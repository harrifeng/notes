<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-12-03 日 09:35 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>tic</title>
<meta name="author" content="harrifeng@outlook.com" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">tic</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgb735f91">1. Why Containers Matter</a>
<ul>
<li><a href="#org9674878">1.1. Modern Application Architecture</a>
<ul>
<li><a href="#orgb615d90">1.1.1. Attribute: Cloud Native</a></li>
<li><a href="#org07dd8f7">1.1.2. Attribute: Modular</a></li>
<li><a href="#orgef2464e">1.1.3. Attribute: Microservice-Based</a></li>
<li><a href="#orgb8cc961">1.1.4. Benefit: Scalability</a></li>
<li><a href="#org7a89567">1.1.5. Benefit: Reliability</a></li>
<li><a href="#orgbbf57b8">1.1.6. Benefit:Resilience</a></li>
</ul>
</li>
<li><a href="#orgdd41d0f">1.2. Why Containers</a>
<ul>
<li><a href="#org4adb5b1">1.2.1. Requirements for Contaienrs</a></li>
<li><a href="#org5f53a55">1.2.2. Requirements for Orchestration</a></li>
</ul>
</li>
<li><a href="#orgb5f6555">1.3. Running Container</a>
<ul>
<li><a href="#orge4f4575">1.3.1. What Container Look Like</a></li>
<li><a href="#org09325f3">1.3.2. Running a Container</a></li>
<li><a href="#orgf72cc1b">1.3.3. Images and Volume Mounts</a></li>
<li><a href="#orgaa70e12">1.3.4. What Containers Really Are</a></li>
</ul>
</li>
<li><a href="#orge2f569c">1.4. Deploying Containers to Kubernetes</a>
<ul>
<li><a href="#org6479a66">1.4.1. Talking to the Kubernetes Cluster</a></li>
<li><a href="#org659821f">1.4.2. Application Overview</a></li>
<li><a href="#org4945f01">1.4.3. Kubernetes Features</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org525bd7b">2. Process Isolation</a>
<ul>
<li><a href="#org61d26b3">2.1. Understanding Isolation</a>
<ul>
<li><a href="#org654e523">2.1.1. Why Process Need Isolation</a></li>
<li><a href="#org6bd613e">2.1.2. File Permissions and Change Root</a></li>
<li><a href="#org12ea6e7">2.1.3. Container Isolation</a></li>
</ul>
</li>
<li><a href="#orga0644c0">2.2. Container Platforms and Container Runtimes</a>
<ul>
<li><a href="#org2781862">2.2.1. Installing containerd</a></li>
<li><a href="#org7df0dd3">2.2.2. Using containerd</a></li>
<li><a href="#org8d87c02">2.2.3. Introducing Linux Namespaces</a></li>
<li><a href="#org3f54337">2.2.4. Containers and Namespaces in CRI-O</a></li>
</ul>
</li>
<li><a href="#org7a7878e">2.3. Running Processes in Namespaces Directly</a></li>
</ul>
</li>
<li><a href="#org4c2f5f2">3. Chapter 3: Resource Limiting</a>
<ul>
<li><a href="#org953f099">3.1. CPU Priorities</a>
<ul>
<li><a href="#org32b7277">3.1.1. Real-Time and Non-Real-Time Policies</a></li>
<li><a href="#orga13f662">3.1.2. Setting Process Priorities</a></li>
</ul>
</li>
<li><a href="#org6fb9d47">3.2. Linux Control Groups</a>
<ul>
<li><a href="#org8d770d5">3.2.1. CPU Quotas with cgroups</a></li>
<li><a href="#org924a25c">3.2.2. CPU Quota with CRI-O and crictl</a></li>
</ul>
</li>
<li><a href="#org910c737">3.3. Memory Limits</a></li>
<li><a href="#org6a46d4a">3.4. Network Bandwidth Limits</a></li>
</ul>
</li>
<li><a href="#org3676f72">4. Chapter 4: Network Namespaces</a>
<ul>
<li><a href="#org5128c45">4.1. Network Isolation</a></li>
<li><a href="#orgd0aff4e">4.2. Network Namespaces</a>
<ul>
<li><a href="#orge385ec3">4.2.1. Inspecting Network Namespaces</a></li>
<li><a href="#org1bfb446">4.2.2. Creating Network Namespaces</a></li>
</ul>
</li>
<li><a href="#orgb316678">4.3. Bridge Interfaces</a>
<ul>
<li><a href="#orgad90c23">4.3.1. Adding Interfaces to a Bridge</a></li>
<li><a href="#orgdc46607">4.3.2. Tracking Traffic</a></li>
</ul>
</li>
<li><a href="#orgff0339b">4.4. Masquerade</a></li>
</ul>
</li>
<li><a href="#orgf83af51">5. Chapter 5</a>
<ul>
<li><a href="#org889ea71">5.1. Filesystem Isolation</a>
<ul>
<li><a href="#org1fa9408">5.1.1. Container Image Contents</a></li>
<li><a href="#orga13d0f2">5.1.2. Image Versions and Layers</a></li>
</ul>
</li>
<li><a href="#orgadcea1e">5.2. Building Container Images</a>
<ul>
<li><a href="#orgb7b19e6">5.2.1. Using a Docerfile</a></li>
<li><a href="#org22d1244">5.2.2. Tagging and Publishing Images</a></li>
</ul>
</li>
<li><a href="#orgedd03ec">5.3. Image and Container Storage</a>
<ul>
<li><a href="#org889af61">5.3.1. Overlay Filesystems</a></li>
<li><a href="#orge44fdb2">5.3.2. Understanding Container Layers</a></li>
<li><a href="#org19bbb09">5.3.3. Practical Image Building Advice</a></li>
</ul>
</li>
<li><a href="#orged586ee">5.4. Open Container Initiative</a></li>
</ul>
</li>
<li><a href="#orgd8611ca">6. Chapter 6: Why Kubernetes Matters</a>
<ul>
<li><a href="#orga996b56">6.1. Running Containers in a Cluster</a>
<ul>
<li><a href="#orgf0b4846">6.1.1. Cross-Cutting Concerns</a></li>
<li><a href="#org65985b7">6.1.2. Kubernetes Concepts</a></li>
</ul>
</li>
<li><a href="#orgc0c3a42">6.2. Cluster Deployment</a>
<ul>
<li><a href="#orgd417171">6.2.1. Prerequisite Packages</a></li>
<li><a href="#org6e76fc0">6.2.2. Kubernetes Packages</a></li>
<li><a href="#org952a255">6.2.3. Cluster Initialization</a></li>
<li><a href="#org431b63d">6.2.4. Joining Nodes to the Cluster</a></li>
</ul>
</li>
<li><a href="#orgdae5b78">6.3. Installing Cluster Add-ons</a>
<ul>
<li><a href="#org127085b">6.3.1. Network Driver</a></li>
<li><a href="#org952ab3e">6.3.2. Installing Storage</a></li>
<li><a href="#org2cc3d6f">6.3.3. Ingress Controller</a></li>
<li><a href="#org96531ce">6.3.4. Metrics Server</a></li>
</ul>
</li>
<li><a href="#org4dc6661">6.4. Exploring a Cluster</a></li>
</ul>
</li>
<li><a href="#orgebd6dfe">7. Chapter 7: Deploying Containers to Kubernetes</a>
<ul>
<li><a href="#org2ba17ce">7.1. Pods</a>
<ul>
<li><a href="#orgfdf3f18">7.1.1. Deploying a Pod</a></li>
<li><a href="#orge734149">7.1.2. Pod Details and Logging</a></li>
</ul>
</li>
<li><a href="#org2ed487b">7.2. Deployments</a>
<ul>
<li><a href="#org19500d5">7.2.1. Creating a Deployment</a></li>
<li><a href="#org2f8c3b8">7.2.2. Monitoring and Scaling</a></li>
<li><a href="#org58eb59d">7.2.3. Autoscaling</a></li>
</ul>
</li>
<li><a href="#org7dcdf09">7.3. Other Controllers</a>
<ul>
<li><a href="#org8938394">7.3.1. Jobs an CronJobs</a></li>
<li><a href="#org3692268">7.3.2. StatefulSets</a></li>
<li><a href="#org829bd08">7.3.3. Daemon Sets</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgb735f91" class="outline-2">
<h2 id="orgb735f91"><span class="section-number-2">1.</span> Why Containers Matter</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li>本章,我们会介绍为什么container在package application和deploy application方面,是一个更好的选择</li>
</ul>
</div>
<div id="outline-container-org9674878" class="outline-3">
<h3 id="org9674878"><span class="section-number-3">1.1.</span> Modern Application Architecture</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>现代软件的关键特性是scale.本章的目的是我们的application需要做什么来达到和维持这种scale</li>
<li>我们首先介绍下modern application的三个关键属性,然后会看看从这三个属性获得的三个收益</li>
</ul>
</div>
<div id="outline-container-orgb615d90" class="outline-4">
<h4 id="orgb615d90"><span class="section-number-4">1.1.1.</span> Attribute: Cloud Native</h4>
<div class="outline-text-4" id="text-1-1-1">
<ul class="org-ul">
<li><p>
cloud的核心是抽象
</p>
<pre class="example" id="org6312673">
At its heart, the cloud is an abstraction.
</pre></li>
<li>在cloud里面,所有的provider都是abstrct的,比如:
<ul class="org-ul">
<li>memory</li>
<li>storage</li>
<li>networking</li>
</ul></li>
<li>使用云服务的用户简单的声明自己的need就可以了,云服务商就可以满足他</li>
<li>为了能够使用云服务,application必须不要把自己绑定在特定的host或者特定的network layout</li>
</ul>
</div>
</div>
<div id="outline-container-org07dd8f7" class="outline-4">
<h4 id="org07dd8f7"><span class="section-number-4">1.1.2.</span> Attribute: Modular</h4>
<div class="outline-text-4" id="text-1-1-2">
<ul class="org-ul">
<li>modular用一句话说就是"高内聚,低耦合"</li>
<li>新时代的架构要求,每个module都有自己的process,不再和其他process通过filesystem或者shared memory,
而是使用socket(也就是网络)进行通信</li>
<li>使用socket通信看起来有些浪费(因为socket会把数据拷贝来,拷贝去),但是却有两个原因来更倾向于使用单独的process
<ol class="org-ol">
<li>由于当前的硬件非常的快, socket通信已经足够快了.使用内存沟通带来的性能提升并不明显(都有良好硬件的情况下)</li>
<li>无论硬件多么的好,一台机器上面能运行的process是有限制的,也就是说'使用内存通信模型'的天花板更低</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgef2464e" class="outline-4">
<h4 id="orgef2464e"><span class="section-number-4">1.1.3.</span> Attribute: Microservice-Based</h4>
<div class="outline-text-4" id="text-1-1-3">
<ul class="org-ul">
<li>微服务的架构,导致我们的服务由很多单独的非常小的process,每个process还会占用至少一台机器.</li>
<li>把这些小的process部署到足够小的机器上面,每个process一个机器,是一个强需求. 而不是申请非常强大的少数机器</li>
<li>把一个大的系统拆成多个小的系统,也便于开发团队的组织:把大系统拆成多个小系统,能够减少测试的复杂度,
同时能够更好的组织大的团队开发</li>
</ul>
</div>
</div>
<div id="outline-container-orgb8cc961" class="outline-4">
<h4 id="orgb8cc961"><span class="section-number-4">1.1.4.</span> Benefit: Scalability</h4>
<div class="outline-text-4" id="text-1-1-4">
<ul class="org-ul">
<li>假设我们有一个简单的executable,运行在单机上面,开始服务3个用户.如果用户激增,我们面临扩容,那么我
们的扩容就必须是全方位的,包括并不限于:
<ul class="org-ul">
<li>机器</li>
<li>网络</li>
<li>数据库</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org7a89567" class="outline-4">
<h4 id="org7a89567"><span class="section-number-4">1.1.5.</span> Benefit: Reliability</h4>
<div class="outline-text-4" id="text-1-1-5">
<ul class="org-ul">
<li>我们刚才的例子中,只有一个server,一旦发生硬件错误,就是single point of failure</li>
<li>cloud native microservice可以让我们克服这种问题</li>
</ul>
</div>
</div>
<div id="outline-container-orgbbf57b8" class="outline-4">
<h4 id="orgbbf57b8"><span class="section-number-4">1.1.6.</span> Benefit:Resilience</h4>
<div class="outline-text-4" id="text-1-1-6">
<ul class="org-ul">
<li>由于可以自动处理硬件和软件错误,所以cloud native还达到了resilience</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgdd41d0f" class="outline-3">
<h3 id="orgdd41d0f"><span class="section-number-3">1.2.</span> Why Containers</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>前面说的,为了控制应用的复杂度,把一个大的应用分拆成很多个小应用.这些小应用容易测试,而且可以分给较
小的团队,便于团队管理</li>
<li>但是这种微服务的方法也有缺点,那就是:如何将翻倍了的,数目很多的小项目成功的发布和维护</li>
<li>我们需要一个广泛应用的方法来为我们的microservice做如下事情:
<ul class="org-ul">
<li>deployment</li>
<li>configuration</li>
<li>maintenance of our microservice</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org4adb5b1" class="outline-4">
<h4 id="org4adb5b1"><span class="section-number-4">1.2.1.</span> Requirements for Contaienrs</h4>
<div class="outline-text-4" id="text-1-2-1">
<ul class="org-ul">
<li>对于单个的microservice,我们需要如下功能:
<ul class="org-ul">
<li>Packaging: 能够打包整个应用(包括dependency),以便进行分发</li>
<li>Verssioning: 全局来唯一性的确定版本,我们要经常更新微服务,要知道更新前是什么版本</li>
<li>Isolation:然后微服务能够和其他微服务进行隔离</li>
<li>Fast startup: 能够快速启动应用,这样才能应对scale和respond to failure</li>
<li>Low overhead: 要让每个微服务自己占用的资源尽可能的小,这样才能在比较小的资源下运行.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org5f53a55" class="outline-4">
<h4 id="org5f53a55"><span class="section-number-4">1.2.2.</span> Requirements for Orchestration</h4>
<div class="outline-text-4" id="text-1-2-2">
<ul class="org-ul">
<li>为了能够让多个微服务在一起工作,我们需要:
<ul class="org-ul">
<li>Clustering: 为跨server的container提供处理器,内存,硬盘</li>
<li>Discovery:能够让一个微服务找到另外的微服务,我们的微服务可能会在cluster上的任意地方运行,还可能
随时换地方运行</li>
<li>Configuration:能够在不rebuild的情况下,配置我们的微服务</li>
<li>Access control:能够控制哪些人才能创建container,保证正确的container运行</li>
<li>Load balancing: 在wokring instance之间要自动分发request,不要让用户来负责记住所有的microservice
instance</li>
<li>Monitoring: 能够监控失败的instance</li>
<li>Resilience: 一旦发现failure,要能自动recover,如果没有这个功能,那么一系列的failure就会让整个
application崩溃</li>
</ul></li>
<li>类似K8s的系统,叫做container orchestration 环境,它能够让我们对待多个server就像是对待一个set的resource一样</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb5f6555" class="outline-3">
<h3 id="orgb5f6555"><span class="section-number-3">1.3.</span> Running Container</h3>
<div class="outline-text-3" id="text-1-3">
</div>
<div id="outline-container-orge4f4575" class="outline-4">
<h4 id="orge4f4575"><span class="section-number-4">1.3.1.</span> What Container Look Like</h4>
<div class="outline-text-4" id="text-1-3-1">
<ul class="org-ul">
<li>在第二章,我们会区分如下两个概念:
<ul class="org-ul">
<li>container platform</li>
<li>container runtime</li>
</ul></li>
<li>本章我们先使用container platform docker来做例子</li>
</ul>
</div>
</div>
<div id="outline-container-org09325f3" class="outline-4">
<h4 id="org09325f3"><span class="section-number-4">1.3.2.</span> Running a Container</h4>
<div class="outline-text-4" id="text-1-3-2">
<ul class="org-ul">
<li>container运行起来,就是和host是完全不一样的机器,在运行之前,我们先来看看host系统的情况
<ul class="org-ul">
<li><p>
代码如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# cat /etc/os-release
<span style="color: #076678;">NAME</span>=<span style="color: #79740e;">"Ubuntu"</span>
<span style="color: #076678;">VERSION</span>=<span style="color: #79740e;">"20.04.5 LTS (Focal Fossa)"</span>
<span style="color: #076678;">ID</span>=ubuntu
<span style="color: #076678;">ID_LIKE</span>=debian
<span style="color: #076678;">PRETTY_NAME</span>=<span style="color: #79740e;">"Ubuntu 20.04.5 LTS"</span>
<span style="color: #076678;">VERSION_ID</span>=<span style="color: #79740e;">"20.04"</span>
<span style="color: #076678;">HOME_URL</span>=<span style="color: #79740e;">"https://www.ubuntu.com/"</span>
<span style="color: #076678;">SUPPORT_URL</span>=<span style="color: #79740e;">"https://help.ubuntu.com/"</span>
<span style="color: #076678;">BUG_REPORT_URL</span>=<span style="color: #79740e;">"https://bugs.launchpad.net/ubuntu/"</span>
<span style="color: #076678;">PRIVACY_POLICY_URL</span>=<span style="color: #79740e;">"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"</span>
<span style="color: #076678;">VERSION_CODENAME</span>=focal
<span style="color: #076678;">UBUNTU_CODENAME</span>=focal
root@host01:/home/vagrant# ps -ef | head
UID          PID    PPID  C STIME TTY          TIME CMD
root           1       0  0 06:06 ?        00:00:14 /sbin/init
root           2       0  0 06:06 ?        00:00:00 <span style="color: #076678;">[</span>kthreadd<span style="color: #076678;">]</span>
root           3       2  0 06:06 ?        00:00:00 <span style="color: #076678;">[</span>rcu_gp<span style="color: #076678;">]</span>
root           4       2  0 06:06 ?        00:00:00 <span style="color: #076678;">[</span>rcu_par_gp<span style="color: #076678;">]</span>
root           6       2  0 06:06 ?        00:00:00 <span style="color: #076678;">[</span>kworker/0:0H-events_highpri<span style="color: #076678;">]</span>
root           8       2  0 06:06 ?        00:00:05 <span style="color: #076678;">[</span>kworker/0:1H-events_highpri<span style="color: #076678;">]</span>
root           9       2  0 06:06 ?        00:00:00 <span style="color: #076678;">[</span>mm_percpu_wq<span style="color: #076678;">]</span>
root          10       2  0 06:06 ?        00:00:18 <span style="color: #076678;">[</span>ksoftirqd/0<span style="color: #076678;">]</span>
root          11       2  0 06:06 ?        00:00:04 <span style="color: #076678;">[</span>rcu_sched<span style="color: #076678;">]</span>
root@host01:/home/vagrant# uname -v
<span style="color: #a89984;">#</span><span style="color: #a89984;">148-Ubuntu SMP Mon Oct 17 16:02:06 UTC 2022</span>
root@host01:/home/vagrant# ip addr | head -n 20
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 02:1c:1b:07:d9:f7 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic enp0s3
       valid_lft 86394sec preferred_lft 86394sec
    inet6 fe80::1c:1bff:fe07:d9f7/64 scope link
       valid_lft forever preferred_lft forever
3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 08:00:27:e1:2d:82 brd ff:ff:ff:ff:ff:ff
    inet 192.168.61.11/24 brd 192.168.61.255 scope global enp0s8
       valid_lft forever preferred_lft forever
    inet6 fe80::a00:27ff:fee1:2d82/64 scope link
       valid_lft forever preferred_lft forever
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:c8:3f:6b:9a brd ff:ff:ff:ff:ff:ff
</pre>
</div></li>
<li>/ect/os-release 用来说明Linux发行版的信息</li>
<li>ip addr 用来获取ip信息,我们可以看到host的ip是192.168.61.11</li>
</ul></li>
<li>下面的例子是使用docker来运行一个rockylinux的命令行
<ul class="org-ul">
<li><p>
命令如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# docker run -ti rockylinux:8
Unable to find image <span style="color: #79740e;">'rockylinux:8'</span> locally
8: Pulling from library/rockylinux
0049b869cecb: Downloading <span style="color: #076678;">[</span>==========================&gt;                        <span style="color: #076678;">]</span>  38.19MB/71.92MB
0049b869cecb: Pull complete
Digest: sha256:afd392a691df0475390df77cb5486f226bc2b4cbaf87c41785115b9237f3203f
Status: Downloaded newer image for rockylinux:8
<span style="color: #076678;">[</span>root@5a13b807526d /<span style="color: #076678;">]</span># cat /etc/os-release
<span style="color: #076678;">NAME</span>=<span style="color: #79740e;">"Rocky Linux"</span>
<span style="color: #076678;">VERSION</span>=<span style="color: #79740e;">"8.6 (Green Obsidian)"</span>
<span style="color: #076678;">ID</span>=<span style="color: #79740e;">"rocky"</span>
<span style="color: #076678;">ID_LIKE</span>=<span style="color: #79740e;">"rhel centos fedora"</span>
<span style="color: #076678;">VERSION_ID</span>=<span style="color: #79740e;">"8.6"</span>
<span style="color: #076678;">PLATFORM_ID</span>=<span style="color: #79740e;">"platform:el8"</span>
<span style="color: #076678;">PRETTY_NAME</span>=<span style="color: #79740e;">"Rocky Linux 8.6 (Green Obsidian)"</span>
<span style="color: #076678;">ANSI_COLOR</span>=<span style="color: #79740e;">"0;32"</span>
<span style="color: #076678;">CPE_NAME</span>=<span style="color: #79740e;">"cpe:/o:rocky:rocky:8:GA"</span>
<span style="color: #076678;">HOME_URL</span>=<span style="color: #79740e;">"https://rockylinux.org/"</span>
<span style="color: #076678;">BUG_REPORT_URL</span>=<span style="color: #79740e;">"https://bugs.rockylinux.org/"</span>
<span style="color: #076678;">ROCKY_SUPPORT_PRODUCT</span>=<span style="color: #79740e;">"Rocky Linux"</span>
<span style="color: #076678;">ROCKY_SUPPORT_PRODUCT_VERSION</span>=<span style="color: #79740e;">"8"</span>
<span style="color: #076678;">REDHAT_SUPPORT_PRODUCT</span>=<span style="color: #79740e;">"Rocky Linux"</span>
<span style="color: #076678;">REDHAT_SUPPORT_PRODUCT_VERSION</span>=<span style="color: #79740e;">"8"</span>
<span style="color: #076678;">[</span>root@5a13b807526d /<span style="color: #076678;">]</span># yum install -y procps iproute

Installed:
  iproute-5.18.0-1.el8.x86_64       libbpf-0.5.0-1.el8.x86_64       libmnl-1.0.4-6.el8.x86_64       procps-ng-3.3.15-9.el8.x86_64       psmisc-23.1-5.el8.x86_64

Complete!
<span style="color: #076678;">[</span>root@5a13b807526d /<span style="color: #076678;">]</span># ps -ef
UID          PID    PPID  C STIME TTY          TIME CMD
root           1       0  0 11:41 pts/0    00:00:00 /bin/bash
root          65       1  0 11:46 pts/0    00:00:00 ps -ef
<span style="color: #076678;">[</span>root@5a13b807526d /<span style="color: #076678;">]</span># ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
19: eth0@if20: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever
<span style="color: #076678;">[</span>root@5a13b807526d /<span style="color: #076678;">]</span># uname -v
<span style="color: #a89984;">#</span><span style="color: #a89984;">148-Ubuntu SMP Mon Oct 17 16:02:06 UTC 2022</span>
<span style="color: #076678;">[</span>root@5a13b807526d /<span style="color: #076678;">]</span># exit
<span style="color: #9d0006;">exit</span>
root@host01:/home/vagrant#
</pre>
</div></li>
<li>我们使用docker run来创建一个运行的容器,参数-ti表示我们要使用交互的shell</li>
<li>/etc/os-release 这里就不再是ubuntu了,而是Rocky Linux</li>
<li>yum安装网络相关的package</li>
<li>在container里面ps显示出来的进程就很少了, 而且我们容器运行的命令bash就是PID为1</li>
<li>IP和host也不一样了,也不再是192.168.61.11了</li>
<li><p>
需要非常注意的是,我们在container里面使用uname -v,返回值和之前在host里面的返回值一样,都是如下
(这说明container和host也不是完全不一样的两个系统)
</p>
<pre class="example" id="orgfcaa03e">
#148-Ubuntu SMP Mon Oct 17 16:02:06 UTC 2022
</pre></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgf72cc1b" class="outline-4">
<h4 id="orgf72cc1b"><span class="section-number-4">1.3.3.</span> Images and Volume Mounts</h4>
<div class="outline-text-4" id="text-1-3-3">
<ul class="org-ul">
<li>第一眼看上去, container像是混合了常规的process和vritual machine的结合体</li>
<li><p>
我们再来看一个Alpine的例子,首先我们pull image,就像是下载虚拟机
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# docker pull alpine:3
3: Pulling from library/alpine
c158987b0551: Pull complete
Digest: sha256:8914eb54f968791faf6a8638949e480fef81e697984fba772b3976835194c6d4
Status: Downloaded newer image for alpine:3
docker.io/library/alpine:3
</pre>
</div></li>
<li>然后我们在docker身上实验两种操作:
<ul class="org-ul">
<li>和host共享某些文件夹: 这是运行virtual machine经常做的事情</li>
<li>通过环境变量来传递写信息:这是运行process经常做的事情</li>
<li><p>
代码如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# docker run -ti -v /:/host -e <span style="color: #076678;">hello</span>=world alpine:3
/ <span style="color: #a89984;"># </span><span style="color: #a89984;">hostname</span>
9c84a63b5ff5
</pre>
</div></li>
<li><p>
/etc/os-release 还是会打印出发行版信息
</p>
<div class="org-src-container">
<pre class="src src-shell">/ <span style="color: #a89984;"># </span><span style="color: #a89984;">cat /etc/os-release</span>
<span style="color: #076678;">NAME</span>=<span style="color: #79740e;">"Alpine Linux"</span>
<span style="color: #076678;">ID</span>=alpine
<span style="color: #076678;">VERSION_ID</span>=3.17.0
<span style="color: #076678;">PRETTY_NAME</span>=<span style="color: #79740e;">"Alpine Linux v3.17"</span>
<span style="color: #076678;">HOME_URL</span>=<span style="color: #79740e;">"https://alpinelinux.org/"</span>
<span style="color: #076678;">BUG_REPORT_URL</span>=<span style="color: #79740e;">"https://gitlab.alpinelinux.org/alpine/aports/-/issues"</span>
</pre>
</div></li>
<li><p>
由于mount了host的文件系统,我们还能打印出host的/etc/os-release, 当然位置是在我们mount的地方/host
</p>
<div class="org-src-container">
<pre class="src src-shell">/ <span style="color: #a89984;"># </span><span style="color: #a89984;">cat /host/etc/os-release</span>
<span style="color: #076678;">NAME</span>=<span style="color: #79740e;">"Ubuntu"</span>
<span style="color: #076678;">VERSION</span>=<span style="color: #79740e;">"20.04.5 LTS (Focal Fossa)"</span>
<span style="color: #076678;">ID</span>=ubuntu
<span style="color: #076678;">ID_LIKE</span>=debian
<span style="color: #076678;">PRETTY_NAME</span>=<span style="color: #79740e;">"Ubuntu 20.04.5 LTS"</span>
<span style="color: #076678;">VERSION_ID</span>=<span style="color: #79740e;">"20.04"</span>
<span style="color: #076678;">HOME_URL</span>=<span style="color: #79740e;">"https://www.ubuntu.com/"</span>
<span style="color: #076678;">SUPPORT_URL</span>=<span style="color: #79740e;">"https://help.ubuntu.com/"</span>
<span style="color: #076678;">BUG_REPORT_URL</span>=<span style="color: #79740e;">"https://bugs.launchpad.net/ubuntu/"</span>
<span style="color: #076678;">PRIVACY_POLICY_URL</span>=<span style="color: #79740e;">"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"</span>
<span style="color: #076678;">VERSION_CODENAME</span>=focal
<span style="color: #076678;">UBUNTU_CODENAME</span>=focal
</pre>
</div></li>
<li><p>
我们还可以打印传入的环境变量
</p>
<div class="org-src-container">
<pre class="src src-shell">/ <span style="color: #a89984;"># </span><span style="color: #a89984;">echo $hello</span>
world
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgaa70e12" class="outline-4">
<h4 id="orgaa70e12"><span class="section-number-4">1.3.4.</span> What Containers Really Are</h4>
<div class="outline-text-4" id="text-1-3-4">
<ul class="org-ul">
<li>虽然container有自己的hostname, filesystem, process space和networking,但是container它不是virtual
machine, 因为它没有自己的kernel,所以它没有自己的kernel module或者device driver</li>
<li>container虽然可以有多个process,但是这些额外的process必须explicitly的被PID为1的process(之前的例
子中PID为1的都是bash)所启动</li>
<li>绝大部分的container没有system service在后台运行,也就没有SSH server</li>
<li>我们再来看一个nginx的例子,这个例子和前面的bash在前台运行不同,是整个container都在后台运行
<ul class="org-ul">
<li><p>
代码如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# docker run -d -p 8080:80 nginx
Unable to find image <span style="color: #79740e;">'nginx:latest'</span> locally
latest: Pulling from library/nginx
a603fa5e3b41: Pull complete
c39e1cda007e: Pull complete
90cfefba34d7: Pull complete
a38226fb7aba: Pull complete
62583498bae6: Pull complete
9802a2cfdb8d: Pull complete
Digest: sha256:e209ac2f37c70c1e0e9873a5f7231e91dcd83fdf1178d8ed36c2ec09974210ba
Status: Downloaded newer image for nginx:latest
d7b73a9beca876ada16a537c1d29149ca2ee1ff92dd557a1645507f37a50d2f0
</pre>
</div></li>
<li>这里我们run跟的参数不再是`-ti`了,而是`-d`,意思是以daemon的形态在后台运行container,普通process
也经常以这种形态运行</li>
<li>`-p 8080:80`就是把container的端口80映射到host的8080,这种用法也是借鉴于virtual machine(vagrant
里面就有类似的配置).这个配置成功后,我们可以通过host的8080来访问container的80端口</li>
</ul></li>
<li><p>
一旦运行起来以后,我们使用docker ps来查看后台的container
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# docker ps
CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS                                   NAMES
d7b73a9beca8   nginx     <span style="color: #79740e;">"/docker-entrypoint.&#8230;"</span>   4 minutes ago   Up 4 minutes   0.0.0.0:8080-&gt;80/tcp, :::8080-&gt;80/tcp   laughing_lehmann
</pre>
</div></li>
<li><p>
由于port forwarding,我们可以从host来访问container
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# curl http://localhost:8080/
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html <span style="color: #076678;">{</span> color-scheme: light dark; <span style="color: #076678;">}</span>
body <span style="color: #076678;">{</span> width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; <span style="color: #076678;">}</span>
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a <span style="color: #076678;">href</span>=<span style="color: #79740e;">"http://nginx.org/"</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a <span style="color: #076678;">href</span>=<span style="color: #79740e;">"http://nginx.com/"</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</pre>
</div></li>
<li>从上面的例子我们可以看出,由于NGINX是被打包在container image里面的,所以我们可以使用一个命令就下
载并且运行nginx,无论host是否装了和NGINX相冲突的软件或者库</li>
<li>我们还可以在host上面通过ps来看到运行在container里面的nginx process,这再次证明container不是virtual
machine.但是我们又不需要在host安装了nginx而运行nginx.换句话说,我们使用container之后,获取了virtual
machine的优势,但是又没有承担virtual machine的负担</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orge2f569c" class="outline-3">
<h3 id="orge2f569c"><span class="section-number-3">1.4.</span> Deploying Containers to Kubernetes</h3>
<div class="outline-text-3" id="text-1-4">
<ul class="org-ul">
<li>为了能够让load balancing和resilience工作,我们需要一个编排的框架,最终的胜利者就是Kubernetes</li>
<li>我们的例子就有web sever和数据库安装好了,我们后续可以学习这个例子</li>
<li>为了使用编排容器,我们要放弃一些control:
<ul class="org-ul">
<li>我们不再是自己运行命令来启动容器</li>
<li>而是,我们告诉k8s,我们需要什么样的容器.然后k8s帮我们启动,并且监控这些容器,并且在需要重启的时候,
帮我们重启</li>
<li>上述配置的方式叫做declarative</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org6479a66" class="outline-4">
<h4 id="org6479a66"><span class="section-number-4">1.4.1.</span> Talking to the Kubernetes Cluster</h4>
<div class="outline-text-4" id="text-1-4-1">
<ul class="org-ul">
<li>k8s cluster自己有一个API server让用户来来获取其状态,并且更改其配置.</li>
<li>正常情况下,我们使用kubectl这个client来和API server沟通,但是这里做demo的话,我们使用k3s内置的kubectl
<ul class="org-ul">
<li><p>
代码如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# k3s kubectl version
Client Version: version.Info<span style="color: #076678;">{</span>Major:<span style="color: #79740e;">"1"</span>, Minor:<span style="color: #79740e;">"23"</span>, GitVersion:<span style="color: #79740e;">"v1.23.4+k3s1"</span>, GitCommit:<span style="color: #79740e;">"43b1cb48200d8f6af85c16ed944d68fcc96b6506"</span>, GitTreeState:<span style="color: #79740e;">"clean"</span>, BuildDate:<span style="color: #79740e;">"2022-02-24T22:38:17Z"</span>, GoVersion:<span style="color: #79740e;">"go1.17.5"</span>, Compiler:<span style="color: #79740e;">"gc"</span>, Platform:<span style="color: #79740e;">"linux/amd64"</span><span style="color: #076678;">}</span>
Server Version: version.Info<span style="color: #076678;">{</span>Major:<span style="color: #79740e;">"1"</span>, Minor:<span style="color: #79740e;">"23"</span>, GitVersion:<span style="color: #79740e;">"v1.23.4+k3s1"</span>, GitCommit:<span style="color: #79740e;">"43b1cb48200d8f6af85c16ed944d68fcc96b6506"</span>, GitTreeState:<span style="color: #79740e;">"clean"</span>, BuildDate:<span style="color: #79740e;">"2022-02-24T22:38:17Z"</span>, GoVersion:<span style="color: #79740e;">"go1.17.5"</span>, Compiler:<span style="color: #79740e;">"gc"</span>, Platform:<span style="color: #79740e;">"linux/amd64"</span><span style="color: #076678;">}</span>
root@host01:/home/vagrant# k3s kubectl get nodes
NAME     STATUS   ROLES                  AGE    VERSION
host01   Ready    control-plane,master   3d4h   v1.23.4+k3s1
</pre>
</div></li>
<li>第一个命令获取了k8s的client和server的版本</li>
<li>第二个命令获取了当前所有node的信息(注意是cluster的node,并且不是高可用的,只是为了让大家方便感受kubectl)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org659821f" class="outline-4">
<h4 id="org659821f"><span class="section-number-4">1.4.2.</span> Application Overview</h4>
<div class="outline-text-4" id="text-1-4-2">
<ul class="org-ul">
<li>我们的例子是一个todo list:
<ul class="org-ul">
<li>拥有web interface</li>
<li>persistent storage</li>
<li>我们可以在nginx启动以后,在运行vagrant的机器上面,访问如下URL就可以看到todo应用 <a href="https://localhost:48080/todo/">https://localhost:48080/todo/</a></li>
</ul></li>
<li>我们的例子也是有两种类型的pod:
<ul class="org-ul">
<li>运行PostgreSQL的container</li>
<li>提供前后端服务的container</li>
</ul></li>
<li>我们使用get pods来让k8s提供pod列表,所谓pod是指一组(或者一个)container,k8s把pod看做是调度和监控
的最小单位
<ul class="org-ul">
<li><p>
使用get pods的代码如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# k3s kubectl get pods
NAME                       READY   STATUS    RESTARTS      AGE
todo-db-585649889f-8blkg   1/1     Running   2 <span style="color: #076678;">(</span>19h ago<span style="color: #076678;">)</span>   3d23h
todo-6bd859fdd5-tstkt      1/1     Running   2 <span style="color: #076678;">(</span>19h ago<span style="color: #076678;">)</span>   3d23h
todo-6bd859fdd5-dlbmx      1/1     Running   2 <span style="color: #076678;">(</span>19h ago<span style="color: #076678;">)</span>   3d23h
todo-6bd859fdd5-92bm8      1/1     Running   3 <span style="color: #076678;">(</span>19h ago<span style="color: #076678;">)</span>   3d23h
</pre>
</div></li>
<li>todo-db开头的,是PostgreSQL</li>
<li>todo-开头的是Node.js 容器</li>
<li>random字符后面会讲解,为了区分不同的容器</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org4945f01" class="outline-4">
<h4 id="org4945f01"><span class="section-number-4">1.4.3.</span> Kubernetes Features</h4>
<div class="outline-text-4" id="text-1-4-3">
<ul class="org-ul">
<li>如果仅仅是运行四个container,那么我们完全可以使用docker run,但是k8s的功能不仅于此</li>
<li>除了运行container,但是k8s还可以监控容器, k8s会持续的保证让这三个instance保持running状态
<ul class="org-ul">
<li><p>
我们可以destroy其中一个pod,然后会发现k8s会自动的recover
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# k3s kubectl get pods
NAME                       READY   STATUS    RESTARTS      AGE
todo-db-585649889f-8blkg   1/1     Running   2 <span style="color: #076678;">(</span>19h ago<span style="color: #076678;">)</span>   3d23h
todo-6bd859fdd5-tstkt      1/1     Running   2 <span style="color: #076678;">(</span>19h ago<span style="color: #076678;">)</span>   3d23h
todo-6bd859fdd5-dlbmx      1/1     Running   2 <span style="color: #076678;">(</span>19h ago<span style="color: #076678;">)</span>   3d23h
todo-6bd859fdd5-92bm8      1/1     Running   3 <span style="color: #076678;">(</span>19h ago<span style="color: #076678;">)</span>   3d23h
root@host01:/home/vagrant# k3s kubectl delete pod todo-6bd859fdd5-tstkt
pod <span style="color: #79740e;">"todo-6bd859fdd5-tstkt"</span> deleted
root@host01:/home/vagrant# k3s kubectl get pods
NAME                       READY   STATUS    RESTARTS      AGE
todo-db-585649889f-8blkg   1/1     Running   2 <span style="color: #076678;">(</span>20h ago<span style="color: #076678;">)</span>   4d
todo-6bd859fdd5-dlbmx      1/1     Running   2 <span style="color: #076678;">(</span>20h ago<span style="color: #076678;">)</span>   4d
todo-6bd859fdd5-92bm8      1/1     Running   3 <span style="color: #076678;">(</span>20h ago<span style="color: #076678;">)</span>   4d
todo-6bd859fdd5-zlzkk      1/1     Running   0             16s
</pre>
</div></li>
</ul></li>
<li>k8s还可以自动的scale我们的application,我们当前使用明确的命令来扩容,但是后面会介绍k8s还可以自动
的scale
<ul class="org-ul">
<li><p>
手动scale的代码如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# k3s kubectl get pods
NAME                       READY   STATUS    RESTARTS      AGE
todo-db-585649889f-8blkg   1/1     Running   2 <span style="color: #076678;">(</span>20h ago<span style="color: #076678;">)</span>   4d
todo-6bd859fdd5-dlbmx      1/1     Running   2 <span style="color: #076678;">(</span>20h ago<span style="color: #076678;">)</span>   4d
todo-6bd859fdd5-92bm8      1/1     Running   3 <span style="color: #076678;">(</span>20h ago<span style="color: #076678;">)</span>   4d
todo-6bd859fdd5-zlzkk      1/1     Running   0             9m30s
root@host01:/home/vagrant# k3s kubectl scale --replicas=5 deployment todo
deployment.apps/todo scaled
root@host01:/home/vagrant# k3s kubectl get pods
NAME                       READY   STATUS    RESTARTS      AGE
todo-db-585649889f-8blkg   1/1     Running   2 <span style="color: #076678;">(</span>20h ago<span style="color: #076678;">)</span>   4d
todo-6bd859fdd5-dlbmx      1/1     Running   2 <span style="color: #076678;">(</span>20h ago<span style="color: #076678;">)</span>   4d
todo-6bd859fdd5-92bm8      1/1     Running   3 <span style="color: #076678;">(</span>20h ago<span style="color: #076678;">)</span>   4d
todo-6bd859fdd5-zlzkk      1/1     Running   0             9m42s
todo-6bd859fdd5-ctmjc      1/1     Running   0             5s
todo-6bd859fdd5-b2mt8      1/1     Running   0             4s
</pre>
</div></li>
<li>这里我们告诉k8s去scale deployment所控制的pod的个数到5</li>
<li>到目前为止,你可以把deployment理解为pod的owner: deployment会monitor,并且控制pod</li>
</ul></li>
<li>为了能够让我们的请求能够水平的分配给不同的pod,我们需要一个类似load balancer的东西,在k8s里面,这
个东西叫做Service
<ul class="org-ul">
<li><p>
查询todo的service
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# k3s kubectl describe service todo
Name:              todo
Namespace:         default
Labels:            <span style="color: #076678;">app</span>=todo
Annotations:       &lt;none&gt;
Selector:          <span style="color: #076678;">app</span>=todo
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.43.99.105
IPs:               10.43.99.105
Port:              web  5000/TCP
TargetPort:        5000/TCP
Endpoints:         10.42.0.25:5000,10.42.0.27:5000,10.42.0.32:5000 + 2 more...
Session Affinity:  None
Events:            &lt;none&gt;
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org525bd7b" class="outline-2">
<h2 id="org525bd7b"><span class="section-number-2">2.</span> Process Isolation</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li>Container使用了Linux Kernel的基础功能,也就是namespace</li>
<li>namespae会为创建如下的资源来隔离process:
<ul class="org-ul">
<li>process identifier</li>
<li>user</li>
<li>filesystem</li>
<li>network interface</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org61d26b3" class="outline-3">
<h3 id="org61d26b3"><span class="section-number-3">2.1.</span> Understanding Isolation</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>我们首先来看看process isolation的动机,为此我们先看看传统的process isolation,并且看看是什么导致了
container使用的isolation功能</li>
</ul>
</div>
<div id="outline-container-org654e523" class="outline-4">
<h4 id="org654e523"><span class="section-number-4">2.1.1.</span> Why Process Need Isolation</h4>
<div class="outline-text-4" id="text-2-1-1">
<ul class="org-ul">
<li>计算机的整体概念就是能够同时运行很多种的任务</li>
<li>最开始的时候计算机是通过大家挨个提交card上的任务来分享计算机的</li>
<li>后面计算机拥有了多任务系统,那么多个用户可以同时使用计算机,看起来好像是每个用户的任务都同时在执行</li>
<li>当然了,既然有东西要分享,那么必须分享的公平,计算机既然分享也要分享的公平</li>
<li>在同一台机器上面运行的process虽然拥有自己的CPU时间片,和自己的memory space,但是它们其实做不到公
平的分享计算机. 这会导致如下的很多问题:
<ul class="org-ul">
<li>某个process使用了过多的CPU, memory, storage, 或者network</li>
<li>覆写了其他process的内存,或者文件</li>
<li>提取了其他process的secret信息</li>
<li>给其他process发送了bad data导致其他的process的不良行为</li>
<li>给其他process发送了过量的request,导致其他process失去响应</li>
</ul></li>
<li>由于有这么多的问题(很多还是严重的安全问题),我们必须要对不同给process进行隔离:
<ul class="org-ul">
<li>最安全的做法是进行物理隔离(每个process一台机器),但是这个做法的缺点是太昂贵</li>
<li>virtual machine可以做到对不同的process,既操作系统隔离,又是硬件共享. 缺点是每个process都要运行
一个操作系统,所以它的速度太慢了,扩展性也不好(虽然相比于物理隔离便宜了一点)</li>
<li><p>
最佳解决方案就是process还是老的proces,但是使用process isolation技术来减小对其他process的支持
</p>
<pre class="example" id="orgc23b36f">
The solution is to run regular processes, but use process isolation
to reduce the risk of affecting other processes.
</pre></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org6bd613e" class="outline-4">
<h4 id="org6bd613e"><span class="section-number-4">2.1.2.</span> File Permissions and Change Root</h4>
<div class="outline-text-4" id="text-2-1-2">
<ul class="org-ul">
<li>process isolation的最大的诉求,就是阻止某个process看到他不该看到的东西.因为一旦process连看都看
不到其他process的文件,那么他根本不可能有意或者无意的造成伤害</li>
<li>container文件系统的可见性设计的和传统的Linux文件的可见性比较一致:
<ul class="org-ul">
<li>visibility control最重要的是filesystem permission: linux有owner和group的概念,并且能够控制两
者的读,写,运行.这套机制有效的防止process看到不该看的文件</li>
<li>这套系统的问题在于,每个process都要运行在拥有必要的权限的用户之下,并且用户要在恰当的group:为了
做到这一点,在linux里面通常是每个服务(比如supervisor)定义一个自己的用户和组(通常是supervisor用
户,supervisor组), 而且通常不给这个用户(supervisor用户)任何的login shell,防止其他人利用这个账
号登录后,做坏事情</li>
</ul></li>
<li>我们来看一个传统的linux文件可见性的例子: rsyslogd service是一个logging服务,它需要写入/var/log下
的某些文件,但是,非它写的文件,它不能看到.我们通过下面代码来了解一下:
<ul class="org-ul">
<li><p>
确认当前系统有这个服务
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ps -ef | grep rsyslogd | grep -v grep
syslog       630       1  0 12:26 ?        00:00:00 /usr/sbin/rsyslogd -n -iNONE
</pre>
</div></li>
<li><p>
当前系统也有syslog这个用户,但是它没有login shell
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# cat /etc/passwd | grep syslog
syslog:x:104:110::/home/syslog:/usr/sbin/nologin
root@host01:~# su syslog
This account is currently not available.
</pre>
</div></li>
<li><p>
rsyslogd需要写入/var/log/auth.log,所以就是这个文件的owner,也就可以写入(注意-rw-r&#x2013;&#x2014;,其中第
一个rw-表示owner的权限, 第二个r&#x2013;,表示group的权限, 这个文件的group是adm)
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ls -l /var/log/auth.log
-rw-r----- 1 syslog adm 47993 Dec  6 13:36 /var/log/auth.log
</pre>
</div></li>
<li><p>
rsyslogd就无法知道/var/log下面还有一个文件夹/var/log/private了,因为这个文件夹只对root:root可
见可写可执行
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ls -ld /var/log/private
drwx------ 2 root root 4096 Dec  3 12:32 /var/log/private
</pre>
</div></li>
<li><p>
关于private文件夹,其他用户用户组能看到private文件夹(因为看到private属于private的父文件夹log
的权限,普通用户是有的),但是去根本没法进入,也无法知道其内部的信息(没有读权限)
</p>
<div class="org-src-container">
<pre class="src src-shell">vagrant@host01:/var/log$ ls -ald .
drwxrwxr-x 8 root syslog 4096 Dec  6 12:26 .
vagrant@host01:/var/log$ ls -ald private
drwx------ 2 root root 4096 Dec  3 12:32 private
vagrant@host01:/var/log$ ls -al private
ls: cannot open directory <span style="color: #79740e;">'private'</span>: Permission denied
vagrant@host01:/var/log$ cd private
bash: cd: private: Permission denied
</pre>
</div></li>
</ul></li>
<li>我们可以看到,传统的permission control还是能比较好的完成任务的.但是其实它不能完全的满足process
isolation的要求,比如:
<ul class="org-ul">
<li>传统的permission control无法避免privilege escalation.所谓privilege escalation是指运行你process
的unix-like系统的root账号被窃取了,自然你所有的control都不再起作用了</li>
</ul></li>
<li>为了应对privilege escalation,Linux发明了一种技术叫做chroot(change root),其核心是让process在文
件系统的一个isolated part来运行. 例子如下
<ul class="org-ul">
<li><p>
首先创建一个文件夹,并且把我们需要的二进制文件(bash, ls)放进去.
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# mkdir /tmp/newroot
root@host01:~# cp --parents /bin/bash /bin/ls /tmp/newroot
</pre>
</div></li>
<li><p>
注意这个&#x2013;parents是指拷贝的时候带着文件夹,所以拷贝完ls和bash之后的newroot是这样的
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# find /tmp/newroot/
/tmp/newroot/
/tmp/newroot/bin
/tmp/newroot/bin/bash
/tmp/newroot/bin/ls
</pre>
</div></li>
<li><p>
只拷贝二进制是不能运行的,还需要拷贝二进制所依赖的动态链接库,代码如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ldd /bin/bash /bin/ls | grep <span style="color: #79740e;">'=&gt;'</span> | awk <span style="color: #79740e;">'{print $3}'</span>
/lib/x86_64-linux-gnu/libtinfo.so.6
/lib/x86_64-linux-gnu/libdl.so.2
/lib/x86_64-linux-gnu/libc.so.6
/lib/x86_64-linux-gnu/libselinux.so.1
/lib/x86_64-linux-gnu/libc.so.6
/lib/x86_64-linux-gnu/libpcre2-8.so.0
/lib/x86_64-linux-gnu/libdl.so.2
/lib/x86_64-linux-gnu/libpthread.so.0
root@host01:~# cp --parents /lib64/ld-linux-x86-64.so.2 $<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">ldd /bin/bash /bin/ls | grep '=&gt;' | awk '{print $3}'</span><span style="color: #076678;">)</span> /tmp/newroot
cp: warning: source file <span style="color: #79740e;">'/lib/x86_64-linux-gnu/libc.so.6'</span> specified more than once
cp: warning: source file <span style="color: #79740e;">'/lib/x86_64-linux-gnu/libdl.so.2'</span> specified more than once
</pre>
</div></li>
<li><p>
拷贝之后的/tmp/newroot里面又多了一些文件夹和文件
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# find /tmp/newroot/
/tmp/newroot/
/tmp/newroot/lib
/tmp/newroot/lib/x86_64-linux-gnu
/tmp/newroot/lib/x86_64-linux-gnu/libselinux.so.1
/tmp/newroot/lib/x86_64-linux-gnu/libpthread.so.0
/tmp/newroot/lib/x86_64-linux-gnu/libtinfo.so.6
/tmp/newroot/lib/x86_64-linux-gnu/libpcre2-8.so.0
/tmp/newroot/lib/x86_64-linux-gnu/libdl.so.2
/tmp/newroot/lib/x86_64-linux-gnu/libc.so.6
/tmp/newroot/bin
/tmp/newroot/bin/bash
/tmp/newroot/bin/ls
/tmp/newroot/lib64
/tmp/newroot/lib64/ld-linux-x86-64.so.2
</pre>
</div></li>
<li><p>
拷贝完必要的文件之后,我们就可以使用chroot啦,第一个参数是新的root地址,第二个参数是在新的root
地址上运行的命令. 注意新的root的/bin里面里面只有我们拷贝的两个二进制啦(正常/bin里面有很多文件)
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# chroot /tmp/newroot /bin/bash
I have no name!@host01:/# ls -l /bin
total 1296
-rwxr-xr-x 1 0 0 1183448 Dec  8 07:26 bash
-rwxr-xr-x 1 0 0  142144 Dec  8 07:26 ls
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org12ea6e7" class="outline-4">
<h4 id="org12ea6e7"><span class="section-number-4">2.1.3.</span> Container Isolation</h4>
<div class="outline-text-4" id="text-2-1-3">
<ul class="org-ul">
<li>虽然一个运行的container拥有如下的配置,但是它其实只不过是一个常规的linux process,运行在isolation
技术下罢了,不是虚拟机:
<ul class="org-ul">
<li>hostname</li>
<li>network</li>
<li>process</li>
<li>filesystem</li>
</ul></li>
<li>除了我们上面讲的filesystem的isolation,container其实还有很多层的isolation,总结如下:
<ul class="org-ul">
<li>Mounted filesystems</li>
<li>Hostanme and domain name</li>
<li>Interprocess communication</li>
<li>Process identifiers</li>
<li>Network device</li>
</ul></li>
<li>有了上面的isolation手段,再加上能够限制每个container所使用的CPU,memory,storage,network资源的数
目,那么我们就可以保障container里面运行process不会影响其他container里面的process啦</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orga0644c0" class="outline-3">
<h3 id="orga0644c0"><span class="section-number-3">2.2.</span> Container Platforms and Container Runtimes</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>如果每次我们自己像上面一样处理需要的二进制,library文件等等,那么整个工作就会非常的繁琐,幸运的是
有我们第一章介绍过的container image,这种image里面就是executable和library的合成体</li>
<li>使用docker,我们可以下载这个image,并且非常容易的运行一个container(比如nginx)</li>
<li>docker就是一种典型的container platform,所谓contaier platform,就是不仅仅有运行container的能力,
并且还提供如下功能:
<ul class="org-ul">
<li>container storage</li>
<li>networking</li>
<li>security</li>
</ul></li>
<li>作为container platform(比如docker),其内部是使用container runtime(或者叫container engine)来完成
主要功能的.最常见的container runtime叫做containerd</li>
<li>container runtime(比如containerd) 提供low-level的功能来运行container</li>
<li>为了让我们有更加直观的理解,我们会使用两种container runtime来启动container</li>
</ul>
</div>
<div id="outline-container-org2781862" class="outline-4">
<h4 id="org2781862"><span class="section-number-4">2.2.1.</span> Installing containerd</h4>
<div class="outline-text-4" id="text-2-2-1">
<ul class="org-ul">
<li><p>
我们直接使用ansible来完成这个部分,安装完之后,我们使用如下命令来确认containerd是否安装成功
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ctr images ls
REF TYPE DIGEST SIZE PLATFORMS LABELS
</pre>
</div></li>
<li>一般来说,用户不会自己安装container runtime. 因为container runtime是更高阶的应用来使用,比如:
<ul class="org-ul">
<li>container platform(docker)</li>
<li>container orchestration(kubernetes)</li>
</ul></li>
<li>container runtime作为low-level的应用主要通过API对外提供服务,但也有命令行工具叫做ctr</li>
</ul>
</div>
</div>
<div id="outline-container-org7df0dd3" class="outline-4">
<h4 id="org7df0dd3"><span class="section-number-4">2.2.2.</span> Using containerd</h4>
<div class="outline-text-4" id="text-2-2-2">
<ul class="org-ul">
<li><p>
之前安装containerd之后,ctr列出image发现没有任何image,那么我们现在就可以下载一个image来体验一下
下载一个比较小的image: busybox
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ctr image pull docker.io/library/busybox:latest
docker.io/library/busybox:latest:                                                 resolved       |++++++++++++++++++++++++++++++++++++++|
index-sha256:3b3128d9df6bbbcc92e2358e596c9fbd722a437a62bafbc51607970e9e3b8869:    done           |++++++++++++++++++++++++++++++++++++++|
manifest-sha256:d345780059f4b200c1ebfbcfb141c67212e1ad4ea7538dcff759895bfcf99e6e: done           |++++++++++++++++++++++++++++++++++++++|
layer-sha256:45a0cdc5c8d3d10ce3d26eec586f3c1f770f373d604c268343518f27d59dc2fb:    done           |++++++++++++++++++++++++++++++++++++++|
config-sha256:334e4a014c81bd4050daa78c7dfd2ae87855e9052721c164ea9d9d9a416ebdd3:   done           |++++++++++++++++++++++++++++++++++++++|
elapsed: 9.0 s                                                                    total:   0.0 B <span style="color: #076678;">(</span>0.0 B/s<span style="color: #076678;">)</span>
unpacking linux/amd64 sha256:3b3128d9df6bbbcc92e2358e596c9fbd722a437a62bafbc51607970e9e3b8869...
<span style="color: #9d0006;">done</span>: 6.719451ms
root@host01:~# ctr images ls
REF                              TYPE                                                      DIGEST                                                                  SIZE    PLATFORMS                                                                                                                          LABELS
docker.io/library/busybox:latest application/vnd.docker.distribution.manifest.list.v2+json sha256:3b3128d9df6bbbcc92e2358e596c9fbd722a437a62bafbc51607970e9e3b8869 2.5 MiB linux/386,linux/amd64,linux/arm/v5,linux/arm/v6,linux/arm/v7,linux/arm64/v8,linux/mips64le,linux/ppc64le,linux/riscv64,linux/s390x -
</pre>
</div></li>
<li><p>
有了image我们就可以运行container了,ctr run和docker run类似,只不过最后一个参数不再是命令而是container id(这里是v1)
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ctr run -t --rm  docker.io/library/busybox:latest v1
/ <span style="color: #a89984;">#</span>
</pre>
</div></li>
<li>我们看看这个container,由于process和network的isolation,我们已经可以不让host system的其他process发现他们了
<ul class="org-ul">
<li><p>
ctr创建的container拥有isolated process space
</p>
<div class="org-src-container">
<pre class="src src-shell">/ <span style="color: #a89984;"># </span><span style="color: #a89984;">ps -ef</span>
ps -ef
PID   USER     TIME  COMMAND
    1 root      0:00 sh
    8 root      0:00 ps -ef
</pre>
</div></li>
<li><p>
ctr创建的container拥有isolated network,并且只有一个loopback interface.
</p>
<div class="org-src-container">
<pre class="src src-shell">/ <span style="color: #a89984;"># </span><span style="color: #a89984;">ip a</span>
ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
</pre>
</div></li>
<li>如果是docker这种container platform的话,创建的container会有额外的interface,这个interface会attached
to a bridge. 能够attached to a bridge 是非常重要的,因为这意味着container能够利用host interface
(通过NAT)去获取万维网的资源.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org8d87c02" class="outline-4">
<h4 id="org8d87c02"><span class="section-number-4">2.2.3.</span> Introducing Linux Namespaces</h4>
<div class="outline-text-4" id="text-2-2-3">
<ul class="org-ul">
<li>所有的container runtime都会使用Linux内核的特性叫做namespace 来隔离container里面的process</li>
<li>process isolation的最大努力方向,是确保process不看到它不该看的,所以,在namespace里面运行的process
只能看到特定的系统资源</li>
<li>linux的namespace是一个非常老的特性,经过多年的演进,有很多不同类型的namespace都被加了进来</li>
<li>我们可以通过使用lsns命令来获取所有process使用namespace的信息,在grep的帮助下可以缩小到某个process ID
由于container其实也就是在host上的一个普通process ID,我们也就能知道某个container到底使用了多少的namespace</li>
<li><p>
保证之前的v1运行的情况下,新开一个varant root terminal首先来看当前有哪些的container
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ctr task ls
TASK    PID      STATUS
v1      85409    RUNNING
</pre>
</div></li>
<li><p>
知道我们的container里面的进程的PID之后,可以在host上面搜索这个pid的信息可以发现sh(85409)的parent
process就是containerd(85385)
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ps -ef | grep 85409 | grep -v grep
root       85409   85385  0 11:38 pts/0    00:00:00 sh
root@host01:~# ps -ef | grep 85385 | grep -v grep
root       85385       1  0 11:38 ?        00:00:00 /usr/bin/containerd-shim-runc-v2 -namespace default -id v1 -address /run/containerd/containerd.sock
root       85409   85385  0 11:38 pts/0    00:00:00 sh
</pre>
</div></li>
<li><p>
我们可以在看看sh(85409)用到了哪些的namespace
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# lsns | grep 85409
4026532198 mnt         1 85409 root            sh
4026532203 uts         1 85409 root            sh
4026532204 ipc         1 85409 root            sh
4026532205 pid         1 85409 root            sh
4026532207 net         1 85409 root            sh
</pre>
</div></li>
<li>我们可以看到,整个containerd使用了如下五种namespace:
<ul class="org-ul">
<li>mnt: Mount points</li>
<li>uts: Unix time sharing(hostname and network domain)</li>
<li>ipc: Interpaocess communication(比如shared memory)</li>
<li>pid: Process identifiers</li>
<li>net: Network(包括interface, routing table, firewall)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org3f54337" class="outline-4">
<h4 id="org3f54337"><span class="section-number-4">2.2.4.</span> Containers and Namespaces in CRI-O</h4>
<div class="outline-text-4" id="text-2-2-4">
<ul class="org-ul">
<li>除了containerd以外, kubernetes还支持其他的container runtime</li>
<li>使用的Kubernetes发行版不同,就会有不同的container runtime:
<ul class="org-ul">
<li>RedHat OpenShift的k8s发行版就使用了CRI-O这种container runtime</li>
<li>CRI-O还被使用在Podman, Buildah, Skopeo等工具中</li>
</ul></li>
<li>安装CRI-O的过程略过,但是由于CRI-O没有提供任何的命令行工具,我们得要使用crictl来控制它,crictl是k8s
的一部分,用来测试container runtime和CRI(Container Runtime Interface)之间的兼容性的.</li>
<li>CRI是kubernetes用来和container runtime 通信的</li>
<li>crictl为了能够和container runtime进行通信,需要一些配置文件,我们也在虚拟机里面准备了一些配置文件:
<ul class="org-ul">
<li><p>
/etc/crictl.yaml:用来告诉crictl如何连接CIR-O的socket
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# cat /etc/crictl.yaml
runtime-endpoint: unix:///var/run/crio/crio.sock
image-endpoint: unix:///var/run/crio/crio.sock
timeout: 10
</pre>
</div></li>
<li><p>
/opt/pod.yaml: 用来告诉crictl如何创建pod(pod是container的一个超集,一个pod里面有最少一个container),
这里就是一个pod里有一个container
</p>
<div class="org-src-container">
<pre class="src src-yaml">root@host01:~# cat /opt/pod.yaml
<span style="color: #a89984;">---</span>
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: busybox
  <span style="color: #076678;">namespace</span>: crio
<span style="color: #076678;">linux</span>:
  <span style="color: #076678;">security_context</span>:
    <span style="color: #076678;">namespace_options</span>:
      <span style="color: #076678;">network</span>: 2
</pre>
</div></li>
<li><p>
/opt/container.yaml: 用来告诉crictl应该启动哪些process,比如这里使用了 /bin/sleep,以防止休眠
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# cat /opt/container.yaml
---
metadata:
  name: busybox
image:
  image: docker.io/library/busybox:latest
args:
  - <span style="color: #79740e;">"/bin/sleep"</span>
  - <span style="color: #79740e;">"36000"</span>
</pre>
</div></li>
</ul></li>
<li><p>
使用crictl来运行CRI-O就比较麻烦了,需要我们自己获取各种ID,例子如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# crictl pull docker.io/library/busybox:latest
Image is up to date for docker.io/library/busybox@sha256:3b3128d9df6bbbcc92e2358e596c9fbd722a437a62bafbc51607970e9e3b8869
root@host01:~# cd /opt
root@host01:/opt# <span style="color: #076678;">POD_ID</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">crictl runp pod.yaml</span><span style="color: #076678;">)</span>
root@host01:/opt# crictl pods
POD ID              CREATED             STATE               NAME                NAMESPACE           ATTEMPT             RUNTIME
e1f87e86b4b8b       3 seconds ago       Ready               busybox             crio                0                   <span style="color: #076678;">(</span>default<span style="color: #076678;">)</span>
root@host01:/opt# <span style="color: #076678;">CONTAINER_ID</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">crictl create $POD_ID container.yaml pod.yaml</span><span style="color: #076678;">)</span>
root@host01:/opt# crictl start $<span style="color: #076678;">CONTAINER_ID</span>
5944fe3b7e8d34f028c38266ba31ad6582ee7ba9608a1331b39f1eb8092a53a3
root@host01:/opt# crictl ps
CONTAINER           IMAGE                              CREATED             STATE               NAME                ATTEMPT             POD ID
5944fe3b7e8d3       docker.io/library/busybox:latest   14 seconds ago      Running             busybox             0                   e1f87e86b4b8b
</pre>
</div></li>
<li>我们可以使用crictl exec来启动一个新的shell命令来查看contaienr的内部情况
<ul class="org-ul">
<li><p>
代码如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# crictl exec -ti  $<span style="color: #076678;">CONTAINER_ID</span> /bin/sh
/ <span style="color: #a89984;"># </span><span style="color: #a89984;">ip a</span>
ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel qlen 1000
    link/ether 02:9a:0a:ae:6b:9e brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic enp0s3
       valid_lft 83081sec preferred_lft 83081sec
    inet6 fe80::9a:aff:feae:6b9e/64 scope link
       valid_lft forever preferred_lft forever
3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel qlen 1000
    link/ether 08:00:27:e6:b5:82 brd ff:ff:ff:ff:ff:ff
    inet 192.168.61.11/24 brd 192.168.61.255 scope global enp0s8
       valid_lft forever preferred_lft forever
    inet6 fe80::a00:27ff:fee6:b582/64 scope link
       valid_lft forever preferred_lft forever
/ <span style="color: #a89984;"># </span><span style="color: #a89984;">ps -ef</span>
ps -ef
PID   USER     TIME  COMMAND
    1 root      0:00 /pause
    7 root      0:00 /bin/sleep 36000
   12 root      0:00 /bin/sh
   20 root      0:00 ps -ef
/ <span style="color: #a89984;"># </span><span style="color: #a89984;">exit</span>
</pre>
</div></li>
<li>和containerd不同的地方有:
<ol class="org-ol">
<li>我们看到,由于我们配置了network: 2, 我们可以看到普通process都能看到的network,而不仅仅是loopback</li>
<li>我们还看到了PID为1的pause进程</li>
<li>我们还能看到sleep,这也是我们之前配置的防止shell休眠的进程</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org7a7878e" class="outline-3">
<h3 id="org7a7878e"><span class="section-number-3">2.3.</span> Running Processes in Namespaces Directly</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>在容器里面运行的最主要困难工作在于让PID为1的process能够履行其责任</li>
<li>为了能够理解这一点,我们不让我们的container runtime为我们创建namespace,而是使用command line来让Linux
kernel把我们的process运行在一个namespace里面(container runtime也是同样的做法,只不过它使用API,而
不是命令行)</li>
<li>我们使用unshare来创建namespace,并且运行进程:
<ul class="org-ul">
<li><p>
在terminal1运行如下命令
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# unshare -f -p --mount-proc -- /bin/sh -c /bin/bash
</pre>
</div></li>
<li>-p 表示我们要创建新的PID namespace</li>
<li>&#x2013;mount-proc,告诉我们要添加新的mount namespace,并且保证/proc能够正确使用</li>
</ul></li>
<li>运行完unshare之后,我们的terminal1就进入了这个新的namespace里面
<ul class="org-ul">
<li><p>
我们可以通过运行ps -ef来看下当前的情形
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ps -ef
UID          PID    PPID  C STIME TTY          TIME CMD
root           1       0  0 11:53 pts/2    00:00:00 /bin/sh -c /bin/bash
root           2       1  0 11:53 pts/2    00:00:00 /bin/bash
root           9       2  0 11:53 pts/2    00:00:00 ps -ef
</pre>
</div></li>
<li>可以发现,只有三个进程,说明是在namespace里面</li>
</ul></li>
<li><p>
我们还可以再terminal1里面看看我们的/proc信息,查找自己的pid,并且加上-l参数,可以看到我们的namespace
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ls -l /proc/self/ns/pid
lrwxrwxrwx 1 root root 0 Apr 12 11:54 /proc/self/ns/pid -&gt; <span style="color: #79740e;">'pid:[4026532190]'</span>
</pre>
</div></li>
<li>这个时候,我们另外开一个terminal2, 在里面运行lsns列举所有的namespace
<ul class="org-ul">
<li><p>
代码如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# lsns
        NS TYPE   NPROCS   PID USER            COMMAND
4026531835 cgroup    120     1 root            /sbin/init
4026531836 pid       118     1 root            /sbin/init
4026531837 user      120     1 root            /sbin/init
4026531838 uts       118     1 root            /sbin/init
4026531839 ipc       120     1 root            /sbin/init
4026531840 mnt       110     1 root            /sbin/init
4026531860 mnt         1    22 root            kdevtmpfs
4026531992 net       120     1 root            /sbin/init
4026532173 mnt         1   386 root            /lib/systemd/systemd-udevd
4026532174 uts         1   386 root            /lib/systemd/systemd-udevd
4026532176 mnt         1  1867 systemd-network /lib/systemd/systemd-networkd
4026532186 mnt         1   603 systemd-resolve /lib/systemd/systemd-resolved
4026532187 mnt         1   766 root            /usr/sbin/ModemManager
4026532189 mnt         3 14035 root            unshare -f -p --mount-proc -- /bin/sh
4026532190 pid         2 14036 root            /bin/sh -c /bin/bash
4026532243 mnt         1   701 root            /usr/sbin/irqbalance --foreground
4026532244 mnt         1   713 root            /lib/systemd/systemd-logind
4026532245 uts         1   713 root            /lib/systemd/systemd-logind
</pre>
</div></li>
<li>我们可以发现,如下两个就是我们自己创建的namespace:
<ol class="org-ol">
<li><p>
mnt namespace, 保证我们的shell能够看到正确的/proc(只看到自己和自己的child,不能看到host信息)
</p>
<pre class="example" id="org1bff4b6">
4026532189 mnt         3 14035 root            unshare -f -p --mount-proc -- /bin/sh
</pre></li>
<li><p>
pid namespace
</p>
<pre class="example" id="org19c900c">
4026532190 pid         2 14036 root            /bin/sh -c /bin/bash
</pre></li>
</ol></li>
</ul></li>
<li>上面的例子我们看到pid namespace的拥有者是sh command,而且其id为1,那么就意味着sh有责任管理好它所有
的child process.</li>
<li><p>
我们在terminal2里面杀掉我们找到的sh的process
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kill -9 14036
</pre>
</div></li>
<li><p>
在terminal1里面会得到如下的输出,证明我们的bash接到了kill的signal
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# Killed
</pre>
</div></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org4c2f5f2" class="outline-2">
<h2 id="org4c2f5f2"><span class="section-number-2">3.</span> Chapter 3: Resource Limiting</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>在前面一章做的process isolation非常重要,因为这样一来,process无法对它都看不到的其他process"作恶"</li>
<li>本章要继续解决的问题是,process还是能看到全量的如下资源.我们要想办法控制,或者让它只看到"适量",而
不是"全量"(因为一旦process看到全量的资源,可能会恶意使用资源让其他process无法获取足够运行的资源)
<ul class="org-ul">
<li>cpu</li>
<li>memory</li>
<li>network</li>
</ul></li>
<li>其实除了上面的三个资源还有一个共享的资源也很重要,那就是storage.不过k8s这种平台,storage是分布式的,
资源是某个集群共享固定容量.和上面三个资源是host上面的容器共享不一样.</li>
</ul>
</div>
<div id="outline-container-org953f099" class="outline-3">
<h3 id="org953f099"><span class="section-number-3">3.1.</span> CPU Priorities</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>为了理解CPU limit,我们首先要理解Linux kernel如何决定哪个process运行,以及哪个process运行多长的时间:
<ul class="org-ul">
<li>在Linux内核里面有个部分叫scheduler</li>
<li>scheduler会维护几个列表:
<ol class="org-ol">
<li>一个列表是所有的process</li>
<li>还有列表是所有准备好运行的process</li>
</ol></li>
<li>scheduler还会记录每个process最近都使用了多少cpu time</li>
<li>scheduler的结构,让它很容易创建一个优先队列来决定哪个process先运行</li>
<li>scheduler的设计原则就是尽可能的让所有的process都有机会运行,同时scheduler也接受来自外界的输入,
以便决定哪些process的优先级更高</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org32b7277" class="outline-4">
<h4 id="org32b7277"><span class="section-number-4">3.1.1.</span> Real-Time and Non-Real-Time Policies</h4>
<div class="outline-text-4" id="text-3-1-1">
<ul class="org-ul">
<li>scheduler支持很多种的policy,我们可以简单的分成两种:
<ul class="org-ul">
<li>real-time policy</li>
<li>non-real-time policy</li>
</ul></li>
<li>realtime意味着这个event对于process来说非常紧急,并且设置了处理的deadline,process必须在deadline
之前处理完毕,否则就会发生问题.比如:
<ul class="org-ul">
<li>我们要从嵌入式设备读取数据,嵌入式设备里面的硬件buffer会在某个时间点之后overflow,那么这个时间
点就是deadline,我们必须要在这个deadline之前处理完毕</li>
</ul></li>
<li>换句话说real-time process当需要cpu的时候,必须马上给它.</li>
<li>为了满足real-time process的需求,Linux要求任意real-time process的优先级高于non-real-time policy</li>
<li>Linux的ps命令告诉我们某个process使用的特定的policy:
<ul class="org-ul">
<li><p>
例子如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ps -e -o pid,class,rtprio,ni,comm
    PID CLS RTPRIO  NI COMMAND
      1 TS       -   0 systemd
      2 TS       -   0 kthreadd
      ...
      6 TS       - -20 kworker/0:0H-events_highpri
      ...
     12 FF      99   - migration/0
     13 FF      50   - idle_inject/0
      ...
     86 FF      99   - watchdogd
      ...
    500 RR      99   - multipathd
      ...
   6534 TS       -   0 ps
</pre>
</div></li>
<li>-e 参数等于-A,意思是读取所有的process</li>
<li>-o 参数能让我们提供自定义信息输出这里我们就写了如下几个信息:
<ul class="org-ul">
<li>pid: 显示就是PID, 意思是进程的ID号</li>
<li>class: 显示是CLS, 意思是调度策略分类:
<ol class="org-ol">
<li>绝大部分进程的CLS是TS(Time-Sharing), 这个是默认的,non-real-time policy, 比如我们的ps就是TS,systemd也是TS</li>
<li>FIFO是(first in-first out)的缩写,这个是real-time policy的一种,比如watchdogd,这个是用来
探测系统中出现了严重的问题,让kernel重启系统的服务</li>
<li>RR是(round-robin)的缩写,这个也是real-time policy的一种,比如multipathd,这个是探测设备更改
的,必须在其他process访问前配置好设备</li>
</ol></li>
<li>rtprio: 显示是RTPRIO,意思是real-time process的priority</li>
<li>ni: 显示是NI(nice的缩写),意思是non-real-time process的nice的程度(越nice,优先级越低),这个值的区间是-20到19</li>
<li>comm: 显示是COMMAND,就是运行这个进程的命令</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga13f662" class="outline-4">
<h4 id="orga13f662"><span class="section-number-4">3.1.2.</span> Setting Process Priorities</h4>
<div class="outline-text-4" id="text-3-1-2">
<ul class="org-ul">
<li>linux允许我们给process配置priority,我们下面来尝试使用priority来控制CPU使用:
<ul class="org-ul">
<li>我们使用了一个叫做stress的程序来尽可能的消耗cpu</li>
<li>我们使用了CRI-O容器版本的stress</li>
<li>我们没有进入容器,是在host上面进行各种测量</li>
</ul></li>
<li>为了让CRI-O能够运行,我们需要两个yaml文件:
<ul class="org-ul">
<li><p>
pod的配置文件po-nolim.yaml
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #a89984;">---</span>
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: stress
  <span style="color: #076678;">namespace</span>: crio
<span style="color: #076678;">linux</span>:
  <span style="color: #076678;">security_context</span>:
    <span style="color: #076678;">namespace_options</span>:
      <span style="color: #076678;">network</span>: 2
</pre>
</div></li>
<li><p>
container的配置文件co-nolim.yaml
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #a89984;">---</span>
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: stress
<span style="color: #076678;">image</span>:
  <span style="color: #076678;">image</span>: docker.io/bookofkubernetes/stress:stable
<span style="color: #076678;">args</span>:
  - <span style="color: #79740e;">"--cpu"</span>
  - <span style="color: #79740e;">"1"</span>
  - <span style="color: #79740e;">"-v"</span>
</pre>
</div></li>
</ul></li>
<li>在host01上面已经安装了CRI-O,我们就可以按照下面的步骤先做实验,看看通过renice的方法,是否能控制cpu的使用
<ul class="org-ul">
<li><p>
首先要启动容器,代码如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# crictl pull docker.io/bookofkubernetes/stress:stable
Image is up to date for docker.io/bookofkubernetes/stress@sha256:0da7b65f89e2473df7845ea171a286501d710e0d5634c15a8ebe7e8ade2d2097
root@host01:~# cd /opt
root@host01:/opt# <span style="color: #076678;">PUL_ID</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">crictl runp po-nolim.yaml</span><span style="color: #076678;">)</span>
root@host01:/opt# <span style="color: #076678;">CUL_ID</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">crictl create $PUL_ID co-nolim.yaml po-nolim.yaml</span><span style="color: #076678;">)</span>
root@host01:/opt# crictl start $<span style="color: #076678;">CUL_ID</span>
6431626fb14b1be38bdb716f5204945ea2d0837b16e163bf9196d427ef64967e
root@host01:/opt# crictl ps
CONTAINER           IMAGE                                      CREATED             STATE               NAME                ATTEMPT             POD ID
6431626fb14b1       docker.io/bookofkubernetes/stress:stable   19 seconds ago      Running             stress              0                   4ceda49e3274f
</pre>
</div></li>
<li><p>
之后我们*不需要*进入容器,在host就能查看进程信息,下面命令的关键,是使用$(pgrep -d , stress)获取stress在host主机的pid, 注意这里有两个stress,因为它运行的时候fork了
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# top -b -n 1 -p $<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">pgrep -d , stress</span><span style="color: #076678;">)</span>
top - 02:22:09 up 1 day,  2:16,  1 user,  load average: 0.53, 0.15, 0.05
Tasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie
%Cpu<span style="color: #076678;">(</span>s<span style="color: #076678;">)</span>: 50.0 us,  0.0 sy,  0.0 ni, 50.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
MiB Mem :   1983.2 total,    127.7 free,    213.1 used,   1642.3 buff/cache
MiB Swap:      0.0 total,      0.0 free,      0.0 used.   1595.1 avail Mem

    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
 105111 root      20   0   23148   3456   1816 R 100.0   0.2   0:48.50 stress-ng
 105080 root      20   0   23148   4316   4064 S   0.0   0.2   0:00.00 stress-ng
</pre>
</div></li>
<li>我们看到没有任何优待的stress-ng的优先级(PR)为20, nice值(NI)为0, 它使用了100%的cpu</li>
<li><p>
我们实验一下给stress-ng调低优先级,其是否可能减少下cpu使用率
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# renice -n 19 -p $<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">pgrep -d ' ' stress</span><span style="color: #076678;">)</span>
<span style="color: #b57614;">105080</span> <span style="color: #076678;">(</span>process ID<span style="color: #076678;">)</span> old priority 0, new priority 19
<span style="color: #b57614;">105111</span> <span style="color: #076678;">(</span>process ID<span style="color: #076678;">)</span> old priority 0, new priority 19
root@host01:/opt# top -b -n 1 -p $<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">pgrep -d , stress</span><span style="color: #076678;">)</span>
top - 02:39:33 up 1 day,  2:34,  1 user,  load average: 1.03, 1.00, 0.73
Tasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie
%Cpu<span style="color: #076678;">(</span>s<span style="color: #076678;">)</span>:  0.0 us,  0.0 sy, 48.3 ni, 51.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
MiB Mem :   1983.2 total,    132.0 free,    207.8 used,   1643.4 buff/cache
MiB Swap:      0.0 total,      0.0 free,      0.0 used.   1610.7 avail Mem

    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
 105111 root      39  19   23148   3456   1816 R 100.0   0.2  18:11.87 stress-ng
 105080 root      39  19   23148   4316   4064 S   0.0   0.2   0:00.00 stress-ng
</pre>
</div></li>
<li>我们可以看到nice值成功的改成了最低的19 (most nice),但是stress还是使用了100% cpu. 原因在于我
们的优先级是一个相对概念,虽然stress的优先级低,但是在容器环境里面,只有它一个进程,所以它还是优
先级最高的,还是会尽可能的吃掉能得到的cpu资源(单线程就是100%)</li>
</ul></li>
<li>上面的例子告诉我们,在容器里面使用优先级不能够控制进程对host主机cpu的消耗.这就导致这个技术没法
用在容器调度环境里面.原因有二:
<ul class="org-ul">
<li>对于容器调度环境(k8s)来说,我不能以优先级来确定使用cpu的排序,因为一个host(甚至一个集群)上面有
很多容器(也就有很多的进程),我们不可能统一的去协调(并且校准)这些容器(进程)的priority</li>
<li>进程使用cpu的多少必须有一个客观可衡量的绝对值(而不是优先级这种相对值),否则我们不知道要把一个
容器调度到哪个host上去.万一一个host上面的容器都是非常消耗cpu的,那么这多个容器相互抢占cpu,最终
就会导致整个host变慢</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org6fb9d47" class="outline-3">
<h3 id="org6fb9d47"><span class="section-number-3">3.2.</span> Linux Control Groups</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li>前面讲了linux的priority无法满足k8s的要求,所以势必要找到新的方法来精确控制进程使用的cpu比例.这个
方法就是Linux Control Group</li>
<li>Linux Control Group的设计,吸取了real-time process调度的经验:
<ul class="org-ul">
<li>对于real-time进程来说,即便它不是compute intensive的(绝大部分都不是compute intensive的,否则rt
调度算法也没办法执行),但是只要它需要cpu的时候,它就必须得到</li>
<li>为了保证所有的real-time 进程都能及时获得cpu,一个普遍的做法是为每个rt进程都预留一点cpu,所谓预留
就是用到的时候给rt进程,不用的时候就空转(这时候我们就理解了为什么rt进程基本都不是compute intensive
的,因为预留也不可能预留多)</li>
</ul></li>
<li>Linux Control Group就是把rt进程的cpu预留法应用到了non-rt进程上面,这样的话一个non-rt进程如果预留
给它50%的cpu的情况下,即便没有其他竞争者,也无法使用全部cpu(最多只能用到50%)</li>
<li>Linux Control Group不仅仅能控制cpu,其能控制如下三种资源.一旦一个进程在某个cgroup里面,那么linux
内核会自动施加资源限制给这个进程(在cgroup里面的多个进程共享一套限制):
<ul class="org-ul">
<li>CPU</li>
<li>Memory</li>
<li>Block Device</li>
</ul></li>
<li>cgroup的创建是通过在特定位置(/sys/fs/cgroup)创建特定的文件,同时这些文件也包含了cgroup的配置信息</li>
<li><p>
我们进入到/sys/fs/cgroup会看到这个文件夹下面有很多子文件夹,每个子文件夹都是一个可以被限制的资源类型
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ls -alF /sys/fs/cgroup
total 0
drwxr-xr-x 15 root root 380 Apr 27 08:29 ./
drwxr-xr-x 10 root root   0 Apr 27 08:29 ../
dr-xr-xr-x  5 root root   0 Apr 27 08:29 blkio/
lrwxrwxrwx  1 root root  11 Apr 27 08:29 cpu -&gt; cpu,cpuacct/
lrwxrwxrwx  1 root root  11 Apr 27 08:29 cpuacct -&gt; cpu,cpuacct/
dr-xr-xr-x  5 root root   0 Apr 27 08:29 cpu,cpuacct/
dr-xr-xr-x  3 root root   0 Apr 27 08:29 cpuset/
dr-xr-xr-x  5 root root   0 Apr 27 08:29 devices/
dr-xr-xr-x  4 root root   0 Apr 27 08:29 freezer/
dr-xr-xr-x  3 root root   0 Apr 27 08:29 hugetlb/
dr-xr-xr-x  5 root root   0 Apr 27 08:29 memory/
lrwxrwxrwx  1 root root  16 Apr 27 08:29 net_cls -&gt; net_cls,net_prio/
dr-xr-xr-x  3 root root   0 Apr 27 08:29 net_cls,net_prio/
lrwxrwxrwx  1 root root  16 Apr 27 08:29 net_prio -&gt; net_cls,net_prio/
dr-xr-xr-x  3 root root   0 Apr 27 08:29 perf_event/
dr-xr-xr-x  5 root root   0 Apr 27 08:29 pids/
dr-xr-xr-x  2 root root   0 Apr 27 08:29 rdma/
dr-xr-xr-x  5 root root   0 Apr 27 08:29 systemd/
dr-xr-xr-x  5 root root   0 Apr 27 08:29 unified/
</pre>
</div></li>
<li>我们只关注cpu这一个文件夹:
<ul class="org-ul">
<li><p>
我们可以看到这个文件夹里面有很多文件和文件夹,其中文件就是这个cgroup的各种限制
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# cd /sys/fs/cgroup/cpu
root@host01:/sys/fs/cgroup/cpu# ls -F
cgroup.clone_children  cgroup.sane_behavior  cpuacct.usage      cpuacct.usage_percpu      cpuacct.usage_percpu_user  cpuacct.usage_user  cpu.cfs_quota_us  cpu.stat     notify_on_release  system.slice/  user.slice/
cgroup.procs           cpuacct.stat          cpuacct.usage_all  cpuacct.usage_percpu_sys  cpuacct.usage_sys          cpu.cfs_period_us   cpu.shares        init.scope/  release_agent      tasks

</pre>
</div></li>
<li><p>
另外的文件夹就是另外一个cgroup,它里面有自己的子文件来配置具体限制
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/sys/fs/cgroup/cpu# cd user.slice/
root@host01:/sys/fs/cgroup/cpu/user.slice# ls -F
cgroup.clone_children  cpuacct.stat   cpuacct.usage_all     cpuacct.usage_percpu_sys   cpuacct.usage_sys   cpu.cfs_period_us  cpu.shares  cpu.uclamp.max  notify_on_release
cgroup.procs           cpuacct.usage  cpuacct.usage_percpu  cpuacct.usage_percpu_user  cpuacct.usage_user  cpu.cfs_quota_us   cpu.stat    cpu.uclamp.min  tasks
</pre>
</div></li>
</ul></li>
</ul>
</div>
<div id="outline-container-org8d770d5" class="outline-4">
<h4 id="org8d770d5"><span class="section-number-4">3.2.1.</span> CPU Quotas with cgroups</h4>
<div class="outline-text-4" id="text-3-2-1">
<ul class="org-ul">
<li><p>
我们下面来看看如何配置cgroup,首先看看之前的cpu使用率信息,还是100%
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/sys/fs/cgroup/cpu# top -b -n 1 -p $<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">pgrep -d , stress</span><span style="color: #076678;">)</span>
top - 06:53:02 up 1 day,  6:47,  1 user,  load average: 1.00, 1.00, 1.00
Tasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie
%Cpu<span style="color: #076678;">(</span>s<span style="color: #076678;">)</span>:  0.0 us,  0.0 sy, 50.0 ni, 50.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
MiB Mem :   1983.2 total,    131.7 free,    213.7 used,   1637.8 buff/cache
MiB Swap:      0.0 total,      0.0 free,      0.0 used.   1604.8 avail Mem

    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
 105111 root      39  19   23148   3456   1816 R 100.0   0.2 271:34.78 stress-ng
 105080 root      39  19   23148   4316   4064 S   0.0   0.2   0:00.00 stress-ng
</pre>
</div></li>
<li><p>
下面就是查找一下,我们的两个进程存在于哪些文件里面,我们可以使用grep来查找
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/sys/fs/cgroup/cpu# pgrep stress-ng
105080
105111
root@host01:/sys/fs/cgroup/cpu# grep -R 105080
system.slice/runc-6431626fb14b1be38bdb716f5204945ea2d0837b16e163bf9196d427ef64967e.scope/cgroup.procs:105080
system.slice/runc-6431626fb14b1be38bdb716f5204945ea2d0837b16e163bf9196d427ef64967e.scope/tasks:105080
root@host01:/sys/fs/cgroup/cpu# grep -R 105111
system.slice/runc-6431626fb14b1be38bdb716f5204945ea2d0837b16e163bf9196d427ef64967e.scope/cgroup.procs:105111
system.slice/runc-6431626fb14b1be38bdb716f5204945ea2d0837b16e163bf9196d427ef64967e.scope/tasks:105111
</pre>
</div></li>
<li><p>
我们进入到这个文件夹后发现,这个文件夹下面有和/sys/fs/cgroup/cpu下面一样的配置文件,说明这也是一个cgroup
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/sys/fs/cgroup/cpu# crictl ps
CONTAINER           IMAGE                                      CREATED             STATE               NAME                ATTEMPT             POD ID
6431626fb14b1       docker.io/bookofkubernetes/stress:stable   5 hours ago         Running             stress              0                   4ceda49e3274f
root@host01:/sys/fs/cgroup/cpu# cd system.slice/runc-$<span style="color: #076678;">{</span><span style="color: #076678;">CUL_ID</span><span style="color: #076678;">}</span>.scope
root@host01:/sys/fs/cgroup/cpu/system.slice/runc-6431626fb14b1be38bdb716f5204945ea2d0837b16e163bf9196d427ef64967e.scope# ls -F
cgroup.clone_children  cpuacct.stat   cpuacct.usage_all     cpuacct.usage_percpu_sys   cpuacct.usage_sys   cpu.cfs_period_us  cpu.shares  cpu.uclamp.max  notify_on_release
cgroup.procs           cpuacct.usage  cpuacct.usage_percpu  cpuacct.usage_percpu_user  cpuacct.usage_user  cpu.cfs_quota_us   cpu.stat    cpu.uclamp.min  tasks
</pre>
</div></li>
<li>下面我们就看看这些配置文件都是配置什么的,比较重要的几个有:
<ul class="org-ul">
<li>cgroup.procs 存储进程的id</li>
<li>cpu.shares 存储cpu slice</li>
<li><p>
cpu.cfs_periods_us, 存储period的长度,以us(microsecond微秒)为单位,这个例子中cfs_period_us的值为100000us
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/sys/fs/cgroup/cpu/system.slice/runc-6431626fb14b1be38bdb716f5204945ea2d0837b16e163bf9196d427ef64967e.scope# cat cpu.cfs_period_us
100000
</pre>
</div></li>
<li><p>
cpu.cfs_quota_us, 存储period中当前的cgroup可以使用的CPU时间,以ms为单位, 这个例子中,默认的cfs_quota_us值为-1,表示可以使用整个period
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/sys/fs/cgroup/cpu/system.slice/runc-6431626fb14b1be38bdb716f5204945ea2d0837b16e163bf9196d427ef64967e.scope# cat cpu.cfs_quota_us
-1
</pre>
</div></li>
</ul></li>
<li>我们可以设置cfs_quota_us为cfs_period_us值的一半,这样就可以保证cpu的使用率也只有一半
<ul class="org-ul">
<li><p>
设置方法如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/sys/fs/cgroup/cpu/system.slice/runc-6431626fb14b1be38bdb716f5204945ea2d0837b16e163bf9196d427ef64967e.scope# echo <span style="color: #79740e;">"50000"</span> &gt;  cpu.cfs_quota_us
root@host01:/sys/fs/cgroup/cpu/system.slice/runc-6431626fb14b1be38bdb716f5204945ea2d0837b16e163bf9196d427ef64967e.scope# cat cpu.cfs_quota_us
50000
</pre>
</div></li>
<li><p>
设置效果如下,由于top测量是两次refresh之间的cpu使用率,所以结果并不是非常精确的50%,但是总体而言是可以达到50%的
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/sys/fs/cgroup/cpu/system.slice/runc-6431626fb14b1be38bdb716f5204945ea2d0837b16e163bf9196d427ef64967e.scope# top -b -n 1 -p $<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">pgrep -d , stress</span><span style="color: #076678;">)</span>
top - 07:10:23 up 1 day,  7:05,  1 user,  load average: 0.92, 0.98, 0.99
Tasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie
%Cpu<span style="color: #076678;">(</span>s<span style="color: #076678;">)</span>:  0.0 us,  0.0 sy, 26.7 ni, 73.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
MiB Mem :   1983.2 total,    129.6 free,    213.2 used,   1640.3 buff/cache
MiB Swap:      0.0 total,      0.0 free,      0.0 used.   1605.3 avail Mem

    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
 105111 root      39  19   23148   3456   1816 R  53.3   0.2 288:43.09 stress-ng
 105080 root      39  19   23148   4316   4064 S   0.0   0.2   0:00.00 stress-ng
</pre>
</div></li>
</ul></li>
<li><p>
配置成功了,但是每次更改配置文件是非常麻烦的,下一节我们会讲到如何在yaml文件里面配置这些限制,所以当前要关闭掉正在运行的stress容器
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# crictl stop $<span style="color: #076678;">CUL_ID</span>
6431626fb14b1be38bdb716f5204945ea2d0837b16e163bf9196d427ef64967e
root@host01:/opt# crictl rm $<span style="color: #076678;">CUL_ID</span>
6431626fb14b1be38bdb716f5204945ea2d0837b16e163bf9196d427ef64967e
root@host01:/opt# crictl stopp $<span style="color: #076678;">PUL_ID</span>
Stopped sandbox 4ceda49e3274f22b17c7636b6a57d74b1e5a4298d4df319b9c39bfabf8a094fa
root@host01:/opt# crictl rmp $<span style="color: #076678;">PUL_ID</span>
Removed sandbox 4ceda49e3274f22b17c7636b6a57d74b1e5a4298d4df319b9c39bfabf8a094fa
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org924a25c" class="outline-4">
<h4 id="org924a25c"><span class="section-number-4">3.2.2.</span> CPU Quota with CRI-O and crictl</h4>
<div class="outline-text-4" id="text-3-2-2">
<ul class="org-ul">
<li>本节我们就介绍把cgroup配置写到yaml文件里面,比如
<ul class="org-ul">
<li><p>
新的pod配置文件(po-clim.yaml)为
</p>
<div class="org-src-container">
<pre class="src src-yaml">
<span style="color: #a89984;">---</span>
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: stress1
  <span style="color: #076678;">namespace</span>: crio
<span style="color: #076678;">linux</span>:
  <span style="color: #076678;">cgroup_parent</span>: pod.slice
  <span style="color: #076678;">security_context</span>:
    <span style="color: #076678;">namespace_options</span>:
      <span style="color: #076678;">network</span>: 2
</pre>
</div></li>
<li>新的pod配置文件专门增加了我们创建cgroup的位置</li>
<li>真正添加资源限制配置的地方在container 配置文件里面设置了如下两个值:
<ol class="org-ol">
<li>cpu_period对应上一节的cpu.cfs_period_us</li>
<li>cpu_quota对应上一节的cpu.cfs_quota_us</li>
</ol></li>
<li><p>
新的container配置文件(co-clim.yaml)为
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #a89984;">---</span>
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: stress1
<span style="color: #076678;">image</span>:
  <span style="color: #076678;">image</span>: docker.io/bookofkubernetes/stress:stable
<span style="color: #076678;">args</span>:
  - <span style="color: #79740e;">"--cpu"</span>
  - <span style="color: #79740e;">"1"</span>
  - <span style="color: #79740e;">"-v"</span>
<span style="color: #076678;">linux</span>:
  <span style="color: #076678;">resources</span>:
    <span style="color: #076678;">cpu_period</span>: 100000
    <span style="color: #076678;">cpu_quota</span>: 10000
</pre>
</div></li>
<li>通过配置我们大概可以猜出新的进程的cpu使用率在10%左右</li>
<li><p>
我们通过如下命令先启动新的容器
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# <span style="color: #076678;">PCL_ID</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">crictl runp po-clim.yaml</span><span style="color: #076678;">)</span>
root@host01:/opt# <span style="color: #076678;">CCL_ID</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">crictl create $PCL_ID co-clim.yaml po-clim.yaml</span><span style="color: #076678;">)</span>
root@host01:/opt# crictl start $<span style="color: #076678;">CCL_ID</span>
c224019f75f69b7affe53a6bfb8720b0eb335eda64c97624160fda414ac30bfa
root@host01:/opt# crictl ps
CONTAINER           IMAGE                                      CREATED             STATE               NAME                ATTEMPT             POD ID
c224019f75f69       docker.io/bookofkubernetes/stress:stable   12 seconds ago      Running             stress1             0                   828d59eebd2e3
</pre>
</div></li>
<li><p>
然后验证下,平均cpu使用率确实是在10%左右
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# top -b -n 1 -p $<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">pgrep -d, stress</span><span style="color: #076678;">)</span>
top - 13:10:33 up 1 day, 13:05,  1 user,  load average: 0.05, 0.11, 0.09
Tasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie
%Cpu<span style="color: #076678;">(</span>s<span style="color: #076678;">)</span>:  2.6 us,  0.0 sy,  0.0 ni, 97.4 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
MiB Mem :   1983.2 total,    110.0 free,    208.2 used,   1665.0 buff/cache
MiB Swap:      0.0 total,      0.0 free,      0.0 used.   1610.4 avail Mem

    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
 134750 root      20   0   23148   3548   1920 R  10.5   0.2   4:20.87 stress-ng
 134718 root      20   0   23148   4236   3988 S   0.0   0.2   0:00.00 stress-ng
</pre>
</div></li>
</ul></li>
<li><p>
我们可以去host的cgroup文件夹看看是不是成功设置了配置
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# cd /sys/fs/cgroup/cpu
root@host01:/sys/fs/cgroup/cpu# ls
cgroup.clone_children  cgroup.sane_behavior  cpuacct.usage      cpuacct.usage_percpu      cpuacct.usage_percpu_user  cpuacct.usage_user  cpu.cfs_quota_us  cpu.stat    notify_on_release  release_agent  tasks
cgroup.procs           cpuacct.stat          cpuacct.usage_all  cpuacct.usage_percpu_sys  cpuacct.usage_sys          cpu.cfs_period_us   cpu.shares        init.scope  pod.slice          system.slice   user.slice
root@host01:/sys/fs/cgroup/cpu# cd pod.slice/
root@host01:/sys/fs/cgroup/cpu/pod.slice# ls
cgroup.clone_children  cpuacct.usage         cpuacct.usage_percpu_sys   cpuacct.usage_user  cpu.shares      cpu.uclamp.min                                                               notify_on_release
cgroup.procs           cpuacct.usage_all     cpuacct.usage_percpu_user  cpu.cfs_period_us   cpu.stat        crio-828d59eebd2e388f2c385f169919033366cc4228740d3816d28c35a891508b1a.scope  tasks
cpuacct.stat           cpuacct.usage_percpu  cpuacct.usage_sys          cpu.cfs_quota_us    cpu.uclamp.max  crio-c224019f75f69b7affe53a6bfb8720b0eb335eda64c97624160fda414ac30bfa.scope
root@host01:/sys/fs/cgroup/cpu/pod.slice# echo $<span style="color: #076678;">CCL_ID</span>
c224019f75f69b7affe53a6bfb8720b0eb335eda64c97624160fda414ac30bfa
root@host01:/sys/fs/cgroup/cpu/pod.slice# cat crio-$<span style="color: #076678;">CCL_ID</span>.scope/cpu.cfs_quota_us
10000
root@host01:/sys/fs/cgroup/cpu/pod.slice# cat crio-$<span style="color: #076678;">CCL_ID</span>.scope/cpu.cfs_period_us
100000
</pre>
</div></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org910c737" class="outline-3">
<h3 id="org910c737"><span class="section-number-3">3.3.</span> Memory Limits</h3>
<div class="outline-text-3" id="text-3-3">
<ul class="org-ul">
<li>内存是另外一个对进程来说非常重要的资源</li>
<li>如果系统没有足够的内存,那么进程申请内存的request会失败,这会导致进程发挥不佳,甚至直接退出</li>
<li>这里插一句,很多Linux系统都会使用硬盘的一部分来做swap space,从而让memory 内容临时换到硬盘里面,这
样系统的可用内存看起来大了一下,但是代价是系统性能下降</li>
<li>出于优化性能的考虑,k8s host不使用swap</li>
<li>确定了swap不再使用后,k8s要解决的另外一个问题,就是如何给一个进程限制其内存的总体使用数量,这样依赖
有两个好处:
<ul class="org-ul">
<li>一来可以不必让一个进程获取太多的内存从而影响同一个host的其他容器</li>
<li>二来可以让我们知道某个容器是否可以调度到其他host上去,因为知道当前进程的内存上限和其他host的内
存剩余</li>
</ul></li>
<li>k8s本来可以直接使用linux继承自Unix的system resource limit方法,这种方法只需要在命令行配置即可:
<ul class="org-ul">
<li><p>
比如
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ulimit -v 262144
</pre>
</div></li>
<li>上面的代码就限制了,所有从当前shell启动的进程,其virtual memory的总使用量不能超过256MB</li>
<li><p>
我们可以使用如下的代码来测试下我们的配置是否起效:
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# cat /dev/zero | head -c 500m | tail
tail: memory exhausted
</pre>
</div></li>
<li>可以看到,我们尝试读取500MB zero到进程里来,结果失败了</li>
</ul></li>
<li>这种limit只能配置到一个process或者一个user,不是很适应k8s的要求,更重要的是,cgroup除了能够控制cpu
还能控制内存,那么显然已经使用cgroup控制cpu的k8s,没必要再选择第二个技术来控制内存</li>
<li>我们还是以之前的stress来做实验, 如果我们再启动了stress之后再施加memory limit,那么可能来不及了,
因为程序启动的时候就已经申请了超过limit的内存了.</li>
<li>我们的办法是在yaml文件里面直接配置这个限制:
<ul class="org-ul">
<li><p>
container yaml代码如下
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #a89984;">---</span>
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: stress2
<span style="color: #076678;">image</span>:
  <span style="color: #076678;">image</span>: docker.io/bookofkubernetes/stress:stable
<span style="color: #076678;">args</span>:
  - <span style="color: #79740e;">"--vm"</span>
  - <span style="color: #79740e;">"1"</span>
  - <span style="color: #79740e;">"--vm-bytes"</span>
  - <span style="color: #79740e;">"512M"</span>
  - <span style="color: #79740e;">"-v"</span>
<span style="color: #076678;">linux</span>:
  <span style="color: #076678;">resources</span>:
    <span style="color: #076678;">memory_limit_in_bytes</span>: 268435456
    <span style="color: #076678;">cpu_period</span>: 100000
    <span style="color: #076678;">cpu_quota</span>: 10000
</pre>
</div></li>
<li>注意这里的memory_limit_in_bytes: 268435456,就是我们对整个cgroup设置的内存限制</li>
<li>同时我们又让我们的stress启动的时候,就申请512MB内存,使用 &#x2013;vm-bytes 512M这个配置</li>
</ul></li>
<li><p>
同时我们还需要一个pod的配置,如下
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #a89984;">---</span>
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: stress2
  <span style="color: #076678;">namespace</span>: crio
<span style="color: #076678;">linux</span>:
  <span style="color: #076678;">cgroup_parent</span>: pod.slice
  <span style="color: #076678;">security_context</span>:
    <span style="color: #076678;">namespace_options</span>:
      <span style="color: #076678;">network</span>: 2
</pre>
</div></li>
<li><p>
我们按照下面的代码启动整个pod
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# <span style="color: #076678;">PML_ID</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">crictl runp po-mlim.yaml</span><span style="color: #076678;">)</span>
root@host01:/opt# <span style="color: #076678;">CML_ID</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">crictl create $PML_ID co-mlim.yaml po-mlim.yaml</span><span style="color: #076678;">)</span>
root@host01:/opt# crictl start $<span style="color: #076678;">CML_ID</span>
6bc98cb013e9e1521150907313b59050950e81ec4346f05b6b6c39a76c8ed4d2
root@host01:/opt# crictl ps
CONTAINER           IMAGE                                      CREATED             STATE               NAME                ATTEMPT             POD ID
6bc98cb013e9e       docker.io/bookofkubernetes/stress:stable   10 seconds ago      Running             stress2             0                   7a08f5dd2a778
</pre>
</div></li>
<li><p>
我们看到crictl ps的结果中stress已经进入了Running状态,但是其实stress只是在不停的申请内存而已,并没
有真正的启动,我们可以通过log来看下stress真正发生了什么
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# crictl logs $<span style="color: #076678;">CML_ID</span>
stress-ng: debug: <span style="color: #076678;">[</span>7<span style="color: #076678;">]</span> stress-ng 0.13.05
stress-ng: debug: <span style="color: #076678;">[</span>7<span style="color: #076678;">]</span> system: Linux host01 5.4.0-148-generic <span style="color: #a89984;">#</span><span style="color: #a89984;">165-Ubuntu SMP Tue Apr 18 08:53:12 UTC 2023 x86_64</span>
stress-ng: debug: <span style="color: #076678;">[</span>7<span style="color: #076678;">]</span> RAM total: 1.9G, RAM free: 1.3G, swap free: 0.0
stress-ng: debug: <span style="color: #076678;">[</span>7<span style="color: #076678;">]</span> 2 processors online, 2 processors configured
stress-ng: debug: <span style="color: #076678;">[</span>7<span style="color: #076678;">]</span> main: can<span style="color: #79740e;">'t set oom_score_adj</span>
<span style="color: #79740e;">stress-ng: info:  [7] defaulting to a 86400 second (1 day, 0.00 secs) run per stressor</span>
<span style="color: #79740e;">stress-ng: info:  [7] dispatching hogs: 1 vm</span>
<span style="color: #79740e;">stress-ng: debug: [7] cache allocate: shared cache buffer size: 6144K</span>
<span style="color: #79740e;">stress-ng: debug: [7] starting stressors</span>
<span style="color: #79740e;">stress-ng: debug: [7] 1 stressor started</span>
<span style="color: #79740e;">stress-ng: debug: [12] stress-ng-vm: can'</span>t set oom_score_adj
stress-ng: debug: <span style="color: #076678;">[</span>12<span style="color: #076678;">]</span> stress-ng-vm: started <span style="color: #076678;">[</span>12<span style="color: #076678;">]</span> <span style="color: #076678;">(</span>instance 0<span style="color: #076678;">)</span>
stress-ng: debug: <span style="color: #076678;">[</span>12<span style="color: #076678;">]</span> stress-ng-vm using method <span style="color: #79740e;">'all'</span>
stress-ng: debug: <span style="color: #076678;">[</span>12<span style="color: #076678;">]</span> stress-ng-vm: child died: signal 9 <span style="color: #79740e;">'SIGKILL'</span> <span style="color: #076678;">(</span>instance 0<span style="color: #076678;">)</span>
stress-ng: debug: <span style="color: #076678;">[</span>12<span style="color: #076678;">]</span> stress-ng-vm: assuming killed by OOM killer, restarting again <span style="color: #076678;">(</span>instance 0<span style="color: #076678;">)</span>
stress-ng: debug: <span style="color: #076678;">[</span>12<span style="color: #076678;">]</span> stress-ng-vm: child died: signal 9 <span style="color: #79740e;">'SIGKILL'</span> <span style="color: #076678;">(</span>instance 0<span style="color: #076678;">)</span>
stress-ng: debug: <span style="color: #076678;">[</span>12<span style="color: #076678;">]</span> stress-ng-vm: assuming killed by OOM killer, restarting again <span style="color: #076678;">(</span>instance 0<span style="color: #076678;">)</span>
stress-ng: debug: <span style="color: #076678;">[</span>12<span style="color: #076678;">]</span> stress-ng-vm: child died: signal 9 <span style="color: #79740e;">'SIGKILL'</span> <span style="color: #076678;">(</span>instance 0<span style="color: #076678;">)</span>
stress-ng: debug: <span style="color: #076678;">[</span>12<span style="color: #076678;">]</span> stress-ng-vm: assuming killed by OOM killer, restarting again <span style="color: #076678;">(</span>instance 0<span style="color: #076678;">)</span>
stress-ng: debug: <span style="color: #076678;">[</span>12<span style="color: #076678;">]</span> stress-ng-vm: child died: signal 9 <span style="color: #79740e;">'SIGKILL'</span> <span style="color: #076678;">(</span>instance 0<span style="color: #076678;">)</span>
stress-ng: debug: <span style="color: #076678;">[</span>12<span style="color: #076678;">]</span> stress-ng-vm: assuming killed by OOM killer, restarting again <span style="color: #076678;">(</span>instance 0<span style="color: #076678;">)</span>
stress-ng: debug: <span style="color: #076678;">[</span>12<span style="color: #076678;">]</span> stress-ng-vm: child died: signal 9 <span style="color: #79740e;">'SIGKILL'</span> <span style="color: #076678;">(</span>instance 0<span style="color: #076678;">)</span>
stress-ng: debug: <span style="color: #076678;">[</span>12<span style="color: #076678;">]</span> stress-ng-vm: assuming killed by OOM killer, restarting again <span style="color: #076678;">(</span>instance 0<span style="color: #076678;">)</span>
</pre>
</div></li>
<li>从日志我们可以看到,系统一直在用OOM killer来关闭stress</li>
<li>OOM killer是Linux的一个特性,一般是用来在系统整体内存比较少的时候,kill 一个或者多个进程来保护系
统的,它通常会发送SIGKILL来关闭进程. SIGKILL这个信号是告诉进程你需要马上结束,甚至不需要做任何的cleanup</li>
<li>需要注意的是,cgroup在进程超过内存的情况下,有多种处理选择,不一定使用OOM killer.</li>
<li>换句话说,使用OOM killer是k8s根据自身的特点做的选择.</li>
<li>k8s的自身特点是什么呢?那就是在一个application里面,每个container都不是特别重要的,每个container不
小心被删掉了(或者关闭了),都是经常发生,不被在意的事情(因为同时有多个container在一个load balancer
后面,类似aws的ec2,也是出问题的概率高达5%)</li>
<li>既然一个container被kill掉都是经常发生的问题,那么内存满了,直接kill掉容器也是可以理解的.毕竟内存满
也是常见bug</li>
<li>另外把一个内存不够用的容器删掉,总比它运行着却什么也不做要好</li>
</ul>
</div>
</div>
<div id="outline-container-org6a46d4a" class="outline-3">
<h3 id="org6a46d4a"><span class="section-number-3">3.4.</span> Network Bandwidth Limits</h3>
<div class="outline-text-3" id="text-3-4">
<ul class="org-ul">
<li>cpu资源是由kernel直接控制每个进程用多少的,所以限制起来很容易</li>
<li>对于内存资源,kernel虽然不能从进程里面扣除内存,但是可以在内存申请的时候进行控制</li>
<li><p>
我们这一节要讨论的网络,是最难以控制limit的,因为Linux的cgroup没法有效的控制网络用量:
</p>
<pre class="example" id="orgaac0a1a">
Although Linux does provide some cgroup capability for network
interfaces, these would only help us prioritize and classify
network traffic
</pre></li>
<li>既然不能使用cgroup,那么,我们只好直接配置linux kernel的traffic control功能来控制网络使用量</li>
<li>整个实验分三步:
<ul class="org-ul">
<li><p>
第一步,我们使用iperf3来测试下两个机器之间的网速,我们可以看到是一个Gbit网络
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# iperf3 -c 192.168.61.12
Connecting to host 192.168.61.12, port 5201
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span> local 192.168.61.11 port 55532 connected to 192.168.61.12 port 5201
<span style="color: #076678;">[</span> ID<span style="color: #076678;">]</span> Interval           Transfer     Bitrate         Retr  Cwnd
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   0.00-1.00   sec   495 MBytes  4.15 Gbits/sec  3080    235 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   1.00-2.00   sec   496 MBytes  4.16 Gbits/sec  3557    215 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   2.00-3.00   sec   506 MBytes  4.25 Gbits/sec  2944    212 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   3.00-4.00   sec   494 MBytes  4.14 Gbits/sec  2626    222 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   4.00-5.00   sec   498 MBytes  4.17 Gbits/sec  2825    246 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   5.00-6.00   sec   504 MBytes  4.23 Gbits/sec  2332    236 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   6.00-7.00   sec   496 MBytes  4.16 Gbits/sec  2293    182 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   7.00-8.00   sec   509 MBytes  4.27 Gbits/sec  2909    219 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   8.00-9.00   sec   504 MBytes  4.23 Gbits/sec  2809    171 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   9.00-10.00  sec   504 MBytes  4.23 Gbits/sec  2737    235 KBytes
- - - - - - - - - - - - - - - - - - - - - - - - -
<span style="color: #076678;">[</span> ID<span style="color: #076678;">]</span> Interval           Transfer     Bitrate         Retr
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   0.00-10.00  sec  4.89 GBytes  4.20 Gbits/sec  28112             sender
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   0.00-10.00  sec  4.88 GBytes  4.20 Gbits/sec                  receiver

iperf Done.
</pre>
</div></li>
<li><p>
第二步,我们使用tc来创建limit,期间用ip addr来确定要限制的network card名字
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# <span style="color: #076678;">IFACE</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">ip -o addr | grep 192.168.61.11 | awk '{print $2}'</span><span style="color: #076678;">)</span>
root@host01:/opt# echo $<span style="color: #076678;">IFACE</span>
enp0s8
root@host01:/opt# tc qdisc add dev $<span style="color: #076678;">IFACE</span> root tbf rate 100mbit burst 256kbit latency 400ms
</pre>
</div></li>
<li><p>
第三步,我们再用iperf3来测试下两个机器之间的网速,我们可以看到,只有100Mbit左右了
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# iperf3 -c 192.168.61.12
Connecting to host 192.168.61.12, port 5201
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span> local 192.168.61.11 port 52802 connected to 192.168.61.12 port 5201
<span style="color: #076678;">[</span> ID<span style="color: #076678;">]</span> Interval           Transfer     Bitrate         Retr  Cwnd
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   0.00-1.00   sec  11.9 MBytes  99.9 Mbits/sec    0    137 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   1.00-2.00   sec  11.2 MBytes  93.8 Mbits/sec    0    137 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   2.00-3.00   sec  10.6 MBytes  88.6 Mbits/sec    0    137 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   3.00-4.00   sec  10.9 MBytes  91.3 Mbits/sec    0    137 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   4.00-5.00   sec  11.5 MBytes  96.4 Mbits/sec    0    137 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   5.00-6.00   sec  11.2 MBytes  93.8 Mbits/sec    0    137 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   6.00-7.00   sec  11.2 MBytes  93.8 Mbits/sec    0    137 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   7.00-8.00   sec  11.2 MBytes  93.8 Mbits/sec    0    137 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   8.00-9.00   sec  11.2 MBytes  93.8 Mbits/sec    0    137 KBytes
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   9.00-10.00  sec  11.2 MBytes  93.8 Mbits/sec    0    137 KBytes
- - - - - - - - - - - - - - - - - - - - - - - - -
<span style="color: #076678;">[</span> ID<span style="color: #076678;">]</span> Interval           Transfer     Bitrate         Retr
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   0.00-10.00  sec   112 MBytes  93.9 Mbits/sec    0             sender
<span style="color: #076678;">[</span>  5<span style="color: #076678;">]</span>   0.00-10.01  sec   111 MBytes  93.3 Mbits/sec                  receiver

iperf Done.
</pre>
</div></li>
</ul></li>
<li>因为我们是通过限制network card来达到限速目的的,那么我们就要保证进程只使用特定的network card,这个下
一章会研究</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org3676f72" class="outline-2">
<h2 id="org3676f72"><span class="section-number-2">4.</span> Chapter 4: Network Namespaces</h2>
<div class="outline-text-2" id="text-4">
<ul class="org-ul">
<li>容器化的微服务里面,最难的就是理解container network,原因有下:
<ul class="org-ul">
<li>即便没有容器,网络也是最复杂的部分,即便是物理机器之间的ping,都是包含了很多层的抽象</li>
<li>容器引入后,网络就更加复杂了,因为每个容器都有自己的虚拟network device</li>
<li>容器的编排框架(k8s)会增加一个overlay network(容器会在这个网络相互通信), 这个overlay network会增加
额外的复杂度</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org5128c45" class="outline-3">
<h3 id="org5128c45"><span class="section-number-3">4.1.</span> Network Isolation</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>在第二章我们学习了isolation的重要性:因为process通常不能影响他们看不到的东西</li>
<li>不影响其他process当然是network也选择isolation的原因,但是还有其他原因,那就是配置的便捷性:
<ul class="org-ul">
<li>如果只有一个网络的话,那么两个process要非常小心才能不让自己的port冲突</li>
<li>使用了network isolation之后,每个container都有一个独立的virtual network interface(独立的ip),那么
这个容器选择使用任何端口提供服务,都不会和其他容器冲突的</li>
</ul></li>
<li>下面我们来看个两个容器可以使用相同端口的例子(ip不同)
<ul class="org-ul">
<li><p>
例子如下
</p>
<div class="org-src-container">
<pre class="src src-shell">
root@host01:/opt# cd /opt/
root@host01:/opt# source nginx.sh
Image is up to date for docker.io/library/nginx@sha256:6b06964cdbbc517102ce5e0cef95152f3c6a7ef703e4057cb574539de91f72e6
09fe9a0ea1f7af3b4214e5551975e0daba6285c793631fa48a4fad7f5a5fe805
0bf98ff9540df4c902854daa3acd35e1823b4983c511fa0bfb2395f28e7554bf
root@host01:/opt# crictl ps
CONTAINER           IMAGE                            CREATED             STATE               NAME                ATTEMPT             POD ID
0bf98ff9540df       docker.io/library/nginx:latest   12 seconds ago      Running             nginx2              0                   3de48a321fdf7
09fe9a0ea1f7a       docker.io/library/nginx:latest   12 seconds ago      Running             nginx1              0                   d90e09319975a
root@host01:/opt# crictl exec $<span style="color: #076678;">N1C_ID</span> cat /proc/net/tcp
  sl  local_address rem_address   st tx_queue rx_queue tr tm-&gt;when retrnsmt   uid  timeout inode
   0: 00000000:0050 00000000:0000 0A 00000000:00000000 00:00000000 00000000     0        0 33667 1 0000000000000000 100 0 0 10 0
root@host01:/opt# crictl exec $<span style="color: #076678;">N2C_ID</span> cat /proc/net/tcp
  sl  local_address rem_address   st tx_queue rx_queue tr tm-&gt;when retrnsmt   uid  timeout inode
   0: 00000000:0050 00000000:0000 0A 00000000:00000000 00:00000000 00000000     0        0 34840 1 0000000000000000 100 0 0 10 0
</pre>
</div></li>
<li>上面例子使用cat 每个容器的/proc/net/tcp来获取网络信息,这是因为普通容器里面是没有netstat命令的:这
是因为每个容器都有自己的mnt namespac,它不会把host的所有二进制都放进来. 只放了必要的,比如cat,ls等</li>
<li>cat /proc/net/tcp的结果是容器的端口是十六进制的50,也就是十进制的80,符合预期</li>
</ul></li>
<li>我们再来看看另外一个例子,我们新创建一个busybox的容器,进入这个容器的内部,看看网络情况
<ul class="org-ul">
<li><p>
例子如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt#
root@host01:/opt# source busybox.sh
Image is up to date for docker.io/library/busybox@sha256:560af6915bfc8d7630e50e212e08242d37b63bd5c1ccf9bd4acccf116e262d5b
5ac8a8a393503ff6f550aead6391497915626e569027f25ae96b38c442056dd1
root@host01:/opt# crictl ps
CONTAINER           IMAGE                              CREATED             STATE               NAME                ATTEMPT             POD ID
5ac8a8a393503       docker.io/library/busybox:latest   9 seconds ago       Running             busybox             0                   4bccdb251c298
0bf98ff9540df       docker.io/library/nginx:latest     19 minutes ago      Running             nginx2              0                   3de48a321fdf7
09fe9a0ea1f7a       docker.io/library/nginx:latest     19 minutes ago      Running             nginx1              0                   d90e09319975a
root@host01:/opt# crictl exec -ti $<span style="color: #076678;">B1C_ID</span> /bin/sh
/ <span style="color: #a89984;"># </span><span style="color: #a89984;">ip addr</span>
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue
    link/ether ba:39:25:25:d7:2a brd ff:ff:ff:ff:ff:ff
    inet 10.85.0.4/16 brd 10.85.255.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::b839:25ff:fe25:d72a/64 scope link
       valid_lft forever preferred_lft forever
/ <span style="color: #a89984;"># </span><span style="color: #a89984;">ping -c 1 192.168.61.11</span>
PING 192.168.61.11 <span style="color: #076678;">(</span>192.168.61.11<span style="color: #076678;">)</span>: 56 data bytes
64 bytes from 192.168.61.11: <span style="color: #076678;">seq</span>=0 <span style="color: #076678;">ttl</span>=64 <span style="color: #076678;">time</span>=0.240 ms

--- 192.168.61.11 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 0.240/0.240/0.240 ms
/ <span style="color: #a89984;"># </span><span style="color: #a89984;">ip route</span>
default via 10.85.0.1 dev eth0
10.85.0.0/16 dev eth0 scope link  src 10.85.0.4
/ <span style="color: #a89984;">#</span>
</pre>
</div></li>
<li>我们创建新的busybox容器后进入容器的shell,使用 ip addr命令得到网络情况.排除loopback网络的话,我们可
以看到容器真正的ip地址是10.85.0.4</li>
<li>我们host的地址是192.168.61.11, 按理说host和容器是网络不通的,但是这里ping却成功了,这是由于路由表里
面增加了中转的一个entity,让两个网络认识了,这个entity通过ip route命令可以看到是10.85.0.1</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgd0aff4e" class="outline-3">
<h3 id="orgd0aff4e"><span class="section-number-3">4.2.</span> Network Namespaces</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>CRI-O 使用了Linux network namespace来创建isolation,我们在第二章已经见识到过了,这里是更详细的介绍</li>
<li>我们可以使用lsns命令来列出所有的network namespace
<ul class="org-ul">
<li><p>
代码如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# lsns -t net
        NS TYPE NPROCS   PID USER    NETNSID NSFS                                            COMMAND
4026531992 net     120     1 root unassigned                                                 /sbin/init
4026532199 net       4  2817 root          0 /run/netns/000a5bb5-2258-42ed-a969-0c9acbacad41 /pause
4026532273 net       4  2951 root          1 /run/netns/d06d38dc-073c-456f-8d45-fa810ffbc03d /pause
4026532338 net       2  4114 root          2 /run/netns/7a511363-9a8b-4e60-8fa1-12bac58bce21 /pause
</pre>
</div></li>
<li>我们可以看到第一个是为所有进程服务的namespace,不是容器的</li>
<li>另外的三个network namespace都是为容器服务的,每个Pod一个namespace(Pod是容器的上级)</li>
<li>Pod的network namespace都是使用了/pause命令,这让这些namespace一直存在,哪怕Pod里面的容器不停的变化</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orge385ec3" class="outline-4">
<h4 id="orge385ec3"><span class="section-number-4">4.2.1.</span> Inspecting Network Namespaces</h4>
<div class="outline-text-4" id="text-4-2-1">
<ul class="org-ul">
<li>我们还可以使用ip nets命令来列出network namespace
<ul class="org-ul">
<li><p>
代码如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns list
<span style="color: #b57614;">7a511363-9a8b-4e60-8fa1-12bac58bce21</span> <span style="color: #076678;">(</span>id: 2<span style="color: #076678;">)</span>
<span style="color: #b57614;">d06d38dc-073c-456f-8d45-fa810ffbc03d</span> <span style="color: #076678;">(</span>id: 1<span style="color: #076678;">)</span>
<span style="color: #b57614;">000a5bb5-2258-42ed-a969-0c9acbacad41</span> <span style="color: #076678;">(</span>id: 0<span style="color: #076678;">)</span>
</pre>
</div></li>
<li>我们看到这里由于配置文件访问的不同,这里没有全局的network namespace啦</li>
<li>这里的id为2的network namespace由于是最晚创建的,那么应该是busybox 容器的network namespace,我们后
面通过其他命令验证了我们的想法</li>
</ul></li>
<li>crictl inspect和jq的组合,可以把容器的详细信息打印出来,获取各种详细的信息
<ul class="org-ul">
<li><p>
我们这里只提取关于network namespace的信息
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# <span style="color: #076678;">NETNS_PATH</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">crictl inspectp $B1P_ID | jq -r '.info.runtimeSpec.linux.namespaces[]|select(.type=="network"</span><span style="color: #79740e;">).path'</span><span style="color: #076678;">)</span>
root@host01:/opt# echo $<span style="color: #076678;">NETNS_PATH</span>
/var/run/netns/7a511363-9a8b-4e60-8fa1-12bac58bce21
root@host01:/opt# <span style="color: #076678;">NETNS</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">basename $NETNS_PATH</span><span style="color: #076678;">)</span>
root@host01:/opt# echo $<span style="color: #076678;">NETNS</span>
7a511363-9a8b-4e60-8fa1-12bac58bce21
</pre>
</div></li>
<li>注意,这里用了jq来解析crictl inspectp 返回的json信息,并且找到network namespace的path并且赋值给NETNS_PATH</li>
<li>我们再把这个full path通过basename来获取其文件名</li>
</ul></li>
<li>得到network namespace之后,我们可以有很多用法:
<ul class="org-ul">
<li><p>
获取这个namespace里面的进程id
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns pids $<span style="color: #076678;">NETNS</span>
4114
4152
</pre>
</div></li>
<li><p>
获取到这些id后,可以通过ps来查看他们的信息
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ps --pid $<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">ip netns pids $NETNS</span><span style="color: #076678;">)</span>
    PID TTY      STAT   TIME COMMAND
   4114 ?        Ss     0:00 /pause
   4152 ?        Ss     0:00 /bin/sleep 36000
</pre>
</div></li>
<li><p>
我们还可以使用ip netns来查看这个namespace里面的网卡信息
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns exec $<span style="color: #076678;">NETNS</span> ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 4e:5b:69:d4:78:b6 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.85.0.7/16 brd 10.85.255.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::4c5b:69ff:fed4:78b6/64 scope link
       valid_lft forever preferred_lft forever
</pre>
</div></li>
<li>注意,ip netns看到的网卡信息,和我们进入到busybox容器,然后ip a的结果是一样的. 这些网卡都是CRI-O创建
之后,放入到network namespace里面的. 我们后面会体验如何创建network namespce和它里面的网卡</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org1bfb446" class="outline-4">
<h4 id="org1bfb446"><span class="section-number-4">4.2.2.</span> Creating Network Namespaces</h4>
<div class="outline-text-4" id="text-4-2-2">
<ul class="org-ul">
<li><p>
你可以使用如下命令来创建一个network namespace,创建后可以马上查找到
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns add myns
root@host01:/opt# ip netns list
myns
<span style="color: #b57614;">6ff41cbb-8930-4553-a719-3fd54cbf5e62</span> <span style="color: #076678;">(</span>id: 2<span style="color: #076678;">)</span>
<span style="color: #b57614;">1f6170da-8d13-49d6-a987-36f00f6a74f8</span> <span style="color: #076678;">(</span>id: 1<span style="color: #076678;">)</span>
<span style="color: #b57614;">97552f2a-3285-447e-b019-96a17068ed1b</span> <span style="color: #076678;">(</span>id: 0<span style="color: #076678;">)</span>
</pre>
</div></li>
<li><p>
刚刚创建好的network namespace只有一个loopback的interface, 而且状态还是DOWN的
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns exec myns ip addr
1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
</pre>
</div></li>
<li><p>
我们启动loopback,并且验证其是否启动(我们可以看到状态变成了UNKNOWN),如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns exec myns ip link set dev lo up
root@host01:/opt# ip netns exec myns ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
</pre>
</div></li>
<li><p>
虽然是UNKNOWN,但是其实是可以用的,我们ping的通127.0.0.1啦
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns exec myns ping -c 1 127.0.0.1
PING 127.0.0.1 <span style="color: #076678;">(</span>127.0.0.1<span style="color: #076678;">)</span> 56<span style="color: #076678;">(</span>84<span style="color: #076678;">)</span> bytes of data.
64 bytes from 127.0.0.1: <span style="color: #076678;">icmp_seq</span>=1 <span style="color: #076678;">ttl</span>=64 <span style="color: #076678;">time</span>=0.022 ms

--- 127.0.0.1 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.022/0.022/0.022/0.000 ms
</pre>
</div></li>
<li>能够ping通loopback非常重要,这桌面我们有能力发送和接受packet</li>
<li>但是想让网络更加的有用,我们必须创建一个新的network device(当然是在这个network namespace里面啦),让这
个device和host能够联通,进而和万维网联通. 为了做到这点,我们需要创建一个叫做veth(virtual Ethernet)的设备</li>
<li>veth虽然名字叫虚拟以太网,但是这个以太网里面只有两个设备.我们把veth想象成网线更容易理解(veth也就常被
称之为veth pair):
<ul class="org-ul">
<li>网线有两个节点</li>
<li>一个节点有数据进入,网线的另外一个节点里面有数据流出</li>
</ul></li>
<li>我们使用如下命令来创建veth pair:
<ul class="org-ul">
<li><p>
代码如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip link add myveth-host type veth peer myveth-myns netns myns
</pre>
</div></li>
<li>上述命令做了三件事:
<ol class="org-ol">
<li>创建veth device myveth-host</li>
<li>创建veth device myveth-myns</li>
<li>把myveth-myns放入到network space myns里面</li>
</ol></li>
</ul></li>
<li><p>
三件事情不包括把myveth-host放入到myns里面,所以我们再host机器上ip addr能发现myveth-host的身影
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip addr
...
5: vethb01d2292@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cni0 state UP group default
    link/ether 7e:a1:99:b4:28:46 brd ff:ff:ff:ff:ff:ff link-netns 97552f2a-3285-447e-b019-96a17068ed1b
    inet6 fe80::7ca1:99ff:feb4:2846/64 scope link
       valid_lft forever preferred_lft forever
6: vetha00e83bc@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cni0 state UP group default
    link/ether f6:3c:26:db:5b:16 brd ff:ff:ff:ff:ff:ff link-netns 1f6170da-8d13-49d6-a987-36f00f6a74f8
    inet6 fe80::f43c:26ff:fedb:5b16/64 scope link
       valid_lft forever preferred_lft forever
7: veth2dd66dba@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cni0 state UP group default
    link/ether 1a:39:2d:2e:8f:1b brd ff:ff:ff:ff:ff:ff link-netns 6ff41cbb-8930-4553-a719-3fd54cbf5e62
    inet6 fe80::1839:2dff:fe2e:8f1b/64 scope link
       valid_lft forever preferred_lft forever
8: myveth-host@if2: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether ea:6b:3c:4f:92:ce brd ff:ff:ff:ff:ff:ff link-netns myns
</pre>
</div></li>
<li><p>
我们可以看到在host上面可见的myveth-host是和network namespace myns相连的,看这句
</p>
<pre class="example" id="org5444b6f">
link-netns myns
</pre></li>
<li><p>
我们也猛然发现其他的veth设备都链接了一个network namespace,也就是我们前面看到的三个namespace(uuid一样)
</p>
<pre class="example" id="org149e0a6">
link-netns 97552f2a-3285-447e-b019-96a17068ed1b
link-netns 1f6170da-8d13-49d6-a987-36f00f6a74f8
link-netns 6ff41cbb-8930-4553-a719-3fd54cbf5e62
</pre></li>
<li><p>
我们使用ip netns exec 可以看到在myns network namespace里面已经有veth pair的另外一个设备myveth-myns了
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns exec myns ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: myveth-myns@if8: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether ee:22:d6:84:75:67 brd ff:ff:ff:ff:ff:ff link-netnsid 0
</pre>
</div></li>
<li>可以看到state还是DOWN的,我们要给这个设备赋ip,然后启动它
<ul class="org-ul">
<li><p>
赋ip命令如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns exec myns ip addr add 10.85.0.254/16 dev myveth-myns
</pre>
</div></li>
<li><p>
启动network namespace里面的myveth-myns
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns exec myns ip link set dev myveth-myns up
</pre>
</div></li>
<li><p>
启动host上的myveth-host
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip link set dev myveth-host up
</pre>
</div></li>
</ul></li>
<li>完成上述操作后,我们马上运行命令去检查设置的结果
<ul class="org-ul">
<li><p>
在network namespace内部已经UP
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns exec myns ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: myveth-myns@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether ee:22:d6:84:75:67 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.85.0.254/16 scope global myveth-myns
       valid_lft forever preferred_lft forever
    inet6 fe80::ec22:d6ff:fe84:7567/64 scope link
       valid_lft forever preferred_lft forever
</pre>
</div></li>
<li><p>
在host内部也UP了
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip addr
...
8: myveth-host@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether ea:6b:3c:4f:92:ce brd ff:ff:ff:ff:ff:ff link-netns myns
    inet6 fe80::e86b:3cff:fe4f:92ce/64 scope link
       valid_lft forever preferred_lft forever
</pre>
</div></li>
<li><p>
我们从network namespace内部ping 10.85.0.254成功了.这个是显然的,因为我们再network namespace内部已经成功启动了这个ip
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns exec myns ping -c 1 10.85.0.254
PING 10.85.0.254 <span style="color: #076678;">(</span>10.85.0.254<span style="color: #076678;">)</span> 56<span style="color: #076678;">(</span>84<span style="color: #076678;">)</span> bytes of data.
64 bytes from 10.85.0.254: <span style="color: #076678;">icmp_seq</span>=1 <span style="color: #076678;">ttl</span>=64 <span style="color: #076678;">time</span>=0.075 ms

--- 10.85.0.254 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.075/0.075/0.075/0.000 ms
</pre>
</div></li>
<li><p>
但是我们从host内部ping 10.85.0.254却失败了.原因是两个设备虽然都启动了,但是没有connect起来
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ping -c 1 10.85.0.254
PING 10.85.0.254 <span style="color: #076678;">(</span>10.85.0.254<span style="color: #076678;">)</span> 56<span style="color: #076678;">(</span>84<span style="color: #076678;">)</span> bytes of data.
From 10.85.0.1 <span style="color: #076678;">icmp_seq</span>=1 Destination Host Unreachable

--- 10.85.0.254 ping statistics ---
1 packets transmitted, 0 received, +1 errors, 100% packet loss, time 0ms
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb316678" class="outline-3">
<h3 id="orgb316678"><span class="section-number-3">4.3.</span> Bridge Interfaces</h3>
<div class="outline-text-3" id="text-4-3">
<ul class="org-ul">
<li>host 的veth当前没有连接到任何的其他veth,那么怎么才能连接到network namespace里面的其他veth呢?我们先
研究下其他的容器是怎么做的:
<ul class="org-ul">
<li><p>
首先看看容器busybox的veth
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip addr
...
7: veth2dd66dba@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cni0 state UP group default
    link/ether 1a:39:2d:2e:8f:1b brd ff:ff:ff:ff:ff:ff link-netns 6ff41cbb-8930-4553-a719-3fd54cbf5e62
    inet6 fe80::1839:2dff:fe2e:8f1b/64 scope link
       valid_lft forever preferred_lft forever
</pre>
</div></li>
<li>我们可以看到,这个veth有一个设置为master cni0,这个设置的意思是这个veth属于一个network bridge</li>
<li>network bridge是把多个interface链接起来的device,你可以把它想成是真实世界的以太网switch,network
bridge根据MAC地址来route traffice</li>
<li><p>
我们再来看看这个cni0的信息
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip addr
...
4: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 22:fc:68:2d:4b:ce brd ff:ff:ff:ff:ff:ff
    inet 10.85.0.1/16 brd 10.85.255.255 scope global cni0
       valid_lft forever preferred_lft forever
    inet6 fe80::20fc:68ff:fe2d:4bce/64 scope link
       valid_lft forever preferred_lft forever
</pre>
</div></li>
<li>这个network bridge有个ip是10.85.0.1, 而属于这个network bridge的busybox的veth却没有ip</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orgad90c23" class="outline-4">
<h4 id="orgad90c23"><span class="section-number-4">4.3.1.</span> Adding Interfaces to a Bridge</h4>
<div class="outline-text-4" id="text-4-3-1">
<ul class="org-ul">
<li>为了能够让host的veth能够和network namespace里面的veth互通,我们显然是要把host的veth加入到cni0这个network
bridge里面:
<ul class="org-ul">
<li><p>
首先我们使用brctl show来查看当前的network bridge都有谁
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# brctl show
bridge name     bridge id               STP enabled     interfaces
cni0            8000.22fc682d4bce       no              veth2dd66dba
                                                        vetha00e83bc
                                                        vethb01d2292
</pre>
</div></li>
<li><p>
然后我们通过brctl addif来给某个network bridge添加veth
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# brctl addif cnio myveth-host
bridge cnio does not exist!
root@host01:/opt# brctl addif cni0 myveth-host
root@host01:/opt# brctl show
bridge name     bridge id               STP enabled     interfaces
cni0            8000.22fc682d4bce       no              myveth-host
                                                        veth2dd66dba
                                                        vetha00e83bc
                                                        vethb01d2292
</pre>
</div></li>
</ul></li>
<li><p>
host的veth加入到network bridge之后,我们从host就能ping通network namespace里面的ip
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ping -c 1 10.85.0.254
PING 10.85.0.254 <span style="color: #076678;">(</span>10.85.0.254<span style="color: #076678;">)</span> 56<span style="color: #076678;">(</span>84<span style="color: #076678;">)</span> bytes of data.
64 bytes from 10.85.0.254: <span style="color: #076678;">icmp_seq</span>=1 <span style="color: #076678;">ttl</span>=64 <span style="color: #076678;">time</span>=0.104 ms

--- 10.85.0.254 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.104/0.104/0.104/0.000 ms
</pre>
</div></li>
<li>我们已经成功建立了host veth和network bridge的connection,也就让host和network namespace联通了网络,后
面我们通过例子来看看traffic是如何flow的</li>
</ul>
</div>
</div>
<div id="outline-container-orgdc46607" class="outline-4">
<h4 id="orgdc46607"><span class="section-number-4">4.3.2.</span> Tracking Traffic</h4>
<div class="outline-text-4" id="text-4-3-2">
<ul class="org-ul">
<li><p>
我们首先在后台开启一个命令,不停的ping 10.85.0.254,并且把输出抛弃(可以使用killall ping命令来删除所有
后台运行的ping命令)
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ping 10.85.0.254 &gt; /dev/null 2&gt;&amp;1 &amp;
<span style="color: #076678;">[</span>1<span style="color: #076678;">]</span> 6234
</pre>
</div></li>
<li>然后我们用如下命令来获取traffic的详细信息
<ul class="org-ul">
<li><p>
命令如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# timeout 1s tcpdump -i any -n icmp
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on any, link-type LINUX_SLL <span style="color: #076678;">(</span>Linux cooked v1<span style="color: #076678;">)</span>, capture size 262144 bytes
09:34:10.833058 IP 10.85.0.1 &gt; 10.85.0.254: ICMP echo request, id 3, seq 15, length 64
09:34:10.833073 IP 10.85.0.1 &gt; 10.85.0.254: ICMP echo request, id 3, seq 15, length 64
09:34:10.833088 IP 10.85.0.254 &gt; 10.85.0.1: ICMP echo reply, id 3, seq 15, length 64
09:34:10.833088 IP 10.85.0.254 &gt; 10.85.0.1: ICMP echo reply, id 3, seq 15, length 64

4 packets captured
7 packets received by filter
0 packets dropped by kernel
</pre>
</div></li>
<li>timeout 1s在这里是只让tcpdump运行一秒钟的意思</li>
<li>tcpdump -i any是指的监听所有端口</li>
<li>tcpdump -n是指不把主机的网络地址转换为名字</li>
<li>tcpdump icmp是指只输出icmp数据</li>
<li>从上面的数据我们可以看出,要到10.85.0.254的数据,都是从10.85.0.1(也就是cni0, bridge interface)流向
10.85.0.254</li>
<li><p>
之所以从bridge interface流出,是由于host的路由表如下(目标为10.85.0.0/16的机器,都通过10.85.0.1)
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip route
default via 10.0.2.2 dev enp0s3 proto dhcp src 10.0.2.15 metric 100
10.0.2.0/24 dev enp0s3 proto kernel scope link src 10.0.2.15
10.0.2.2 dev enp0s3 proto dhcp scope link src 10.0.2.15 metric 100
10.85.0.0/16 dev cni0 proto kernel scope link src 10.85.0.1
192.168.61.0/24 dev enp0s8 proto kernel scope link src 192.168.61.11
</pre>
</div></li>
</ul></li>
<li>我们再来看看从network namespace里面ping host是否成功
<ul class="org-ul">
<li><p>
命令如下,我们可以看到ping没有成功
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns exec myns ping -c 1 192.168.61.11
ping: connect: Network is unreachable
</pre>
</div></li>
<li><p>
原因在于,虽然host veth和container veth连接起来了(可以理解为物理连接起来了),但是container veth不
知道怎么寻找192.168.61.11,因为container的路由表没有数据,我们可以通过如下命令添加
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns exec myns ip route add default via 10.85.0.1
</pre>
</div></li>
<li><p>
添加成功后,从network namespace可以访问到192.168.61.11了,如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns exec myns ping -c 1 192.168.61.11
PING 192.168.61.11 <span style="color: #076678;">(</span>192.168.61.11<span style="color: #076678;">)</span> 56<span style="color: #076678;">(</span>84<span style="color: #076678;">)</span> bytes of data.
64 bytes from 192.168.61.11: <span style="color: #076678;">icmp_seq</span>=1 <span style="color: #076678;">ttl</span>=64 <span style="color: #076678;">time</span>=0.035 ms

--- 192.168.61.11 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.035/0.035/0.035/0.000 ms
</pre>
</div></li>
</ul></li>
<li>我们已经更改了很多设置,但是我们自己配置的network namespace距离真正的容器网络配置还有一点没有做到,
那就是从容器里面访问host01以外的网络(这里的例子是host02, ip为192.168.61.12)
<ul class="org-ul">
<li><p>
我们首先看看容器里面是可以访问的到host02的
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# crictl ps
CONTAINER           IMAGE                              CREATED             STATE               NAME                ATTEMPT             POD ID
1d763855358fa       docker.io/library/busybox:latest   51 minutes ago      Running             busybox             0                   4d9c69c701ccf
7c05c9b6fdd8e       docker.io/library/nginx:latest     52 minutes ago      Running             nginx2              0                   5e320641249e2
4c2dae8e0fbac       docker.io/library/nginx:latest     52 minutes ago      Running             nginx1              0                   501c3d1ad87a0

root@host01:/opt# crictl exec 1d763855358fa ping -c 1 192.168.61.12
PING 192.168.61.12 <span style="color: #076678;">(</span>192.168.61.12<span style="color: #076678;">)</span>: 56 data bytes
64 bytes from 192.168.61.12: <span style="color: #076678;">seq</span>=0 <span style="color: #076678;">ttl</span>=63 <span style="color: #076678;">time</span>=0.644 ms

--- 192.168.61.12 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 0.644/0.644/0.644 ms
</pre>
</div></li>
<li><p>
但是我们自己的network namespace是访问不到的
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns exec myns ping -c 1 192.168.61.12
PING 192.168.61.12 <span style="color: #076678;">(</span>192.168.61.12<span style="color: #076678;">)</span> 56<span style="color: #076678;">(</span>84<span style="color: #076678;">)</span> bytes of data.

--- 192.168.61.12 ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms
</pre>
</div></li>
<li>我们之前设置过,所有从容器出来的流量都要经过cni0,也就是10.85.0.1,cni0知道192.168.61.11,进而也可能知道
192.168.61.12. 但是流量是相互的192.168.61.12是不知道cni0的,那么reply的流量显然是无法到达我们创建
的network namespace的</li>
<li>所以ping不到192.168.61.12是合理的,那么我们下面就是要解决为什么busybox container能够访问到192.168.61.12了.
答案就是下一节要讲的masquerade</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgff0339b" class="outline-3">
<h3 id="orgff0339b"><span class="section-number-3">4.4.</span> Masquerade</h3>
<div class="outline-text-3" id="text-4-4">
<ul class="org-ul">
<li>Masquerade,也叫NAT(Network Address Translation),是网络上常用的技术.
<ul class="org-ul">
<li><p>
其核心目的就是:
</p>
<pre class="example" id="org19cd6ef">
一个以太网内的多个设备共享以太网出口路由的一个IP.这样可以让我们仅仅为一个路
由器赋一个万维网可解析的IP即可.而不需要为这个以太网内的所有设备赋一个万维网
可解析的地址
</pre></li>
<li>其为达到目的做了如下两件事(通过路由器):
<ol class="org-ol">
<li>在流量流出的时候更改sourceIP为路由器IP,以便对方reply的时候能够可寻址</li>
<li>在流量进入的时候,根据自己的记录,更改reply的detinationIP,以便数据能够到达以太网内的具体设备</li>
</ol></li>
</ul></li>
<li>busybox 容器是实现了Masquerade的,我们可以通过例子看出
<ul class="org-ul">
<li><p>
首先在busybox后台启动一个ping服务
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# crictl exec $<span style="color: #076678;">B1C_ID</span> ping 192.168.61.12 &gt; /dev/null 2&gt;&amp;1 &amp;
<span style="color: #076678;">[</span>1<span style="color: #076678;">]</span> 4956
</pre>
</div></li>
<li><p>
运行1秒的tcpdump,抓取icmp
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# timeout 1s tcpdump -i any -n icmp
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on any, link-type LINUX_SLL <span style="color: #076678;">(</span>Linux cooked v1<span style="color: #076678;">)</span>, capture size 262144 bytes
03:12:19.179635 IP 10.85.0.13 &gt; 192.168.61.12: ICMP echo request, id 12, seq 10, length 64
03:12:19.179635 IP 10.85.0.13 &gt; 192.168.61.12: ICMP echo request, id 12, seq 10, length 64
03:12:19.179662 IP 192.168.61.11 &gt; 192.168.61.12: ICMP echo request, id 12, seq 10, length 64
03:12:19.179970 IP 192.168.61.12 &gt; 192.168.61.11: ICMP echo reply, id 12, seq 10, length 64
03:12:19.179980 IP 192.168.61.12 &gt; 10.85.0.13: ICMP echo reply, id 12, seq 10, length 64
03:12:19.179984 IP 192.168.61.12 &gt; 10.85.0.13: ICMP echo reply, id 12, seq 10, length 64
</pre>
</div></li>
<li>当前busybox容器的ip是10.85.0.13, 最开始的:
<ol class="org-ol">
<li>source是10.85.0.13</li>
<li>destination是192.168.61.12</li>
</ol></li>
<li>经过masquerade之后:
<ol class="org-ol">
<li>source变成了192.168.61.11</li>
<li>destination还是192.168.61.12</li>
</ol></li>
<li>ICMP reply最开始:
<ol class="org-ol">
<li>source是192.168.61.12</li>
<li>destination是192.168.61.11</li>
</ol></li>
<li>ICMP经过masquerade之后
<ol class="org-ol">
<li>source还是是192.168.61.12</li>
<li>destination变成了10.85.0.13</li>
</ol></li>
</ul></li>
<li>我们自己的network namespace的目的就是达到busybox的效果,为此我们需要使用iptables命令,我们先用iptable
来看看其他容器是如何设置的
<ul class="org-ul">
<li><p>
我们先看看busybox等容器的iptable配置
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# iptables -t nat -n -L
Chain PREROUTING <span style="color: #076678;">(</span>policy ACCEPT<span style="color: #076678;">)</span>
target     prot opt source               destination

Chain INPUT <span style="color: #076678;">(</span>policy ACCEPT<span style="color: #076678;">)</span>
target     prot opt source               destination

Chain OUTPUT <span style="color: #076678;">(</span>policy ACCEPT<span style="color: #076678;">)</span>
target     prot opt source               destination

Chain POSTROUTING <span style="color: #076678;">(</span>policy ACCEPT<span style="color: #076678;">)</span>
target     prot opt source               destination
CNI-2f6196b6cf7f0a19d48ad251  all  --  10.85.0.8            0.0.0.0/0            /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"501c3d1ad87a09fc26cf9432be1861229225af3832c0ba2e274469c3201870d3"</span> */
CNI-430d2454234ecb5e4b41effb  all  --  10.85.0.9            0.0.0.0/0            /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"5e320641249e2570e91dfc7aa1fdfdfcfb8b87d8b7ecb762698fadcefbf278bf"</span> */
CNI-1270384b3b4eced2e7366c2b  all  --  10.85.0.10           0.0.0.0/0            /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"4d9c69c701ccfce2b4781a217dd23102acfca3dc9a249ca2aa732c10d46738ea"</span> */

Chain CNI-1270384b3b4eced2e7366c2b <span style="color: #076678;">(</span>1 references<span style="color: #076678;">)</span>
target     prot opt source               destination
ACCEPT     all  --  0.0.0.0/0            10.85.0.0/16         /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"4d9c69c701ccfce2b4781a217dd23102acfca3dc9a249ca2aa732c10d46738ea"</span> */
MASQUERADE  all  --  0.0.0.0/0           !224.0.0.0/4          /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"4d9c69c701ccfce2b4781a217dd23102acfca3dc9a249ca2aa732c10d46738ea"</span> */

Chain CNI-2f6196b6cf7f0a19d48ad251 <span style="color: #076678;">(</span>1 references<span style="color: #076678;">)</span>
target     prot opt source               destination
ACCEPT     all  --  0.0.0.0/0            10.85.0.0/16         /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"501c3d1ad87a09fc26cf9432be1861229225af3832c0ba2e274469c3201870d3"</span> */
MASQUERADE  all  --  0.0.0.0/0           !224.0.0.0/4          /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"501c3d1ad87a09fc26cf9432be1861229225af3832c0ba2e274469c3201870d3"</span> */

Chain CNI-430d2454234ecb5e4b41effb <span style="color: #076678;">(</span>1 references<span style="color: #076678;">)</span>
target     prot opt source               destination
ACCEPT     all  --  0.0.0.0/0            10.85.0.0/16         /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"5e320641249e2570e91dfc7aa1fdfdfcfb8b87d8b7ecb762698fadcefbf278bf"</span> */
MASQUERADE  all  --  0.0.0.0/0           !224.0.0.0/4          /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"5e320641249e2570e91dfc7aa1fdfdfcfb8b87d8b7ecb762698fadcefbf278bf"</span> */

Chain KUBE-MARK-MASQ <span style="color: #076678;">(</span>0 references<span style="color: #076678;">)</span>
target     prot opt source               destination
</pre>
</div></li>
<li>上例中有三个CNI-chain,分别对应三个pod,配置一致. 三个CNI-chain,都是为某个destination(都是容器的内部IP)服务的:
<ol class="org-ol">
<li>CNI-2f6196b6cf7f0a19d48ad251  为IP 10.85.0.8 服务</li>
<li>CNI-430d2454234ecb5e4b41effb  为IP 10.85.0.9 服务</li>
<li>CNI-1270384b3b4eced2e7366c2b  为IP 10.85.0.10 服务</li>
</ol></li>
<li>每个CNI-chain都是做了这么两件事情:
<ol class="org-ol">
<li>首先ACCEPT所有的local流量(也就是和机器在同一个以太网的流量,这个例子就是10.85.0.0/16)</li>
<li>除了local流量以外的所有地址(除了224.0.0.0/4,这个地址是广播地址,无法处理)都进行masquerade</li>
</ol></li>
</ul></li>
<li>我们为了能够让我们的network namespace能够达到上面的效果,我们要为10.85.254创建一个Chain POSTEROUTING和一个Chain CNI
<ul class="org-ul">
<li><p>
首先创建Chain CNI
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# iptables -t nat -N chain-myns
</pre>
</div></li>
<li><p>
然后模拟其他容器,让所有的内部流量都直接ACCEPT(不要进行masquerade)
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# iptables -t nat -A chain-myns -d 10.85.0.0/16 -j ACCEPT
</pre>
</div></li>
<li><p>
模拟其他容器,让非内部流量的所有其他流量(除了224.0.0.0/4以外),都被masquerade
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# iptables -t nat -A chain-myns ! -d 224.0.0.0/4 -j MASQUERADE
</pre>
</div></li>
<li><p>
设置好Chain CNI之后,设定这个Chain CNI为ip 10.85.0.254服务
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# iptables -t nat -A POSTROUTING -s 10.85.0.254 -j chain-myns
</pre>
</div></li>
<li><p>
所有都创建之后,我们确认下配置是写入成功的
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# iptables -t nat -n -L
Chain PREROUTING <span style="color: #076678;">(</span>policy ACCEPT<span style="color: #076678;">)</span>
target     prot opt source               destination

Chain INPUT <span style="color: #076678;">(</span>policy ACCEPT<span style="color: #076678;">)</span>
target     prot opt source               destination

Chain OUTPUT <span style="color: #076678;">(</span>policy ACCEPT<span style="color: #076678;">)</span>
target     prot opt source               destination

Chain POSTROUTING <span style="color: #076678;">(</span>policy ACCEPT<span style="color: #076678;">)</span>
target     prot opt source               destination
CNI-2f6196b6cf7f0a19d48ad251  all  --  10.85.0.8            0.0.0.0/0            /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"501c3d1ad87a09fc26cf9432be1861229225af3832c0ba2e274469c3201870d3"</span> */
CNI-430d2454234ecb5e4b41effb  all  --  10.85.0.9            0.0.0.0/0            /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"5e320641249e2570e91dfc7aa1fdfdfcfb8b87d8b7ecb762698fadcefbf278bf"</span> */
CNI-1270384b3b4eced2e7366c2b  all  --  10.85.0.10           0.0.0.0/0            /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"4d9c69c701ccfce2b4781a217dd23102acfca3dc9a249ca2aa732c10d46738ea"</span> */
chain-myns  all  --  10.85.0.254          0.0.0.0/0

Chain CNI-1270384b3b4eced2e7366c2b <span style="color: #076678;">(</span>1 references<span style="color: #076678;">)</span>
target     prot opt source               destination
ACCEPT     all  --  0.0.0.0/0            10.85.0.0/16         /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"4d9c69c701ccfce2b4781a217dd23102acfca3dc9a249ca2aa732c10d46738ea"</span> */
MASQUERADE  all  --  0.0.0.0/0           !224.0.0.0/4          /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"4d9c69c701ccfce2b4781a217dd23102acfca3dc9a249ca2aa732c10d46738ea"</span> */

Chain CNI-2f6196b6cf7f0a19d48ad251 <span style="color: #076678;">(</span>1 references<span style="color: #076678;">)</span>
target     prot opt source               destination
ACCEPT     all  --  0.0.0.0/0            10.85.0.0/16         /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"501c3d1ad87a09fc26cf9432be1861229225af3832c0ba2e274469c3201870d3"</span> */
MASQUERADE  all  --  0.0.0.0/0           !224.0.0.0/4          /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"501c3d1ad87a09fc26cf9432be1861229225af3832c0ba2e274469c3201870d3"</span> */

Chain CNI-430d2454234ecb5e4b41effb <span style="color: #076678;">(</span>1 references<span style="color: #076678;">)</span>
target     prot opt source               destination
ACCEPT     all  --  0.0.0.0/0            10.85.0.0/16         /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"5e320641249e2570e91dfc7aa1fdfdfcfb8b87d8b7ecb762698fadcefbf278bf"</span> */
MASQUERADE  all  --  0.0.0.0/0           !224.0.0.0/4          /* name: <span style="color: #79740e;">"crio"</span> id: <span style="color: #79740e;">"5e320641249e2570e91dfc7aa1fdfdfcfb8b87d8b7ecb762698fadcefbf278bf"</span> */

Chain KUBE-MARK-MASQ <span style="color: #076678;">(</span>0 references<span style="color: #076678;">)</span>
target     prot opt source               destination

Chain chain-myns <span style="color: #076678;">(</span>1 references<span style="color: #076678;">)</span>
target     prot opt source               destination
ACCEPT     all  --  0.0.0.0/0            10.85.0.0/16
MASQUERADE  all  --  0.0.0.0/0           !224.0.0.0/4
</pre>
</div></li>
<li><p>
确认配置写入成功后,我们从我们的network namespace去ping host2(192.168.61.12)这次是成功了
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# ip netns exec myns ping -c 1 192.168.61.12
PING 192.168.61.12 <span style="color: #076678;">(</span>192.168.61.12<span style="color: #076678;">)</span> 56<span style="color: #076678;">(</span>84<span style="color: #076678;">)</span> bytes of data.
64 bytes from 192.168.61.12: <span style="color: #076678;">icmp_seq</span>=1 <span style="color: #076678;">ttl</span>=63 <span style="color: #076678;">time</span>=0.559 ms

--- 192.168.61.12 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.559/0.559/0.559/0.000 ms
root@host01:/opt#
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgf83af51" class="outline-2">
<h2 id="orgf83af51"><span class="section-number-2">5.</span> Chapter 5</h2>
<div class="outline-text-2" id="text-5">
<ul class="org-ul">
<li>为了能够让process运行,我们需要存储process的依赖.而容器化的软件,其最大的优势就是能够把application和
其dependency一起打包</li>
<li>这些dependency包括:
<ul class="org-ul">
<li>shared library</li>
<li>configuration flie</li>
<li>log</li>
<li>进程需要操作的data</li>
</ul></li>
<li>所有的这些dependency和executable都要和其他的容器隔离,否则会影响到其他容器或者host</li>
<li>所有以上的对容器的要求,都会需要巨大的存储空间,这也意味着容器引擎要提供一些特性来更有效率的进行存储,
进而节省硬盘空间和网络带宽</li>
<li>本章我们就会看到容器镜像通过layered filesystem来达到更有效率的存储的目的.</li>
</ul>
</div>
<div id="outline-container-org889ea71" class="outline-3">
<h3 id="org889ea71"><span class="section-number-3">5.1.</span> Filesystem Isolation</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>在第二章,我们看到了如何通过chroot来创建一个只包含进程,已经进程所需要的依赖的文件夹系统,在这个文件系统
内部运行进程不会影响其他进程,而且所需要的文件是刚刚好的.</li>
<li>在chroot例子中,我们是在host上面创建隔离的文件系统的,但是这个在容器使用方面不是很合适,容器的做法,是
把一个isolated filesystem打包成container image</li>
</ul>
</div>
<div id="outline-container-org1fa9408" class="outline-4">
<h4 id="org1fa9408"><span class="section-number-4">5.1.1.</span> Container Image Contents</h4>
<div class="outline-text-4" id="text-5-1-1">
<ul class="org-ul">
<li>本节我们会看看NGINX 容器里面的内容,我们这里使用最常见的容器工具docker</li>
<li><p>
我们首先通过docker pull从一个image registry上面下载nginx image
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# docker pull nginx
Using default tag: latest
latest: Pulling from library/nginx
5b5fe70539cd: Pull complete
441a1b465367: Pull complete
3b9543f2b500: Pull complete
ca89ed5461a9: Pull complete
b0e1283145af: Pull complete
4b98867cde79: Pull complete
4a85ce26214d: Pull complete
Digest: sha256:593dac25b7733ffb7afe1a72649a43e574778bf025ad60514ef40f6b5d606247
Status: Downloaded newer image for nginx:latest
docker.io/library/nginx:latest
</pre>
</div></li>
<li>然后我们可以通过docker images命令来查看本机上的所有的image
<ul class="org-ul">
<li><p>
例子如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# docker images
REPOSITORY   TAG       IMAGE ID       CREATED        SIZE
nginx        latest    eb4a57159180   12 days ago    187MB
</pre>
</div></li>
<li>我们可以看到SIZE来表示这个容器的大小是187MB</li>
<li>IMAGE ID是一个唯一标识容器的UUID,这个ID每个版本都会有所不同</li>
<li>这个image的size不是仅仅nginx的二进制和其依赖库的大小,还包括了一个小的linux发行版(这里是Debian)</li>
</ul></li>
<li>我们通过下面的例子来看看Debian内部的情况
<ul class="org-ul">
<li><p>
首先需要启动一个容器
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# docker run --name nginx -d nginx
ec8c798c45e8043f69612604a98d89c8b21dade272a6e7cdde5cfbf44d9bd8dc
root@host01:~# docker ps
CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS        PORTS     NAMES
ec8c798c45e8   nginx     <span style="color: #79740e;">"/docker-entrypoint.&#8230;"</span>   2 seconds ago   Up 1 second   80/tcp    nginx
</pre>
</div></li>
<li>docker run里面的参数其中&#x2013;name用来给container一个友好的名字 -d表示让容器在后台运行</li>
<li><p>
然后我们就可以进入到容器内部查看容器的情况
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# docker exec -ti nginx /bin/bash
root@ec8c798c45e8:/# ldd $<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">which nginx</span><span style="color: #076678;">)</span>
        linux-vdso.so.1 <span style="color: #076678;">(</span>0x00007ffd7d5a4000<span style="color: #076678;">)</span>
        libcrypt.so.1 =&gt; /lib/x86_64-linux-gnu/libcrypt.so.1 <span style="color: #076678;">(</span>0x00007f84928f6000<span style="color: #076678;">)</span>
        libpcre2-8.so.0 =&gt; /lib/x86_64-linux-gnu/libpcre2-8.so.0 <span style="color: #076678;">(</span>0x00007f849285c000<span style="color: #076678;">)</span>
        libssl.so.3 =&gt; /lib/x86_64-linux-gnu/libssl.so.3 <span style="color: #076678;">(</span>0x00007f84927b3000<span style="color: #076678;">)</span>
        libcrypto.so.3 =&gt; /lib/x86_64-linux-gnu/libcrypto.so.3 <span style="color: #076678;">(</span>0x00007f8492332000<span style="color: #076678;">)</span>
        libz.so.1 =&gt; /lib/x86_64-linux-gnu/libz.so.1 <span style="color: #076678;">(</span>0x00007f8492313000<span style="color: #076678;">)</span>
        libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 <span style="color: #076678;">(</span>0x00007f8492130000<span style="color: #076678;">)</span>
        /lib64/ld-linux-x86-64.so.2 <span style="color: #076678;">(</span>0x00007f8492b6e000<span style="color: #076678;">)</span>
</pre>
</div></li>
<li>我们可以看到,所有的nginx以来的库,都是我们容器的一部分,所以nginx可以不依赖host上的任何资源就能成功运行</li>
<li><p>
我们再运行一下会发现,不仅仅是nginx运行所必须的文件会包含在image里面,nginx运行不需要的也被引入了
image,比如systemd相关文件(容器完全不能运行systemd)
</p>
<div class="org-src-container">
<pre class="src src-shell">root@ec8c798c45e8:/# ls -l /etc
total 308
-rw-r--r-- 1 root root    3040 May 25 15:54 adduser.conf
drwxr-xr-x 2 root root    4096 Jun 12 00:00 alternatives
drwxr-xr-x 1 root root    4096 Jun 12 00:00 apt
-rw-r--r-- 1 root root    1994 Apr 23 21:23 bash.bashrc
-rw-r--r-- 1 root root     367 Sep 22  2022 bindresvport.blacklist
drwxr-xr-x 3 root root    4096 Jun 14 07:16 ca-certificates
-rw-r--r-- 1 root root    5989 Jun 14 07:16 ca-certificates.conf
drwxr-xr-x 2 root root    4096 Jun 12 00:00 cron.d
drwxr-xr-x 2 root root    4096 Jun 12 00:00 cron.daily
-rw-r--r-- 1 root root    2969 Jan  8 21:50 debconf.conf
-rw-r--r-- 1 root root       5 Mar  2 09:00 debian_version
drwxr-xr-x 1 root root    4096 Jun 14 07:16 default
-rw-r--r-- 1 root root    1706 May 25 15:54 deluser.conf
drwxr-xr-x 4 root root    4096 Jun 12 00:00 dpkg
...
drwxr-xr-x 1 root root    4096 Sep 18  2022 systemd
...
</pre>
</div></li>
<li>之所以同时包含这些用不到的文件,基于如下两个原因:
<ol class="org-ol">
<li>很多人进程源代码在编译的时候,是assume某些文件必须在的</li>
<li>这些额外的文件(比如systemd)是linux distrubution基础镜像就带着的,而我们从基础镜像编译自己的镜像很明显更容易</li>
</ol></li>
<li><p>
运行着的容器里面的文件系统也是可写的,我们可以试着用dd命令写入10MB的文件到/tmp文件夹
</p>
<div class="org-src-container">
<pre class="src src-shell">root@ec8c798c45e8:/# dd <span style="color: #076678;">if</span>=/dev/urandom <span style="color: #076678;">of</span>=/tmp/data <span style="color: #076678;">bs</span>=1M <span style="color: #076678;">count</span>=10
10+0 records<span style="color: #9d0006;"> in</span>
10+0 records out
10485760 bytes <span style="color: #076678;">(</span>10 MB, 10 MiB<span style="color: #076678;">)</span> copied, 0.0355285 s, 295 MB/s
root@ec8c798c45e8:/# ls -alh /tmp/data
-rw-r--r-- 1 root root 10M Jun 27 01:35 /tmp/data
root@ec8c798c45e8:/# exit
</pre>
</div></li>
<li><p>
从容器内部退出后,我们可以使用docker inspect命令来查看当前容器的大小
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# docker inspect -s nginx | jq <span style="color: #79740e;">'.[0].SizeRw'</span>
10486955
</pre>
</div></li>
<li>这里的-s告诉docker inspect要获取容器size</li>
<li>docker inspect返回的json很长,我们使用jq来进行选取相应内容</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga13d0f2" class="outline-4">
<h4 id="orga13d0f2"><span class="section-number-4">5.1.2.</span> Image Versions and Layers</h4>
<div class="outline-text-4" id="text-5-1-2">
<ul class="org-ul">
<li>docker还可以通过tag来区分不同版本的redis,我们来看一个两个版本redis的例子:
<ul class="org-ul">
<li><p>
首先通过docker pull命令来下载两个不同版本的redis镜像
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# docker pull redis:6.0.13-alpine
6.0.13-alpine: Pulling from library/redis
540db60ca938: Pull complete
29712d301e8c: Pull complete
8173c12df40f: Pull complete
2e184ab6a377: Pull complete
57f8e2c13cfc: Pull complete
63122aa9c3e3: Pull complete
Digest: sha256:43f89b3cdf9bc609f32dd62662422cb97b3cffc2a5bf7b29afe3a54c283ab02f
Status: Downloaded newer image for redis:6.0.13-alpine
docker.io/library/redis:6.0.13-alpine
root@host01:~# docker pull redis:6.2.3-alpine
6.2.3-alpine: Pulling from library/redis
540db60ca938: Already exists
29712d301e8c: Already exists
8173c12df40f: Already exists
a77b7ddf4978: Pull complete
3f34a000c6b3: Pull complete
275dfaedaf41: Pull complete
Digest: sha256:f8f0e809a4281714c33edf86f6da6cc2d4058c8549e44d8c83303c28b3123072
Status: Downloaded newer image for redis:6.2.3-alpine
docker.io/library/redis:6.2.3-alpine
</pre>
</div></li>
<li>冒号后面的字符,就是image tag,在容器里面扮演着版本的作用.</li>
<li>如果冒号后面没有image tag,那么,就是默认的image tag,叫做 latest</li>
<li>使用image tag的好处,在于即便是新的版本出现了,我们还是可以坚持使用老版本的redis</li>
<li>image tag不像版本那么的死板,可以包含任意字符,所以可以有很多额外的信息,比如这里的iamge tag最后的-alpine
就表示这个image是基于Alpine Linux的</li>
<li><p>
另外一个有趣的现象就是,我们下载第二个redis的时候,很多地方显示了Already exists,如下
</p>
<pre class="example" id="org8437178">
540db60ca938: Already exists
29712d301e8c: Already exists
8173c12df40f: Already exists
</pre></li>
<li><p>
这些uuid也存在于第一个redis的下载列表中
</p>
<pre class="example" id="orgea54cee">
540db60ca938: Pull complete
29712d301e8c: Pull complete
8173c12df40f: Pull complete
</pre></li>
<li><p>
这每一个uuid在image里面叫做一个layer,如果两个image共用一个layer,那么我们可以只下次一次,并且只存储
一份,我们可以通过查看容器的共享容量来确认这一点
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# docker images | grep redis
redis        6.0.13-alpine   a556c77d3dce   2 years ago    31.3MB
redis        6.2.3-alpine    efb4fa30f1cf   2 years ago    32.3MB
root@host01:~# docker system df -v
Images space usage:

REPOSITORY   TAG             IMAGE ID       CREATED        SIZE      SHARED SIZE   UNIQUE SIZE   CONTAINERS
redis        6.0.13-alpine   a556c77d3dce   2 years ago    31.33MB   6.905MB       24.42MB       0
redis        6.2.3-alpine    efb4fa30f1cf   2 years ago    32.31MB   6.905MB       25.4MB        0
</pre>
</div></li>
<li><p>
不同版本的redis可以同时运行相互不影响
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# docker run -d --name redis1 redis:6.0.13-alpine
e1efb9c0433767dc20c6311669cf917c541bc0b930446fa71b7f8c6d8d69e274
root@host01:~# docker run -d --name redis2 redis:6.2.3-alpine
f23a2dac3496d402dda2b2b36ed7c96f182e41ba3397981ece2c1ffac093fe6b
root@host01:~# docker logs redis1 | grep version
1:C 27 Jun 2023 01:42:29.293 <span style="color: #a89984;"># </span><span style="color: #a89984;">Redis version=6.0.13, bits=64, commit=00000000, modified=0, pid=1, just started</span>
root@host01:~# docker logs redis2 | grep version
1:C 27 Jun 2023 01:42:36.014 <span style="color: #a89984;"># </span><span style="color: #a89984;">Redis version=6.2.3, bits=64, commit=00000000, modified=0, pid=1, just started</span>
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgadcea1e" class="outline-3">
<h3 id="orgadcea1e"><span class="section-number-3">5.2.</span> Building Container Images</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>前面的例子我们看到,两个不同版本redis的容器镜像共享了layer. 其实任意两个容器镜像都可以共享layer,并不需要是同一种软件(redis)的容器</li>
<li>每个容器镜像都是从base image(一个已经build好的image)来的. 每当在base image里面运行一个build step,就会创建一个layer, 这个layer只包含这个build step引入的文件改变</li>
<li>base image是一个已经build好的image, 也会从其他base image开始build,最终会落到一个最原始的base image,一般是不同的Linux发行商提供的</li>
</ul>
</div>
<div id="outline-container-orgb7b19e6" class="outline-4">
<h4 id="orgb7b19e6"><span class="section-number-4">5.2.1.</span> Using a Docerfile</h4>
<div class="outline-text-4" id="text-5-2-1">
<ul class="org-ul">
<li>最常见的创建image的方法是使用Dockerfile
<ul class="org-ul">
<li><p>
下面就是一个Dockerfile的例子
</p>
<div class="org-src-container">
<pre class="src src-dockerfile"><span style="color: #9d0006;">FROM</span> <span style="color: #8f3f71; font-weight: bold;">nginx</span>

<span style="color: #a89984;"># </span><span style="color: #a89984;">Add index.html</span>
<span style="color: #9d0006;">RUN</span> echo <span style="color: #79740e;">"&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hello World!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;"</span> <span style="color: #79740e;">\</span>
    &gt;/usr/share/nginx/html/index.html
</pre>
</div></li>
<li>FROM: 用来指示这次build的base image</li>
<li>RUN: 在容器内部运行一个命令</li>
<li>COPY: 把文件拷贝进容器</li>
<li>ENV: 指定一个环境变量</li>
<li>ENTRYPOINT: 配置容器的启动进程</li>
<li>CMD: 设置启动进程的默认参数</li>
</ul></li>
<li>我们使用docker build来把Dockerfile变成一个image
<ul class="org-ul">
<li><p>
例子如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# cd /opt/hello/
root@host01:/opt/hello# docker build -t hello .
Sending build context to Docker daemon  2.048kB
Step 1/2 : FROM nginx
 ---&gt; eb4a57159180
Step 2/2 : RUN echo <span style="color: #79740e;">"&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hello World!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;"</span>     &gt;/usr/share/nginx/html/index.html
 ---&gt; Running<span style="color: #9d0006;"> in</span> 27a6e2113e3d
Removing intermediate container 27a6e2113e3d
 ---&gt; f5d37eccfe26
Successfully built f5d37eccfe26
Successfully tagged hello:latest
</pre>
</div></li>
<li>`-t` 参数是给新的image取名字的意思,这里没有给image tag,默认使用latest,所以最后的名字是hello:latest</li>
<li>我们从上面编译的文字说明可以看到image编译的步骤:
<ol class="org-ol">
<li>Docker会把build context传送给docker daemon(所以具体的操作都是在docker daemon里面完成的).所谓
docker context,就是一个文件夹以及这个文件夹下的所有子文件和子文件夹(在这里例子里面就是当前文
件夹"."), 注意我们COPY命令所有可达的数据只有build context的内容(COPY ..这种操作是不会成功的).</li>
<li>Docker会识别出我们的base image是nginx</li>
<li>docker执行RUN指示的命令,这些命令其实都是在nginx base image里面运行的,只有这个image里面有的命令,才能执行</li>
</ol></li>
<li><p>
编译出来的image可以像其他image一样运行,这里我们可以看到我们成功使用curl命令确认了容器成功运行
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt/hello# docker images | grep hello
hello        latest          f5d37eccfe26   50 seconds ago   187MB
root@host01:/opt/hello# docker run -d -p 8080:80 hello
597ef1a2dedb5b5771c2b0ffa54a4e0e095b7f830dd1938c74f5461f00f9d125
root@host01:/opt/hello# curl http://localhost:8080/
&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hello World!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org22d1244" class="outline-4">
<h4 id="org22d1244"><span class="section-number-4">5.2.2.</span> Tagging and Publishing Images</h4>
<div class="outline-text-4" id="text-5-2-2">
<ul class="org-ul">
<li>image的build的介绍就结束了,下面介绍发布步骤</li>
<li>为了能够让image可以发布,我们必须给它一个全网唯一的名字,这就会在我们原来名字的基础上加上两个部分:
<ol class="org-ol">
<li>full registry host</li>
<li>full registry path</li>
</ol></li>
<li>比如我们举个例子, quay.io/quay/busybox:latest, 其中:
<ul class="org-ul">
<li>quay.io/就是full registry host</li>
<li>quay/就是full registry path</li>
<li>busybox:lastest 就是容器名加容器tag</li>
</ul></li>
<li><p>
上面我们介绍的quay.io下载下来的busybox和我们默认host和path(其实就是docker.io/library)下载下来的容器是不一样的
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt/hello# docker pull quay.io/quay/busybox
Using default tag: latest
latest: Pulling from quay/busybox
9c075fe2c773: Pull complete
ee780d08a5b4: Pull complete
Digest: sha256:92f3298bf80a1ba949140d77987f5de081f010337880cd771f7e7fc928f8c74d
Status: Downloaded newer image for quay.io/quay/busybox:latest
quay.io/quay/busybox:latest
root@host01:/opt/hello# docker pull busybox
Using default tag: latest
latest: Pulling from library/busybox
71d064a1ac7d: Pull complete
Digest: sha256:6e494387c901caf429c1bf77bd92fb82b33a68c0e19f6d1aa6a3ac8d27a7049d
Status: Downloaded newer image for busybox:latest
docker.io/library/busybox:latest
root@host01:/opt/hello# docker images | grep busybox
busybox                latest          b539af69bc01   2 weeks ago     4.86MB
quay.io/quay/busybox   latest          e3121c769e39   2 years ago     1.22MB
</pre>
</div></li>
<li><p>
所以我们的hello:latest如果我们不改名的话,其实就是docker.io/library/hello:latest, 但是显然我们是没有
权限push到这个地址的,所以我们要改名,我们本地刚好起了一个registry.local的registry server,于是就有
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt/hello# docker start registry
registry
root@host01:/opt/hello# docker ps
CONTAINER ID   IMAGE                 COMMAND                  CREATED          STATUS          PORTS                                             NAMES
597ef1a2dedb   hello                 <span style="color: #79740e;">"/docker-entrypoint.&#8230;"</span>   3 minutes ago    Up 3 minutes    0.0.0.0:8080-&gt;80/tcp, :::8080-&gt;80/tcp             priceless_visvesvaraya
f23a2dac3496   redis:6.2.3-alpine    <span style="color: #79740e;">"docker-entrypoint.s&#8230;"</span>   13 minutes ago   Up 13 minutes   6379/tcp                                          redis2
e1efb9c04337   redis:6.0.13-alpine   <span style="color: #79740e;">"docker-entrypoint.s&#8230;"</span>   13 minutes ago   Up 13 minutes   6379/tcp                                          redis1
ec8c798c45e8   nginx                 <span style="color: #79740e;">"/docker-entrypoint.&#8230;"</span>   22 minutes ago   Up 22 minutes   80/tcp                                            nginx
1ba1abb1546e   registry:2            <span style="color: #79740e;">"/entrypoint.sh /etc&#8230;"</span>   6 months ago     Up 1 second     0.0.0.0:443-&gt;443/tcp, :::443-&gt;443/tcp, 5000/tcp   registry
root@host01:/opt/hello# docker tag hello registry.local/hello
root@host01:/opt/hello# docker push registry.local/hello
Using default tag: latest
The push refers to repository <span style="color: #076678;">[</span>registry.local/hello<span style="color: #076678;">]</span>
ca1db7cb22af: Pushed
9e96226c58e7: Pushed
12a568acc014: Pushed
7757099e19d2: Pushed
bf8b62fb2f13: Pushed
4ca29ffc4a01: Pushed
a83110139647: Pushed
ac4d164fef90: Pushed
latest: digest: sha256:bfc98b9c04b13a46310d0a0f729be0a8548958f8e829bc446985041aa2bcc252 size: 1985
</pre>
</div></li>
<li>由于我们的registry.local开始是空的,所以所有layer都没有,第一次所有layer都得全部上传,但是我们再tag好
包含已有layer的image再传的时候,已有layer就不需要再传了</li>
<li><p>
也不是只有自己build的可以retag, 非自己build的也能retag并且上传,比如我们的官方busybox,可以被我们retag并上传
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt/hello# docker tag busybox registry.local/busybox
root@host01:/opt/hello# docker push registry.local/busybox
Using default tag: latest
The push refers to repository <span style="color: #076678;">[</span>registry.local/busybox<span style="color: #076678;">]</span>
0b7d464440dc: Pushed
latest: digest: sha256:1b0a26bd07a3d17473d8d8468bea84015e27f87124b283b91d781bce13f61370 size: 528
</pre>
</div></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgedd03ec" class="outline-3">
<h3 id="orgedd03ec"><span class="section-number-3">5.3.</span> Image and Container Storage</h3>
<div class="outline-text-3" id="text-5-3">
</div>
<div id="outline-container-org889af61" class="outline-4">
<h4 id="org889af61"><span class="section-number-4">5.3.1.</span> Overlay Filesystems</h4>
<div class="outline-text-4" id="text-5-3-1">
<ul class="org-ul">
<li>当我们运行容器的时候,我们在容器内部看到的文件系统是一体的,并没有区分出不同的layer,这是拜overlay
filesystem所赐</li>
<li>一个overlay filesystem有三个部分:
<ul class="org-ul">
<li>lower directory, 这里存储着base layer(可能有很多个lower directory)</li>
<li>upper directory, 这里存储着overlay layer(所谓overlay layer,就是从lower directory拷贝过来改动的layer)</li>
<li>mount directory, 这里是lower directory和upper directory组合起来以后的样子,这个layer是展示给用户的</li>
</ul></li>
<li>我们的overlay filesystem的原理就是,所有对mount directory的改动,其实都是改动的upper directory.</li>
<li><p>
而upper directory是把lower directory的东西先拷贝再改动,俗称Copy On Write, 这样做的好处是: lower directory
的东西从来不变,可以和其他容器分享.
</p>
<pre class="example" id="org3dc821b">
Multiple users can share the lower directory without conflict because it is only read from, never written to.
</pre></li>
<li>overlay filesystem不仅仅对容器有用,其实我们的路由器就是用的overlay filesystem: 不变的部分被写入到firmware,
后续的所有改动,都在upper directory,如果一旦出现问题需要回滚到最初的已知的状态(firmware的状态),那么就
reset:清空upper directory,回到最初的firmware状态</li>
<li>overlay filesystem是linux 内核提供的功能,性能很高,我们下面来看看一个overlay filesystem的例子
<ul class="org-ul">
<li><p>
首先我们创建四个文件夹(其中三个我们会涉及到, work文件夹是overlay filesystem用来做临时文件用的),并
且在其中的lower和upper创建两个文件
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/tmp# cd
root@host01:~# mkdir /tmp/<span style="color: #076678;">{</span>lower,upper,work,mount<span style="color: #076678;">}</span>
root@host01:~# echo <span style="color: #79740e;">"hello1"</span> &gt; /tmp/lower/hello1
root@host01:~# echo <span style="color: #79740e;">"hello2"</span> &gt; /tmp/upper/hello2
</pre>
</div></li>
<li><p>
然后我们通过mount命令来创建overlay filesystem
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# mount -t overlay -o rw,<span style="color: #076678;">lowerdir</span>=/tmp/lower,<span style="color: #076678;">upperdir</span>=/tmp/upper,<span style="color: #076678;">workdir</span>=/tmp/work overlay /tmp/mount
</pre>
</div></li>
<li><p>
当前/tmp/mount文件夹就是merged 内容
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ls -l /tmp/mount
total 8
-rw-r--r-- 1 root root 7 Jul  1 09:15 hello1
-rw-r--r-- 1 root root 7 Jul  1 09:15 hello2
root@host01:~# cat /tmp/mount/hello1
hello1
root@host01:~# cat /tmp/mount/hello2
hello2
</pre>
</div></li>
<li><p>
所有对mount location的改动,都会在upper directory里面提现,但是lower一直保持不变
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# echo <span style="color: #79740e;">"hello3"</span> &gt; /tmp/mount/hello3
root@host01:~# ls -l /tmp/mount
total 12
-rw-r--r-- 1 root root 7 Jul  1 09:15 hello1
-rw-r--r-- 1 root root 7 Jul  1 09:15 hello2
-rw-r--r-- 1 root root 7 Jul  1 09:16 hello3
root@host01:~# ls -l /tmp/lower
total 4
-rw-r--r-- 1 root root 7 Jul  1 09:15 hello1
root@host01:~# ls -l /tmp/upper
total 8
-rw-r--r-- 1 root root 7 Jul  1 09:15 hello2
-rw-r--r-- 1 root root 7 Jul  1 09:16 hello3
</pre>
</div></li>
<li><p>
即便是我们删除文件,也不会影响lower directory
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# rm /tmp/mount/hello1
root@host01:~# ls -l /tmp/mount
total 8
-rw-r--r-- 1 root root 7 Jul  1 09:15 hello2
-rw-r--r-- 1 root root 7 Jul  1 09:16 hello3
root@host01:~# ls -l /tmp/lower
total 4
-rw-r--r-- 1 root root 7 Jul  1 09:15 hello1
root@host01:~# ls -l /tmp/upper
total 8
c--------- 1 root root 0, 0 Jul  1 09:17 hello1
-rw-r--r-- 1 root root    7 Jul  1 09:15 hello2
-rw-r--r-- 1 root root    7 Jul  1 09:16 hello3
root@host01:~#
</pre>
</div></li>
<li>注意,上面的c是character special file的意思,linux为了不让这个文件展示在mount directory里面,给他设
置了一个标志,意思是不要merge这个文件到mount directory了</li>
<li><p>
由于可以重用lower directory,我们可以使用如下命令来创建新的mount2 ,但是重用lower.我们会发现lower
文件夹下的hello1还是会出现(因为一直都是只读,从来没有动过lower里面的hello1)
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# mkdir /tmp/<span style="color: #076678;">{</span>upper2,work2,mount2<span style="color: #076678;">}</span>
root@host01:~# mount -t overlay -o rw,<span style="color: #076678;">lowerdir</span>=/tmp/lower,<span style="color: #076678;">upperdir</span>=/tmp/upper2,<span style="color: #076678;">workdir</span>=/tmp/work2 overlay /tmp/mount2
root@host01:~# ls -l /tmp/mount2
total 4
-rw-r--r-- 1 root root 7 Jul  1 09:15 hello1
root@host01:~#
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orge44fdb2" class="outline-4">
<h4 id="orge44fdb2"><span class="section-number-4">5.3.2.</span> Understanding Container Layers</h4>
<div class="outline-text-4" id="text-5-3-2">
<ul class="org-ul">
<li>理解了overlay filesystem,我们可以探索下我们运行中的nginx容器的filesystem
<ul class="org-ul">
<li><p>
首先查找正在运行的nginx的filesystem
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# <span style="color: #076678;">ROOT</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">docker inspect nginx | jq -r '.[0].GraphDriver.Data.MergedDir'</span><span style="color: #076678;">)</span>
root@host01:~# echo $<span style="color: #076678;">ROOT</span>
/var/lib/docker/overlay2/e7966cd9fe73d8ebcee4bc1069d5eea6496d968e951781bdd9600427d9153b1d/merged
</pre>
</div></li>
<li>我们还是使用jq来选择我们希望查找我们想要的field,这里查找到的是最终mount point的位置,在容器里面叫merged</li>
<li><p>
我们再来看看这个mount point对应的lowerdir, upperdir, workdir地址
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# mount | grep $<span style="color: #076678;">ROOT</span> | tr <span style="color: #076678;">[</span>:,<span style="color: #076678;">]</span> <span style="color: #79740e;">'\n'</span>
overlay on /var/lib/docker/overlay2/e7966cd9fe73d8ebcee4bc1069d5eea6496d968e951781bdd9600427d9153b1d/merged type overlay <span style="color: #076678;">(</span>rw
relatime
<span style="color: #076678;">lowerdir</span>=/var/lib/docker/overlay2/l/BBJDCJ5XARDOSESCMCL6X2QDGD
/var/lib/docker/overlay2/l/CAZCAOGD7UFIT2IFX5AWSV5CIU
/var/lib/docker/overlay2/l/GZX2CMXOG6VFMCUB3UACD5KZLM
/var/lib/docker/overlay2/l/2KIUAYH3TMRBF7NQNDVQZPPEOI
/var/lib/docker/overlay2/l/GW6ABLHCXLFHDRZYWQ3EIMQ6G6
/var/lib/docker/overlay2/l/DDIJ2BJFQTG24FHAEK3VIVY4W2
/var/lib/docker/overlay2/l/VWZXOYJQ3F26BMTDVBQXTQ6B5W
/var/lib/docker/overlay2/l/S54SMZR7FJK4SVY6MQJLCQ3XRC
<span style="color: #076678;">upperdir</span>=/var/lib/docker/overlay2/e7966cd9fe73d8ebcee4bc1069d5eea6496d968e951781bdd9600427d9153b1d/diff
<span style="color: #076678;">workdir</span>=/var/lib/docker/overlay2/e7966cd9fe73d8ebcee4bc1069d5eea6496d968e951781bdd9600427d9153b1d/work
<span style="color: #076678;">xino</span>=off<span style="color: #076678;">)</span>
</pre>
</div></li>
<li>注意,上面的tr命令,是把分号和逗号都变成新的一行来使得多个lowerdir更容易查看</li>
<li><p>
我们之前创建了一个10MB的data文件,这个文件显示在mountdir,其实是存储在upperdir(也就是上面
的/var/lib/docker/overlay2/e7966cd9fe73d8ebcee4bc1069d5eea6496d968e951781bdd9600427d9153b1d/diff)
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ls -l $<span style="color: #076678;">ROOT</span>/tmp/data
-rw-r--r-- 1 root root 10485760 Jun 27 01:35 /var/lib/docker/overlay2/e7966cd9fe73d8ebcee4bc1069d5eea6496d968e951781bdd9600427d9153b1d/merged/tmp/data
root@host01:~# ls -l $<span style="color: #076678;">ROOT</span>/../diff/tmp/data
-rw-r--r-- 1 root root 10485760 Jun 27 01:35 /var/lib/docker/overlay2/e7966cd9fe73d8ebcee4bc1069d5eea6496d968e951781bdd9600427d9153b1d/merged/../diff/tmp/data
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org19bbb09" class="outline-4">
<h4 id="org19bbb09"><span class="section-number-4">5.3.3.</span> Practical Image Building Advice</h4>
<div class="outline-text-4" id="text-5-3-3">
<ul class="org-ul">
<li>由于容器使用了overlay filesystem,我们在build image的时候,就有如下的最佳实践:
<ul class="org-ul">
<li>由于overlay filesystem有多个lower directory(但是整个的merge过程性能损失极小), 所以我们可以模块化
的创建image: 在一个base image的基础上,安装不同的软件,获得不同的新的image</li>
<li>由于在upper layer删除存在于lower layer的文件其实并没有真的删除(因为要保证lower layer文件的只读性)
所以我们在创建lower layer(每个Dockerfile的RUN命令都会创建一个layer)的时候要非常小心:
<ol class="org-ol">
<li>如果RUN(COPY)创建了大的临时文件,必须在RUN(COPY)这一行就删除,否则RUN(COPY)这一行就已经建立了一个
lowerdir,后续的RUN即便是删除也不会改变临时文件已经占据一层lowdir的事实</li>
<li>不要使用COPY(ENV)把机密的文件或者参数传入image,如果不再COPY(ENV)这一行删除,那么这个机密文件(参
数)就会变成image的一个lowerdir</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orged586ee" class="outline-3">
<h3 id="orged586ee"><span class="section-number-3">5.4.</span> Open Container Initiative</h3>
<div class="outline-text-3" id="text-5-4">
<ul class="org-ul">
<li>container image不仅仅包括之前讲的overlay filesystem,它还包括一些metadata,比如:
<ul class="org-ul">
<li>初始化运行的命令(CMD)</li>
<li>程序所需要的环境变量(ENV)</li>
</ul></li>
<li><p>
这些metadata的信息存储有一个固定的格式,就是OCI format. skopeo命令就是能把内置的docker image转换为
OCI format文件夹
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# skopeo copy docker-daemon:busybox:latest oci:busybox:latest
Getting image source signatures
Copying blob 0b7d464440dc done
Copying config 75e92172e5 done
Writing manifest to image destination
Storing signatures
</pre>
</div></li>
<li><p>
OCI format 文件夹内容如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ls -l busybox
total 12
drwxr-xr-x 3 root root 4096 Jul  1 16:08 blobs
-rw-r--r-- 1 root root  247 Jul  1 16:08 index.json
-rw-r--r-- 1 root root   31 Jul  1 16:08 oci-layout
</pre>
</div></li>
<li><p>
oci-layout显示了image使用的OCI version
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# jq . busybox/oci-layout
<span style="color: #076678;">{</span>
  <span style="color: #79740e;">"imageLayoutVersion"</span>: <span style="color: #79740e;">"1.0.0"</span>
<span style="color: #076678;">}</span>
</pre>
</div></li>
<li><p>
index.json介绍了image的具体信息
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# jq . busybox/index.json
<span style="color: #076678;">{</span>
  <span style="color: #79740e;">"schemaVersion"</span>: 2,
  <span style="color: #79740e;">"manifests"</span>: <span style="color: #b16286;">[</span>
    <span style="color: #8ec07c;">{</span>
      <span style="color: #79740e;">"mediaType"</span>: <span style="color: #79740e;">"application/vnd.oci.image.manifest.v1+json"</span>,
      <span style="color: #79740e;">"digest"</span>: <span style="color: #79740e;">"sha256:2b71f601f021f9f0e1c57cf7ee13eee44c39b1595ca00a08180549e4c387ead0"</span>,
      <span style="color: #79740e;">"size"</span>: 348,
      <span style="color: #79740e;">"annotations"</span>: <span style="color: #d65d0e;">{</span>
        <span style="color: #79740e;">"org.opencontainers.image.ref.name"</span>: <span style="color: #79740e;">"latest"</span>
      <span style="color: #d65d0e;">}</span>
    <span style="color: #8ec07c;">}</span>
  <span style="color: #b16286;">]</span>
<span style="color: #076678;">}</span>
</pre>
</div></li>
<li>实际的文件是以每个layer为一个tar文件,存储在blob文件夹里面的,我们的busybox只有一个layer,也就只有一个
tar文件.我们下面就来查看下这个tar文件
<ul class="org-ul">
<li><p>
index.json里面的sha256文件,其实是整个image的manifest文件,我们先找到这个文件
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# <span style="color: #076678;">MANIFEST</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">jq -r .manifests</span><span style="color: #b16286; font-weight: bold;">[</span><span style="color: #076678; font-weight: bold;">0</span><span style="color: #b16286; font-weight: bold;">]</span><span style="color: #076678; font-weight: bold;">.digest busybox/index.json | sed -e 's/sha256://'</span><span style="color: #076678;">)</span>
root@host01:~# echo $<span style="color: #076678;">MANIFEST</span>
2b71f601f021f9f0e1c57cf7ee13eee44c39b1595ca00a08180549e4c387ead0
root@host01:~# jq . busybox/blobs/sha256/$<span style="color: #076678;">MANIFEST</span>
<span style="color: #076678;">{</span>
  <span style="color: #79740e;">"schemaVersion"</span>: 2,
  <span style="color: #79740e;">"config"</span>: <span style="color: #b16286;">{</span>
    <span style="color: #79740e;">"mediaType"</span>: <span style="color: #79740e;">"application/vnd.oci.image.config.v1+json"</span>,
    <span style="color: #79740e;">"digest"</span>: <span style="color: #79740e;">"sha256:75e92172e5cd78fe6f944333403b5709eaccbec868c455f2f0913c66aa272e02"</span>,
    <span style="color: #79740e;">"size"</span>: 575
  <span style="color: #b16286;">}</span>,
  <span style="color: #79740e;">"layers"</span>: <span style="color: #b16286;">[</span>
    <span style="color: #8ec07c;">{</span>
      <span style="color: #79740e;">"mediaType"</span>: <span style="color: #79740e;">"application/vnd.oci.image.layer.v1.tar+gzip"</span>,
      <span style="color: #79740e;">"digest"</span>: <span style="color: #79740e;">"sha256:4262504e0aaf2e52231efa124ad2062f3bbbf601ad311971ff2f91bf76fdebbd"</span>,
      <span style="color: #79740e;">"size"</span>: 2657131
    <span style="color: #8ec07c;">}</span>
  <span style="color: #b16286;">]</span>
<span style="color: #076678;">}</span>
</pre>
</div></li>
<li>我们可以看到,这个文件其实是一个metadata json文件,这个metadata文件里面两个sha256文件:
<ol class="org-ol">
<li><p>
第一个sha256是容器的命令和环境变量等信息
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# jq . busybox/blobs/sha256/75e92172e5cd78fe6f944333403b5709eaccbec868c455f2f0913c66aa272e02
<span style="color: #076678;">{</span>
  <span style="color: #79740e;">"created"</span>: <span style="color: #79740e;">"2023-06-10T00:19:54.795108463Z"</span>,
  <span style="color: #79740e;">"architecture"</span>: <span style="color: #79740e;">"amd64"</span>,
  <span style="color: #79740e;">"os"</span>: <span style="color: #79740e;">"linux"</span>,
  <span style="color: #79740e;">"config"</span>: <span style="color: #b16286;">{</span>
    <span style="color: #79740e;">"Env"</span>: <span style="color: #8ec07c;">[</span>
      <span style="color: #79740e;">"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"</span>
    <span style="color: #8ec07c;">]</span>,
    <span style="color: #79740e;">"Cmd"</span>: <span style="color: #8ec07c;">[</span>
      <span style="color: #79740e;">"sh"</span>
    <span style="color: #8ec07c;">]</span>
  <span style="color: #b16286;">}</span>,
  <span style="color: #79740e;">"rootfs"</span>: <span style="color: #b16286;">{</span>
    <span style="color: #79740e;">"type"</span>: <span style="color: #79740e;">"layers"</span>,
    <span style="color: #79740e;">"diff_ids"</span>: <span style="color: #8ec07c;">[</span>
      <span style="color: #79740e;">"sha256:0b7d464440dc672e08617a7520ac064ba1d6db2c855c185a2a71f1b20e728875"</span>
    <span style="color: #8ec07c;">]</span>
  <span style="color: #b16286;">}</span>,
  <span style="color: #79740e;">"history"</span>: <span style="color: #b16286;">[</span>
    <span style="color: #8ec07c;">{</span>
      <span style="color: #79740e;">"created"</span>: <span style="color: #79740e;">"2023-06-10T00:19:54.657258095Z"</span>,
      <span style="color: #79740e;">"created_by"</span>: <span style="color: #79740e;">"/bin/sh -c #(nop) ADD file:06946025f3ffea04544a154140a48acf32ddfec8205c9b8b5bc7e94abb0c2879 in / "</span>
    <span style="color: #8ec07c;">}</span>,
    <span style="color: #8ec07c;">{</span>
      <span style="color: #79740e;">"created"</span>: <span style="color: #79740e;">"2023-06-10T00:19:54.795108463Z"</span>,
      <span style="color: #79740e;">"created_by"</span>: <span style="color: #79740e;">"/bin/sh -c #(nop)  CMD [\"sh\"]"</span>,
      <span style="color: #79740e;">"empty_layer"</span>: true
    <span style="color: #8ec07c;">}</span>
  <span style="color: #b16286;">]</span>
<span style="color: #076678;">}</span>
</pre>
</div></li>
<li>第二个sha256文件就是我们的tar文件</li>
</ol></li>
<li><p>
我们把第二个tar文件提取出来,如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# <span style="color: #076678;">LAYER</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">jq -r .layers</span><span style="color: #b16286; font-weight: bold;">[</span><span style="color: #076678; font-weight: bold;">0</span><span style="color: #b16286; font-weight: bold;">]</span><span style="color: #076678; font-weight: bold;">.digest busybox/blobs/sha256/$MANIFEST | sed -e 's/sha256://'</span><span style="color: #076678;">)</span>
root@host01:~# echo $<span style="color: #076678;">LAYER</span>
4262504e0aaf2e52231efa124ad2062f3bbbf601ad311971ff2f91bf76fdebbd
</pre>
</div></li>
<li><p>
再用tar tvf 命令把tar文件的解压出来,详细信息如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# tar tvf busybox/blobs/sha256/$<span style="color: #076678;">LAYER</span>
drwxr-xr-x 0/0               0 2023-06-08 23:48 bin/
-rwxr-xr-x 0/0         1033728 2023-06-08 23:48 bin/<span style="color: #076678;">[</span>
hrwxr-xr-x 0/0               0 2023-06-08 23:48 bin/<span style="color: #b16286;">[</span><span style="color: #8ec07c;">[</span> link to bin/<span style="color: #d65d0e;">[</span>
hrwxr-xr-x 0/0               0 2023-06-08 23:48 bin/acpid link to bin/<span style="color: #4db5bd;">[</span>
hrwxr-xr-x 0/0               0 2023-06-08 23:48 bin/add-shell link to bin/<span style="color: #076678;">[</span>
hrwxr-xr-x 0/0               0 2023-06-08 23:48 bin/addgroup link to bin/<span style="color: #8ec07c;">[</span>
...
-rw-r--r-- 0/0           93000 2023-04-19 21:17 lib/libresolv.so.2
lrwxrwxrwx 0/0               0 2023-06-08 23:48 lib64 -&gt; lib
drwx------ 0/0               0 2023-06-08 23:48 root/
drwxrwxrwt 0/0               0 2023-06-08 23:48 tmp/
drwxr-xr-x 0/0               0 2023-06-08 23:48 usr/
drwxr-xr-x 0/0               0 2023-06-08 23:48 usr/bin/
lrwxrwxrwx 0/0               0 2023-06-08 23:48 usr/bin/env -&gt; ../../bin/env
drwxr-xr-x 1/1               0 2023-06-08 23:48 usr/sbin/
drwxr-xr-x 0/0               0 2023-06-08 23:48 var/
drwxr-xr-x 0/0               0 2023-06-08 23:48 var/spool/
drwxr-xr-x 8/8               0 2023-06-08 23:48 var/spool/mail/
drwxr-xr-x 0/0               0 2023-06-08 23:48 var/www/
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgd8611ca" class="outline-2">
<h2 id="orgd8611ca"><span class="section-number-2">6.</span> Chapter 6: Why Kubernetes Matters</h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li>容器允许我们改变之前打包和部署application的方式</li>
<li>但是这还不够,我们还需要一个容器编排的框架来帮我们在一个集群内调度容器.这样才能满足优秀架构的如下三个特性:
<ul class="org-ul">
<li>scalability</li>
<li>reliability</li>
<li>resiliency</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orga996b56" class="outline-3">
<h3 id="orga996b56"><span class="section-number-3">6.1.</span> Running Containers in a Cluster</h3>
<div class="outline-text-3" id="text-6-1">
<ul class="org-ul">
<li>现代的网络应用,在多个server之间调度application是非常迫切想需求:
<ul class="org-ul">
<li>一来可以保证扩展性和可靠性,分散load</li>
<li>二来可以避免单点故障</li>
</ul></li>
<li>前面我们已经讲了在容器里面运行application,但是这并没有保证容器能够在多个server运行以规避单点故障</li>
<li>只有容器编排框架能够解决这个问题: 因为容器编排框架可以跨server调度容器</li>
<li>这也给了容器编排框架一个巨大的难题: 容器编排框架需要在多个机器之间调度任意类型的容器</li>
</ul>
</div>
<div id="outline-container-orgf0b4846" class="outline-4">
<h4 id="orgf0b4846"><span class="section-number-4">6.1.1.</span> Cross-Cutting Concerns</h4>
<div class="outline-text-4" id="text-6-1-1">
<ul class="org-ul">
<li>在任意机器山运行任意的container让我们的灵活性得到了巨大提高,但是极大的增加了k8s的复杂度:
<ul class="org-ul">
<li>k8s事前不知道它被要求运行什么类型的容器</li>
<li>容器的workload随着新的应用的加入或者离开,而不停的在改变</li>
</ul></li>
<li>除了上面的问题,k8s还需要满足如下的design pattern:
<ul class="org-ul">
<li>Dynamic scheduling: 新的容器必须分配给server,而且还能根据配置的改变,而改变容器所在的server</li>
<li>Distributed state: 整个集群必须知道哪些容器,在哪里运行,即便是硬件或网络错误期间</li>
<li>Multitenancy: 单个机器可以运行多个application</li>
<li>Hardware isolation: 集群可以运行在cloud环境,也可以运行在用户的机器上面,要和不同的运行环境解耦</li>
</ul></li>
<li>上面的这些个design pattern整合起来用一个术语来表达,就是cross-cutting concern</li>
</ul>
</div>
</div>
<div id="outline-container-org65985b7" class="outline-4">
<h4 id="org65985b7"><span class="section-number-4">6.1.2.</span> Kubernetes Concepts</h4>
<div class="outline-text-4" id="text-6-1-2">
<ul class="org-ul">
<li>为了达到cross-cutting concern, k8s架构需要允许所有的东西随时来,随时走:这里说到了所有的东西,那么就不
仅仅是容器化之后的app可以随时来随时走. 甚至是包括底层的硬件,比如:
<ul class="org-ul">
<li>server</li>
<li>network</li>
<li>storage</li>
</ul></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orgcbd9dc6"></a>Separate Control Plane<br />
<div class="outline-text-5" id="text-6-1-2-1">
<ul class="org-ul">
<li>k8s作为编排框架,首先需要能够运行容器. 具体做这项工作的是一系列的worker机器,叫做node</li>
<li>每个node上面都会运行一个kubelet service,用来和底层的container runtime进行通信,并且监控容器</li>
<li>k8s 为了管理node和在node上面的容器,也编写了一系列的core software component, 它们统一叫做control plane</li>
<li>这些core software component 通常都和worker node 分开部署,这也就意味着一个有趣的现象: 我们可以使用worker node
上面的容器来运行这些control plane</li>
<li>独立的control plane意味着k8s有更高的可配置性,比如cloud controller manager组件就是用来部署k8s到cloud
provider的,这个组件提高了hardware isolation</li>
</ul>
</div>
</li>
<li><a id="orgfdd0d6a"></a>Declarative API<br />
<div class="outline-text-5" id="text-6-1-2-2">
<ul class="org-ul">
<li>k8s的API是声明式的,也就是说其endpoint是如create, read, update, delete, 但是具体的参数从cluster configuration中获取</li>
<li>声明式API的优点是可以设计幂等(idempotence)类型的API,所谓幂等,就是API无论运行多少次,都是一次的效果.
这在网络和硬件不稳定的分布式系统中,很受欢迎</li>
</ul>
</div>
</li>
<li><a id="org69f7075"></a>Self-Healing<br />
<div class="outline-text-5" id="text-6-1-2-3">
<ul class="org-ul">
<li>在声明式API的帮助下,k8s被设计成了自我治愈(self-healing)的架构</li>
<li>所谓自我治愈架构,是说control plane实时监控如下两个事情,并且确保它们一直是对的齐的:
<ul class="org-ul">
<li>cluster configuration</li>
<li>cluster state</li>
</ul></li>
<li>k8s的这种把configuration和state分离的做法让k8s非常大的弹性(resilient,或者翻译成修复力),举个例子:
<ul class="org-ul">
<li>假设开始有一个容器在Running的状态正常运行</li>
<li>如果control plane失去了和这个容器的server之间的联系, 那么control plane立马就设置容器status为Unknown,
然后,要么尝试重连,要么重建一个容器</li>
</ul></li>
<li>k8s的这种声明式API和自我治愈架构,会带来一个不太寻常的体验:
<ul class="org-ul">
<li>我们发送给k8s的一个success的反馈,只是说cluster configuration改变了,而不是说cluster的state改变了.
它俩是分开的,k8s需要一段时间达到align configuration和state的效果</li>
<li>所有我们在获取success反馈后,还要实时监控容器的状态,看看control plane align的时候有什么问题</li>
</ul></li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-orgc0c3a42" class="outline-3">
<h3 id="orgc0c3a42"><span class="section-number-3">6.2.</span> Cluster Deployment</h3>
<div class="outline-text-3" id="text-6-2">
<ul class="org-ul">
<li>第一章我们使用了prebuilt的k8s发行版来部署,这是生产环境最好的选择</li>
<li>本章我们为了理解,我们会自己部署vanilla k8s, 使用kubeadm管理工具</li>
<li>我们自己的k8s集群会部署在四台virtual machine,分别是:
<ul class="org-ul">
<li>host01</li>
<li>host02</li>
<li>host03</li>
<li>host04</li>
</ul></li>
<li>我们使用其中的前三台(host01,host02,host03)来作为control plane component,因为这是高可用cluster最低机器数目的要求.</li>
<li>host04作为唯一的一台worker</li>
<li>本例子中,我们会使用control plane node来运行常规容器,但是注意在生产环境中这是需要避免的.</li>
<li>我们使用vagrant初始化机器的过程当中,就已经安装了如下两个软件:
<ul class="org-ul">
<li>containerd: 最常见的container runtime</li>
<li>crictl: k8s的一部分,用来测试container runtime和CRI之间兼容性的</li>
</ul></li>
<li>vagrant初始化的过程中,我们还会设置kube-vip来提供类似高可用的Load balancer功能</li>
<li><p>
注意: 本章所有内容,都可以使用ansible的 extra provisioning script来自动化完成
</p>
<pre class="example" id="org6a2ccf5">
You can install Kubernets automatically using the xtra provisioning script provided with this chapter's examples
</pre></li>
</ul>
</div>
<div id="outline-container-orgd417171" class="outline-4">
<h4 id="orgd417171"><span class="section-number-4">6.2.1.</span> Prerequisite Packages</h4>
<div class="outline-text-4" id="text-6-2-1">
<ul class="org-ul">
<li>第一步是要保证br_netfilter kernel module开启并且在启动的时候加载了.k8s使用这个特性来处理跨集群的networking
<ul class="org-ul">
<li><p>
使用如下命令来安装并在开机时候加载br_netfilter
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# k8s-all modprobe br_netfilter
Running command locally
Running command on 192.168.61.12
Running command on 192.168.61.13
Running command on 192.168.61.14
root@host01:/home/vagrant# k8s-all <span style="color: #79740e;">"echo 'br_netfilter' &gt; /etc/modules-load.d/k8s.conf"</span>
Running command locally
Running command on 192.168.61.12
Running command on 192.168.61.13
Running command on 192.168.61.14
</pre>
</div></li>
<li>第一条命令是安装这个module</li>
<li>第二条命令是配置这个module在启动时候就加载</li>
</ul></li>
<li>network across cluster还需要开启很多高级网络特性
<ul class="org-ul">
<li><p>
我们使用如下命令完成操作
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# k8s-all sysctl -w net.ipv4.ip_forward=1 <span style="color: #79740e;">\</span>
&gt; net.bridge.bridge-nf-call-ip6tables=1 <span style="color: #79740e;">\</span>
&gt; net.bridge.bridge-nf-call-iptables=1
Running command locally
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
Running command on 192.168.61.12
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
Running command on 192.168.61.13
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
Running command on 192.168.61.14
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
</pre>
</div></li>
<li>上面的命令开启了如下linux 内核网络特性:
<ol class="org-ol">
<li>net.ip4.ip_forward: 从一个network interface转到另外一个network interface(比如从容器的namespace的一个interface到host的network)</li>
<li>net.bridge-nf-call-ip6tables: 能让IPv6 bridge traffice 通过iptables firewall</li>
<li>net.bridge-nf-call-iptables: 能让IPv4 bridge traffice 通过iptables firewall</li>
</ol></li>
<li>使用sysctl进行配置的话,每次重启要重新配置(ansible extra provisioning script会持久化这个配置)</li>
</ul></li>
<li>下面是使用apt安装一些需要的package
<ul class="org-ul">
<li><p>
命令如下
</p>
<div class="org-src-container">
<pre class="src src-shell">k8s-all apt install -y apt-transport-https open-iscsi nfs-common
</pre>
</div></li>
<li>apt-transport-https允许apt使用HTTP地址</li>
<li>open-iscsi nfs-common 是cluster add-on需要的</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org6e76fc0" class="outline-4">
<h4 id="org6e76fc0"><span class="section-number-4">6.2.2.</span> Kubernetes Packages</h4>
<div class="outline-text-4" id="text-6-2-2">
<ul class="org-ul">
<li>下面就要安装kubeadm了,这个工具我们后面用来设置cluster</li>
<li>首先需要添加GPG key,用来验证package 签名
<ul class="org-ul">
<li><p>
命令如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# k8s-all <span style="color: #79740e;">"curl -fsSL \</span>
<span style="color: #79740e;">&gt; https://packages.cloud.google.com/apt/doc/apt-key.gpg | \</span>
<span style="color: #79740e;">&gt; gpg --dearmor -o /usr/share/keyrings/google-cloud-keyring.gpg"</span>
Running command locally
Running command on 192.168.61.12
Running command on 192.168.61.13
Running command on 192.168.61.14
</pre>
</div></li>
<li>上述命令使用curl来下载GPG key,然后使用gpg来格式化,之后把格式化后的key写入到/user/share/keyrings</li>
<li>`-fsSL`参数组合起来起到了如下效果(这些效果对于后面的pipeline很有用):
<ol class="org-ol">
<li>避免了不必要的output</li>
<li>能follow server的redirect</li>
<li>如果出现了问题,直接结束命令</li>
</ol></li>
</ul></li>
<li>下面我们来添加k8s的repo配置
<ul class="org-ul">
<li><p>
命令如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# k8s-all <span style="color: #79740e;">"echo 'deb [arch=amd64' \</span>
<span style="color: #79740e;">&gt; 'signed-by=/usr/share/keyrings/google-cloud-keyring.gpg]' \</span>
<span style="color: #79740e;">&gt; 'https://apt.kubernetes.io/ kubernetes-xenial main' &gt; \</span>
<span style="color: #79740e;">&gt; /etc/apt/sources.list.d/kubernetes.list"</span>
Running command locally
Running command on 192.168.61.12
Running command on 192.168.61.13
Running command on 192.168.61.14
</pre>
</div></li>
<li>这里把kubernetes-xenial作为发行版,这个名字的意思是从xenial发行版以后的ubuntu都用这个,而不仅仅是xenial</li>
</ul></li>
<li>安装完repository之后,我们就可以更新并且安装了
<ul class="org-ul">
<li><p>
首先每台机器进行更新
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# k8s-all apt update
</pre>
</div></li>
<li><p>
然后安装我们需要的版本
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# source /opt/k8sver
root@host01:/home/vagrant# k8s-all apt install -y <span style="color: #076678;">kubelet</span>=$<span style="color: #076678;">K8SV</span> <span style="color: #076678;">kubeadm</span>=$<span style="color: #076678;">K8SV</span> <span style="color: #076678;">kubectl</span>=$<span style="color: #076678;">K8SV</span>
</pre>
</div></li>
<li><p>
source 命令会读取一个文件,里面会设置一些环境变量,保证我们安装的k8s版本都是一致的
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# cat /opt/k8sver
<span style="color: #af3a03;">export</span> <span style="color: #076678;">K8SV</span>=<span style="color: #79740e;">"1.23.4-*"</span>
<span style="color: #af3a03;">export</span> <span style="color: #076678;">calico_url</span>=<span style="color: #79740e;">"https://docs.projectcalico.org/archive/v3.22/manifests/tigera-operator.yaml"</span>
<span style="color: #af3a03;">export</span> <span style="color: #076678;">longhorn_url</span>=<span style="color: #79740e;">"https://raw.githubusercontent.com/longhorn/longhorn/v1.2.4/deploy/longhorn.yaml"</span>
<span style="color: #af3a03;">export</span> <span style="color: #076678;">metrics_url</span>=<span style="color: #79740e;">"https://github.com/kubernetes-sigs/metrics-server/releases/download/metrics-server-helm-chart-3.8.2/components.yaml"</span>
<span style="color: #af3a03;">export</span> <span style="color: #076678;">ingress_url</span>=<span style="color: #79740e;">"https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.1/deploy/static/provider/cloud/deploy.yaml"</span>
<span style="color: #af3a03;">export</span> <span style="color: #076678;">prometheus_url</span>=<span style="color: #79740e;">"https://github.com/prometheus-operator/kube-prometheus/archive/refs/heads/release-0.10.zip"</span>
</pre>
</div></li>
<li>apt命令安装的三个package的介绍如下:
<ol class="org-ol">
<li>kubelet: 所有worker node上面都有的service,用来和container engine沟通,并且运行control plane要求的container</li>
<li>kubeadm: 管理工具,用来安装k8s,并且维持集群运转</li>
<li>kubectl: 命令行工具用来创建和删除资源,并且获取k8s集群的状态</li>
</ol></li>
</ul></li>
<li><p>
kubelet packag立马开始工作,但是由于我们没有安装control plane,所以service当前是failed的状态
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# systemctl status kubelet
&#9679; kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded <span style="color: #076678;">(</span>/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled<span style="color: #076678;">)</span>
    Drop-In: /etc/systemd/system/kubelet.service.d
             &#9492;&#9472;10-kubeadm.conf
     Active: activating <span style="color: #076678;">(</span>auto-restart<span style="color: #076678;">)</span> <span style="color: #076678;">(</span>Result: exit-code<span style="color: #076678;">)</span> since Tue 2023-07-18 06:37:07 UTC; 4s ago
       Docs: https://kubernetes.io/docs/home/
    Process: 19170 <span style="color: #076678;">ExecStart</span>=/usr/bin/kubelet $<span style="color: #076678;">KUBELET_KUBECONFIG_ARGS</span> $<span style="color: #076678;">KUBELET_CONFIG_ARGS</span> $<span style="color: #076678;">KUBELET_KUBEADM_ARGS</span> $<span style="color: #076678;">KUBELET_EXTRA_ARGS</span> <span style="color: #076678;">(</span><span style="color: #076678;">code</span>=exited, <span style="color: #076678;">status</span>=1/FAILURE<span style="color: #076678;">)</span>
   Main PID: 19170 <span style="color: #076678;">(</span><span style="color: #076678;">code</span>=exited, <span style="color: #076678;">status</span>=1/FAILURE<span style="color: #076678;">)</span>
</pre>
</div></li>
<li><p>
上面安装的三个k8s package我们希望它们稳定,我们不希望系统在apt full-upgrade的时候,不小心也升级了这三
个k8s package的版本.所以我们用下面的命令来固定它们的版本.固定之后,如果再想升级这些package,必须是apt
install加specific版本
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# k8s-all apt-mark hold kubelet kubeadm kubectl
Running command locally
kubelet set on hold.
kubeadm set on hold.
kubectl set on hold.
Running command on 192.168.61.12
kubelet set on hold.
kubeadm set on hold.
kubectl set on hold.
Running command on 192.168.61.13
kubelet set on hold.
kubeadm set on hold.
kubectl set on hold.
Running command on 192.168.61.14
kubelet set on hold.
kubeadm set on hold.
kubectl set on hold.
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org952a255" class="outline-4">
<h4 id="org952a255"><span class="section-number-4">6.2.3.</span> Cluster Initialization</h4>
<div class="outline-text-4" id="text-6-2-3">
<ul class="org-ul">
<li>下一个命令kubeadm init,会做两件事:
<ul class="org-ul">
<li>初始化contrl plane</li>
<li>为所有的node提供kubelet worker node service</li>
</ul></li>
<li>我们会在集群的一台机器上先运行kubeadm init, 然后在其他node上面运行kubeadmin join, 让其他机器加入到已经存在的集群</li>
<li>为了运行kubeadmin init, 我们要先创建一个YAML配置文件.这个配置文件里放的是之前命令行的参数</li>
<li>使用YAML来存储配置文件参数有几个好处:
<ul class="org-ul">
<li>首先,我们不必要记忆太多的command line flag</li>
<li>其次,我们的YAML配置文件可以放在cluster的文件系统,需要更改的时候,更改YAML的一个位置,可以再次运行</li>
</ul></li>
<li>自动运行的YAML文件存储在了/etc/kubernetes, 我们首先使用的就是kubeadm-init.yaml
<ul class="org-ul">
<li><p>
文件内容如下
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #a89984;">---</span>
<span style="color: #076678;">apiVersion</span>: kubeadm.k8s.io/v1beta3
<span style="color: #076678;">kind</span>: InitConfiguration
<span style="color: #076678;">bootstrapTokens</span>:
- <span style="color: #076678;">groups</span>:
  - system:bootstrappers:kubeadm:default-node-token
  <span style="color: #076678;">token</span>: 1d8fb1.2875d52d62a3282d
  <span style="color: #076678;">ttl</span>: 2h0m0s
  <span style="color: #076678;">usages</span>:
  - signing
  - authentication
<span style="color: #076678;">nodeRegistration</span>:
  <span style="color: #076678;">kubeletExtraArgs</span>:
    <span style="color: #076678;">node-ip</span>: 192.168.61.11
  <span style="color: #076678;">taints</span>: []
<span style="color: #076678;">localAPIEndpoint</span>:
  <span style="color: #076678;">advertiseAddress</span>: 192.168.61.11
<span style="color: #076678;">certificateKey</span>: <span style="color: #79740e;">"5a7e07816958efb97635e9a66256adb1"</span>
<span style="color: #a89984;">---</span>
<span style="color: #076678;">apiVersion</span>: kubeadm.k8s.io/v1beta3
<span style="color: #076678;">kind</span>: ClusterConfiguration
<span style="color: #076678;">kubernetesVersion</span>: 1.23.4
<span style="color: #076678;">apiServer</span>:
  <span style="color: #076678;">extraArgs</span>:
    <span style="color: #076678;">service-node-port-range</span>: 80-32767
<span style="color: #076678;">networking</span>:
  <span style="color: #076678;">podSubnet</span>: <span style="color: #79740e;">"172.31.0.0/16"</span>
<span style="color: #076678;">controlPlaneEndpoint</span>: <span style="color: #79740e;">"192.168.61.10:6443"</span>
<span style="color: #a89984;">---</span>
<span style="color: #076678;">apiVersion</span>: kubelet.config.k8s.io/v1beta1
<span style="color: #076678;">kind</span>: KubeletConfiguration
<span style="color: #076678;">serverTLSBootstrap</span>: <span style="color: #8f3f71;">true</span>
</pre>
</div></li>
<li>YAML文件分成三个部分,通过dash(&#x2014;)分开:
<ol class="org-ol">
<li>第一个部分是初始化cluster的</li>
<li>第二个部分是generic configuration</li>
<li>第三个部分是用来提供对所有node的kubelet进行配置的</li>
</ol></li>
<li>我们来介绍下YAML里面的配置项:
<ol class="org-ol">
<li>apiVersion: k8s版本</li>
<li>kind: 资源名称</li>
<li>bootstrapToksn: 配置的一个secret用来让其他node join cluster用的,这个token在生产环境要保密的.
同时,这个secret两个小时就过期,如果两个小时都join不完,那么要新创建secret</li>
<li>nodeRegistration: 传递给kubelet(只在host01)服务的配置, 其中的node-ip用来保证kubelet注册正确的
IP地址, taints为空,表示本机也接受常规容器被调度过来</li>
<li>localAPIEndpoint: 为API server指定IP,由于我们的vagrant有多个ip,我们要指定某个ip给API server</li>
<li>certificateKey: 其他node访问API server需要的鉴权,所有的API server共享这个key,所以在production环境要保护好这个key</li>
<li>networking: 集群里面所有的容器都会得到一个IP(podSubnet来设置这个网段). 后面我们会按照一个network driver用来确保
host上的所有容器都可以相互通信</li>
<li>controlPlaneEndpoint: API server的外部IP, 这个部分是对于高可用集群最重要的部分,这是一个IP
address能够到达所有的API server. 我们应该首先创建这个IP. 如果是云服务,那么可以利用其ELB(aws)来
做到.这里我们是用的vagrant,所以我们使用了kub-vip, 地址就是192.168.61.10:6443</li>
<li>serverTSLBootstrap: 指导kubelet使用controller manager的鉴权</li>
</ol></li>
</ul></li>
<li>所有的参数都已经在yaml文件里面了,我们可以使用命令来初始化集群了
<ul class="org-ul">
<li><p>
命令如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# /usr/bin/kubeadm init --config /etc/kubernetes/kubeadm-init.yaml --upload-certs
<span style="color: #076678;">[</span>init<span style="color: #076678;">]</span> Using Kubernetes version: v1.23.4
<span style="color: #076678;">[</span>preflight<span style="color: #076678;">]</span> Running pre-flight checks
<span style="color: #076678;">[</span>preflight<span style="color: #076678;">]</span> Pulling images required for setting up a Kubernetes cluster
<span style="color: #076678;">[</span>preflight<span style="color: #076678;">]</span> This might take a minute or two, depending on the speed of your internet connection
<span style="color: #076678;">[</span>preflight<span style="color: #076678;">]</span> You can also perform this action<span style="color: #9d0006;"> in</span> beforehand using <span style="color: #79740e;">'kubeadm config images pull'</span>
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> Using certificateDir folder <span style="color: #79740e;">"/etc/kubernetes/pki"</span>
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> Generating <span style="color: #79740e;">"ca"</span> certificate and key
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> Generating <span style="color: #79740e;">"apiserver"</span> certificate and key
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> apiserver serving cert is signed for DNS names <span style="color: #076678;">[</span>host01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local<span style="color: #076678;">]</span> and IPs <span style="color: #076678;">[</span>10.96.0.1 192.168.61.11 192.168.61.10<span style="color: #076678;">]</span>
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> Generating <span style="color: #79740e;">"apiserver-kubelet-client"</span> certificate and key
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> Generating <span style="color: #79740e;">"front-proxy-ca"</span> certificate and key
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> Generating <span style="color: #79740e;">"front-proxy-client"</span> certificate and key
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> Generating <span style="color: #79740e;">"etcd/ca"</span> certificate and key
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> Generating <span style="color: #79740e;">"etcd/server"</span> certificate and key
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> etcd/server serving cert is signed for DNS names <span style="color: #076678;">[</span>host01 localhost<span style="color: #076678;">]</span> and IPs <span style="color: #076678;">[</span>192.168.61.11 127.0.0.1 ::1<span style="color: #076678;">]</span>
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> Generating <span style="color: #79740e;">"etcd/peer"</span> certificate and key
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> etcd/peer serving cert is signed for DNS names <span style="color: #076678;">[</span>host01 localhost<span style="color: #076678;">]</span> and IPs <span style="color: #076678;">[</span>192.168.61.11 127.0.0.1 ::1<span style="color: #076678;">]</span>
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> Generating <span style="color: #79740e;">"etcd/healthcheck-client"</span> certificate and key
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> Generating <span style="color: #79740e;">"apiserver-etcd-client"</span> certificate and key
<span style="color: #076678;">[</span>certs<span style="color: #076678;">]</span> Generating <span style="color: #79740e;">"sa"</span> key and public key
<span style="color: #076678;">[</span>kubeconfig<span style="color: #076678;">]</span> Using kubeconfig folder <span style="color: #79740e;">"/etc/kubernetes"</span>
<span style="color: #076678;">[</span>kubeconfig<span style="color: #076678;">]</span> Writing <span style="color: #79740e;">"admin.conf"</span> kubeconfig file
<span style="color: #076678;">[</span>kubeconfig<span style="color: #076678;">]</span> Writing <span style="color: #79740e;">"kubelet.conf"</span> kubeconfig file
<span style="color: #076678;">[</span>kubeconfig<span style="color: #076678;">]</span> Writing <span style="color: #79740e;">"controller-manager.conf"</span> kubeconfig file
<span style="color: #076678;">[</span>kubeconfig<span style="color: #076678;">]</span> Writing <span style="color: #79740e;">"scheduler.conf"</span> kubeconfig file
<span style="color: #076678;">[</span>kubelet-start<span style="color: #076678;">]</span> Writing kubelet environment file with flags to file <span style="color: #79740e;">"/var/lib/kubelet/kubeadm-flags.env"</span>
<span style="color: #076678;">[</span>kubelet-start<span style="color: #076678;">]</span> Writing kubelet configuration to file <span style="color: #79740e;">"/var/lib/kubelet/config.yaml"</span>
<span style="color: #076678;">[</span>kubelet-start<span style="color: #076678;">]</span> Starting the kubelet
<span style="color: #076678;">[</span>control-plane<span style="color: #076678;">]</span> Using manifest folder <span style="color: #79740e;">"/etc/kubernetes/manifests"</span>
<span style="color: #076678;">[</span>control-plane<span style="color: #076678;">]</span> Creating static Pod manifest for <span style="color: #79740e;">"kube-apiserver"</span>
<span style="color: #076678;">[</span>control-plane<span style="color: #076678;">]</span> Creating static Pod manifest for <span style="color: #79740e;">"kube-controller-manager"</span>
<span style="color: #076678;">[</span>control-plane<span style="color: #076678;">]</span> Creating static Pod manifest for <span style="color: #79740e;">"kube-scheduler"</span>
<span style="color: #076678;">[</span>etcd<span style="color: #076678;">]</span> Creating static Pod manifest for local etcd<span style="color: #9d0006;"> in</span> <span style="color: #79740e;">"/etc/kubernetes/manifests"</span>
<span style="color: #076678;">[</span>wait-control-plane<span style="color: #076678;">]</span> Waiting for the kubelet to boot up the control plane as static Pods from directory <span style="color: #79740e;">"/etc/kubernetes/manifests"</span>. This can take up to 4m0s
<span style="color: #076678;">[</span>apiclient<span style="color: #076678;">]</span> All control plane components are healthy after 18.710166 seconds
<span style="color: #076678;">[</span>upload-config<span style="color: #076678;">]</span> Storing the configuration used<span style="color: #9d0006;"> in</span> ConfigMap <span style="color: #79740e;">"kubeadm-config"</span><span style="color: #9d0006;"> in</span> the <span style="color: #79740e;">"kube-system"</span> Namespace
<span style="color: #076678;">[</span>kubelet<span style="color: #076678;">]</span> Creating a ConfigMap <span style="color: #79740e;">"kubelet-config-1.23"</span><span style="color: #9d0006;"> in</span> namespace kube-system with the configuration for the kubelets<span style="color: #9d0006;"> in</span> the cluster
NOTE: The <span style="color: #79740e;">"kubelet-config-1.23"</span> naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just <span style="color: #79740e;">"kubelet-config"</span>. Kubeadm upgrade will handle this transition transparently.
<span style="color: #076678;">[</span>upload-certs<span style="color: #076678;">]</span> Storing the certificates<span style="color: #9d0006;"> in</span> Secret <span style="color: #79740e;">"kubeadm-certs"</span><span style="color: #9d0006;"> in</span> the <span style="color: #79740e;">"kube-system"</span> Namespace
<span style="color: #076678;">[</span>upload-certs<span style="color: #076678;">]</span> Using certificate key:
5a7e07816958efb97635e9a66256adb1
<span style="color: #076678;">[</span>mark-control-plane<span style="color: #076678;">]</span> Marking the node host01 as control-plane by adding the labels: <span style="color: #076678;">[</span>node-role.kubernetes.io/master<span style="color: #b16286;">(</span>deprecated<span style="color: #b16286;">)</span> node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers<span style="color: #076678;">]</span>
<span style="color: #076678;">[</span>bootstrap-token<span style="color: #076678;">]</span> Using token: 1d8fb1.2875d52d62a3282d
<span style="color: #076678;">[</span>bootstrap-token<span style="color: #076678;">]</span> Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
<span style="color: #076678;">[</span>bootstrap-token<span style="color: #076678;">]</span> configured RBAC rules to allow Node Bootstrap tokens to get nodes
<span style="color: #076678;">[</span>bootstrap-token<span style="color: #076678;">]</span> configured RBAC rules to allow Node Bootstrap tokens to post CSRs<span style="color: #9d0006;"> in</span> order for nodes to get long term certificate credentials
<span style="color: #076678;">[</span>bootstrap-token<span style="color: #076678;">]</span> configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
<span style="color: #076678;">[</span>bootstrap-token<span style="color: #076678;">]</span> configured RBAC rules to allow certificate rotation for all node client certificates<span style="color: #9d0006;"> in</span> the cluster
<span style="color: #076678;">[</span>bootstrap-token<span style="color: #076678;">]</span> Creating the <span style="color: #79740e;">"cluster-info"</span> ConfigMap<span style="color: #9d0006;"> in</span> the <span style="color: #79740e;">"kube-public"</span> namespace
<span style="color: #076678;">[</span>kubelet-finalize<span style="color: #076678;">]</span> Updating <span style="color: #79740e;">"/etc/kubernetes/kubelet.conf"</span> to point to a rotatable kubelet client certificate and key
<span style="color: #076678;">[</span>addons<span style="color: #076678;">]</span> Applied essential addon: CoreDNS
<span style="color: #076678;">[</span>addons<span style="color: #076678;">]</span> Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $<span style="color: #076678;">HOME</span>/.kube
  sudo cp -i /etc/kubernetes/admin.conf $<span style="color: #076678;">HOME</span>/.kube/config
  sudo chown $<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">id -u</span><span style="color: #076678;">)</span>:$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">id -g</span><span style="color: #076678;">)</span> $<span style="color: #076678;">HOME</span>/.kube/config

Alternatively, if you are the root user, you can run:

  <span style="color: #af3a03;">export</span> <span style="color: #076678;">KUBECONFIG</span>=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run <span style="color: #79740e;">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 192.168.61.10:6443 --token 1d8fb1.2875d52d62a3282d <span style="color: #79740e;">\</span>
        --discovery-token-ca-cert-hash sha256:18679f52b136afa9247cbb152e7a337e247712bfef74488b9018c9c4d8acb59e <span style="color: #79740e;">\</span>
        --control-plane --certificate-key 5a7e07816958efb97635e9a66256adb1

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted<span style="color: #9d0006;"> in</span> two hours; If necessary, you can use
<span style="color: #79740e;">"kubeadm init phase upload-certs --upload-certs"</span> to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.61.10:6443 --token 1d8fb1.2875d52d62a3282d <span style="color: #79740e;">\</span>
        --discovery-token-ca-cert-hash sha256:18679f52b136afa9247cbb152e7a337e247712bfef74488b9018c9c4d8acb59e
</pre>
</div></li>
<li>`&#x2013;config` 参数指向了YAML配置文件的地址</li>
<li>`&#x2013;upload-certs` 告诉kubeadm,需要把API server的certificate上传到cluster的分布式存储,其他的control
plane node就可以在join cluster的时候,下载这些certificate了.我们的certificate还是使用certificateKey
来加密的,所以后面解密还需要这个certificateKey</li>
<li>kubeadm init用来在host01上面初始化control plane, control plane本身就运行在container里面,并且被kubelet
service所管理(因为是容器化运行,所以很容易升级), 由于要下载容器,所以这个命令的执行会慢一点</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org431b63d" class="outline-4">
<h4 id="org431b63d"><span class="section-number-4">6.2.4.</span> Joining Nodes to the Cluster</h4>
<div class="outline-text-4" id="text-6-2-4">
<ul class="org-ul">
<li>kubeadm init 命令会打印出kubeadmin join命令,我们可以在其他机器上使用这个命令来加入集群.</li>
<li>但是,我们这个例子中就不再使用kubeadm init显示的命令了.我们自己把参数还是存在了yaml文件里面
<ul class="org-ul">
<li><p>
下面是host02上面join的时候需要的配置文件kubeadm-join.yaml
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #076678;">apiVersion</span>: kubeadm.k8s.io/v1beta3
<span style="color: #076678;">kind</span>: JoinConfiguration
<span style="color: #076678;">discovery</span>:
  <span style="color: #076678;">bootstrapToken</span>:
    <span style="color: #076678;">apiServerEndpoint</span>: 192.168.61.10:6443
    <span style="color: #076678;">token</span>: 1d8fb1.2875d52d62a3282d
    <span style="color: #076678;">unsafeSkipCAVerification</span>: <span style="color: #8f3f71;">true</span>
  <span style="color: #076678;">timeout</span>: 5m0s
<span style="color: #076678;">nodeRegistration</span>:
  <span style="color: #076678;">kubeletExtraArgs</span>:
    <span style="color: #076678;">node-ip</span>: 192.168.61.12
  <span style="color: #076678;">taints</span>: []

  <span style="color: #076678;">ignorePreflightErrors</span>:
    - DirAvailable--etc-kubernetes-manifests
<span style="color: #076678;">controlPlane</span>:
  <span style="color: #076678;">localAPIEndpoint</span>:
    <span style="color: #076678;">advertiseAddress</span>: 192.168.61.12
  <span style="color: #076678;">certificateKey</span>: <span style="color: #79740e;">"5a7e07816958efb97635e9a66256adb1"</span>
</pre>
</div></li>
<li>此配置文件的kind变成了JoinConfiguration,但是其他的配置基本和init时候的一致</li>
<li>不同的地方有一个,就是ignorePreflightErrors, 这是因为我们使用了kube-vip,所以/etc/kubernetes/mainfest
文件夹下已经有配置文件了.所以我们要告诉kubeadm不要因为这个报错</li>
<li><p>
由于需要下载image,所以我们要给containerd配置proxy(vagrant-proxyconf还不支持systemd的配置),我们需要如下配置
</p>
<div class="org-src-container">
<pre class="src src-shell">systemctl set-environment <span style="color: #076678;">HTTP_PROXY</span>=192.168.61.1:7890
systemctl set-environment <span style="color: #076678;">HTTPS_PROXY</span>=192.168.61.1:7890
systemctl restart containerd.service
</pre>
</div></li>
<li><p>
下面就是kubeadm join的结果
</p>
<pre class="example" id="orga4e8f62">
root@host02:~# systemctl set-environment HTTP_PROXY=192.168.61.1:7890
root@host02:~# systemctl set-environment HTTPS_PROXY=192.168.61.1:7890
root@host02:~# systemctl restart containerd.service
root@host02:~# /usr/bin/kubeadm join --config /etc/kubernetes/kubeadm-join.yaml
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0720 09:04:21.722194   89527 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[preflight] Running pre-flight checks before initializing the new control plane instance
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [host02 localhost] and IPs [192.168.61.12 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [host02 localhost] and IPs [192.168.61.12 127.0.0.1 ::1]
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [host02 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.61.12 192.168.61.10]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[certs] Using the existing "sa" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[check-etcd] Checking that the etcd cluster is healthy
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[etcd] Announced new etcd member joining to the existing etcd cluster
[etcd] Creating static Pod manifest for "etcd"
[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s
The 'update-status' phase is deprecated and will be removed in a future release. Currently it performs no operation
[mark-control-plane] Marking the node host02 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane (master) label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.
* A new etcd member was added to the local/stacked etcd cluster.

To start administering your cluster from this node, you need to run the following as a regular user:

        mkdir -p $HOME/.kube
        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
        sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.
</pre></li>
<li><p>
同样的我们在host03运行一样的命令,效果如下
</p>
<pre class="example" id="org074cc37">
root@host03:~# systemctl set-environment HTTP_PROXY=192.168.61.1:7890
root@host03:~# systemctl set-environment HTTPS_PROXY=192.168.61.1:7890
root@host03:~# systemctl restart containerd.service
root@host03:~# /usr/bin/kubeadm join --config /etc/kubernetes/kubeadm-join.yaml
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0720 09:12:23.612397   90765 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[preflight] Running pre-flight checks before initializing the new control plane instance
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [host03 localhost] and IPs [192.168.61.13 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [host03 localhost] and IPs [192.168.61.13 127.0.0.1 ::1]
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [host03 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.61.13 192.168.61.10]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[certs] Using the existing "sa" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[check-etcd] Checking that the etcd cluster is healthy
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[etcd] Announced new etcd member joining to the existing etcd cluster
[etcd] Creating static Pod manifest for "etcd"
[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s
The 'update-status' phase is deprecated and will be removed in a future release. Currently it performs no operation
[mark-control-plane] Marking the node host03 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane (master) label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.
* A new etcd member was added to the local/stacked etcd cluster.

To start administering your cluster from this node, you need to run the following as a regular user:

        mkdir -p $HOME/.kube
        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
        sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.
</pre></li>
</ul></li>
<li>host03运行成功后,我们的control plane就已经配置成功了, 下面这条命令开始就开始设置worker node
<ul class="org-ul">
<li><p>
下面是host04上面kubeadm-join的配置文件
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #076678;">apiVersion</span>: kubeadm.k8s.io/v1beta3
<span style="color: #076678;">kind</span>: JoinConfiguration
<span style="color: #076678;">discovery</span>:
  <span style="color: #076678;">bootstrapToken</span>:
    <span style="color: #076678;">apiServerEndpoint</span>: 192.168.61.10:6443
    <span style="color: #076678;">token</span>: 1d8fb1.2875d52d62a3282d
    <span style="color: #076678;">unsafeSkipCAVerification</span>: <span style="color: #8f3f71;">true</span>
  <span style="color: #076678;">timeout</span>: 5m0s
<span style="color: #076678;">nodeRegistration</span>:
  <span style="color: #076678;">kubeletExtraArgs</span>:
    <span style="color: #076678;">node-ip</span>: 192.168.61.14
  <span style="color: #076678;">taints</span>: []

  <span style="color: #076678;">ignorePreflightErrors</span>:
    - DirAvailable--etc-kubernetes-manifests
</pre>
</div></li>
<li><p>
这个命令由于不需要下载镜像,所以运行较快.运行效果如下
</p>
<pre class="example" id="org6a38f95">
root@host04:~# /usr/bin/kubeadm join --config /etc/kubernetes/kubeadm-join.yaml
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0720 09:21:38.219369   92111 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
</pre></li>
</ul></li>
<li>当前集群已经有四台机器了,我们可以通过kubectl来查看
<ul class="org-ul">
<li><p>
命令运行效果如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# export <span style="color: #076678;">KUBECONFIG</span>=/etc/kubernetes/admin.conf &amp;&amp; kubectl get nodes
NAME     STATUS     ROLES                  AGE     VERSION
host01   NotReady   control-plane,master   62m     v1.23.4
host02   NotReady   control-plane,master   22m     v1.23.4
host03   NotReady   control-plane,master   14m     v1.23.4
host04   NotReady   &lt;none&gt;                 6m50s   v1.23.4
</pre>
</div></li>
<li><p>
我们首先为kubectl指定了配置文件,为/etc/kubernetes/admin.conf(这个文件是kubeadm init的时候自动创建
的), 内容如下
</p>
<div class="org-src-container">
<pre class="src src-conf">apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSU...
    server: https://192.168.61.10:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ...
    client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLR...
</pre>
</div></li>
<li><p>
当前的四个节点都是NotReady,这是由于我们还没有为集群安装network driver,这个信息可以从kubectl describe获得(container runtime network not ready)
</p>
<div class="org-src-container">
<pre class="src src-shell">
root@host01:~# kubectl describe node host04
Name:               host04
...
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 20 Jul 2023 09:32:06 +0000   Thu, 20 Jul 2023 09:21:43 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 20 Jul 2023 09:32:06 +0000   Thu, 20 Jul 2023 09:21:43 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 20 Jul 2023 09:32:06 +0000   Thu, 20 Jul 2023 09:21:43 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            False   Thu, 20 Jul 2023 09:32:06 +0000   Thu, 20 Jul 2023 09:21:43 +0000   KubeletNotReady              container runtime network not ready: <span style="color: #076678;">NetworkReady</span>=false ...
</pre>
</div></li>
<li><p>
k8s是通过在node的配置文件里面的taint数组里面增加一些配置来让这些机器不用背分配工作的(不会被分配容器)
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kubectl get node -o json | jq <span style="color: #79740e;">'.items[]|.metadata.name,.spec.taints[]'</span>
<span style="color: #79740e;">"host01"</span>
<span style="color: #076678;">{</span>
  <span style="color: #79740e;">"effect"</span>: <span style="color: #79740e;">"NoSchedule"</span>,
  <span style="color: #79740e;">"key"</span>: <span style="color: #79740e;">"node.kubernetes.io/not-ready"</span>
<span style="color: #076678;">}</span>
<span style="color: #79740e;">"host02"</span>
<span style="color: #076678;">{</span>
  <span style="color: #79740e;">"effect"</span>: <span style="color: #79740e;">"NoSchedule"</span>,
  <span style="color: #79740e;">"key"</span>: <span style="color: #79740e;">"node.kubernetes.io/not-ready"</span>
<span style="color: #076678;">}</span>
<span style="color: #79740e;">"host03"</span>
<span style="color: #076678;">{</span>
  <span style="color: #79740e;">"effect"</span>: <span style="color: #79740e;">"NoSchedule"</span>,
  <span style="color: #79740e;">"key"</span>: <span style="color: #79740e;">"node.kubernetes.io/not-ready"</span>
<span style="color: #076678;">}</span>
<span style="color: #79740e;">"host04"</span>
<span style="color: #076678;">{</span>
  <span style="color: #79740e;">"effect"</span>: <span style="color: #79740e;">"NoSchedule"</span>,
  <span style="color: #79740e;">"key"</span>: <span style="color: #79740e;">"node.kubernetes.io/not-ready"</span>
<span style="color: #076678;">}</span>
</pre>
</div></li>
<li>在生产环境中, control plane几台机器的taint总是有值的,这样它们不会被分配容器运行任务</li>
<li>我们的vagrant环境当中,由于control plane也要承担容器运行任务.所以,后面我们的driver都安装成功后,我
们会把这个taint设置为空(包括control plane),这样,四台host都可以被安排容器运行任务</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgdae5b78" class="outline-3">
<h3 id="orgdae5b78"><span class="section-number-3">6.3.</span> Installing Cluster Add-ons</h3>
<div class="outline-text-3" id="text-6-3">
<ul class="org-ul">
<li>我们已经在四台机器上安装了kubelet,并且配置了其中三台作为control plane,后面我们就会在这三台control plane上进行操作了</li>
<li>在control plane上进行的第一个操作,就是安装add-ons</li>
<li>所谓add-ons,就是包含k8s资源并且运行在容器里面,通杀为我们提供关键的服务</li>
<li>为了让一个cluster能够成功的运行,我们需要安装三个add-on:
<ul class="org-ul">
<li>network driver</li>
<li>storage driver</li>
<li>ingress controller</li>
</ul></li>
<li>我们还会额外安装一个监控的add-on:metric server</li>
</ul>
</div>
<div id="outline-container-org127085b" class="outline-4">
<h4 id="org127085b"><span class="section-number-4">6.3.1.</span> Network Driver</h4>
<div class="outline-text-4" id="text-6-3-1">
<ul class="org-ul">
<li>k8s的networking需要满足CNI(Container Network Interface), 所有满足这个标准的driver都可以使用.</li>
<li>在第8章,我们会展示不同的network add-on,本章,我们使用大多数人都会使用的Calico network driver</li>
<li>首先我们要下载Calico的YAML
<ul class="org-ul">
<li><p>
例子如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# cd
root@host01:~# cd /etc/kubernetes/components/
root@host01:/etc/kubernetes/components# source /opt/k8sver
root@host01:/etc/kubernetes/components# echo $<span style="color: #076678;">calico_url</span>
https://docs.projectcalico.org/archive/v3.22/manifests/tigera-operator.yaml
root@host01:/etc/kubernetes/components# mv tigera-operator.yaml /tmp/
root@host01:/etc/kubernetes/components# curl -L -O $<span style="color: #076678;">calico_url</span>
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100    82  100    82    0     0     84      0 --:--:-- --:--:-- --:--:--    84
100  304k  100  304k    0     0   122k      0  0:00:02  0:00:02 --:--:--  319k
root@host01:/etc/kubernetes/components# ls
custom-resources.yaml  ingress-patch.yaml  tigera-operator.yaml
</pre>
</div></li>
<li>首先我们进入/etc/kubernetes/components</li>
<li>在/opt/k8sver文件里面写了calico的url地址,我们先source一下,然后使用curl 下载</li>
<li>curl的-L参数是让curl follow http redirect</li>
<li>curl的-O参数是让curl使用url里面的文件名(这里是tigera-operator.yaml)来作为本地文件名</li>
<li><p>
除了刚才下载的YAML文件,我们还有一个YAML文件叫做custom-resources.yaml,用来提供集群必要的配置,下面
例子就是按照这两个文件来安装必要组件
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/etc/kubernetes/components# kubectl apply -f tigera-operator.yaml
root@host01:/etc/kubernetes/components# kubectl apply -f custom-resources.yaml
root@host01:/etc/kubernetes/components#
</pre>
</div></li>
</ul></li>
<li><p>
k8s会花费一定的时间来下载image并且启动image,一旦Calico在我们的cluster里面运行了,我们的nodes就要报Ready状态了,如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/etc/kubernetes/components# kubectl get nodes
NAME     STATUS   ROLES                  AGE   VERSION
host01   Ready    control-plane,master   13h   v1.23.4
host02   Ready    control-plane,master   13h   v1.23.4
host03   Ready    control-plane,master   13h   v1.23.4
host04   Ready    &lt;none&gt;                 13h   v1.23.4
</pre>
</div></li>
<li>Calico安装的时候,是安装了一个DaemonSet.
<ul class="org-ul">
<li>所谓DaemonSet,就是在所有的node里面都安装一个或者多个容器</li>
</ul></li>
<li>这里看似有一个悖论,因为我们所有的node之前是有taint的,这个taint告诉k8s不要接受任何容器被部署到自己node上</li>
<li>解决这个悖论的办法是k8s的另外一个特性,叫做tolerations:
<ul class="org-ul">
<li>所谓toleration,就是一个resource上面的configuration setting用来指导k8s安装容器的(即便设置了taint)</li>
<li><p>
我们可以通过如下设置来看到Calico设置的toleration
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/etc/kubernetes/components# kubectl -n calico-system get daemonsets -o json | jq <span style="color: #79740e;">'.items[].spec.template.spec.tolerations[]'</span>
<span style="color: #076678;">{</span>
  <span style="color: #79740e;">"key"</span>: <span style="color: #79740e;">"CriticalAddonsOnly"</span>,
  <span style="color: #79740e;">"operator"</span>: <span style="color: #79740e;">"Exists"</span>
<span style="color: #076678;">}</span>
<span style="color: #076678;">{</span>
  <span style="color: #79740e;">"effect"</span>: <span style="color: #79740e;">"NoSchedule"</span>,
  <span style="color: #79740e;">"operator"</span>: <span style="color: #79740e;">"Exists"</span>
<span style="color: #076678;">}</span>
<span style="color: #076678;">{</span>
  <span style="color: #79740e;">"effect"</span>: <span style="color: #79740e;">"NoExecute"</span>,
  <span style="color: #79740e;">"operator"</span>: <span style="color: #79740e;">"Exists"</span>
<span style="color: #076678;">}</span>
</pre>
</div></li>
<li>上面命令的-n参数选择了calico-system Namespace.</li>
<li>Namespace是一种让k8s 某个集群内的一类相关性高的容器能够聚集的方法.这样,不同的namespace也能起到隔离作用</li>
<li>上面的DaemonSet有三个toleration, 第二个是我们需要的,这个设置是对schedule的toleration</li>
<li>第二个scheduler toleration告诉k8s schduler让他大胆的schedule calico的容器到所有node(即便这个node有NoSchedule taint)</li>
<li>一旦Calico成功运行,所有的node就会更改自己的状态为Ready,那么NoSchedule taint就会取消,普通的容器也
就能schedule到所有node上了(本例子是所有node,production是所有worker node)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org952ab3e" class="outline-4">
<h4 id="org952ab3e"><span class="section-number-4">6.3.2.</span> Installing Storage</h4>
<div class="outline-text-4" id="text-6-3-2">
<ul class="org-ul">
<li>当前的cluster node都已经ready了,那么我们就可以部署常规容器了,容器也能够运行.</li>
<li>但是如果这个容器需要持久化的存储,那么容器会失败,因为我们还没有安装storage driver</li>
<li>和network driver一样,k8s对storage driver也提供了一个标准,叫做CSI(Container Storage Interface),遵守
这个协议的就能被安装为storage driver</li>
<li>最常用的storage driver是Longhorn,这个是来自Rancher,它的特点是容易安装不需要额外底层硬件</li>
<li><p>
Longhorn使用了我们之前配置好的iSCSI和NFS软件,我们通过如下命令来在所有的node都enable这种软件
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/etc/kubernetes/components# k8s-all systemctl enable --now iscsid
Running command locally
Synchronizing state of iscsid.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable iscsid
Created symlink /etc/systemd/system/sysinit.target.wants/iscsid.service  /lib/systemd/system/iscsid.service.
Running command on 192.168.61.12
Synchronizing state of iscsid.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable iscsid
Created symlink /etc/systemd/system/sysinit.target.wants/iscsid.service  /lib/systemd/system/iscsid.service.
Running command on 192.168.61.13
Synchronizing state of iscsid.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable iscsid
Created symlink /etc/systemd/system/sysinit.target.wants/iscsid.service  /lib/systemd/system/iscsid.service.
Running command on 192.168.61.14
Synchronizing state of iscsid.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable iscsid
Created symlink /etc/systemd/system/sysinit.target.wants/iscsid.service  /lib/systemd/system/iscsid.service.
</pre>
</div></li>
<li><p>
下面是下载longhorn的YAML和安装longhorn的过程示例
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/etc/kubernetes/components# echo $<span style="color: #076678;">longhorn_url</span>
https://raw.githubusercontent.com/longhorn/longhorn/v1.2.4/deploy/longhorn.yaml
root@host01:/etc/kubernetes/components# curl -LO $<span style="color: #076678;">longhorn_url</span>
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 33127  100 33127    0     0   5887      0  0:00:05  0:00:05 --:--:--  8924
root@host01:/etc/kubernetes/components# kubectl apply -f longhorn.yaml
namespace/longhorn-system created
serviceaccount/longhorn-service-account created
clusterrole.rbac.authorization.k8s.io/longhorn-role created
clusterrolebinding.rbac.authorization.k8s.io/longhorn-bind created
customresourcedefinition.apiextensions.k8s.io/engines.longhorn.io created
customresourcedefinition.apiextensions.k8s.io/replicas.longhorn.io created
customresourcedefinition.apiextensions.k8s.io/settings.longhorn.io created
customresourcedefinition.apiextensions.k8s.io/volumes.longhorn.io created
customresourcedefinition.apiextensions.k8s.io/engineimages.longhorn.io created
customresourcedefinition.apiextensions.k8s.io/nodes.longhorn.io created
customresourcedefinition.apiextensions.k8s.io/instancemanagers.longhorn.io created
customresourcedefinition.apiextensions.k8s.io/sharemanagers.longhorn.io created
customresourcedefinition.apiextensions.k8s.io/backingimages.longhorn.io created
customresourcedefinition.apiextensions.k8s.io/backingimagemanagers.longhorn.io created
customresourcedefinition.apiextensions.k8s.io/backingimagedatasources.longhorn.io created
customresourcedefinition.apiextensions.k8s.io/backuptargets.longhorn.io created
customresourcedefinition.apiextensions.k8s.io/backupvolumes.longhorn.io created
customresourcedefinition.apiextensions.k8s.io/backups.longhorn.io created
customresourcedefinition.apiextensions.k8s.io/recurringjobs.longhorn.io created
configmap/longhorn-default-setting created
Warning: policy/v1beta1 PodSecurityPolicy is deprecated<span style="color: #9d0006;"> in</span> v1.21+, unavailable<span style="color: #9d0006;"> in</span> v1.25+
podsecuritypolicy.policy/longhorn-psp created
role.rbac.authorization.k8s.io/longhorn-psp-role created
rolebinding.rbac.authorization.k8s.io/longhorn-psp-binding created
configmap/longhorn-storageclass created
daemonset.apps/longhorn-manager created
service/longhorn-backend created
service/longhorn-engine-manager created
service/longhorn-replica-manager created
deployment.apps/longhorn-ui created
service/longhorn-frontend created
deployment.apps/longhorn-driver-deployer created
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org2cc3d6f" class="outline-4">
<h4 id="org2cc3d6f"><span class="section-number-4">6.3.3.</span> Ingress Controller</h4>
<div class="outline-text-4" id="text-6-3-3">
<ul class="org-ul">
<li>我们已经安装了networking和storage driver.现在我们要安装ingress controller,它的作用是让集群外的网络
也能访问我们的容器. 也就是开放我们的服务给集群外的网络,甚至是公网</li>
<li>安装ingress controller的过程和之前network driver, storage driver类似
<ul class="org-ul">
<li><p>
示例如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/etc/kubernetes/components# echo $<span style="color: #076678;">ingress_url</span>
https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.1/deploy/static/provider/cloud/deploy.yaml
root@host01:/etc/kubernetes/components# curl -Lo ingress-controller.yaml $<span style="color: #076678;">ingress_url</span>
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 19299  100 19299    0     0  15601      0  0:00:01  0:00:01 --:--:-- 15601
root@host01:/etc/kubernetes/components# kubectl apply -f ingress-controller.yaml
namespace/ingress-nginx created
serviceaccount/ingress-nginx created
configmap/ingress-nginx-controller created
clusterrole.rbac.authorization.k8s.io/ingress-nginx created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created
role.rbac.authorization.k8s.io/ingress-nginx created
rolebinding.rbac.authorization.k8s.io/ingress-nginx created
service/ingress-nginx-controller-admission created
service/ingress-nginx-controller created
deployment.apps/ingress-nginx-controller created
ingressclass.networking.k8s.io/nginx created
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created
serviceaccount/ingress-nginx-admission created
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
role.rbac.authorization.k8s.io/ingress-nginx-admission created
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
job.batch/ingress-nginx-admission-create created
job.batch/ingress-nginx-admission-patch created
</pre>
</div></li>
<li>安装ingress controller的过程会安装很多资源,但是主要分成两个部分:
<ol class="org-ol">
<li>一个Nginx web server,其主要用来routing HTTP traffic</li>
<li>一个component用来监督容器中ingress资源的改动,同时动态的更改nginx的配置,让网络访问感知不到ingress
资源的改动</li>
</ol></li>
</ul></li>
<li>ingress controller在安装后,还需要额外的配置,这是因为:
<ul class="org-ul">
<li>在安装后,ingress controller会试图获取一个external IP,用来让集群外部流量能够到达ingress controller</li>
<li>由于我们的vagrant无法获取外部的IP,所以上述策略无法成功</li>
<li>我们为了完成工作,让ingress controller通过port forwarding的方式来工作.</li>
<li>但是当前ingress controller当前使用的是random port,我们需要额外的配置来让ingress在固定port工作</li>
<li><p>
为了能够做额外的配置,我们需要使用kubectl patch命令
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/etc/kubernetes/components# kubectl patch -n ingress-nginx service/ingress-nginx-controller --patch-file ingress-patch.yaml
service/ingress-nginx-controller patched
</pre>
</div></li>
<li><p>
其中的YAML内容如下,这个YAML文件指定了用来port forwarding的port number
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #a89984;">---</span>
<span style="color: #076678;">apiVersion</span>: v1
<span style="color: #076678;">kind</span>: Service
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: ingress-nginx-controller
  <span style="color: #076678;">namespace</span>: ingress-nginx
<span style="color: #076678;">spec</span>:
  <span style="color: #076678;">ports</span>:
    - <span style="color: #076678;">port</span>: 80
      <span style="color: #076678;">nodePort</span>: 80
    - <span style="color: #076678;">port</span>: 443
      <span style="color: #076678;">nodePort</span>: 443
<span style="color: #a89984;">---</span>
<span style="color: #076678;">apiVersion</span>: networking.k8s.io/v1
<span style="color: #076678;">kind</span>: IngressClass
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: nginx
  <span style="color: #076678;">namespace</span>: ingress-nginx
  <span style="color: #076678;">annotations</span>:
    <span style="color: #076678;">ingressclass.kubernetes.io/is-default-class</span>: <span style="color: #79740e;">"true"</span>
</pre>
</div></li>
<li><p>
最后,我们还需要apply annotaion,示例如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/etc/kubernetes/components# kubectl annotate -n ingress-nginx ingressclass/nginx ingressclass.kubernetes.io/is-default-class=<span style="color: #79740e;">"true"</span>
ingressclass.networking.k8s.io/nginx annotated
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org96531ce" class="outline-4">
<h4 id="org96531ce"><span class="section-number-4">6.3.4.</span> Metrics Server</h4>
<div class="outline-text-4" id="text-6-3-4">
<ul class="org-ul">
<li>我们最后需要安装的add-on是metrics server</li>
<li>metrics server对于k8s集群来说非常重要,原因在于只有了解metric,才能做到autoscaling</li>
<li>从安全方面的考虑,我们要使用HTTP/S来访问kubelet,并且需要从controller manager获取certificate</li>
<li><p>
setup的过程当中, kubelet会创建对每个node的certificate request,但是这个request部署自动被批准的,我们可以使用如下方法查看这些request
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/etc/kubernetes/components# kubectl get csr
NAME        AGE     SIGNERNAME                      REQUESTOR            REQUESTEDDURATION   CONDITION
csr-2664h   84m     kubernetes.io/kubelet-serving   system:node:host03   &lt;none&gt;              Pending
csr-26777   14h     kubernetes.io/kubelet-serving   system:node:host01   &lt;none&gt;              Pending
csr-2tw29   14h     kubernetes.io/kubelet-serving   system:node:host03   &lt;none&gt;              Pending
csr-54vkx   13s     kubernetes.io/kubelet-serving   system:node:host01   &lt;none&gt;              Pending
csr-54wsj   69m     kubernetes.io/kubelet-serving   system:node:host03   &lt;none&gt;              Pending
csr-64k6t   87m     kubernetes.io/kubelet-serving   system:node:host02   &lt;none&gt;              Pending
csr-6bwm7   41m     kubernetes.io/kubelet-serving   system:node:host02   &lt;none&gt;              Pending
csr-6s2r2   53m     kubernetes.io/kubelet-serving   system:node:host03   &lt;none&gt;              Pending
csr-6z4c6   14h     kubernetes.io/kubelet-serving   system:node:host02   &lt;none&gt;              Pending
csr-78t7b   68m     kubernetes.io/kubelet-serving   system:node:host04   &lt;none&gt;              Pending
csr-7wv9t   99m     kubernetes.io/kubelet-serving   system:node:host03   &lt;none&gt;              Pending
csr-82wx8   31m     kubernetes.io/kubelet-serving   system:node:host01   &lt;none&gt;              Pending
csr-8646x   10m     kubernetes.io/kubelet-serving   system:node:host02   &lt;none&gt;              Pending
csr-8w8zf   14h     kubernetes.io/kubelet-serving   system:node:host01   &lt;none&gt;              Pending
csr-9bvfm   22m     kubernetes.io/kubelet-serving   system:node:host03   &lt;none&gt;              Pending
csr-bnlr9   83m     kubernetes.io/kubelet-serving   system:node:host04   &lt;none&gt;              Pending
csr-dkmz6   14h     kubernetes.io/kubelet-serving   system:node:host04   &lt;none&gt;              Pending
csr-fgkw8   107m    kubernetes.io/kubelet-serving   system:node:host01   &lt;none&gt;              Pending
csr-fmk9d   38m     kubernetes.io/kubelet-serving   system:node:host03   &lt;none&gt;              Pending
csr-fxp8f   52m     kubernetes.io/kubelet-serving   system:node:host04   &lt;none&gt;              Pending
csr-gjnqg   98m     kubernetes.io/kubelet-serving   system:node:host04   &lt;none&gt;              Pending
csr-hhvwn   21m     kubernetes.io/kubelet-serving   system:node:host04   &lt;none&gt;              Pending
csr-hzk4n   25m     kubernetes.io/kubelet-serving   system:node:host02   &lt;none&gt;              Pending
csr-jxhz9   77m     kubernetes.io/kubelet-serving   system:node:host01   &lt;none&gt;              Pending
csr-jzwvm   6m21s   kubernetes.io/kubelet-serving   system:node:host04   &lt;none&gt;              Pending
csr-kqh4w   72m     kubernetes.io/kubelet-serving   system:node:host02   &lt;none&gt;              Pending
csr-lmjqf   46m     kubernetes.io/kubelet-serving   system:node:host01   &lt;none&gt;              Pending
csr-n2jct   56m     kubernetes.io/kubelet-serving   system:node:host02   &lt;none&gt;              Pending
csr-n827k   37m     kubernetes.io/kubelet-serving   system:node:host04   &lt;none&gt;              Pending
csr-nccpt   7m22s   kubernetes.io/kubelet-serving   system:node:host03   &lt;none&gt;              Pending
csr-rj6vk   62m     kubernetes.io/kubelet-serving   system:node:host01   &lt;none&gt;              Pending
csr-tdsgp   92m     kubernetes.io/kubelet-serving   system:node:host01   &lt;none&gt;              Pending
csr-tnfhw   14h     kubernetes.io/kubelet-serving   system:node:host02   &lt;none&gt;              Pending
csr-wfjmn   15m     kubernetes.io/kubelet-serving   system:node:host01   &lt;none&gt;              Pending
csr-wnf65   102m    kubernetes.io/kubelet-serving   system:node:host02   &lt;none&gt;              Pending
</pre>
</div></li>
<li>注意,这里的certificate request主要是kubelet-serving的,而不是bootstrap阶段kubelet来验证API server的</li>
<li><p>
我们使用如下命令来一次性approve这些certificate request
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/etc/kubernetes/components# kubectl certificate approve $<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">kubectl get csr --field-selector spec.signerName=kubernetes.io/kubelet-serving -o name</span><span style="color: #076678;">)</span>
certificatesigningrequest.certificates.k8s.io/csr-2664h approved
certificatesigningrequest.certificates.k8s.io/csr-26777 approved
certificatesigningrequest.certificates.k8s.io/csr-2tw29 approved
certificatesigningrequest.certificates.k8s.io/csr-54vkx approved
certificatesigningrequest.certificates.k8s.io/csr-54wsj approved
certificatesigningrequest.certificates.k8s.io/csr-64k6t approved
certificatesigningrequest.certificates.k8s.io/csr-6bwm7 approved
certificatesigningrequest.certificates.k8s.io/csr-6s2r2 approved
certificatesigningrequest.certificates.k8s.io/csr-6z4c6 approved
certificatesigningrequest.certificates.k8s.io/csr-78t7b approved
certificatesigningrequest.certificates.k8s.io/csr-7wv9t approved
certificatesigningrequest.certificates.k8s.io/csr-82wx8 approved
certificatesigningrequest.certificates.k8s.io/csr-8646x approved
certificatesigningrequest.certificates.k8s.io/csr-8w8zf approved
certificatesigningrequest.certificates.k8s.io/csr-9bvfm approved
certificatesigningrequest.certificates.k8s.io/csr-bnlr9 approved
certificatesigningrequest.certificates.k8s.io/csr-dkmz6 approved
certificatesigningrequest.certificates.k8s.io/csr-fgkw8 approved
certificatesigningrequest.certificates.k8s.io/csr-fmk9d approved
certificatesigningrequest.certificates.k8s.io/csr-fxp8f approved
certificatesigningrequest.certificates.k8s.io/csr-gjnqg approved
certificatesigningrequest.certificates.k8s.io/csr-hhvwn approved
certificatesigningrequest.certificates.k8s.io/csr-hzk4n approved
certificatesigningrequest.certificates.k8s.io/csr-jxhz9 approved
certificatesigningrequest.certificates.k8s.io/csr-jzwvm approved
certificatesigningrequest.certificates.k8s.io/csr-kqh4w approved
certificatesigningrequest.certificates.k8s.io/csr-lmjqf approved
certificatesigningrequest.certificates.k8s.io/csr-n2jct approved
certificatesigningrequest.certificates.k8s.io/csr-n827k approved
certificatesigningrequest.certificates.k8s.io/csr-nccpt approved
certificatesigningrequest.certificates.k8s.io/csr-rj6vk approved
certificatesigningrequest.certificates.k8s.io/csr-tdsgp approved
certificatesigningrequest.certificates.k8s.io/csr-tnfhw approved
certificatesigningrequest.certificates.k8s.io/csr-wfjmn approved
certificatesigningrequest.certificates.k8s.io/csr-wnf65 approved
</pre>
</div></li>
<li><p>
metric server的安装过程和前面类似,先下载YAML,然后apply
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/etc/kubernetes/components# echo $<span style="color: #076678;">metrics_url</span>
https://github.com/kubernetes-sigs/metrics-server/releases/download/metrics-server-helm-chart-3.8.2/components.yaml
root@host01:/etc/kubernetes/components# curl -Lo metrics-server.yaml $<span style="color: #076678;">metrics_url</span>
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  4181  100  4181    0     0   2293      0  0:00:01  0:00:01 --:--:-- 10172
root@host01:/etc/kubernetes/components# kubectl apply -f metrics-server.yaml
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
root@host01:/etc/kubernetes/components# cd
root@host01: ~&#7;root@host01:~#
</pre>
</div></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org4dc6661" class="outline-3">
<h3 id="org4dc6661"><span class="section-number-3">6.4.</span> Exploring a Cluster</h3>
<div class="outline-text-3" id="text-6-4">
<ul class="org-ul">
<li>我们的集群到现在就部署完毕了,我们在创建第一个application之前,先看看我们的集群到底有哪些内容</li>
<li><p>
我们使用crictl来查看container的情况
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# crictl ps
CONTAINER           IMAGE               CREATED             STATE               NAME                       ATTEMPT             POD ID
c826101ae78e7       92291586da370       9 minutes ago       Running             replica-manager            0                   4eb3f16d182c8
eb8e988a2ab28       92291586da370       9 minutes ago       Running             engine-manager             0                   66a114856e8bb
cfdd023cea165       f0c119aae83c5       12 minutes ago      Running             engine-image-ei-4dbdb778   0                   d36b5f7c9369a
77797f990d552       6561caaf4d7e6       15 minutes ago      Running             longhorn-manager           0                   4ee7777358b43
bce6f427c0c9e       30308f97f742f       2 hours ago         Running             calico-node                0                   9615157305275
fd778efd475e0       25444908517a5       15 hours ago        Running             kube-controller-manager    1                   d6021f6ae1100
bbcbd4243c19c       aceacb6244f9f       15 hours ago        Running             kube-scheduler             1                   615dadf661d25
037d6fa6b0b3b       2114245ec4d6b       15 hours ago        Running             kube-proxy                 0                   3c7d753242bea
8050c0b63d6ea       ad59e82b53c12       15 hours ago        Running             kube-vip                   0                   250317f40657c
bb046989ce34b       25f8c7f3da61c       15 hours ago        Running             etcd                       0                   165f31d8e452f
4130fe0f6356f       62930710c9634       15 hours ago        Running             kube-apiserver             0                   f4f8653f588fb
</pre>
</div></li>
<li><p>
从容器的维度看,有些杂乱. 所幸kubectl可以从不同的namespace查看(注意!如果不使用-n指定namespace,那么
kubectl查看的是default namespace),我们使用如下命令来查看有哪些namespace
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kubectl get namespaces
NAME              STATUS   AGE
calico-system     Active   14h
default           Active   14h
ingress-nginx     Active   15m
kube-node-lease   Active   14h
kube-public       Active   14h
kube-system       Active   14h
longhorn-system   Active   25m
tigera-operator   Active   14h
</pre>
</div></li>
<li><p>
我们查看当前的default namespace(不使用-n就是指default namespace)有哪些pod
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kubectl get pods
No resources found<span style="color: #9d0006;"> in</span> default namespace.
</pre>
</div></li>
<li><p>
k8s主要的系统级别的pod都在kube-system namespace
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kubectl -n kube-system get pods
NAME                              READY   STATUS    RESTARTS      AGE
coredns-64897985d-95fgf           1/1     Running   0             14h
coredns-64897985d-pfp92           1/1     Running   0             14h
etcd-host01                       1/1     Running   0             14h
etcd-host02                       1/1     Running   0             14h
etcd-host03                       1/1     Running   0             14h
kube-apiserver-host01             1/1     Running   0             14h
kube-apiserver-host02             1/1     Running   0             14h
kube-apiserver-host03             1/1     Running   0             14h
kube-controller-manager-host01    1/1     Running   1 <span style="color: #076678;">(</span>14h ago<span style="color: #076678;">)</span>   14h
kube-controller-manager-host02    1/1     Running   0             14h
kube-controller-manager-host03    1/1     Running   0             14h
kube-proxy-6gt6m                  1/1     Running   0             14h
kube-proxy-bw676                  1/1     Running   0             14h
kube-proxy-cpqwh                  1/1     Running   0             14h
kube-proxy-rmwth                  1/1     Running   0             14h
kube-scheduler-host01             1/1     Running   1 <span style="color: #076678;">(</span>14h ago<span style="color: #076678;">)</span>   14h
kube-scheduler-host02             1/1     Running   0             14h
kube-scheduler-host03             1/1     Running   0             14h
kube-vip-host01                   1/1     Running   0             14h
kube-vip-host02                   1/1     Running   0             14h
kube-vip-host03                   1/1     Running   0             14h
metrics-server-847dcc659d-f2btj   1/1     Running   0             6m31s
</pre>
</div></li>
<li><p>
上面的每个control plane我们在第11章都会介绍,这里先看其中一个pod: host01上的API server,我们可以使用
kubectl describe来列出其中的信息
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kubectl -n kube-system describe pod kube-apiserver-host01
Name:                 kube-apiserver-host01
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 host01/192.168.61.11
Start Time:           Mon, 24 Jul 2023 12:16:28 +0000
Labels:               <span style="color: #076678;">component</span>=kube-apiserver
                      <span style="color: #076678;">tier</span>=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.61.11:6443
                      kubernetes.io/config.hash: 645997a588449d51a45905e226ef55ae
                      kubernetes.io/config.mirror: 645997a588449d51a45905e226ef55ae
                      kubernetes.io/config.seen: 2023-07-24T12:16:27.911452688Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   192.168.61.11
IPs:
  IP:           192.168.61.11
Controlled By:  Node/host01
Containers:
  kube-apiserver:
    Container ID:  containerd://4130fe0f6356fdcc68f835acaf59277241822638f65cf0c88c5dfa6ec7e17162
    Image:         k8s.gcr.io/kube-apiserver:v1.23.4
    Image ID:      k8s.gcr.io/kube-apiserver@sha256:d129cd825d6de9c61cfa0be3f85a630e8a9094e929c3718db6b7fd00957ff5b5
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    ...
</pre>
</div></li>
<li><p>
我们还可以查看Calico的pod,如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kubectl -n calico-system get pods
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6c4c678f97-smxcr   1/1     Running   0          14h
calico-node-8wq6m                          0/1     Running   0          14h
calico-node-c7rmj                          0/1     Running   0          14h
calico-node-j4dv7                          1/1     Running   0          14h
calico-node-n4jz9                          1/1     Running   0          14h
calico-typha-675545d5d9-lf8f2              1/1     Running   0          14h
calico-typha-675545d5d9-zh7mv              1/1     Running   0          14h
</pre>
</div></li>
<li><p>
还可以看下longhorn的pod
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kubectl -n longhorn-system get pods
NAME                                        READY   STATUS    RESTARTS      AGE
csi-attacher-6454556647-255xl               1/1     Running   0             6m39s
csi-attacher-6454556647-hmgj2               1/1     Running   0             6m39s
csi-attacher-6454556647-x98w5               1/1     Running   0             6m39s
csi-provisioner-869bdc4b79-2cjzj            1/1     Running   0             6m39s
csi-provisioner-869bdc4b79-85s5x            1/1     Running   0             6m39s
csi-provisioner-869bdc4b79-dzzsn            1/1     Running   0             6m39s
csi-resizer-6d8cf5f99f-cwlg5                1/1     Running   0             6m38s
csi-resizer-6d8cf5f99f-lcmjv                1/1     Running   0             6m38s
csi-resizer-6d8cf5f99f-z6x8n                1/1     Running   0             6m38s
csi-snapshotter-588457fcdf-7zvd5            1/1     Running   0             6m37s
csi-snapshotter-588457fcdf-c942q            1/1     Running   0             6m37s
csi-snapshotter-588457fcdf-tnmpr            1/1     Running   0             6m37s
engine-image-ei-4dbdb778-bxmmt              1/1     Running   0             24m
engine-image-ei-4dbdb778-cprpt              1/1     Running   0             24m
engine-image-ei-4dbdb778-vdxxs              1/1     Running   0             24m
engine-image-ei-4dbdb778-whl9d              1/1     Running   0             24m
instance-manager-e-0742a57f                 1/1     Running   0             22m
instance-manager-e-17c56f4b                 1/1     Running   0             22m
instance-manager-e-7d5af3e1                 1/1     Running   0             24m
instance-manager-e-d4ce7497                 1/1     Running   0             24m
instance-manager-r-0299105f                 1/1     Running   0             24m
instance-manager-r-0c88e64e                 1/1     Running   0             24m
instance-manager-r-d10dfcb6                 1/1     Running   0             22m
instance-manager-r-d5bb8b00                 1/1     Running   0             22m
longhorn-csi-plugin-7pzfv                   2/2     Running   0             6m37s
longhorn-csi-plugin-gz7hx                   2/2     Running   0             6m37s
longhorn-csi-plugin-nd22x                   2/2     Running   0             6m37s
longhorn-csi-plugin-nx8mw                   2/2     Running   0             6m37s
longhorn-driver-deployer-7c85dc8c69-jtg22   1/1     Running   8 <span style="color: #076678;">(</span>11m ago<span style="color: #076678;">)</span>   27m
longhorn-manager-bzxkg                      1/1     Running   0             28m
longhorn-manager-jnjb5                      1/1     Running   0             28m
longhorn-manager-m422f                      1/1     Running   0             28m
longhorn-manager-zn4s9                      1/1     Running   0             28m
longhorn-ui-6dcd69998-7njs6                 1/1     Running   0             28m
</pre>
</div></li>
<li>花费了如此巨大的经历创建了集群,我们可以开始创建一个application容器啦:
<ul class="org-ul">
<li><p>
命令如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kubectl run nginx --image=nginx
pod/nginx created
root@host01:~# kubectl get pods -o wide
NAME    READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
nginx   0/1     ContainerCreating   0          8s    &lt;none&gt;   host04   &lt;none&gt;           &lt;none&gt;
</pre>
</div></li>
<li>上面的命令看最终是kubectl会根据我们的image和name来创建Pod,并且分配cluster的资源给这个Pod,让他运行</li>
</ul></li>
<li>我们可以使用如下命令来inspect这个pod
<ul class="org-ul">
<li><p>
命令如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kubectl get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          84s   172.31.15.83   host04   &lt;none&gt;           &lt;none&gt;
</pre>
</div></li>
<li>上面的-o wide是让我们打印Pod额外的信息:
<ol class="org-ol">
<li>比如IP,注意IP是和Pod绑定的,即便Pod内部的container重启,也不影响container外部的Pod IP. 这里的IP
是来自Pod CIDR,由Calico自动赋值的</li>
<li>比如Node, 这里我们的容器是被schedule到了host04</li>
</ol></li>
<li><p>
Calico同时能够让我们从外部访问Pod(calico浮在routing traffic), 比如我们的常规ping是成功的
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# ping -c1 172.31.15.83
PING 172.31.15.83 <span style="color: #076678;">(</span>172.31.15.83<span style="color: #076678;">)</span> 56<span style="color: #076678;">(</span>84<span style="color: #076678;">)</span> bytes of data.
64 bytes from 172.31.15.83: <span style="color: #076678;">icmp_seq</span>=1 <span style="color: #076678;">ttl</span>=63 <span style="color: #076678;">time</span>=0.439 ms

--- 172.31.15.83 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.439/0.439/0.439/0.000 ms
</pre>
</div></li>
<li><p>
然后我们可以通过如下示例确认nginx web server也是可以工作的
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# curl http://172.31.15.83
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
...
&lt;/html&gt;
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgebd6dfe" class="outline-2">
<h2 id="orgebd6dfe"><span class="section-number-2">7.</span> Chapter 7: Deploying Containers to Kubernetes</h2>
<div class="outline-text-2" id="text-7">
<ul class="org-ul">
<li>不同的container拥有不同的use case:
<ul class="org-ul">
<li>有些需要完全相同的instance来在load balancer后面提供可自动伸缩的服务</li>
<li>有些只运行one-time command(只需要运行一次的命令)</li>
<li>还有些需要按照固定的顺序运行</li>
</ul></li>
<li>k8s应对这些不同的use case的办法是位不同use case提供不同的controller resource type</li>
<li>我们首先看一下最新的controller resource type: Pod</li>
</ul>
</div>
<div id="outline-container-org2ba17ce" class="outline-3">
<h3 id="org2ba17ce"><span class="section-number-3">7.1.</span> Pods</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>每个Pod里面有一个或者多个container</li>
<li>Linux kernel namespace有些是在Pod level应用的,有些是在container level应用的,具体为:
<ul class="org-ul">
<li>mnt: 每个container有自己的/root, 其他的mounts都是倍Pod内部的所有container所共享的</li>
<li>uts: Unix time, 隔离应用在Pod level</li>
<li>ipc: 进程间通信, 隔离在Pod level</li>
<li>pid: 进程标志, 隔离在container level</li>
<li>net: 网络, 隔离在Pod level</li>
</ul></li>
<li>这种隔离的设置让k8s一个Pod内的容器可以像一个vm里面的process一样使用localhost进行通信,同时又能使用
不同的container image</li>
</ul>
</div>
<div id="outline-container-orgfdf3f18" class="outline-4">
<h4 id="orgfdf3f18"><span class="section-number-4">7.1.1.</span> Deploying a Pod</h4>
<div class="outline-text-4" id="text-7-1-1">
<ul class="org-ul">
<li>我们首先来直接创建Pod, 和之前章节不同,之前我们使用kubectl run加命令行参数,而这里我们使用YAML文件来
存储配置参数信息</li>
<li>使用配置文件能够配置更多的pod属性,也是绝大多数controller创建pod的方式</li>
<li>下面的例子是我们在vagrant中配置的k8s集群,这个集群:
<ul class="org-ul">
<li>总共有三台机器</li>
<li>三台机器全部运行control plane</li>
<li>不再配置worker机器,所有容器还是运行在这三台机器上(也就是说control plane机器也运行容器,这在production
上面是不推荐的)</li>
</ul></li>
<li><p>
本章例子如下遇到如下问题,需要初始化.kube/config文件
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/opt# kubectl get pods
The connection to the server localhost:8080 was refused - did you specify the right host or port?
root@host01:~# cd ~/.kube/
root@host01:~/.kube# ls
cache
root@host01:~/.kube#  cp -i /etc/kubernetes/admin.conf $<span style="color: #076678;">HOME</span>/.kube/config
root@host01:~/.kube# kubectl get pods
No resources found<span style="color: #9d0006;"> in</span> default namespace.
</pre>
</div></li>
<li>我们的ansible脚本还创建了本文所需要的YAML文件:
<ul class="org-ul">
<li><p>
第一个例子是创建一个nginx容器,YAML文件(/opt/nginx-pod.yaml)如下
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #a89984;">---</span>
<span style="color: #076678;">apiVersion</span>: v1
<span style="color: #076678;">kind</span>: Pod
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: nginx
<span style="color: #076678;">spec</span>:
  <span style="color: #076678;">containers</span>:
  - <span style="color: #076678;">name</span>: nginx
    <span style="color: #076678;">image</span>: nginx
</pre>
</div></li>
<li>apiVersion是v1,因为pod从最开始版本(v1)就存在</li>
<li>kind写成了Pod,说明我们这里的controller是Pod</li>
<li>metadata有很多成员的,我们这里只设置了其中一个成员:name</li>
<li>metadata另外比较知名的成员是namespace,这里没有设置,那么默认就是default</li>
<li>spec是提供其他具体信息给k8s,这里也有很多成员,我们只设置了一个成员containers: containers就是设置pod
中运行的一系列容器(container是复数), 这里我们只运行一个容器,那么就列出这个容器的名字和image</li>
</ul></li>
<li>下面我们把这个YAML配置的容器放入集群
<ul class="org-ul">
<li><p>
命令如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kubectl apply -f /opt/nginx-pod.yaml
pod/nginx created
</pre>
</div></li>
<li><p>
我们可以看到运行效果,注意,可能要经过一段时间才显示Running. 如果不想一直敲击这个命令,可以在命令前加watch
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kubectl get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          2m19s   172.31.25.202   host03   &lt;none&gt;           &lt;none&gt;
</pre>
</div></li>
<li>我们的-o wide参数让我们能看到Node和IP信息</li>
</ul></li>
<li>这个例子里面Pod被调度到了host03,如果我们想从Pod维度来查看信息,那么我们要到host03来查看
<ul class="org-ul">
<li><p>
在host03我们使用如下命令来查看pod的信息
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host03:~# crictl pods --name nginx
POD ID              CREATED             STATE               NAME                NAMESPACE           ATTEMPT             RUNTIME
86e42d40d21a5       5 minutes ago       Ready               nginx               default             0                   <span style="color: #076678;">(</span>default<span style="color: #076678;">)</span>
</pre>
</div></li>
<li><p>
由于只有一个nginx的pod,那么我们可以使用如下命令来获取这个pod的id
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host03:~# <span style="color: #076678;">POD_ID</span>=$<span style="color: #076678;">(</span><span style="color: #076678; font-weight: bold;">crictl pods -q --name nginx</span><span style="color: #076678;">)</span>
root@host03:~# crictl ps --pod $<span style="color: #076678;">POD_ID</span>
CONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID
a82cc5c33ff6b       89da1fb6dcb96       4 minutes ago       Running             nginx               0                   86e42d40d21a5
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orge734149" class="outline-4">
<h4 id="orge734149"><span class="section-number-4">7.1.2.</span> Pod Details and Logging</h4>
<div class="outline-text-4" id="text-7-1-2">
<ul class="org-ul">
<li>使用crictl来和底层container engine进行通信虽然也很好用,但是却要求在pod所在的,特定的host上面,这在
production环境是不可取的</li>
<li>我们再production环境,一般使用kubectl来inspect我们的pod. 因为kubectl只需要连接cluster的API server就可以了
<ul class="org-ul">
<li><p>
查看pod的信息,我们就kubectl describe pod
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kubectl describe pod nginx
Name:         nginx
Namespace:    default
Priority:     0
Node:         host01/192.168.61.11
Start Time:   Thu, 03 Aug 2023 02:44:02 +0000
Labels:       &lt;none&gt;
Annotations:  cni.projectcalico.org/containerID: ffb2e14723d60e2639069d940a67a7310c639c0e41bf060523daea8b34cce39e
              cni.projectcalico.org/podIP: 172.31.239.193/32
              cni.projectcalico.org/podIPs: 172.31.239.193/32
Status:       Running
IP:           172.31.239.193
IPs:
  IP:  172.31.239.193
Containers:
  nginx:
    Container ID:   containerd://f822c044828027e67dc2b52f410e766f2aa3a1ebce7e1668d8f23f1ae280333a
...
Events:
  Type     Reason                  Age                From               Message
  ----     ------                  ----               ----               -------
  Warning  FailedScheduling        25m                default-scheduler  0/3 nodes are available: 3 node<span style="color: #076678;">(</span>s<span style="color: #076678;">)</span> had taint <span style="color: #076678;">{</span>node.kubernetes.io/not-ready: <span style="color: #076678;">}</span>, that the pod didn<span style="color: #79740e;">'t tolerate.</span>
<span style="color: #79740e;">  Warning  FailedScheduling        24m                default-scheduler  0/3 nodes are available: 3 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn'</span>t tolerate.
  Normal   Scheduled               23m                default-scheduler  Successfully assigned default/nginx to host01
  Warning  FailedCreatePodSandBox  23m                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox <span style="color: #79740e;">"e8c92f00109d9ba422f50b8873e0646ea4433a6c3fb13aa5cdda75c00c0744cd"</span>: plugin <span style="color: #076678;">type</span>=<span style="color: #79740e;">"calico"</span> failed <span style="color: #076678;">(</span>add<span style="color: #076678;">)</span>: stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/
  Normal   SandboxChanged          21m <span style="color: #076678;">(</span>x9 over 23m<span style="color: #076678;">)</span>  kubelet            Pod sandbox changed, it will be killed and re-created.
  Normal   Pulling                 21m                kubelet            Pulling image <span style="color: #79740e;">"nginx"</span>
  Normal   Pulled                  19m                kubelet            Successfully pulled image <span style="color: #79740e;">"nginx"</span><span style="color: #9d0006;"> in</span> 1m38.942683852s
  Normal   Created                 19m                kubelet            Created container nginx
  Normal   Started                 19m                kubelet            Started container nginx
</pre>
</div></li>
<li>注意,这里的namespace是default,本书所有例子都是default,但是实践当中推荐给类似的多个应用同一个namespace</li>
<li>kubectl describe的最后,是一个event log,这里的日志通常会展示我们启动容器时候遇到的问题</li>
<li>通过event log我们看到一般的容器运行的步骤是:
<ol class="org-ol">
<li>需要一个有足够资源的node被schedule到来接收这个容器</li>
<li>一旦某个node被调度到之后,控制权会转换到这个node的kubelet, 这个kubelet会和container engine联系,
让engine来拉取镜像,创建容器,并且启动</li>
<li>一旦启动成功,kubelet就会收集这个标准输入和标准输出</li>
</ol></li>
<li><p>
我们可以使用如下命令来查看kubectl收集的标准输入和标准输出
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kubectl logs nginx
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts<span style="color: #9d0006;"> in</span> /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6<span style="color: #9d0006;"> in</span> /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2023/08/03 02:47:36 <span style="color: #076678;">[</span>notice<span style="color: #076678;">]</span> 1#1: using the <span style="color: #79740e;">"epoll"</span> event method
2023/08/03 02:47:36 <span style="color: #076678;">[</span>notice<span style="color: #076678;">]</span> 1#1: nginx/1.25.1
2023/08/03 02:47:36 <span style="color: #076678;">[</span>notice<span style="color: #076678;">]</span> 1#1: built by gcc 12.2.0 <span style="color: #076678;">(</span>Debian 12.2.0-14<span style="color: #076678;">)</span>
2023/08/03 02:47:36 <span style="color: #076678;">[</span>notice<span style="color: #076678;">]</span> 1#1: OS: Linux 5.4.0-135-generic
2023/08/03 02:47:36 <span style="color: #076678;">[</span>notice<span style="color: #076678;">]</span> 1#1: getrlimit<span style="color: #076678;">(</span>RLIMIT_NOFILE<span style="color: #076678;">)</span>: 1048576:1048576
2023/08/03 02:47:36 <span style="color: #076678;">[</span>notice<span style="color: #076678;">]</span> 1#1: start worker processes
2023/08/03 02:47:36 <span style="color: #076678;">[</span>notice<span style="color: #076678;">]</span> 1#1: start worker process 29
2023/08/03 02:47:36 <span style="color: #076678;">[</span>notice<span style="color: #076678;">]</span> 1#1: start worker process 30
</pre>
</div></li>
<li><p>
最后我们使用如下方法进行删除.当前的例子只有一个pod,但是YAML里面有多个pod的情况也是可以这样删除的
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:~# kubectl delete -f /opt/nginx-pod.yaml
pod <span style="color: #79740e;">"nginx"</span> deleted
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org2ed487b" class="outline-3">
<h3 id="org2ed487b"><span class="section-number-3">7.2.</span> Deployments</h3>
<div class="outline-text-3" id="text-7-2">
<ul class="org-ul">
<li>为了运行容器,我们需要Pod.但是这并不意味着我们每次都要直接创建Pod</li>
<li>因为直接创建一个Pod的话,只能让Pod在一个host运行.不能带来如下特性:
<ul class="org-ul">
<li>scalability: 我们无法根据流量来自动在新的host上创建新的Pod来迎接新的流量</li>
<li>failover: 我们无法某个host挂掉的情况下,重新把这个host上的Pod重新调度到其他机器上</li>
</ul></li>
<li>为了能够做到scalability和failover,我们需要创建controller来管理pod.本节介绍这些controller里的第一种:
Deployment</li>
</ul>
</div>
<div id="outline-container-org19500d5" class="outline-4">
<h4 id="org19500d5"><span class="section-number-4">7.2.1.</span> Creating a Deployment</h4>
<div class="outline-text-4" id="text-7-2-1">
<ul class="org-ul">
<li>Deployment是管理一个或者多个"完全相同的k8s Pod"的controller</li>
<li>当我们给Deployment提供了Pod template之后, Deployment会在ReplicaSet的帮助下创建多个Pod</li>
<li>这里我们要额外岔开讲一下ReplicaSet:
<ul class="org-ul">
<li>ReplicaSet主要负责Pod管理,包括:
<ul class="org-ul">
<li>监控Pod状态</li>
<li>在出现问题的情况下重启服务Pod</li>
</ul></li>
<li>ReplicaSet的前身是ReplicationController(这个名字听起来就是一个Controller)</li>
<li>ReplicationController的功能太过复杂,最终把ReplicationController拆成了两个部分进行工作:
<ol class="org-ol">
<li>ReplicaSet</li>
<li>Deployment</li>
</ol></li>
<li>我们使用k8s的时候,往往只会和Deployment打交道,那是因为Deployment创建了自己的ReplicaSet.所以本书中
提到Deployment也有监控Pod的功能,指的,就是Deployment通过ReplicaSet获得的这些能力</li>
<li>Deployment除了从头ReplicaSet获得的能力以外,还可以监控Pod template的变动(configuration改动,或者image
改动),从而做对应改动</li>
</ul></li>
<li>下面我们来看一个Deployment的例子
<ul class="org-ul">
<li><p>
/opt/nginx-deploy.yaml配置文件如下
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #a89984;">---</span>
<span style="color: #076678;">kind</span>: Deployment
<span style="color: #076678;">apiVersion</span>: apps/v1
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: nginx
<span style="color: #076678;">spec</span>:
  <span style="color: #076678;">replicas</span>: 3
  <span style="color: #076678;">selector</span>:
    <span style="color: #076678;">matchLabels</span>:
      <span style="color: #076678;">app</span>: nginx
  <span style="color: #076678;">template</span>:
    <span style="color: #076678;">metadata</span>:
      <span style="color: #076678;">labels</span>:
        <span style="color: #076678;">app</span>: nginx
    <span style="color: #076678;">spec</span>:
      <span style="color: #076678;">containers</span>:
      - <span style="color: #076678;">name</span>: nginx
        <span style="color: #076678;">image</span>: nginx
        <span style="color: #076678;">resources</span>:
          <span style="color: #076678;">requests</span>:
            <span style="color: #076678;">cpu</span>: <span style="color: #79740e;">"100m"</span>
</pre>
</div></li>
<li>由于Deployment在apps的API group,所以我们设置apiVersion为apps/v1</li>
<li>和所有的k8s resource一样,我们要为这个资源提供一个唯一的名字以便和其他Deployment区分</li>
<li>第一个spec内部有很多的设置:
<ol class="org-ol">
<li>replicas用来告诉Deployment要创建几个相同的pod</li>
<li>selector用来让Deployment来找到自己的pod,这里用了matchLabels,那就必须全部字符相同才能找到</li>
</ol></li>
<li>我们会发现下面还有一个spec,这个是template的spec. 为什么template里面还有spec呢? 因为template的定义,
是这个Deployment创建的N个相同的pod所需要的配置信息. 我们知道Pod的定义里面也有spec,那么template下面
有spec就是非常正常的了.</li>
<li><p>
我们这里贴一下nginx-pod.yaml的信息
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #a89984;">---</span>
<span style="color: #076678;">apiVersion</span>: v1
<span style="color: #076678;">kind</span>: Pod
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: nginx
<span style="color: #076678;">spec</span>:
  <span style="color: #076678;">containers</span>:
  - <span style="color: #076678;">name</span>: nginx
    <span style="color: #076678;">image</span>: nginx
</pre>
</div></li>
<li>我们发现,除了新增的resources区域以外,其他的部分是相同的</li>
</ul></li>
<li><p>
我们下面来创建这个depoyment
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl apply -f /opt/nginx-deploy.yaml
deployment.apps/nginx created
</pre>
</div></li>
<li><p>
我们可以使用kubectl get来获取Deployment的信息
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl get deployment nginx
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
nginx   3/3     3            3           3m3s
</pre>
</div></li>
<li><p>
上面的READY 3/3就是说,计划有3个replica,当前确实有3个replica,我们可以使用get pods来确认一下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-5f4ff46877-kgzng   1/1     Running   0          3m6s
nginx-5f4ff46877-tppsr   1/1     Running   0          3m6s
nginx-5f4ff46877-x7c28   1/1     Running   0          3m6s
</pre>
</div></li>
<li><p>
上面属于一个Deployment的不同Pod的名字:后面明显是random字符串. 前半部分是一样的,其实是对应的ReplicaSet
的名字,我们下面来验证一下,通过get replicasets获取ReplicaSet的情况
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl get replicasets
NAME               DESIRED   CURRENT   READY   AGE
nginx-5f4ff46877   3         3         3       3m19s
</pre>
</div></li>
<li>我们可以看到,前缀都是nginx-5f4ff46877,也就是ReplicaSet的名字</li>
<li>注意,我们并没有在Deployment里面设置ReplicaSet的信息,ReplicaSet是自动和Deployment一同创建的. 了解这
个非常重要,因为有些Pod的错误不会再Deployment的日志里面,但是却会在ReplicaSet的日志里面</li>
<li>nginx prefix出现在了ReplicaSet和Pod的name里面,但是Deployment缺不是依靠name来match它的Pod的,Deployment
是依靠selector来match Pod(也就是某个deployment的selector必须和其Pod的Lables能对的上):
<ul class="org-ul">
<li><p>
我们可以通过desscribe任意一个pod来查看器Lables
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl describe pod nginx-5f4ff46877-kgzng
Name:         nginx-5f4ff46877-kgzng
Namespace:    default
Priority:     0
Node:         host01/192.168.61.11
Start Time:   Fri, 22 Sep 2023 07:11:50 +0000
Labels:       <span style="color: #076678;">app</span>=nginx
...
</pre>
</div></li>
<li><p>
我们在Deployment的selector里面也能看到app=nginx
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl describe deployment nginx
Name:                   nginx
Namespace:              default
CreationTimestamp:      Fri, 22 Sep 2023 07:11:50 +0000
Labels:                 &lt;none&gt;
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               <span style="color: #076678;">app</span>=nginx
...
</pre>
</div></li>
<li><p>
既然Pod和ReplicaSet都是依靠Lables来组合起来的,那么我们必然也可以使用查询某些Label的方法来获取信息,
如下. 注意,由于是match label,所以Pod和ReplicaSet都被显示出来了.
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl get all -l <span style="color: #076678;">app</span>=nginx
NAME                         READY   STATUS    RESTARTS   AGE
pod/nginx-5f4ff46877-kgzng   1/1     Running   0          5m31s
pod/nginx-5f4ff46877-tppsr   1/1     Running   0          5m31s
pod/nginx-5f4ff46877-x7c28   1/1     Running   0          5m31s

NAME                               DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-5f4ff46877   3         3         3       5m31s
</pre>
</div></li>
<li>让Deployment使用selector来追踪自己的Pod,而不是直接记录自己的所有创建的Pod,是k8s刻意为之,这是因为:
<ol class="org-ol">
<li>k8s的node在任何时刻都有可能因为网络原因而offline</li>
<li>在某个Pod offline的情况下,k8s要额外启动一个Pod</li>
<li>当某个Pod重新online的情况下,k8s还要删除这个Pod</li>
<li>使用selector的好处就是Deployment不用维护自己曾经创建过多少Pod,只要保证当前有需要的N个Pod就可以,
少了就补充,多了就删除</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org2f8c3b8" class="outline-4">
<h4 id="org2f8c3b8"><span class="section-number-4">7.2.2.</span> Monitoring and Scaling</h4>
<div class="outline-text-4" id="text-7-2-2">
<ul class="org-ul">
<li><p>
由于Deployment会监控Pod来确认正确的replica个数的Pod在运行,所以我们随机删除一个Pod会自动创建一个新的Pod
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl delete pod nginx-5f4ff46877-kgzng
pod <span style="color: #79740e;">"nginx-5f4ff46877-kgzng"</span> deleted
root@host01:/home/vagrant# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-5f4ff46877-tppsr   1/1     Running   0          6m5s
nginx-5f4ff46877-x7c28   1/1     Running   0          6m5s
nginx-5f4ff46877-zqpnc   1/1     Running   0          6s
</pre>
</div></li>
<li>同样的,我们更改了replica的数目,Pod也会根据新的replica数目而新增或者减少
<ul class="org-ul">
<li><p>
例子如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl scale --replicas=4 deployment nginx
deployment.apps/nginx scaled
root@host01:/home/vagrant# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-5f4ff46877-jql4d   1/1     Running   0          4s
nginx-5f4ff46877-tppsr   1/1     Running   0          6m27s
nginx-5f4ff46877-x7c28   1/1     Running   0          6m27s
nginx-5f4ff46877-zqpnc   1/1     Running   0          28s
</pre>
</div></li>
<li>我们这里是使用kubectl scale来提升replica的,也更改YAML文件并且重新运行kubectl apply</li>
<li><p>
我们可以看到我们scale了replica之后,k8s自动的更新了Pod的数目,上面的例子是增加,下面的例子是减少
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl scale --replicas=2 deployment nginx
deployment.apps/nginx scaled
root@host01:/home/vagrant# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-5f4ff46877-tppsr   1/1     Running   0          6m56s
nginx-5f4ff46877-x7c28   1/1     Running   0          6m56s
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org58eb59d" class="outline-4">
<h4 id="org58eb59d"><span class="section-number-4">7.2.3.</span> Autoscaling</h4>
<div class="outline-text-4" id="text-7-2-3">
<ul class="org-ul">
<li>上一节已经介绍了如何手动的提升或者降低replica,但是在实际工作当中,我们不可能人工的在高峰时候提升
replica,在低谷的时候降低replica. 这样太复杂了.</li>
<li><p>
一个可行的自动化的方法是自动监控,并且autoscale,在k8s里面有这样一类的cluster来做这个工作,这种cluster
叫做HorizontalPodAutoscaler
</p>
<pre class="example" id="orgd039260">
这里的"水平"是指的"相同类型的Pod", 我们在云服务中经常讲的水平扩展也是指的扩展的服务器是相同规格的
</pre></li>
<li>配置autoscale的方法是创建一种新的resource(会refer我们的Deployment)来配置Deployment如何scale
<ul class="org-ul">
<li>可以使用kubectl atuoscale, 但是这个不太直观,而且不能版本管理</li>
<li>更多的时候,在YAML文件里面配置相应的部分</li>
</ul></li>
<li>下面是一个增加了autoscale配置的HorizontalPodAutoscaler的resource文件
<ul class="org-ul">
<li><p>
代码如下(/opt/nginx-scaler.yaml):
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #a89984;">---</span>
<span style="color: #076678;">apiVersion</span>: autoscaling/v2
<span style="color: #076678;">kind</span>: HorizontalPodAutoscaler
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: nginx
  <span style="color: #076678;">labels</span>:
    <span style="color: #076678;">app</span>: nginx
<span style="color: #076678;">spec</span>:
  <span style="color: #076678;">scaleTargetRef</span>:
    <span style="color: #076678;">apiVersion</span>: apps/v1
    <span style="color: #076678;">kind</span>: Deployment
    <span style="color: #076678;">name</span>: nginx
  <span style="color: #076678;">minReplicas</span>: 1
  <span style="color: #076678;">maxReplicas</span>: 10
  <span style="color: #076678;">metrics</span>:
  - <span style="color: #076678;">type</span>: Resource
    <span style="color: #076678;">resource</span>:
      <span style="color: #076678;">name</span>: cpu
      <span style="color: #076678;">target</span>:
        <span style="color: #076678;">type</span>: Utilization
        <span style="color: #076678;">averageUtilization</span>: 50
</pre>
</div></li>
<li>我们增加了label:nginx, 前面说了,这种label的方法是k8s用来group一系列应用的常用方法(不管之前是否down过,
只关注眼前,所以不用id). 这个label能让我们在kubectl get的时候把当前这个resource给展现出来</li>
<li>注意这里的apiVersion是autoscaling/v2, 这种版本管理的方法能让k8s在保持向前兼容的情况下,不断开发新功能</li>
<li>我们使用如下三个参数来唯一定位我们要监控的Deployment:
<ol class="org-ol">
<li>apiVersion</li>
<li>kind</li>
<li>name</li>
</ol></li>
<li>我们设置了cpu利用率50%这个线,autoscaler会不停的增加或者减少Pod,让我们的cpu利用率不超过这个线</li>
<li>Pod不是没有边界的增加或者减小,其范围是1-10</li>
</ul></li>
<li><p>
我们使用上述配置文件来创建autoscaler
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl apply -f /opt/nginx-scaler.yaml
horizontalpodautoscaler.autoscaling/nginx created
</pre>
</div></li>
<li>使用如下命令查看是否创建成功
<ul class="org-ul">
<li><p>
代码如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl get hpa
NAME    REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx   Deployment/nginx   0%/50%    1         10        2          23s
</pre>
</div></li>
<li>输出会展示如下内容:
<ol class="org-ol">
<li>atarget reference</li>
<li>当前的cpu利用率/目标cpu利用率</li>
<li>最大Pod数目,最小Pod数目</li>
<li>当前的Pod数目</li>
</ol></li>
<li>我们使用hpa来作为horizontalpodautoscaler的缩写,这是k8s这种21世纪应用贴心的地方</li>
<li>k8s贴心的地方还有其命令可以
<ol class="org-ol">
<li>用单数也可用复数,比如:pod和pods是等价的</li>
<li>用简写来替代全拼,比如:deployment和deploy是等价的</li>
</ol></li>
</ul></li>
<li>autoscaler之所以能够搜集到CPU利用率的信息,在于我们我们在cluster上面安装了metrics server</li>
<li><p>
我们这里并没有使用nginx,所以nginx的cpu使用率一直是0, 经过一段时间后,我们的Deployment就会把Pod减少
到只有一个Pod
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-5f4ff46877-tppsr   1/1     Running   0          17m
</pre>
</div></li>
<li>上面的例子奏效需要一段时间,因为为了不过分频繁的启动和关闭Pod,所有的调度都是有一定时间间隔的(同时,
刚启动的Pod的CPU信息不被使用,防止刚启动还没使用cpu被判断为cpu使用率过低)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org7dcdf09" class="outline-3">
<h3 id="org7dcdf09"><span class="section-number-3">7.3.</span> Other Controllers</h3>
<div class="outline-text-3" id="text-7-3">
<ul class="org-ul">
<li>Deployment是最常用的controller,除此之外经常使用的其他Controller还有:
<ul class="org-ul">
<li>Job</li>
<li>ControJob</li>
<li>StatefullSet</li>
<li>DaemonSet</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org8938394" class="outline-4">
<h4 id="org8938394"><span class="section-number-4">7.3.1.</span> Jobs an CronJobs</h4>
<div class="outline-text-4" id="text-7-3-1">
<ul class="org-ul">
<li>Deployment的特点是永远不间断的运行</li>
<li>但是有时候我们希望这个程序只运行一次(或者定时每天某个时间运行一次或多次).这种情况下的Controller就
叫做Job.两者的对比如下:
<ul class="org-ul">
<li>Deployment确认所有停止运行的container必须restarted</li>
<li>Job则是通过exit code来确认是否运行成功,在exit code不是0(表示失败)的情况下,会restart这个job</li>
</ul></li>
<li><p>
一个Job的定义yaml如下
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #076678;">apiVersion</span>: batch/v1
<span style="color: #076678;">kind</span>: Job
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: sleep
<span style="color: #076678;">spec</span>:
  <span style="color: #076678;">template</span>:
    <span style="color: #076678;">spec</span>:
      <span style="color: #076678;">containers</span>:
      - <span style="color: #076678;">name</span>: sleep
        <span style="color: #076678;">image</span>: busybox
        <span style="color: #076678;">command</span>:
          - <span style="color: #79740e;">"/bin/sleep"</span>
          - <span style="color: #79740e;">"30"</span>
      <span style="color: #076678;">restartPolicy</span>: OnFailureroot@host01:/home/vagrant# kubectl apply -f /opt/sleep-job.yaml
</pre>
</div></li>
<li><p>
我们可以使用如下代码来运行并查看job的运行情况
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl get job
NAME    COMPLETIONS   DURATION   AGE
sleep   0/1           5s         5s
root@host01:/home/vagrant# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-5f4ff46877-tppsr   1/1     Running   0          20m
sleep-s7rww              1/1     Running   0          22s
root@host01:/home/vagrant# kubectl get jobs
NAME    COMPLETIONS   DURATION   AGE
sleep   1/1           40s        42s
root@host01:/home/vagrant# kubectl get pods
NAME                     READY   STATUS      RESTARTS   AGE
nginx-5f4ff46877-tppsr   1/1     Running     0          21m
sleep-s7rww              0/1     Completed   0          48s
</pre>
</div></li>
<li>运行完的pod依然还是存在的,这样我们可以去查看这个pod的log,但是这个pod的状态显示未Completed,所以k8s不
会去试图重启这个pod</li>
<li>CronJob也是一种controller,其和Job的不同,在于CronJob是定时运行的(cron是crontab的缩写)
<ul class="org-ul">
<li><p>
一个cronjob的例子: sleep-cronjob.yaml
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #076678;">apiVersion</span>: batch/v1
<span style="color: #076678;">kind</span>: CronJob
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: sleep
<span style="color: #076678;">spec</span>:
  <span style="color: #076678;">schedule</span>: <span style="color: #79740e;">"0 3 * * *"</span>
  <span style="color: #076678;">jobTemplate</span>:
    <span style="color: #076678;">spec</span>:
      <span style="color: #076678;">template</span>:
        <span style="color: #076678;">spec</span>:
          <span style="color: #076678;">containers</span>:
          - <span style="color: #076678;">name</span>: sleep
            <span style="color: #076678;">image</span>: busybox
            <span style="color: #076678;">command</span>:
              - <span style="color: #79740e;">"/bin/sleep"</span>
              - <span style="color: #79740e;">"30"</span>
          <span style="color: #076678;">restartPolicy</span>: OnFailureroot@host01:/home/vagrant#
</pre>
</div></li>
<li>我们可以看到jobTemplate的内容就是前面的Job controller</li>
<li>和Job controller不同的是,我们这里有schedule部分,用来控制定时任务运行的时间</li>
<li><p>
运行后的例子如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl apply -f /opt/sleep-cronjob.yaml
cronjob.batch/sleep created
root@host01:/home/vagrant# kubectl get jobs
NAME    COMPLETIONS   DURATION   AGE
sleep   1/1           40s        97s
root@host01:/home/vagrant# kubectl get pods
NAME                     READY   STATUS      RESTARTS   AGE
nginx-5f4ff46877-tppsr   1/1     Running     0          24m
sleep-s7rww              0/1     Completed   0          4m38s
</pre>
</div></li>
</ul></li>
<li>注意,CronJob会在每次定时任务的时候创建新的Job,所以拥有Cronjob的集群,在运行一一段时间后,会有很多Completed
的job</li>
</ul>
</div>
</div>
<div id="outline-container-org3692268" class="outline-4">
<h4 id="org3692268"><span class="section-number-4">7.3.2.</span> StatefulSets</h4>
<div class="outline-text-4" id="text-7-3-2">
<ul class="org-ul">
<li>目前为止,我们看到的controller,都是创建的完全相同的Pod.这些Pod即便失败了也没关系,重启一个就是</li>
<li>使用完全相同Pod的前提,是Pod对其内部的存储是不在意的.因为这种失败了可以重启的Pod,新创建出来的文件系
统,要么是新的,要么是和其他Pod共享的</li>
<li>如果我们希望即便Pod失败了,重启后文件系统还维持之前的内容,我们就用到了StatefulSet(名字中的State给了暗示)</li>
<li>我们来看一个StatefulSet的例子:
<ul class="org-ul">
<li><p>
配置文件如下
</p>
<div class="org-src-container">
<pre class="src src-yaml"><span style="color: #a89984;">---</span>
<span style="color: #076678;">apiVersion</span>: apps/v1
<span style="color: #076678;">kind</span>: StatefulSet
<span style="color: #076678;">metadata</span>:
  <span style="color: #076678;">name</span>: sleep
<span style="color: #076678;">spec</span>:
  <span style="color: #076678;">serviceName</span>: sleep
  <span style="color: #076678;">replicas</span>: 2
  <span style="color: #076678;">selector</span>:
    <span style="color: #076678;">matchLabels</span>:
      <span style="color: #076678;">app</span>: sleep
  <span style="color: #076678;">template</span>:
    <span style="color: #076678;">metadata</span>:
      <span style="color: #076678;">labels</span>:
        <span style="color: #076678;">app</span>: sleep
    <span style="color: #076678;">spec</span>:
      <span style="color: #076678;">containers</span>:
      - <span style="color: #076678;">name</span>: sleep
        <span style="color: #076678;">image</span>: busybox
        <span style="color: #076678;">command</span>:
          - <span style="color: #79740e;">"/bin/sleep"</span>
          - <span style="color: #79740e;">"3600"</span>
        <span style="color: #076678;">volumeMounts</span>:
        - <span style="color: #076678;">name</span>: sleep-volume
          <span style="color: #076678;">mountPath</span>: /storagedir
  <span style="color: #076678;">volumeClaimTemplates</span>:
  - <span style="color: #076678;">metadata</span>:
      <span style="color: #076678;">name</span>: sleep-volume
    <span style="color: #076678;">spec</span>:
      <span style="color: #076678;">storageClassName</span>: longhorn
      <span style="color: #076678;">accessModes</span>:
        - ReadWriteOnce
      <span style="color: #076678;">resources</span>:
        <span style="color: #076678;">requests</span>:
          <span style="color: #076678;">storage</span>: 100Mi
</pre>
</div></li>
<li>这个例子中的/storagedir是Pod重启后依然能够维持之前内容的文件路径</li>
<li>我们这里创建了Service, 这个Service会把我们这个StatefulSet和k8s Service联系在一起,并且会为每个Pod
创建一个DNS.Service具体内容在第九章会讲到</li>
<li>我们还需要创建template来请求persistent storage</li>
<li><p>
启动方法如下
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl apply -f /opt/sleep-set.yaml
service/sleep created
statefulset.apps/sleep created
root@host01:/home/vagrant# kubectl get statefulsets
NAME    READY   AGE
sleep   0/2     7s
root@host01:/home/vagrant# kubectl get pods
NAME                     READY   STATUS              RESTARTS   AGE
nginx-5f4ff46877-tppsr   1/1     Running             0          31m
sleep-0                  0/1     ContainerCreating   0          11s
sleep-s7rww              0/1     Completed           0          11m
root@host01:/home/vagrant# kubectl get pods
NAME                     READY   STATUS              RESTARTS   AGE
nginx-5f4ff46877-tppsr   1/1     Running             0          31m
sleep-0                  1/1     Running             0          30s
sleep-1                  0/1     ContainerCreating   0          10s
sleep-s7rww              0/1     Completed           0          11m
</pre>
</div></li>
<li>我们可以注意到StatefulSet的Pod,其文件名并不是random string,而是一个数字</li>
<li><p>
我们做个实验,向两个Pod的/storagedir文件夹下面写入一些特定字符,删除掉这些Pod,并等待Pod重启之后,发
现字符串还是依然存在
</p>
<div class="org-src-container">
<pre class="src src-shell">root@host01:/home/vagrant# kubectl exec sleep-0 -- /bin/sh -c <span style="color: #79740e;">'hostname &gt; /storagedir/myhost'</span>
root@host01:/home/vagrant# kubectl exec sleep-0 -- /bin/cat /storagedir/myhost
sleep-0
root@host01:/home/vagrant# kubectl exec sleep-1 -- /bin/sh -c <span style="color: #79740e;">'hostname &gt; /storagedir/myhost'</span>
root@host01:/home/vagrant# kubectl exec sleep-1 -- /bin/cat /storagedir/myhost
sleep-1
root@host01:/home/vagrant# kubectl delete pod sleep-0
pod <span style="color: #79740e;">"sleep-0"</span> deleted
root@host01:/home/vagrant# kubectl get pods
NAME                     READY   STATUS      RESTARTS   AGE
nginx-5f4ff46877-tppsr   1/1     Running     0          35m
sleep-0                  1/1     Running     0          46s
sleep-1                  1/1     Running     0          4m1s
sleep-s7rww              0/1     Completed   0          15m
root@host01:/home/vagrant# kubectl exec sleep-0 -- /bin/cat /storagedir/myhost
sleep-0
</pre>
</div></li>
<li><p>
并且我们发现,重启后的Pod,连名字都没有改(还是数字),这是和Deployment不一样的
</p>
<pre class="example" id="orgb8d5b7b">
For StatefulSet, new Pod is crated with the same name, which is different from
the Deployment for which a random name was generate for every new Pod
</pre></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org829bd08" class="outline-4">
<h4 id="org829bd08"><span class="section-number-4">7.3.3.</span> Daemon Sets</h4>
<div class="outline-text-4" id="text-7-3-3">
<ul class="org-ul">
<li>DeemonSet controller是和StatefulSet controller很相似的Controller:
<ul class="org-ul">
<li>它也在controller下面运行固定数目的Pod</li>
<li>它的Pod也是有unique identity(而不是random string)</li>
</ul></li>
<li>DeemonSet controller和StatefulSet controller不同的地方在于:
<ul class="org-ul">
<li>它是精确的每个node(集群的node)只有一个Pod: 通常用于cluster的control plane在每个node的client代表</li>
</ul></li>
<li>我们的集群已经安装了很多DaemonSet,比如calico-node,我们可以通过如下命令发现,每个node都刚好有一个
calico-node的pod在运行
#+begin_src shell
  root@host01:/home/vagrant# kubectl -n calico-system get pods -l k8s-app=calico-node -o wide
  NAME                READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES
  calico-node-7bbzx   0/1     Running   0          70m   192.168.61.12   host02   &lt;none&gt;           &lt;none&gt;
  calico-node-kc98v   0/1     Running   0          70m   192.168.61.11   host01   &lt;none&gt;           &lt;none&gt;
  calico-node-sb4gk   0/1     Running   0          70m   192.168.61.13   host03   &lt;none&gt;           &lt;none&gt;
#+end_srcs</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: harrifeng@outlook.com</p>
<p class="date">Created: 2023-12-03 日 09:35</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>