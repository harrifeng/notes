<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-10-22 Wed 20:17 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>ms</title>
<meta name="author" content="harrifeng@outlook.com" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="src/readtheorg_theme/css/readtheorg.css"/>
<script type="text/javascript" src="src/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="src/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="src/readtheorg_theme/js/readtheorg.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">ms</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org3d284a0">1. Chapter 01: Introduction to this book</a>
<ul>
<li><a href="#orgdf310f3">1.1. What is statistics and why learn it?</a></li>
<li><a href="#org77df17d">1.2. Statistics, data science, machine learning, etc.</a>
<ul>
<li><a href="#orgf5b0033">1.2.1. Data science</a></li>
<li><a href="#org11e45d3">1.2.2. Data mining, analytics, informatics, etc.</a></li>
</ul>
</li>
<li><a href="#org71d6fe5">1.3. Target audience</a></li>
<li><a href="#org2d28ec0">1.4. Prerequisites</a>
<ul>
<li><a href="#org8552856">1.4.1. The obvious</a></li>
<li><a href="#org896bf03">1.4.2. High-school math</a></li>
<li><a href="#orgf9a8d3f">1.4.3. Calculus and linear algebra</a></li>
<li><a href="#org8204a44">1.4.4. Programming</a></li>
</ul>
</li>
<li><a href="#org03c82a7">1.5. Exercises</a></li>
<li><a href="#orgb0815df">1.6. Learning from simulated data</a>
<ul>
<li><a href="#org2257424">1.6.1. 现代计算方法:模拟数据学习</a></li>
</ul>
</li>
<li><a href="#orgadf6b9a">1.7. Using the code with this book</a>
<ul>
<li><a href="#org814fed0">1.7.1. Which language to use?</a></li>
<li><a href="#orgf64a56a">1.7.2. Following along with Python</a></li>
<li><a href="#org911d54c">1.7.3. Following along with R</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgdd202cb">2. Chapter 02: What Are (Is?) Data?</a>
<ul>
<li><a href="#orgf7c11be">2.1. Is "data" singular or plural?</a></li>
<li><a href="#org3400c18">2.2. Where do data come from, what do they mean?</a></li>
<li><a href="#org5684ccf">2.3. What do data look like?</a></li>
<li><a href="#org89203fb">2.4. Limitations of data</a>
<ul>
<li><a href="#org0b93086">2.4.1. Data come from imperfect measurements</a></li>
<li><a href="#org901ed4c">2.4.2. Data my contains noise</a></li>
<li><a href="#orgc374366">2.4.3. Outliers may skew statistics</a></li>
<li><a href="#org7c05ea8">2.4.4. What is a measurement unit?</a></li>
<li><a href="#orge9fa869">2.4.5. Conclusion</a></li>
</ul>
</li>
<li><a href="#orge2da577">2.5. Accuracy, precision, resolution, range</a>
<ul>
<li><a href="#org2152e3e">2.5.1. Accuracy 准确度</a></li>
<li><a href="#org08869a5">2.5.2. Precision 精密度</a></li>
<li><a href="#org102e455">2.5.3. Resolution 分辨率</a></li>
<li><a href="#org8b94230">2.5.4. Range 量程</a></li>
</ul>
</li>
<li><a href="#org51d1894">2.6. Data types</a>
<ul>
<li><a href="#org23d11cd">2.6.1. 数值型数据(Numerical data)</a></li>
<li><a href="#orgd1b12ad">2.6.2. 离散数据(Discrete data)</a></li>
<li><a href="#org5e48561">2.6.3. 区间数据(Interval data)</a></li>
<li><a href="#orgd2b9de0">2.6.4. 比率数据(Ratio data)</a></li>
<li><a href="#org46f6e1c">2.6.5. 分类数据(Categorical data)</a></li>
<li><a href="#orgbd4875e">2.6.6. 名义数据(Nominal data)</a></li>
<li><a href="#org68632cb">2.6.7. 有序数据(Ordinal data)</a></li>
<li><a href="#org8fa7b44">2.6.8. 数据类型的变化</a></li>
<li><a href="#orgee124a5">2.6.9. 有序数据的数学运算</a></li>
</ul>
</li>
<li><a href="#orge32a6de">2.7. From anecdotes to populations</a>
<ul>
<li><a href="#orgdfb2efb">2.7.1. Sample vs. population</a></li>
<li><a href="#orgba8ed98">2.7.2. "Big enough" samples</a></li>
<li><a href="#org64d4895">2.7.3. Problems with N=1 studies</a></li>
</ul>
</li>
<li><a href="#org714e5f8">2.8. Data management</a></li>
<li><a href="#org7141434">2.9. The ethics of making up data</a></li>
</ul>
</li>
<li><a href="#org365ba1f">3. Chapter 03: Visualizing Data</a>
<ul>
<li><a href="#org239c030">3.1. Why visualize data?</a></li>
<li><a href="#org7c9bbd5">3.2. How to visualize data</a></li>
<li><a href="#orgdf3c35d">3.3. Bar plots</a>
<ul>
<li><a href="#org07a2e5f">3.3.1. Bar plots for grouped data</a></li>
<li><a href="#org2825b7a">3.3.2. Error bars</a></li>
</ul>
</li>
<li><a href="#org0ee0b32">3.4. Pie charts</a></li>
<li><a href="#orgb2d1cdc">3.5. Box plots</a></li>
<li><a href="#orgcba3b4b">3.6. Histograms</a>
<ul>
<li><a href="#org71edcf1">3.6.1. Histogram vs. bar plot</a></li>
<li><a href="#orgb622b41">3.6.2. Counts vs. proprotions</a></li>
</ul>
</li>
<li><a href="#org4a74b94">3.7. Lines vs. bars in a histogram</a></li>
<li><a href="#org1809512">3.8. Violin plots</a></li>
<li><a href="#org5a8af80">3.9. Linear vs. logarithmic axis scaling</a></li>
<li><a href="#org9230b76">3.10. Discretizing continuous data</a></li>
<li><a href="#org735b15a">3.11. Radial plots</a></li>
<li><a href="#org10b45a7">3.12. Color</a>
<ul>
<li><a href="#org3c98801">3.12.1. 选择哪些颜色?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org660caae">4. Chapter 04: Descriptive Statistics</a>
<ul>
<li><a href="#orgae3cde0">4.1. Descriptive vs. inferential statistics</a></li>
<li><a href="#orgb08896d">4.2. Data distributons</a>
<ul>
<li><a href="#org6146251">4.2.1. Empirical vs. analytical distributions</a></li>
<li><a href="#org8fde5f4">4.2.2. The usese of data distributions</a></li>
<li><a href="#org4282a47">4.2.3. Examples of distributions</a></li>
<li><a href="#org923a48d">4.2.4. Quantifying qualitative characteristics</a></li>
</ul>
</li>
<li><a href="#org5e0e545">4.3. Central tendency</a>
<ul>
<li><a href="#org32e9574">4.3.1. Mean</a></li>
<li><a href="#org8c585a0">4.3.2. Median</a></li>
<li><a href="#org27910c5">4.3.3. Mode</a></li>
</ul>
</li>
<li><a href="#org33d260b">4.4. Measures of dispersion</a>
<ul>
<li><a href="#orgf462a52">4.4.1. Variance</a></li>
<li><a href="#org15a1a45">4.4.2. Standard deviation</a></li>
<li><a href="#org715d54e">4.4.3. Heteroscedasticity and Homoscedasticity</a></li>
<li><a href="#orga8dd0b6">4.4.4. Full width at half maximum (FWHM)</a></li>
<li><a href="#org19bad29">4.4.5. Fano factor and CV</a></li>
</ul>
</li>
<li><a href="#orgb882094">4.5. Interquartile range(IQR)</a></li>
<li><a href="#orgb249cb2">4.6. QQ plots</a></li>
<li><a href="#orgf2738a7">4.7. Statistical "moments"</a>
<ul>
<li><a href="#orge94dbbc">4.7.1. Unstandardized and standardized moments</a></li>
<li><a href="#org2f808c9">4.7.2. First moment: mean</a></li>
<li><a href="#org0288765">4.7.3. Second moment: variance</a></li>
<li><a href="#org57f5bf2">4.7.4. Third moment: skew</a></li>
<li><a href="#orgd432049">4.7.5. Fourth moment: kurtosis</a></li>
<li><a href="#orgde422fb">4.7.6. What to memorize</a></li>
</ul>
</li>
<li><a href="#org208cc56">4.8. Histograms part 2: Number of bins</a>
<ul>
<li><a href="#org8c04b92">4.8.1. Other descriptive stats</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org13d2f88">5. Chapter 05: Simulating Data</a>
<ul>
<li><a href="#org71eb223">5.1. Why simulate data?</a></li>
<li><a href="#orge775dff">5.2. Random data from distributions</a>
<ul>
<li><a href="#orgbd007d7">5.2.1. Normally distributed random data</a></li>
<li><a href="#org4737b6e">5.2.2. Uniformly distributed data</a></li>
<li><a href="#org5cc9cf8">5.2.3. Random data from other distributions</a></li>
<li><a href="#orgfe133de">5.2.4. Random integers</a></li>
</ul>
</li>
<li><a href="#org8ea8336">5.3. Random elements of a set</a></li>
<li><a href="#org8da8c38">5.4. Random permutations</a></li>
<li><a href="#org83497a9">5.5. Reproducing randomness</a></li>
<li><a href="#org77d9db9">5.6. Runningexperiments with random numbers</a>
<ul>
<li><a href="#orga5226b0">5.6.1. Experiment: Impact of standard deviation on mean</a></li>
</ul>
</li>
<li><a href="#org24d59fc">5.7. The amazing world of data-simulations</a></li>
<li><a href="#orgda2ef02">5.8. Finding publicly available real datasets</a></li>
</ul>
</li>
<li><a href="#orgf9b4e6a">6. Chapter 06: Transformations</a>
<ul>
<li><a href="#org30400b2">6.1. Wht, why, and how of data transformations</a>
<ul>
<li><a href="#orge8a51d7">6.1.1. What are data transformations?</a></li>
<li><a href="#org9463e7e">6.1.2. Why transform data?</a></li>
<li><a href="#orgbbab3f9">6.1.3. How to transform data?</a></li>
<li><a href="#org44802b6">6.1.4. What kinds of transformations are there?</a></li>
</ul>
</li>
<li><a href="#org98ec065">6.2. Z-score standardization</a>
<ul>
<li><a href="#org569f2e9">6.2.1. Z-score math</a></li>
<li><a href="#org85e646f">6.2.2. Interpretation</a></li>
<li><a href="#orgff11754">6.2.3. Hard and soft assumptions</a></li>
<li><a href="#orgbe74e30">6.2.4. The modified z-score method</a></li>
</ul>
</li>
<li><a href="#orgc68e23e">6.3. Min-max normalization</a>
<ul>
<li><a href="#org7b1adfb">6.3.1. Interpretation</a></li>
</ul>
</li>
<li><a href="#org2097ae9">6.4. Z-scoring vs. min-max scaling</a></li>
<li><a href="#org9d6acc9">6.5. Percent change</a></li>
<li><a href="#org7bccb24">6.6. Nonlinear data transformations</a>
<ul>
<li><a href="#org669e1c6">6.6.1. Rank-transform</a></li>
<li><a href="#org72902b6">6.6.2. Logarithm and square root transformation</a></li>
<li><a href="#org069d664">6.6.3. Fisher-Z</a></li>
<li><a href="#org2f2cdee">6.6.4. Transform any distribution to Gaussian</a></li>
</ul>
</li>
<li><a href="#org88595a5">6.7. Interpreting transformed data</a>
<ul>
<li><a href="#org362663a">6.7.1. When to transform your  data</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgc3d033e">7. Chapter 07: Assess And Improve Data Quality</a>
<ul>
<li><a href="#orgdfd730c">7.1. Data quality matters</a>
<ul>
<li><a href="#org66f097a">7.1.1. Data quality influences data-driven decisions</a></li>
</ul>
</li>
<li><a href="#orga81fd01">7.2. Data cleaning phases</a>
<ul>
<li><a href="#org04fa1d6">7.2.1. 获取数据之前</a></li>
<li><a href="#org72e4931">7.2.2. 数据收集期间</a></li>
<li><a href="#org53b21ab">7.2.3. 数据收集之后</a></li>
<li><a href="#orgc9411eb">7.2.4. 数据分析期间</a></li>
</ul>
</li>
<li><a href="#org6255f8c">7.3. Assessing data quality</a>
<ul>
<li><a href="#orga3a046f">7.3.1. 定性质量评估(Qualitative quality assessments)</a></li>
<li><a href="#org3cc07c7">7.3.2. 定量质量评估(Quantitative quality assessments)</a></li>
</ul>
</li>
<li><a href="#orgbf4432e">7.4. Improving data quality through  transformations</a></li>
<li><a href="#org4a7d4d4">7.5. What are outliers?</a>
<ul>
<li><a href="#orgb0eed98">7.5.1. How to think about outliers</a></li>
</ul>
</li>
<li><a href="#org3e6f188">7.6. Identifying outliers</a>
<ul>
<li><a href="#orgf3a33f3">7.6.1. Absolute threshold detection</a></li>
<li><a href="#org6d5ac83">7.6.2. The z-score method</a></li>
<li><a href="#org46d96a8">7.6.3. Iterative z-score method</a></li>
<li><a href="#orgeee4681">7.6.4. Removing data by trimming</a></li>
<li><a href="#orged3aa11">7.6.5. Manual, automatic, and semi-automatic cleaning</a></li>
<li><a href="#org8d77941">7.6.6. What happends to rejected outliers?</a></li>
</ul>
</li>
<li><a href="#org1b4f9fc">7.7. Analysis-based solutions to outliers</a>
<ul>
<li><a href="#orgcd41681">7.7.1. 非参数分析(Nonparametric analyses)</a></li>
<li><a href="#org51cd2d7">7.7.2. 基于置换的检验(Permutation-based tests)</a></li>
<li><a href="#org22f1e85">7.7.3. 加权分析(Weighted analyses)</a></li>
<li><a href="#org46dd478">7.7.4. 子群分析(Subgroups analysis)</a></li>
</ul>
</li>
<li><a href="#org7762099">7.8. Missing data</a>
<ul>
<li><a href="#orga0ebccf">7.8.1. 按行删除(Row-wise removal)</a></li>
<li><a href="#orgeb50e14">7.8.2. 针对特定分析的行删除(Analysis-specific row removal)</a></li>
<li><a href="#org7e9eb3c">7.8.3. 插补(Imputation)</a></li>
<li><a href="#org931d770">7.8.4. 预测建模(Predictive modeling)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org0891341">8. Chapter 08: Probability Theory</a>
<ul>
<li><a href="#org2894f02">8.1. From descriptive to inferential statistics</a></li>
<li><a href="#org718422a">8.2. What is probability?</a>
<ul>
<li><a href="#org1d24925">8.2.1. The problem with  probability</a></li>
<li><a href="#org1d7f295">8.2.2. When do we need probabilities?</a></li>
</ul>
</li>
<li><a href="#orgb8582ab">8.3. Probability vs. proportion</a></li>
<li><a href="#orgc0cc022">8.4. Computing probabilities</a>
<ul>
<li><a href="#org17554be">8.4.1. Computing analytical probabilities</a></li>
<li><a href="#org3e44330">8.4.2. Computing expirical probabilities</a></li>
</ul>
</li>
<li><a href="#org0a74a57">8.5. Probability functions, mass, and density</a></li>
<li><a href="#orged92777">8.6. Cumulative distribution function (cdf)</a></li>
<li><a href="#orga20b75b">8.7. Expected value</a>
<ul>
<li><a href="#orgd9cc4fa">8.7.1. Computing expected value</a></li>
<li><a href="#org898ac73">8.7.2. Expected value and statistical moments</a></li>
</ul>
</li>
<li><a href="#org03b2234">8.8. Softmax</a>
<ul>
<li><a href="#org914a240">8.8.1. Does \(\sigma(x)\) really produce a probability mass function</a></li>
<li><a href="#org80e84c7">8.8.2. What is softmax used for?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org07f6afe">9. Chapter 09: Sampling And Distributions</a>
<ul>
<li><a href="#org7feb53e">9.1. Sampling variability and its annoyances</a>
<ul>
<li><a href="#org454b3b6">9.1.1. An example with random data</a></li>
<li><a href="#org2668231">9.1.2. Where does sampling variability come from ?</a></li>
</ul>
</li>
<li><a href="#org5468a37">9.2. Creating sample estimate distributions</a></li>
<li><a href="#orga6e48ae">9.3. Standard error of the mean</a>
<ul>
<li><a href="#org6473f33">9.3.1. Standard error of the mean vs. standard deviation</a></li>
</ul>
</li>
<li><a href="#org683fbdf">9.4. Random and representative sampling</a>
<ul>
<li><a href="#org90eb774">9.4.1. Independent and identically distributed data</a></li>
</ul>
</li>
<li><a href="#orgf06ebbf">9.5. The Law of Large Numbers</a>
<ul>
<li><a href="#org04f1e9b">9.5.1. LLN(Law of Large Number) and sample size (LLN demo #1)</a></li>
<li><a href="#orgd7491f8">9.5.2. LLN and repeated samples (LLN demo #2)</a></li>
</ul>
</li>
<li><a href="#org7362fa6">9.6. The Central Limit Theorem</a>
<ul>
<li><a href="#orga0a0fdb">9.6.1. CLT part 1: sampling distributions</a></li>
<li><a href="#orgb38d7c4">9.6.2. CLT part2: mixing variables</a></li>
<li><a href="#orgc4f25ab">9.6.3. The distribution of sample means</a></li>
<li><a href="#org93a361b">9.6.4. Implications of the CLT</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org9874baa">10. Chapter 10: Hypothesis Testing</a>
<ul>
<li><a href="#orgd6c1423">10.1. Hypotheses</a>
<ul>
<li><a href="#org09d0539">10.1.1. How to specify a hypothesis</a></li>
<li><a href="#orga9584f6">10.1.2. (Why) do we need hypotheses?</a></li>
<li><a href="#orgf8fdb83">10.1.3. Strong and weak hypotheses</a></li>
</ul>
</li>
<li><a href="#orgf526ca9">10.2. IVS, DVs, models, and other stats lingo</a></li>
<li><a href="#org5e2acbc">10.3. Can you prove a hypothesis?</a></li>
<li><a href="#org1d39533">10.4. Sample distributions under \(H_0\) and \(H_A\)</a></li>
<li><a href="#org77a98e9">10.5. Where do H0 distribution come from?</a></li>
<li><a href="#orgcff832e">10.6. P-values: definition and misinterpretations</a>
<ul>
<li><a href="#orgcb49e48">10.6.1. P-values and statistical significance</a></li>
<li><a href="#org709b82f">10.6.2. P-values and distribution tails</a></li>
<li><a href="#org126e9e8">10.6.3. Where do p-values come from?</a></li>
<li><a href="#orge378172">10.6.4. P-z combinations to memorize</a></li>
<li><a href="#org76abb8f">10.6.5. Misinterpretations</a></li>
<li><a href="#org163f348">10.6.6. Problems with p-values</a></li>
</ul>
</li>
<li><a href="#org7aae517">10.7. P-values and significance categorization</a></li>
<li><a href="#org90276e6">10.8. Type-I and Type-II errors</a>
<ul>
<li><a href="#org6fd6378">10.8.1. The balance of Type-I and Type-II errors</a></li>
</ul>
</li>
<li><a href="#orgbb55af2">10.9. Various interpretations of "significant"</a></li>
<li><a href="#org3f5cfc3">10.10. Multiple comparisons</a>
<ul>
<li><a href="#org73b5fd4">10.10.1. Solutions to the multiple comparisons problem</a></li>
</ul>
</li>
<li><a href="#org37504b4">10.11. Degrees of freedom</a></li>
</ul>
</li>
<li><a href="#org53c7704">11. Chapter 11: The T-Test Family</a>
<ul>
<li><a href="#org4e0a05b">11.1. Purpose and interpretation of the t-test</a>
<ul>
<li><a href="#orgdbe4979">11.1.1. The purpose of a t-test</a></li>
<li><a href="#org135f68b">11.1.2. General t-test formula</a></li>
<li><a href="#orgd1506fb">11.1.3. Degrees of freeedom of t-tests</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org3d284a0" class="outline-2">
<h2 id="org3d284a0"><span class="section-number-2">1.</span> Chapter 01: Introduction to this book</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-orgdf310f3" class="outline-3">
<h3 id="orgdf310f3"><span class="section-number-3">1.1.</span> What is statistics and why learn it?</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>统计学是关于在不确定性面前利用数据帮助决策的学科.有时候不确定性很小,你根本不需要统计学.例如:
<ul class="org-ul">
<li>地球和月球哪个更大?回答这个问题绝对不需要统计学.</li>
<li>另一个问题:女性比男性更爱说闲话吗?虽然文化刻板印象提供了答案,但您肯定认识爱闲聊的男性和守口如
瓶的女性.因此,要普遍回答这个问题,我们需要收集数据,并需要对这些数据进行统计分析.</li>
</ul></li>
<li>但"对数据进行统计分析"意味着什么?
<ul class="org-ul">
<li>统计学是一套广泛的算法体系,用于将数值数据转化为少量可解释的数值</li>
<li><p>
这些数值既能描述世界,也能体现我们对解读世界的确信程度(图1.1).
</p>

<div id="org91e3a2e" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/1-1.png" alt="1-1.png" />
</p>
<p><span class="figure-number">Figure 1: </span>ms/1-1.png</p>
</div></li>
</ul></li>
<li>许多统计程序最终都会产生一个"p值",其中p代表概率.p值的精妙解释将在本书后续章节说明,但基本上它表示
你在数据中观察到的效应实际上是 <b>由偶然因素而非真实效应导致的概率</b> :
<ul class="org-ul">
<li>接近零的p值意味着数据中的模式不太可能是偶然现象.因此,p值越小,我们越相信该效应的真实性;</li>
<li>若p值越接近1,我们则判定任何表面效应都只是偶然现象,不太可能被重复观测到.</li>
</ul></li>
<li>统计学在现代人类生活的众多领域都发挥着关键作用:
<ul class="org-ul">
<li>从量子力学到医学再到金融退休规划无不如此.</li>
<li>从亚马逊这样的大型企业到一人规模的小型初创公司,越来越多的企业正在运用数据指导决策和商业策略.</li>
</ul></li>
<li>因此,掌握数据处理,分析,可视化及解读的能力,正对越来越多的职业变得日益重要.</li>
</ul>
</div>
</div>
<div id="outline-container-org77df17d" class="outline-3">
<h3 id="org77df17d"><span class="section-number-3">1.2.</span> Statistics, data science, machine learning, etc.</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>统计学,数据科学,机器学习等. 这些术语之间有何区别?它们真的彼此不同吗?</li>
<li>广义而言:并无不同,这些不同术语只是相同流程的重新包装.我们可以达成共识:
<ul class="org-ul">
<li>"数据科学"听起来新奇有趣;</li>
<li>"机器学习"充满未来感和科幻感;</li>
<li>"商业分析"则显得严肃且具有竞争力.</li>
<li>相比之下,"统计学"听起来陈旧,枯燥,数学化且乏味.</li>
</ul></li>
<li>从广义上讲,所有上述术语都共享一个共同目标:
<ul class="org-ul">
<li>以数据为输入,应用某种算法或算法集,</li>
<li>最终提供可解释的数值输出,进而帮助我们做出决策并理解世界.</li>
</ul></li>
<li>但仍存在一些微妙而重要的区别值得关注:
<ul class="org-ul">
<li>统计学:维基百科给出了一个严谨的定义:"一门专注于数据收集,整理,分析,阐释与呈现的学科".本书大
部分内容都符合这一定义.</li>
<li>机器学习:尽管常被互换使用&#x2013;且采用相同的分析方法&#x2013;机器学习与统计学的目标存在细微差异.
<ol class="org-ol">
<li>推论统计学的目标是基于样本数据(sample data)推断总体特征(population inferernce),</li>
<li>而机器学习的目标则是利用数据中的模式来预测结果或进行分类.</li>
</ol></li>
</ul></li>
<li>在许多场景中,统计学与机器学习会使用相同的分析方法和算法,但研究者的关注点有所不同.例如:双方都使用
称为"回归分析"的技术,
<ul class="org-ul">
<li>但统计学家会重点关注各个回归变量的统计显著性及其阐释,</li>
<li>而机器学习专家则会聚焦于模型对数据样本进行分类的准确度.</li>
</ul></li>
<li>若您难以理解这种区别,请不必担心:
<ul class="org-ul">
<li>需要掌握更多统计学与机器学习知识才能透彻理解.我们将在本书后续章节重新讨论这一话题.</li>
<li>关键在于:统计学与机器学习的相似之处远超过它们的差异,且多数分歧仅涉及对最终结果的解读方式,而非
数学原理或代码实现层面的根本分歧.</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orgf5b0033" class="outline-4">
<h4 id="orgf5b0033"><span class="section-number-4">1.2.1.</span> Data science</h4>
<div class="outline-text-4" id="text-1-2-1">
<ul class="org-ul">
<li>关于数据科学是否以及如何区别于统计学,不同学者持有不同观点.</li>
<li>我个人对此没有强烈立场,但数据科学确实更侧重于现代应用领域,并包含传统统计学长期忽略的数据类型,如:
<ul class="org-ul">
<li>图像</li>
<li>时间序列</li>
<li>文本等.</li>
</ul></li>
<li>必须承认,"科学"一词的附加让我有些不适:
<ul class="org-ul">
<li>科学本意为追求认知而探索事物本质,</li>
<li>但数据科学实质上并非研究数据本身的科学,而是关于运用数据辅助决策的学科.即使不理解数据本质,您仍
可利用数据做出更优决策&#x2013;这或许正是"数据工程"术语逐渐流行的原因.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org11e45d3" class="outline-4">
<h4 id="org11e45d3"><span class="section-number-4">1.2.2.</span> Data mining, analytics, informatics, etc.</h4>
<div class="outline-text-4" id="text-1-2-2">
<ul class="org-ul">
<li>恕我直言,尽管这些术语体系的创立必然存在合理缘由,且特定领域确实需要比"统计学"更精细的术语来体现细
微差别,但此种程度的术语辨析并非我愿意投入时间深究的范畴.</li>
</ul>
<hr />
<ul class="org-ul">
<li>本文讨论的核心在于:纵然统计学与机器学习并非完全相同的学科,但它们存在足够的重叠领域.</li>
<li>本书所奠定的坚实基础,将足以应对您未来雇主在职位描述中可能使用的任何特定术语.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org71d6fe5" class="outline-3">
<h3 id="org71d6fe5"><span class="section-number-3">1.3.</span> Target audience</h3>
<div class="outline-text-3" id="text-1-3">
<ul class="org-ul">
<li>我撰写本书时,始终将自学读者放在心上.
<ul class="org-ul">
<li>也许您在校期间并未意识到自己需要掌握统计学知识,</li>
<li>抑或您虽正在修读大学课程,却对推荐的教材不甚满意.</li>
</ul></li>
<li>的确,许多统计学教科书数学密度过高,令非数学专业的学生望而生畏</li>
<li>或者成书于个人计算机尚未普及和强大的时代,曾为经典却已不合时宜.</li>
<li>我期望本书能成为课堂内外皆有益的学习资源.</li>
<li>现今众多的统计学教材过度侧重数学推演,强烈聚焦于抽象概念而非实际应用.我此言并非批评:
<ul class="org-ul">
<li>数理统计学本身是一个丰富且极具智力挑战的领域,为应用统计学奠定了基石.</li>
<li>然而,对于那些希望将统计学作为理解数据的工具,并依据数据做出决策的学习者而言,纯数学化的统计学论
述往往艰深难懂甚至难以入门.</li>
</ul></li>
<li>本书的目标,是以一种易于理解和掌握的方式呈现应用统计学,着重强调那些与实际操作,具体应用和结果解读
紧密关联的概念</li>
</ul>
</div>
</div>
<div id="outline-container-org2d28ec0" class="outline-3">
<h3 id="org2d28ec0"><span class="section-number-3">1.4.</span> Prerequisites</h3>
<div class="outline-text-3" id="text-1-4">
</div>
<div id="outline-container-org8552856" class="outline-4">
<h4 id="org8552856"><span class="section-number-4">1.4.1.</span> The obvious</h4>
<div class="outline-text-4" id="text-1-4-1">
<ul class="org-ul">
<li>虽不言自明但仍需强调:
<ul class="org-ul">
<li>学习统计学首先需要强烈的动机.</li>
<li>统计学既非艰深莫测,也非轻而易举.最重要的是保持学习意愿,并愿意为此投入时间与精力.相较之下,以
下各点都属次要.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org896bf03" class="outline-4">
<h4 id="org896bf03"><span class="section-number-4">1.4.2.</span> High-school math</h4>
<div class="outline-text-4" id="text-1-4-2">
<ul class="org-ul">
<li>需熟练掌握算术与基础代数知识.若能解答4x²=9这类方程,即具备继续学习所需的代数基础.其他数学概念将在
需要时引入讲解.</li>
<li>本书虽包含公式,方程和算法,但力求以通俗易懂的语言阐释,配以图表说明,并提供Python与R代码示例,方便
您通过模拟和可视化方式理解数学原理.</li>
</ul>
</div>
</div>
<div id="outline-container-orgf9a8d3f" class="outline-4">
<h4 id="orgf9a8d3f"><span class="section-number-4">1.4.3.</span> Calculus and linear algebra</h4>
<div class="outline-text-4" id="text-1-4-3">
<ul class="org-ul">
<li>无需基础.需说明的是:
<ul class="org-ul">
<li>统计学作为数学分支,掌握其他数学领域知识固然有助于学习,但本书专注应用统计学而非数理统计学,因此
省略了依赖微积分的证明,侧重直观理解.</li>
<li>书中虽涉及少量微积分与线性代数内容,但已确保即使不理解全部数学符号,也能通过直观方式理解算法与证
明过程.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org8204a44" class="outline-4">
<h4 id="org8204a44"><span class="section-number-4">1.4.4.</span> Programming</h4>
<div class="outline-text-4" id="text-1-4-4">
<ul class="org-ul">
<li>现代应用统计学完全依赖编程&#x2013;此言绝非夸张.若不具备至少基础编码能力,根本无法实施统计操作.</li>
<li>所幸现有成熟稳定的代码库能实现底层技术细节.这意味着您无需成为专业程序员即可轻松应用统计学,但确实
需要具备代码编写能力.</li>
<li>本书采用Python与R语言&#x2013;这两者堪称现代统计学领域最流行的编程语言.我确信它们不会永远占据主导地位;未
来必将出现更高效,更易用,更快速的新语言.</li>
<li>但好消息是:所有编程语言都存在共通性,因此通过Python或R学习统计学将助您在任意其他语言中应用统计知识</li>
<li>换言之,编程学习是值得投入的时间投资而非浪费&#x2013;即使您实践中使用不同语言.况且ChatGPT(或其他先进语言AI)
已能相当准确地将代码转换为SAS,MATLAB,Julia等其他语言.</li>
<li>需要明确:阅读本书无需编程基础.您可跳过所有代码与练习,专注概念与解读部分.但本书的设计理念在于:
<ul class="org-ul">
<li>唯有通过代码实践与练习,方能获得对知识的深层理解.</li>
</ul></li>
<li>亲身体验胜过纸上谈兵&#x2013;这正是我写作与教学的核心哲学.
<ul class="org-ul">
<li>人们通过亲眼观察与亲手实践才能达到最佳学习效果,而非仅让目光掠过文字与公式.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org03c82a7" class="outline-3">
<h3 id="org03c82a7"><span class="section-number-3">1.5.</span> Exercises</h3>
<div class="outline-text-3" id="text-1-5">
<ul class="org-ul">
<li>我确信您曾听过这个说法:数学绝非观赏性运动.</li>
<li>若您仅阅读本书而不尝试任何练习,固然能有所收获,我也希望您觉得本书有用.</li>
<li>但若要真正理解统计学,您必须亲手解决统计学问题.</li>
<li>本人设计的练习需投入一定精力与创造力,且必须通过编码完成.</li>
<li>这些练习将为您提供独特机会,通过编程方式探索概念,可视化效果与参数调整:这些体验仅靠阅读难以实现.</li>
<li>本人强烈建议您认真完成这些练习.
<ul class="org-ul">
<li>它们并非填充时间的无效劳作,而是通过实践来巩固,探索和扩展统计学认知的重要途径,其效果是单纯阅读
章节无法企及的.</li>
<li>这些练习同时提供了丰富的代码资源,可供您持续学习并将统计学应用于自身研究数据.</li>
</ul></li>
<li>虽然我会提供全部练习的代码参考答案,但请注意:
<ul class="org-ul">
<li>编程解决方案本无定法.重点在于通过编码探索和理解统计学,而非精确复现我的代码.</li>
</ul></li>
<li>请随时随意查阅参考答案,
<ul class="org-ul">
<li>这绝非作弊行为&#x2013;尤其当您是Python或R语言新手时:若已理解统计学概念但暂受编码语法困扰,参考答案正是
为您铺设的学习阶梯.</li>
<li>这些练习的根本目的是提供学习探索与知识拓展的契机,而非进行量化考核.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb0815df" class="outline-3">
<h3 id="orgb0815df"><span class="section-number-3">1.6.</span> Learning from simulated data</h3>
<div class="outline-text-3" id="text-1-6">
<ul class="org-ul">
<li>统计学作为一门方法论科学,传统上主要通过三种方式学习. 每种方式各有特点,但也存在明显局限性.而现代
计算方法特别是模拟数据技术,为统计学教育提供了新的可能.</li>
<li>三种传统学习方式
<ol class="org-ol">
<li>教科书公式导向法: 这种方法侧重于钻研统计学教科书中的方程式,这些教材通常由荣誉退休的统计学教授编
写.部分教授编写教材的目的是向同行展示而非教育学习者.如果您对纯数理统计学感兴趣,这种方法很棒;
但如果您关注现实世界的应用统计学,则可能令人望而生畏</li>
<li>案例脱离实际法: 这种方法通过阅读充满案例的统计学书籍来学习,但这些书籍使用的数据很少向读者公开,
且这些数据的特征或主题与学习者实际可能接触到的任何数据集或应用场景毫无关联</li>
<li>软件菜单操作法: 这种方法通过阅读软件教程书籍来学习哪些菜单选项可以执行哪些分析.如果您已经了解统
计学并且需要学习特定软件的实施细节,这种方法很棒;但如果您还不理解统计学原理,则可能造成困惑和误导</li>
</ol></li>
</ul>
</div>
<div id="outline-container-org2257424" class="outline-4">
<h4 id="org2257424"><span class="section-number-4">1.6.1.</span> 现代计算方法:模拟数据学习</h4>
<div class="outline-text-4" id="text-1-6-1">
<ul class="org-ul">
<li>本书旨在通过模拟具有特定特征的数据集然后对这些数据进行统计分析,提供一种现代的,计算驱动的统计学学习方式</li>
<li>模拟数据的本质与价值:
<ul class="org-ul">
<li>本质上,模拟提供了一种在虚拟环境中试验定制数据集的方法.通过多次迭代运行,操纵数据特征并观察结果,
您可以了解统计分析的行为方式以及对现实场景的预期.第五章开头提供了使用模拟数据的详细动机和优势.</li>
<li>与健身房的类比:这就像在健身房锻炼:器械和自由重量器械奇怪且不自然,但它们能增强力量和耐力,从而帮
助你在现实世界中的行为.同样,您可能认为模拟数据奇怪且不自然,但我相信(希望)模拟让您获得的数学,
概念和实施技能将有助于您在现实世界中的数据分析</li>
</ul></li>
<li>方法论的多样性与包容性:
<ul class="org-ul">
<li>并非每个人都喜欢这种方法,我尊重意见和偏好学习风格的差异.但据我所知,有无数统计学书籍不依赖模拟
数据作为深入探究统计学数学,算法和解释的机制;而至少有一本书严重依赖模拟数据(就是您正在阅读的这本!).</li>
<li>我理解并尊重不同的人有不同的意见和偏好,并且我保证,如果您不喜欢这种教育方法,我不会个人感到被冒犯</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgadf6b9a" class="outline-3">
<h3 id="orgadf6b9a"><span class="section-number-3">1.7.</span> Using the code with this book</h3>
<div class="outline-text-3" id="text-1-7">
<ul class="org-ul">
<li>这本书不包含 Python 或 R 语言的教程.我已尽力确保代码中的注释足够详细,以便初学者能够理解并将代码改
编用于自己的应用.但我确实假设读者具备一些基础的编程知识.如果你理解变量,for循环,函数,导入库以及
基础绘图,那么你就具备了学习本书代码所需的基础.</li>
<li>如果你是完全不熟悉 Python 或 R 的新手,那么我建议你先学习一些编程基础再继续.</li>
</ul>
</div>
<div id="outline-container-org814fed0" class="outline-4">
<h4 id="org814fed0"><span class="section-number-4">1.7.1.</span> Which language to use?</h4>
<div class="outline-text-4" id="text-1-7-1">
<ul class="org-ul">
<li>我不确定是否值得同时深入钻研 Python 和 R 的代码.最好的做法可能是选择一门语言,然后将你的时间最大限
度地投入那门语言.</li>
<li>你该选择哪门语言?我无法下定论.我的建议是,专注于你所在部门,公司,大学或任何环境中大多数人使用的
那门语言.如果你不知道应该使用哪门语言,那么选择 Python 可能更好,因为它是一门通用编程语言,而 R 的
应用范围更专注于统计学.</li>
<li>请记住,R 和 Python 是使用不同库的不同语言.基于相同数据的统计分析结果在两种语言中应该趋于一致,但
并不保证完全 identical(完全相同),尤其是对于那些依赖复杂数学的高级分析.在一个给定的项目中,最好
坚持使用一种语言</li>
</ul>
</div>
</div>
<div id="outline-container-orgf64a56a" class="outline-4">
<h4 id="orgf64a56a"><span class="section-number-4">1.7.2.</span> Following along with Python</h4>
<div class="outline-text-4" id="text-1-7-2">
<ul class="org-ul">
<li>其实就是jupyter notebook, 我们的笔记也会主要以jupyter notebook为主</li>
</ul>
</div>
</div>
<div id="outline-container-org911d54c" class="outline-4">
<h4 id="org911d54c"><span class="section-number-4">1.7.3.</span> Following along with R</h4>
<div class="outline-text-4" id="text-1-7-3">
<ul class="org-ul">
<li>本笔记不涉及R语言</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orgdd202cb" class="outline-2">
<h2 id="orgdd202cb"><span class="section-number-2">2.</span> Chapter 02: What Are (Is?) Data?</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-orgf7c11be" class="outline-3">
<h3 id="orgf7c11be"><span class="section-number-3">2.1.</span> Is "data" singular or plural?</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>请原谅我对语法用法的小小抱怨;我会尽量简短.</li>
<li>从语法角度看,data 是一个复数名词.Data 表示多个数据点,即多于一条的数据&#x2013;一个可数的数据值集合.</li>
<li>一条数据&#x2013;也就是一个数据点&#x2013;用 datum 表示.
<ul class="org-ul">
<li>Datum 是 data 的单数形式.一个数字是一个 datum;</li>
<li>两个数字是 data.</li>
</ul></li>
<li>因此,像 "this data shows" 或 "the data was uploaded" 这样的表达,其实算是非英语母语者可以被原谅的
语法错误.</li>
<li>正确的语法应该是 "these data show" 和 "the data were uploaded"(或者 "this datum shows" 和
"the datum was uploaded").</li>
<li>但实际上,不知为何&#x2013;令语法较真的统计学家们十分懊恼的是&#x2013;data 成了"既是单数又是复数"的词.</li>
<li>"This data shows" 这种用法在媒体和传播领域被视为可接受的表达.甚至 Reddit 上还有一个子版块叫
"Data is beautiful".</li>
<li>这种用法让我听起来很刺耳,简直像一位百岁女巫用她过长的指甲刮黑板,同时唱着阿金托(Argento)电影
《阴风阵阵》(Suspiria)里那首令人毛骨悚然的小调.</li>
<li>当然,我的语法并不完美,我确信如果用语言学的精密梳子来细查,这本书里一定也会发现一些语法上的小瑕疵.</li>
<li>但我会始终坚持将 data 视作复数名词,将 datum 视作单数名词.</li>
<li>感谢你,亲爱的读者,包容我这一番抱怨.现在,让我们进入一个关于数据更有意义的讨论吧.</li>
</ul>
</div>
</div>
<div id="outline-container-org3400c18" class="outline-3">
<h3 id="org3400c18"><span class="section-number-3">2.2.</span> Where do data come from, what do they mean?</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>宇宙是一个极其,极其,极其无法想象的复杂之地.</li>
<li>我们甚至并不了解它到底有多复杂&#x2013;事实上,宇宙中大约 95% 是神秘的暗物质和暗能量.</li>
<li>然而,我们是充满好奇的生物.人类长期以来一直试图通过故事,寓言,宗教,哲学,观察和科学来理解并掌控
自己的环境.</li>
<li>科学的方法是:构建测量装置,将某些可观测的物理量转化为一个或一组数字.这些数字就叫作数据.</li>
<li>举个例子:想象你在一家想更好了解顾客的公司工作.</li>
<li>当然,人是极其复杂且多样的.为了帮助你理解顾客,你设计了一种测量装置&#x2013;问卷调查&#x2013;让顾客用数字来回答
关于他们对各种产品或设计的偏好,或者他们愿意为某些产品或服务支付多少钱.这些数字,就是数据.</li>
<li>重要的是要明白:这些数字并不等同于现实;现实要复杂且微妙得多.
<ul class="org-ul">
<li>比如,顾客对某个产品的满意度并不是真的"9分(满分10分)";那是一种主观体验,涉及情感,记忆,期望,
基因等多种因素的交织,还可能受到早餐吃了什么,配偶的压力水平等各种细微因素的影响.</li>
</ul></li>
<li>而这种由生物—心理—社会—情感动态共同作用所产生的满意度,是极其复杂的,要想开始理解它,甚至准确测量它
都可能需要一生的科学研究.</li>
<li>所以,我们会去用某种方式捕捉这种现实的某一部分特征,把它用一个可以存入电子表格的数字表示出来.</li>
<li>这些数字&#x2013;也就是数据&#x2013;并不是真实本身,但它们反映了对现实的测量结果.</li>
<li>数据的来源就在于此:我们开发测量技术,将物理和生物现象转化为可以储存在计算机中的数字.
<ul class="org-ul">
<li>这些测量技术可能非常先进,比如大型强子对撞机,核磁共振成像(MRI)机器,扫描电子显微镜;</li>
<li>也可能很简单,比如问卷调查,体重秤,或者医生对患者疾病严重程度的评分.</li>
</ul></li>
<li>测量设备的质量很重要,因为它直接关系到数据的准确性和精确性(本书 2.5 节会详细讨论这些概念).</li>
<li>但从概念上来看,所有数据的本质是相同的:
<ul class="org-ul">
<li>你在宇宙中测量某个事物</li>
<li>测量产生一个或一串数字(或文字,再转换成数字)</li>
<li>我们称这些数字为数据.</li>
<li>然后,你对这些数据进行统计分析,希望能够更好地理解你正在研究的系统.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org5684ccf" class="outline-3">
<h3 id="org5684ccf"><span class="section-number-3">2.3.</span> What do data look like?</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>为了进行处理和分析,数据会被数字化并存储在计算机中.因此,从根本上说,数据是以一系列存储单元的二进
制状态进行编码的,但我们会用更熟悉的数字和字母来表示和可视化它们.</li>
<li>大多数数据集以表格形式表示,其中行对应数据样本,列对应所测量的特征.换句话说,
<ul class="org-ul">
<li>行代表观测值(observations)</li>
<li>列代表特征(features).</li>
</ul></li>
<li>图 2.2 给出了一个示例数据集:我在写这一章时,记录了我在联合办公空间里下午早些时候喝的咖啡评分.
<ul class="org-ul">
<li><p>
图2-2
</p>

<div id="orgb5f42a5" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/2-2.png" alt="2-2.png" />
</p>
<p><span class="figure-number">Figure 2: </span>ms/2-2.png</p>
</div></li>
<li>在表格中,每一行是一条观测记录,每一列是一个特征.这个例子说明,数据特征可以是类别型(用文本编码
表示),也可以是数值型.</li>
<li>表格中还有一个缺失值(那天是周六,我有点宿醉,所以没去联合办公空间).</li>
</ul></li>
<li>数据也可以是图像的形式(见图 2.3),它们的本质是数字,这些数字会映射为像素的颜色强度.
<ul class="org-ul">
<li><p>
图2-3
</p>

<div id="org08d124b" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/2-3.png" alt="2-3.png" />
</p>
<p><span class="figure-number">Figure 3: </span>ms/2-3.png</p>
</div></li>
</ul></li>
<li>有些数据集可能会存储为多维数组,比如三维立方体,而不是二维表格&#x2013;例如彩色图像通常会存储成一个立方体,
其三个维度分别表示宽度,高度和 RGB 颜色通道.</li>
<li>不过,在本书所讨论的统计分析中,我们关心的数据集通常是电子表格的形式:
<ul class="org-ul">
<li>行表示观测值(样本)</li>
<li>列表示特征(测量变量)</li>
</ul></li>
<li><p>
就像图 2.4 那样的结构.
</p>

<div id="orga6d7884" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/2-4.png" alt="2-4.png" />
</p>
<p><span class="figure-number">Figure 4: </span>ms/2-4.png</p>
</div></li>
<li>数据可以是数值型的,也可以是文本型的,你在图 2.2 中已经看到过这两种形式.</li>
<li>然而,非数值型数据通常会在分析之前被转换成数字.
<ul class="org-ul">
<li>例如,"Coffee"(咖啡)这一列的数据,可以用 0 表示卡布奇诺,用 1 表示浓缩咖啡,等等.</li>
</ul></li>
<li>需要注意的是,这种把数据类别映射成数字的方法是任意的:为什么不能用 1 表示卡布奇诺,用 2.3 表示浓缩
咖啡呢?</li>
<li>在某些情况下,这种数字映射会影响结果的解释,例如在回归分析中.你将在本书后面学到原因.</li>
<li>目前你只需要知道,文本数据在分析时通常会被转换成数字.</li>
<li>对整个数据集有一个鸟瞰式的整体视图很好,但对于庞大或多维度的数据集来说,这种方法并不适用.</li>
<li>此外,仅仅看着一堆电子表格中的数字并不会自动带来洞察,也不太可能单凭肉眼就从原始数据中看出有意义的
模式.因此,数据可视化是很有必要的,你将在下一章学习到许多数据可视化的方法.</li>
</ul>
</div>
</div>
<div id="outline-container-org89203fb" class="outline-3">
<h3 id="org89203fb"><span class="section-number-3">2.4.</span> Limitations of data</h3>
<div class="outline-text-3" id="text-2-4">
<ul class="org-ul">
<li>如果数据来自对宇宙的测量,那我们为什么还需要数据处理和统计分析呢?</li>
<li>难道我们不能直接相信原始数据吗?</li>
<li>很遗憾,并不是所有数据都能立即被信任,因此我们需要对数据进行清洗,准备,归一化或转换,并结合统计分析.</li>
<li>下面列出了一些困扰"原始"(未处理)数据的问题,这些问题促使我们在解释数据之前,先对其应用算法和分析方法.</li>
</ul>
</div>
<div id="outline-container-org0b93086" class="outline-4">
<h4 id="org0b93086"><span class="section-number-4">2.4.1.</span> Data come from imperfect measurements</h4>
<div class="outline-text-4" id="text-2-4-1">
<ul class="org-ul">
<li>数据的质量受限于测量的质量.以下是几个说明这种限制的例子:</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org99dc390"></a>记忆偏差<br />
<div class="outline-text-5" id="text-2-4-1-1">
<ul class="org-ul">
<li>你让人们回忆 10 年前的购买偏好,并将调查放到网上,收集到数以万计的回答.</li>
<li>但这些数据究竟意味着什么?它们真的反映了人们当时的经济决策吗?</li>
<li>还是其实在测量他们的记忆,或者是他们对过去自我的理想化印象?</li>
</ul>
</div>
</li>
<li><a id="org1726aff"></a>测量环境限制<br />
<div class="outline-text-5" id="text-2-4-1-2">
<ul class="org-ul">
<li>你需要测量发动机内部的温度,但把温度计放进发动机会被融化.</li>
<li>于是你可能会把温度计放在发动机外面,但那样测到的温度其实是发动机温度 + 外界温度 + 热量经过发动机外
壳传递的影响的综合结果.</li>
</ul>
</div>
</li>
<li><a id="org35e1306"></a>数据与现实不完全吻合<br />
<div class="outline-text-5" id="text-2-4-1-3">
<ul class="org-ul">
<li>2020 年初,新冠病毒在全球传播.当时各国都尝试统计感染人数.</li>
<li>但虽然病毒会让部分人死亡或出现严重健康问题,很多人无症状或症状轻微.</li>
<li>因此,数据呈现的是"报告的确诊病例数",而不是真正的"感染人数".</li>
<li>实际的感染人数比报告病例更重要,但在自测工具普及之前,并且只检测出现症状的人时,这个数字几乎无法准
确测量.</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org901ed4c" class="outline-4">
<h4 id="org901ed4c"><span class="section-number-4">2.4.2.</span> Data my contains noise</h4>
<div class="outline-text-4" id="text-2-4-2">
<ul class="org-ul">
<li>另一个限制是,数据中往往会包含噪声.</li>
<li>"噪声"是一个出乎意料难以精确定义的概念,但一般可以理解为数据中不需要的变化.</li>
<li>有些噪声来源已知,例如:在没有做好屏蔽的电路中,会出现 50/60 Hz 的市电干扰噪声;</li>
<li>有些噪声来自测量设备的故障,例如手机摄像头镜头上有一块油渍,就会降低拍摄图像的质量;</li>
<li>还有一些噪声来自未知或无法量化的因素,例如人的体重取决于基因,年龄,生活方式及其他多种复杂因素的组合.</li>
<li>噪声可以是非系统性的(对所有测量产生大致相同的影响),也可以是系统性的(以某种方式影响测量,从而可能引入偏差).</li>
<li>有些类型的噪声在统计分析中容易被最小化或忽略,而另一些噪声则更具破坏性.</li>
<li>在第 7 章(以及本书的许多其他章节)中,你会学到更多关于识别与处理噪声的方法.</li>
<li>现在,你只需要理解:噪声会给数据引入不需要的变动,从而使数据更难解释
<ul class="org-ul">
<li><p>
参见图 2-5
</p>

<div id="org18d1647" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/2-5.png" alt="2-5.png" />
</p>
<p><span class="figure-number">Figure 5: </span>ms/2-5.png</p>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgc374366" class="outline-4">
<h4 id="orgc374366"><span class="section-number-4">2.4.3.</span> Outliers may skew statistics</h4>
<div class="outline-text-4" id="text-2-4-3">
<ul class="org-ul">
<li>数据集中有时会包含相对于其他数据来说异常的少量数据点.</li>
<li><p>
这些数据点被称为离群值(outliers)或非代表性样本(non-representative samples)(见图 2.6).
</p>

<div id="org8a17d05" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/2-6.png" alt="2-6.png" />
</p>
<p><span class="figure-number">Figure 6: </span>ms/2-6.png</p>
</div></li>
<li>离群值可能由多种原因造成,例如测量误差或特殊样本.</li>
<li>离群值对分析结果的影响可能非常大,也可能几乎没有影响&#x2013;这取决于离群值的性质,大小,以及所进行的统计分析类型.</li>
<li>因此,在进行分析之前,通常有必要检查数据中是否存在离群值,并在必要时将其移除.</li>
<li>更多相关内容会在第 7 章中详细讨论.</li>
</ul>
</div>
</div>
<div id="outline-container-org7c05ea8" class="outline-4">
<h4 id="org7c05ea8"><span class="section-number-4">2.4.4.</span> What is a measurement unit?</h4>
<div class="outline-text-4" id="text-2-4-4">
<ul class="org-ul">
<li>我们使用的大多数计量单位&#x2013;秒,千克,英里,主观评分等等&#x2013;其实都是人为设定,任意创造的.例如:
<ul class="org-ul">
<li>一秒 的定义基于铯-133 原子的能态跃迁频率(但为什么是这个同位素,而不是别的呢?)</li>
<li>一千克 等于一千克(1000 克),而 1 克最初被定义为一立方厘米水的重量(但为什么用立方厘米?为什么选
特定温度下的水?又为什么用十进制?)</li>
<li>一英里 等于 5280 英尺,至于为什么&#x2026; 嗯,你懂我的意思了.</li>
</ul></li>
<li>当然,也有一些计量单位似乎是宇宙通用的,也就是说,无论在宇宙何处,它们都是相同的.</li>
<li>一个例子是光速,物理学家相信光速在宇宙各处都是恒定的.</li>
<li>所以,如果你必须向一个外星人解释地球上的交通情况,说我们开车的速度是 8.946989587×10⁻⁸ c(c 代表光速)
会比说60 英里/小时更好&#x2013;毕竟"英里"和"小时"都是不具普适性的任意单位.</li>
<li>当然,这些人类自创的单位本身没有错,它们方便,易用,并能让我们有效沟通.</li>
<li><p>
我想表达的只是:这些不具有普遍意义,带有文化色彩的测量单位,只是把真实的宇宙与数据集中的数值隔开了
又一层.
</p>
<pre class="example" id="orga334481">
Non-universal, cultural measurement units are just one more step in between the actual universe
and the numerical values in our datasets.
</pre></li>
</ul>
</div>
</div>
<div id="outline-container-orge9fa869" class="outline-4">
<h4 id="orge9fa869"><span class="section-number-4">2.4.5.</span> Conclusion</h4>
<div class="outline-text-4" id="text-2-4-5">
<ul class="org-ul">
<li>虽然我们喜欢认为数据直接来自那些我们想要理解的宇宙中的事物,但事实并非如此.</li>
<li>实际上,数据来自测量设备.
<ul class="org-ul">
<li>如果测量设备不精确,存在噪声或有缺陷,那么数据将会很难甚至无法正确解读.</li>
<li>如果我们没能理解宇宙与数据之间的区别,就可能得出具有误导性甚至完全错误的结论.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orge2da577" class="outline-3">
<h3 id="orge2da577"><span class="section-number-3">2.5.</span> Accuracy, precision, resolution, range</h3>
<div class="outline-text-3" id="text-2-5">
<ul class="org-ul">
<li>测量设备的质量可以通过多种方式进行量化.在本节中,你将学习到本节标题所列四个术语的定义及其区别.</li>
</ul>
</div>
<div id="outline-container-org2152e3e" class="outline-4">
<h4 id="org2152e3e"><span class="section-number-4">2.5.1.</span> Accuracy 准确度</h4>
<div class="outline-text-4" id="text-2-5-1">
<ul class="org-ul">
<li>指的是测量数据相对于其所测量特征的正确程度</li>
<li>例如,一个廉价制造的心率监测仪报告你的心跳是每分钟80次,而你的实际心率是50 bpm(准确度低).相比之下,
更高质量的心率监测仪报告你的心率为52 bpm(准确度更高,尽管仍不完美).</li>
</ul>
</div>
</div>
<div id="outline-container-org08869a5" class="outline-4">
<h4 id="org08869a5"><span class="section-number-4">2.5.2.</span> Precision 精密度</h4>
<div class="outline-text-4" id="text-2-5-2">
<ul class="org-ul">
<li>是设备在重复测量中提供相同数据值的能力</li>
<li>例如,想象一下你在一个数字体重秤上上下下称了五次.你的体重在如此短的时间内不会波动,但测量值却是
72.4,73.1,70.4,73.2和72.1公斤.这个秤的精密度低.</li>
<li>相反,一个精密度更高的秤可能会显示72.4,72.5,72.4,72.2和72.3公斤(轻微的波动可能来自秤上姿势和体
重分布的变化).</li>
</ul>
</div>
</div>
<div id="outline-container-org102e455" class="outline-4">
<h4 id="org102e455"><span class="section-number-4">2.5.3.</span> Resolution 分辨率</h4>
<div class="outline-text-4" id="text-2-5-3">
<ul class="org-ul">
<li>指的是连续两次测量之间的数值差距</li>
<li>例如,厨房烤箱中一个低分辨率的温度传感器报告的温度精确到25°C(例如,200°,225°,250°),而分辨率更
高的烤箱则报告精确到1°C的温度.在信号处理和时间序列分析中,分辨率被称为采样率或离散化率;在数字图像
处理中,分辨率由像素数量决定.</li>
</ul>
</div>
</div>
<div id="outline-container-org8b94230" class="outline-4">
<h4 id="org8b94230"><span class="section-number-4">2.5.4.</span> Range 量程</h4>
<div class="outline-text-4" id="text-2-5-4">
<ul class="org-ul">
<li>是传感器能够测量的最小值和最大值</li>
<li>例如,你放在嘴里,用来判断能否说服父母让你请假在家打游戏或躺一整天的体温计,其量程可能是35°C至42°C.
让这种测量设备报告低至-100°C或高达300°C的温度是没有意义的.</li>
</ul>
<hr />
<ul class="org-ul">
<li>准确度和量程很少被混淆,因为这些都是日常生活中使用的概念.</li>
<li>然而,精密度和分辨率则更常被混淆,请务必理解它们之间的区别,因为你肯定想成为那个纠正别人错误的"讨厌
鬼",而不是那个被纠正的尴尬者.</li>
<li>图2.7展示了一系列常用于说明这些概念的靶心图.
<ul class="org-ul">
<li><p>
如图
</p>

<div id="orgb5a9eea" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/2-7.png" alt="2-7.png" />
</p>
<p><span class="figure-number">Figure 7: </span>ms/2-7.png</p>
</div></li>
<li>一个理想的测量设备只会将点打在靶心中心.</li>
<li>最下面一排的靶心图缺少的描述文字为:
<ol class="org-ol">
<li>A⬇️R⬆️P⬆️</li>
<li>A⬇️R⬆️P⬇️</li>
<li>A⬆️R⬇️P⬇️</li>
</ol></li>
</ul></li>
<li>毋庸置疑,准确度,分辨率和精密度都应该被最大化.但这并不总是可能:
<ul class="org-ul">
<li>部分是由于技术限制</li>
<li>部分是由于你试图测量的数据特性.例如,心理学家使用自我报告问卷来评估人格风格,但人们可能无法非常
准确地洞察自身能力(例如,65%的美国人认为自己比50%的人口更聪明).</li>
</ul></li>
<li>低精密度和低分辨率可以通过重复测量和取平均值来克服,例如称重五次然后取平均值作为你的"真实"体重.</li>
<li>低准确度则很成问题,因为它可能引入系统性偏差,从而导致误解.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org51d1894" class="outline-3">
<h3 id="org51d1894"><span class="section-number-3">2.6.</span> Data types</h3>
<div class="outline-text-3" id="text-2-6">
<ul class="org-ul">
<li>因为本书涉猎了coding部分,所以我们需要区别如下两个概念:
<ul class="org-ul">
<li>data type in computer science</li>
<li>data type in statistics</li>
</ul></li>
<li>在计算机科学中,数据类型指的是数据存储的格式(例如:int,string,float,bool).数据类型会影响变量
可以进行的操作种类,以及这些变量在计算机内存中所占用的存储空间大小.</li>
<li>在统计学中,数据类型是指根据数据的性质,将数据归纳到的某个类别.数据类型的类别会影响可以对数据应用
的可视化方式以及统计分析方法.</li>
<li>人们已经区分出了许多不同的数据类型.总体上,数据可以分为数值型或分类型(有时也称为标签型).在这两
个大类中,还可以进一步细分.
<ul class="org-ul">
<li><p>
图 2.8 提供了一个概览.
</p>

<div id="orgc9afdc4" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/2-8.png" alt="2-8.png" />
</p>
<p><span class="figure-number">Figure 8: </span>ms/2-8.png</p>
</div></li>
</ul></li>
<li>我们下面挨个来介绍下图2-8中的各种数据类型</li>
</ul>
</div>
<div id="outline-container-org23d11cd" class="outline-4">
<h4 id="org23d11cd"><span class="section-number-4">2.6.1.</span> 数值型数据(Numerical data)</h4>
<div class="outline-text-4" id="text-2-6-1">
<ul class="org-ul">
<li>由数字组成.但并不仅仅是"看起来是数字"就能算数值型,因为分类数据(categorical data)有时也会用数字
表示.</li>
<li>数值型数据的定义特征在于:数据值与它所反映的现实世界数量之间有有意义的联系.
<ul class="org-ul">
<li>例如,如果你有 47 个苹果,那么"47"与一个实际存在的物理数量密切相关.</li>
<li>反之,如果我们规定 BMW=1,Mercedes=2,Honda=3,那么"1"和 BMW 之间并没有内在意义上的映射关系;我本
可以给 BMW 指定成任何一个数字(或者根本不用数字).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgd1b12ad" class="outline-4">
<h4 id="orgd1b12ad"><span class="section-number-4">2.6.2.</span> 离散数据(Discrete data)</h4>
<div class="outline-text-4" id="text-2-6-2">
<ul class="org-ul">
<li>离散数据反映的是可数的事物,通常用整数来表示.</li>
<li>例如,一个城市的人口就是可数的,并且一定是整数
<ul class="org-ul">
<li>新西兰惠灵顿市的人口可能是 422,000 人,</li>
<li>但不会是 422,000.2 人.</li>
</ul></li>
<li>这意味着数据的精度限制在整数范围内,而这个限制来自数据本身,而不是来自测量工具.</li>
<li>这就引出了**区间数据(Interval data)**的概念(因为Interval data的精度由工具决定).</li>
</ul>
</div>
</div>
<div id="outline-container-org5e48561" class="outline-4">
<h4 id="org5e48561"><span class="section-number-4">2.6.3.</span> 区间数据(Interval data)</h4>
<div class="outline-text-4" id="text-2-6-3">
<ul class="org-ul">
<li>区间数据是具有有意义的间隔(interval)且精度由测量工具决定的数值数据.</li>
<li>所谓"测量精度由工具限制",是指数据本身没有精度限制,而是测量设备决定了我们能测得多精确.</li>
<li>举例来说,如果室外温度是 24 °C,一个更精确的温度计可能读出 24.43 °C,更精确的甚至可能读到
24.4323492184 °C,依此类推.</li>
<li>测量精度取决于仪器和应用场景(比如在实验室监控化学反应需要远高于测量一个城市的平均室温的精度).</li>
<li>需要注意的是,0 °C 并不表示"没有温度";它依然是一个具体的温度值,–24 °C 也是如此.</li>
<li>"零点"的意义在统计数据类型中非常重要,它直接决定了区间数据与比率数据(Ratio data)的区别.</li>
<li>为什么"0"的含义很重要?:
<ul class="org-ul">
<li>这是因为"0"的含义决定了我们能对数据做什么数学运算.</li>
</ul></li>
<li>我们可以对区间数据做加法和减法,但不能做乘法或除法.
<ul class="org-ul">
<li>例如:20 °C 比 10 °C 高 10 °C,但不能说它是"两倍热".这是因为 0 °C 的物理意义是:在标准大气压下,
纯水冻结时原子所具有的某个动能水平.</li>
<li>原子完全可以有两倍的动能,但 2×0 °C 依然是 0 °C,这个运算在物理意义上不成立.</li>
<li>另一个例子:–10 °C 的动能翻倍显然不会得到 –20 °C.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgd2b9de0" class="outline-4">
<h4 id="orgd2b9de0"><span class="section-number-4">2.6.4.</span> 比率数据(Ratio data)</h4>
<div class="outline-text-4" id="text-2-6-4">
<ul class="org-ul">
<li>比率数据是具有有意义零点的区间数据,因此我们可以对它进行乘法和除法.
<ul class="org-ul">
<li>例如身高:一个 10 米高的建筑确实是一个 5 米高建筑的两倍高,而 0 米 代表完全没有高度.</li>
</ul></li>
<li>比率数据既可以相加,也可以相乘.</li>
<li>比率数据通常不能取负值&#x2013;没有"负高度",就像一条线不能有"负长度",气球也不能有"负体积".</li>
<li>不过在一些情境中,为了方便,我们会用负数表示相反方向的概念,例如:"负高度"可能用来表示地下建筑;"负货币"表示负债.</li>
</ul>
</div>
</div>
<div id="outline-container-org46f6e1c" class="outline-4">
<h4 id="org46f6e1c"><span class="section-number-4">2.6.5.</span> 分类数据(Categorical data)</h4>
<div class="outline-text-4" id="text-2-6-5">
<ul class="org-ul">
<li>也叫 标记数据(labeled data).</li>
<li>这类数据由不同的类别构成,而这些类别之间可能在许多方面存在差异.
<ul class="org-ul">
<li>例如,"高中"与"大学"是变量"教育水平"的两个类别,但它们之间的差异不仅仅是一个学位名称的不同,还可
能涉及教育,学术,社会,文化,经济等多方面的差别.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgbd4875e" class="outline-4">
<h4 id="orgbd4875e"><span class="section-number-4">2.6.6.</span> 名义数据(Nominal data)</h4>
<div class="outline-text-4" id="text-2-6-6">
<ul class="org-ul">
<li>名义数据是离散的,不可排序的类别.</li>
<li>这意味着我们可以给不同的类别分配数字,但这种映射是任意的,数字与类别之间没有内在联系,数字彼此之间
也没有关系.
<ul class="org-ul">
<li>比如,我们做一个问卷调查,让人们填写他们最近看过的电影类型.类型可能包括:科幻片(sci-fi),浪漫
喜剧(romcom),纪录片(documentary),动作片(action),剧情片(drama)等.</li>
<li>我们可以设置映射:科幻片 = 1,浪漫喜剧 = 2.但这种数字分配与电影类型本身没有任何关系,我完全可以
把浪漫喜剧设为 1,把科幻片设为 2.</li>
</ul></li>
<li>这种编号不能拿来做数学运算:两个"科幻片"并不等于一个"浪漫喜剧",浪漫喜剧也不是科幻片的"两倍类型".</li>
</ul>
</div>
</div>
<div id="outline-container-org68632cb" class="outline-4">
<h4 id="org68632cb"><span class="section-number-4">2.6.7.</span> 有序数据(Ordinal data)</h4>
<div class="outline-text-4" id="text-2-6-7">
<ul class="org-ul">
<li>有序数据是可以按某种指标排序的离散数据.以"教育水平"为例:
<ul class="org-ul">
<li>可能的取值包括初中,高中,本科,硕士,博士.</li>
<li>这些类别是离散的,并且可以排序:本科高于高中,硕士高于本科.</li>
<li>我们也可以给它们分配数字:初中 = 1,高中 = 2,本科 = 3,硕士 = 4,博士 = 5.</li>
</ul></li>
<li>但是,类别间的差距以及分配的数字并不是等距的:高中和本科的差别并不像本科和硕士的差别那样在数量上相
等.两个高中学历的人也不会"等于"一个硕士学位.</li>
</ul>
</div>
</div>
<div id="outline-container-org8fa7b44" class="outline-4">
<h4 id="org8fa7b44"><span class="section-number-4">2.6.8.</span> 数据类型的变化</h4>
<div class="outline-text-4" id="text-2-6-8">
<ul class="org-ul">
<li>通过改变收集数据的方式,我们有时可以将数据类型从名义数据变为有序数据.
<ul class="org-ul">
<li>比如刚才的电影类型例子,如果我们让受访者按喜好顺序排列电影类型(比如某人最喜欢科幻片,其次动作片
其次纪录片),那么这些数据就可以排序,也能分配有意义的数字,就成了有序数据,而不是名义数据.</li>
</ul></li>
<li>在下一章你会看到,区间数据和比率数据通常会被转化成离散数据用来绘制直方图.</li>
</ul>
</div>
</div>
<div id="outline-container-orgee124a5" class="outline-4">
<h4 id="orgee124a5"><span class="section-number-4">2.6.9.</span> 有序数据的数学运算</h4>
<div class="outline-text-4" id="text-2-6-9">
<ul class="org-ul">
<li>理论上,我们不能对有序数据进行算术运算(原因同上),不过在实践中,很多地方都会这么做.</li>
<li>虽然解释上会有问题,但这种方法足够常见,以至于我们习惯接受它.举个例子:
<ul class="org-ul">
<li>我希望你喜欢读这本书,如果你喜欢,也希望你在购买网站给它评分.评分可能是 1,2,3,4 或 5 星.</li>
</ul></li>
<li>但问题是:
<ul class="org-ul">
<li>1 星和 2 星的差异,和 4 星与 5 星的差异是一样的吗?</li>
<li>4 星是否正好是 2 星的两倍好?</li>
<li>两个低分能不能相加变成一个高分?</li>
</ul></li>
<li>答案都是 不成立.评分是有序数据,而不是数值数据.因此,它不应该被加总或平均.</li>
<li>然而,现实中每个有评分的网站都会显示平均分,消费者也会用这些平均分来做购买决策.</li>
<li>这是严重的统计学违规行为并会造成严重后果吗?不是.</li>
<li>平均分确实有参考价值,我们也没必要在每一种数据类型都严格套用规则.</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org1a1b378"></a>从纯统计到应用统计<br />
<div class="outline-text-5" id="text-2-6-9-1">
<ul class="org-ul">
<li>这个例子虽小,却深刻反映出统计的现实:
<ul class="org-ul">
<li>统计学中充满了假设,要求和规则,有些必须遵守,有些则可以在一定条件下弯曲,甚至打破.</li>
<li>纯统计(pure statistics) 是算法开发和严格数学证明所必需的;</li>
<li>应用统计(applied statistics) 则允许我们在复杂,混乱的现实世界中运用统计方法.</li>
</ul></li>
<li>现实世界很混乱,如果一味死守规则,我们可能寸步难行.</li>
<li>我希望在读完本书后,你能理解 什么时候,为什么可以稍微弯曲规则,甚至在某些假设不成立的情况下继续分析</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-orge32a6de" class="outline-3">
<h3 id="orge32a6de"><span class="section-number-3">2.7.</span> From anecdotes to populations</h3>
<div class="outline-text-3" id="text-2-7">
<ul class="org-ul">
<li>你可能知道,datum 指的是一个单独的数据点,而 data 指的是多个数据点.</li>
<li>但是,数据还有更细致的层次划分,这些划分会影响你能对数据进行的统计分析类型&#x2013;以及你能得出的结论类型
我会先列出并简要定义这些层次,然后再进行更深入的讨论.</li>
<li>在此之前,先让我来定义一下样本量(sample size):它指的是一个数据集中观测值的数量.
<ul class="org-ul">
<li>例如,如果你调查了 1000 个人,询问他们的生活幸福感和咖啡消费情况,那么你的样本量就是 1000.</li>
</ul></li>
<li>不要把样本量(sample size)和数据点总数(total number of data points)混淆:在这个例子中,总数据点数是
2000(1000 个人分别给出关于幸福感和咖啡的回答),由 1000 个样本和两个特征组成.</li>
<li>样本量通常用大写字母 N(有时用小写 n)表示,所以在这个例子中,N = 1000.</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org44a93fc"></a>推测或理论(Speculation or theory, N = 0)<br />
<div class="outline-text-5" id="text-2-7-0-1">
<ul class="org-ul">
<li>开发新的想法,假设和理论,即使一个数据点都没有,也是可以完成的.虽然这对科学发展至关重要,但这种非
实证研究与统计学教材并不直接相关.</li>
</ul>
</div>
</li>
<li><a id="org065f58a"></a>轶事(Anecdote, N = 1)<br />
<div class="outline-text-5" id="text-2-7-0-2">
<ul class="org-ul">
<li>轶事是关于一个人所发生事情的故事.</li>
</ul>
</div>
</li>
<li><a id="org000ec68"></a>病例报告(Case report, N = 1)<br />
<div class="outline-text-5" id="text-2-7-0-3">
<ul class="org-ul">
<li>病例报告本质上是一种在医学背景下的轶事,通常是对某位患有罕见病症患者的诊断或治疗的描述.病例报告有
时可能包含多个患者(即 N &gt; 1),但样本量仍然很小.</li>
</ul>
</div>
</li>
<li><a id="org7da6e82"></a>样本(Sample)<br />
<div class="outline-text-5" id="text-2-7-0-4">
<ul class="org-ul">
<li>在前文中我定义了样本量(sample size).样本指的是你进行测量并收集数据的个体集合.例如,如果你想知道
狐獴的体重,不可能测量所有曾经存在过的狐獴;相反,你会测量其中的一部分&#x2013;这部分就是样本.</li>
</ul>
</div>
</li>
<li><a id="orgd3a6ca6"></a>预研 / 试点研究(Pilot study,小样本 N)<br />
<div class="outline-text-5" id="text-2-7-0-5">
<ul class="org-ul">
<li>"试点研究"并不是关于驾驶飞机的飞行员的研究项目.它的真正含义是:
<ul class="org-ul">
<li>使用太小而不足以进行严格统计分析的样本量进行的研究,</li>
<li>但样本量又大到足以判断设备和实验过程 <b>是否可行</b></li>
</ul></li>
<li>试点研究(也叫"探索性研究"或"概念验证研究")的作用包括:
<ul class="org-ul">
<li>在正式收集全量数据前,有机会调整实验方案;</li>
<li>向资助机构证明研究团队有能力完成更大型的研究计划.</li>
</ul></li>
</ul>
</div>
</li>
<li><a id="org9628e12"></a>小规模 vs. 大规模研究(Small-scale vs. large-scale studies)<br />
<div class="outline-text-5" id="text-2-7-0-6">
<ul class="org-ul">
<li>这两个术语有一定模糊性,因为并没有一个明确的样本量界定何为"小"或"大";它取决于多个因素,例如效:
<ul class="org-ul">
<li>应量大小</li>
<li>数据获取难度</li>
<li>以及该研究领域的常见样本量.</li>
</ul></li>

<li>大规模研究的主要优势是:
<ul class="org-ul">
<li>具有更高的统计效力以控制混杂因素;</li>
<li>能够识别更小的效应量;</li>
<li>结果更容易推广到更广泛的人群.</li>
</ul></li>
</ul>
</div>
</li>
<li><a id="orgd175059"></a>观察性研究(Observational study)<br />
<div class="outline-text-5" id="text-2-7-0-7">
<ul class="org-ul">
<li>这个术语有点容易误导&#x2013;数据都是"观察"得到的,那么是不是所有有数据的研究都是观察性研究?
<ul class="org-ul">
<li>其实不然.观察性研究是指在没有任何实验性操控的情况下收集数据,即没有引入干预或控制.</li>
<li>例如:研究男性或女性在 1 月 vs. 6 月加入健身房的倾向.</li>
</ul></li>
<li>与之相对的,是实验性研究(experimental study),即研究人员会操控变量进行干预.</li>
</ul>
</div>
</li>
<li><a id="org3370c49"></a>便利样本(Convenience sample)<br />
<div class="outline-text-5" id="text-2-7-0-8">
<ul class="org-ul">
<li>便利样本是指研究人员几乎不费力气就获取到的样本.比如只调查你的朋友和家人,就是一种便利样本.</li>
<li>便利抽样的结果往往缺乏普遍性,因为这个易获取的群体可能与更广泛的人群存在差异.尽量避免这种采样方式,
除非是在试点研究阶段,它才是可接受的.</li>
</ul>
</div>
</li>
<li><a id="orgb69c62f"></a>总体(Population)<br />
<div class="outline-text-5" id="text-2-7-0-9">
<ul class="org-ul">
<li>总体是指你想要了解的所有对象的集合.在很多情况下,总体是不可能被完全测量的,例如宇宙中所有的恒星.
这也是为何我们用样本来进行分析的原因.</li>
</ul>
</div>
</li>
</ol>
<div id="outline-container-orgdfb2efb" class="outline-4">
<h4 id="orgdfb2efb"><span class="section-number-4">2.7.1.</span> Sample vs. population</h4>
<div class="outline-text-4" id="text-2-7-1">
<ul class="org-ul">
<li>虽然个体的情况可能很有趣,但大多数研究的目标是理解总体.
<ul class="org-ul">
<li>举例来说,一个生产小工具的公司,并不想只把小工具卖给一个人,而是希望卖给很多人;</li>
<li>心理学家想要理解人们的思维,情感和行为,而不仅仅是理解某一个人;</li>
<li>肿瘤科医生的目标是治愈所有人的所有癌症,而不是只为某一个人治好某一种癌症.</li>
</ul></li>
<li>有些总体很小,可以全部测量.例如:
<ul class="org-ul">
<li>如果我们对某家公司某个部门里所有员工的薪资感兴趣;</li>
<li>或者我们想知道一个动物园里所有狮子的年龄&#x2013;这些都是总体,而且我们可以测量总体中每一个成员.</li>
</ul></li>
<li>但是,很多总体是无法被完整测量的.比如,
<ul class="org-ul">
<li>如果我们想知道全体意大利人的平均身高,维基百科上有人声称,意大利人大约有 1.4 亿人.要找到每一个意
大利人并测量他们的身高,显然不可行.</li>
<li>因此,我们会选择一个较小数量的意大利人&#x2013;比如几百人&#x2013;来测量他们的身高,这就是样本.</li>
</ul></li>
<li>抽样的核心目的是:我们希望把结论推广到整个总体.
<ul class="org-ul">
<li>仔细想想这里的逻辑:我们想知道所有意大利人的身高,</li>
<li>但我们只测量了 100 个意大利人,</li>
<li>我们并不是真的只关心这 100 个具体的人,而是想根据样本,对整个意大利人群做出科学的结论.</li>
<li>换句话说,我们希望能从样本推广到总体.</li>
</ul></li>
<li>但是,这个逻辑站得住吗?
<ul class="org-ul">
<li>我们真的能根据样本对总体做出结论吗?</li>
<li>答案是有条件的"可以".</li>
</ul></li>
<li>是的,我们可以推广,但前提是满足特定条件,即样本必须是
<ul class="org-ul">
<li>随机的(random)</li>
<li>具有代表性(representative)</li>
<li>且样本量足够大(sufficiently large)</li>
</ul></li>
<li>我将在第 9 章详细讨论这个问题,但因为从样本推广到总体极其重要,所以我希望现在就把这个概念种在你的脑
海里&#x2013;它正是统计学的核心所在.</li>
</ul>
</div>
</div>
<div id="outline-container-orgba8ed98" class="outline-4">
<h4 id="orgba8ed98"><span class="section-number-4">2.7.2.</span> "Big enough" samples</h4>
<div class="outline-text-4" id="text-2-7-2">
<ul class="org-ul">
<li>多大的样本量才足够让我们将样本推论至总体呢?</li>
<li>这是一个非常重要的问题,但遗憾的是,无法给出一个通用的答案.</li>
<li>我真希望能像这样轻松下结论:"N = 30 就是一个足够大的样本量".</li>
<li>但现实是,合适的样本量取决于多个因素,包括:
<ul class="org-ul">
<li>效应量(effect size);</li>
<li>样本与总体的变异程度(variability);</li>
<li>样本特征与总体特征的匹配程度;</li>
<li>样本是如何收集的.</li>
</ul></li>
<li>除此之外,还有一些实际因素会影响样本量:
<ul class="org-ul">
<li>某些类型的病人群体十分稀少或难以接触;</li>
<li>有些研究成本高且受预算限制;</li>
<li>有些研究耗时长,而研究人员可能是迫于论文答辩压力的博士生,需要尽快完成工作.</li>
</ul></li>
<li>你将在第 17 章学到更多有关估算合理样本量的知识;目前你只需记住这样一个关键点:
<ul class="org-ul">
<li>样本量越大,通常越好("更好"意味着统计结果中出现偶然偏差的风险更小,结论更容易推广到抽样来源的总
体);</li>
<li>但现实中,非常大的样本量并不总是可行的.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org64d4895" class="outline-4">
<h4 id="org64d4895"><span class="section-number-4">2.7.3.</span> Problems with N=1 studies</h4>
<div class="outline-text-4" id="text-2-7-3">
<ul class="org-ul">
<li>案例研究和轶事(anecdote)可以成为很精彩的故事,但在解读时必须格外谨慎,尤其当研究目标是为了理解总
体时.</li>
<li>我并不是说 N = 1 的报告不应该存在&#x2013;它们可能很有趣,也能激发灵感.</li>
<li>有些人甚至可能仅仅基于一个轶事或一个临床病例报告,就发展出完整的研究方向.但是,真正带有严格统计分
析并且具有可推广性的研究,往往发生在大规模研究中.</li>
<li>为什么 N = 1 的研究难以解读呢?
<ul class="org-ul">
<li>主要限制在于,这种观察结果很可能缺乏代表性.</li>
</ul></li>
<li>事实上,这类案例之所以被报道,是因为它不寻常,否则就不会引起关注.</li>
<li>这就像新闻总是报道那些重大,罕见的事件,而不会去报道某个普通人度过了和几百万人一模一样的平凡一天.</li>
<li>这意味着,在 N = 1 的研究中,几乎不可能进行统计推断,也很难将结论推广到总体.</li>
<li>另一方面,拥有大样本量也并不能保证就能获得完美的统计推断和可推广性.依然可能出现的问题包括:
<ul class="org-ul">
<li>抽样变异(sampling variability)</li>
<li>噪声或离群值(noise / outliers)</li>
<li>实验设计缺陷</li>
<li>统计方法使用错误等等.</li>
</ul></li>
<li>关键结论是:
<ul class="org-ul">
<li><p>
案例报告和轶事无法推广到总体;而大规模研究才有可能具备推广性.
</p>
<pre class="example" id="org4dfe81d">
Case reports and anecdotes cannot be generalized, whereas larger-scale studies proffer the
possibility of generalizability.
</pre></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org714e5f8" class="outline-3">
<h3 id="org714e5f8"><span class="section-number-3">2.8.</span> Data management</h3>
<div class="outline-text-3" id="text-2-8">
<ul class="org-ul">
<li>数据管理(Data management) 指的是一种用于记录,存储,和归档数据的系统.</li>
<li>根据数据的性质,合理的数据管理可能包括安全限制或匿名化处理.</li>
<li>数据管理的范围还包括你用来处理,可视化,和分析数据的代码.</li>
<li>数据管理的重要性在于,格式整齐,结构清晰的数据更容易使用和共享.</li>
<li>良好的数据管理还能帮助你预防问题,并方便在需要时共享数据.</li>
<li>我们在这个美丽的地球上时间有限,而恰当的数据管理能让我们把更多时间花在有趣且有洞察力的分析上,而不
是被缺乏文档,格式混乱的数据搞得焦头烂额.</li>
<li>好消息是,数据管理并不复杂,只要提前投入一些精力,就能确保高质量的数据.
<ul class="org-ul">
<li>要管理好数据,你需要一个计划.</li>
</ul></li>
<li>数据管理计划(Data Management Plan,DMP)需要针对每个数据集和实验方案进行定制化设计,因此我无法提供
一个通用的"一招通"的方案.</li>
<li>所以,在正式收集数据集之前,请先设计或调整一份数据管理计划,
<ul class="org-ul">
<li>记录你的命名规则,组织方式,数据与代码文件的存储方法,</li>
<li>并考虑你将在代码中如何识别和导入数据文件.</li>
</ul></li>
<li>最后,一个良好的数据管理计划必须包含数据备份.备份的方法取决于数据的性质和大小,但最重要的规则是:
<ul class="org-ul">
<li>务必保留多份数据副本.</li>
</ul></li>
<li>一个可靠的备份方案可能包括:
<ul class="org-ul">
<li>一份数据存在办公室里的外接硬盘上;</li>
<li>一份数据存在笔记本电脑里;</li>
<li>另一份数据放在云存储中.</li>
</ul></li>
<li>许多研究机构和大型公司都有内部的数据归档系统.</li>
<li>但需要注意,某些敏感数据(如医疗或财务信息)可能被禁止存储在非安全地点,因此在备份时一定要遵守相应
法律与政策要求.</li>
</ul>
</div>
</div>
<div id="outline-container-org7141434" class="outline-3">
<h3 id="org7141434"><span class="section-number-3">2.9.</span> The ethics of making up data</h3>
<div class="outline-text-3" id="text-2-9">
<ul class="org-ul">
<li>听起来很简单,对吧?"不要捏造数据."&#x2013;好像一句话就能结束讨论.</li>
<li>但这里需要一些细微区分.捏造数据本身并不是有问题的.
<ul class="org-ul">
<li>事实上,我为这本书创造了大量的虚拟数据,</li>
<li>在你用这本书学习时,我也会鼓励你创造大量的虚拟数据.</li>
<li>我在其他书籍,在线课程以及线下教学中,同样经常使用虚拟数据.</li>
</ul></li>
<li>有些人会用"生成数据(generating data)"或"模拟数据(simulating data)"这样的说法,但归根到底,这和
"捏造假数据"是同一回事.</li>
<li>生成虚拟数据是有价值的
<ul class="org-ul">
<li>模拟数据是学习统计的无价方法.</li>
<li>通过创造并使用虚拟数据,你可以在统计分析方面建立信心和直觉,这些是仅使用真实数据往往难以获得的.</li>
</ul></li>
<li>我会在第 5 章详细解释和论证这一点,但提前说结论:
<ul class="org-ul">
<li>数据创造过程使你能够控制效应量,噪声特征,数据分布形状,样本量等关键因素;</li>
<li>而这些因素在真实数据中往往无法人为操控,却对理解统计原理至关重要.</li>
</ul></li>
<li>将虚拟数据冒充真实数据是极其有害的
<ul class="org-ul">
<li>当人们讨论"捏造数据的伦理"时,一般指的是这一种情况.</li>
<li>问题不在于"假数据"本身,而在于把假数据当成真数据来表现和使用.</li>
</ul></li>
<li>我们可以区分两种不诚信行为:
<ul class="org-ul">
<li>捏造数据(faking data):凭空编造整组数据集;</li>
<li>篡改数据(manipulating data):为了得到特定结果,故意修改真实数据集.</li>
</ul></li>
<li>这两种行为在伦理上没有区别:都不要做.</li>
<li>如果你被发现将假数据冒充为真数据,可能会失去工作,失去声誉;</li>
<li>如果你与他人合作,他们也可能受到牵连.</li>
<li>如果数据在现实世界中有直接影响(例如制造工厂的安全数据),</li>
<li>那么捏造或篡改这些数据可能会直接危及他人的生命安全.</li>
<li>即使你没有被发现,你也不想背负那种长期撒谎带来的压力,焦虑与恐惧.我们生活在这个复杂多变的世界中,
已经有足够多的社会,经济,健康和气候压力,不要再给自己加上"假数据"这个烦恼来源.</li>
<li>犯错与造假是两回事
<ul class="org-ul">
<li>每个人都会犯错&#x2013;这是人之常情.</li>
<li>错误(mistake)与操纵(manipulation)的区别是:诚实的错误是无意的,而捏造或篡改数据是有意的.</li>
<li>如果你意识到自己犯了错,那就承认,诚恳道歉,改正它,从中学习,然后继续前进.</li>
</ul></li>

<li>总结起来:
<ul class="org-ul">
<li>捏造数据(虚拟数据)是极佳的方式,可以帮助你深入理解数据可视化,描述统计与推断统计,分析局限性,
以及数据特征(如样本量,方差,离群值,分布)对结果的影响.</li>
<li>虚拟数据同样非常适合用来向学生,同事和客户展示假设性的数据模式.</li>
</ul></li>
<li>问题在于:当假数据或被篡改的数据被当作真实数据呈现时,就会引发严重的伦理问题.</li>
<li>所以,必须明确说明数据是假的(可以用更温和的术语,如"模拟的(simulated)""生成的(generated)""假设
的(hypothetical)"或"理论的(theorized)").不要留下任何可能让人误解数据真实性的空间.</li>
<li>更成熟,诚实的讨论
<ul class="org-ul">
<li>在教材里写一句"不要做有伦理争议的事"很容易,但现实世界从来不是黑白分明的.</li>
<li>在数据采集,筛选,清洗,以及选择分析方法和参数的过程中,我们会做出无数决策;</li>
<li>你可能会在无意识状态下,为了得到某个更可能"理想"的结果,而在分析流程中引入微妙的偏差.</li>
</ul></li>
<li>一些严苛的统计学"纯粹派"可能会反对这种说法,
<ul class="org-ul">
<li>他们会建议整个数据处理与分析流程在数据收集前就全部固定,</li>
<li>并且只有在收集到全新数据集的情况下才能做任何调整.</li>
</ul></li>
<li>我对这种理想化想法并不反感&#x2013;预先设定的分析流程确实是每位数据科学家值得追求的目标.</li>
<li>但是我更愿意面对现实:
<ul class="org-ul">
<li>人类在不断开发更先进的方法来测量生物与物理世界;</li>
<li>数据集越来越庞大且更复杂;</li>
<li>在拿到数据之前,最佳的分析方式可能根本无法完全预知;</li>
<li>科学需要不断改进与保持灵活性.</li>
</ul></li>
<li>现实是:
<ul class="org-ul">
<li>统计分析往往是迭代进行的,每一次迭代都会改进和优化分析流程.</li>
<li>这种迭代可能让我们发现一些细微而有价值的数据模式,但也会带来引入偏差,影响结果的风险.</li>
</ul></li>
<li>在保持分析灵活性的同时减少偏差,是有原则可循的&#x2013;你会在第 18 章学到更多.</li>
<li>我想表达的是:尽管"绝对化"的数据伦理立场听起来权威且容易写下,但面对现代数据分析的现实,成熟而细腻
的讨论才更有意义.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org365ba1f" class="outline-2">
<h2 id="org365ba1f"><span class="section-number-2">3.</span> Chapter 03: Visualizing Data</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org239c030" class="outline-3">
<h3 id="org239c030"><span class="section-number-3">3.1.</span> Why visualize data?</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>我非常热爱数据的可视化.
<ul class="org-ul">
<li>不仅仅是我&#x2013;任何处理数据的人都会同意,数据可视化是理解和传达数据最重要的方式之一.</li>
</ul></li>
<li>你应该进行数据可视化,有几个重要原因:
<ul class="org-ul">
<li>可视化能够帮助你发现单看数字或标签可能很难甚至不可能察觉的模式.</li>
<li>大脑是一个惊人的模式识别机器,而我们人类本质上是视觉型的生物.事实上,我们大脑大约三分之一的区域
专注于视觉处理,并且还有更多的大脑区域与视觉密切相关,例如由视觉引导的运动.因此,把数据呈现成一
种大自然塑造我们大脑去处理的形式,符合我们的天性.</li>
<li>查看可视化数据可以发现离群值,错误,或其他可能引入伪影(artifacts)或对分析造成负面影响的问题.</li>
<li>使用图表展示数据,可以帮助非专业人士理解分析中的重要结论.换句话说,良好的数据可视化能帮助你塑造
数据故事(data narrative),并将这个故事传递给别人.</li>
<li>数据是美的,而美的事物值得我们细细品味.</li>
</ul></li>
<li><p>
为了说明可视化在理解数据(以及更广泛地说,理解数学)方面的重要性,请看看以下这个方程:
</p>
\begin{equation}
\left(x^{2} + y^{2} - 1\right)^{3} - x^{2} y^{3}  = 0 \tag{3.1}
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-org7c9bbd5" class="outline-3">
<h3 id="org7c9bbd5"><span class="section-number-3">3.2.</span> How to visualize data</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li>数据可视化之所以是一项技能&#x2013;甚至说是一门艺术&#x2013;是因为有些数据具有内在特性,可以直接决定它们该如何被
可视化.
<ul class="org-ul">
<li>例如,不同邮政编码区域的人口数可以在地图上用颜色来表示;</li>
<li>而另外一些数据,并不天然地对应某种特定的可视化类型.</li>
</ul></li>
<li>所以,我们必须决定,对于不同的数据集,该使用哪些可视化方式,以及是否应该使用特定的方式.这并不是一
个简单的任务,尤其是在处理大型多变量数据集时会更具挑战性.</li>
<li>对于小数据集,你可以直接查看全部数据.
<ul class="org-ul">
<li>举个例子,我曾记录了一周内每天晚上开始吃晚餐的时间.</li>
<li><p>
图 3.1 在电子表格中展示了这些数据.
</p>

<div id="org6f850b0" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-1.png" alt="3-1.png" />
</p>
<p><span class="figure-number">Figure 9: </span>ms/3-1.png</p>
</div></li>
<li>这实际上就是整个数据集,所以数据的可视化,就只是看一眼这些数字那么简单.</li>
</ul></li>
<li>然而,大多数数据集(我甚至敢说&#x2013;所有你在本书,其他书/课程以及现实世界中遇到的数据集)都比这个例子庞
大得多:可能包含几百,几千,甚至几百万行数据,以及几十到上百个数据列.</li>
<li>在这种情况下,重要的模式仅靠肉眼浏览原始数字是很难,甚至根本不可能看出来的.</li>
<li>关键在于:查看原始数据虽然有用,也能提供一些信息,但是,图形化的数据表达往往更有洞察力,更美观,也
更容易解释.</li>
<li>在本章的接下来的部分,你将学习最常见的数据可视化方式&#x2013;它们长什么样,怎么解读,以及可以用于哪种类型的数据.</li>
<li>在整本书中,你也会看到大量数据可视化的实例.</li>
</ul>
</div>
</div>
<div id="outline-container-orgdf3c35d" class="outline-3">
<h3 id="orgdf3c35d"><span class="section-number-3">3.3.</span> Bar plots</h3>
<div class="outline-text-3" id="text-3-3">
<ul class="org-ul">
<li>让我们先看一个例子.假设我们进行了一项调查,向人们询问他们从哪里获取新闻¹.他们可以选择的答案是:
<ul class="org-ul">
<li>电视(TV)</li>
<li>报纸(newspaper)</li>
<li>互联网(Internet)</li>
<li>或口口相传(word of mouth).</li>
</ul></li>
<li>我们计算选择每一种新闻来源的参与者所占的百分比.
<ul class="org-ul">
<li><p>
图 3.3 展示了这一调查结果,并用**条形图(bar plot)**进行可视化.
</p>

<div id="org08ebae9" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-3.png" alt="3-3.png" />
</p>
<p><span class="figure-number">Figure 10: </span>ms/3-3.png</p>
</div></li>
<li>横轴(x 轴)上的刻度对应于各类新闻来源,</li>
<li>纵轴(y 轴)&#x2013;也就是每个柱子的高度&#x2013;对应选择这一新闻来源的人的百分比.</li>
</ul></li>
<li>请注意:
<ul class="org-ul">
<li>x 轴表示的是分类变量(categorical variable);</li>
<li>y 轴表示的是连续的数值型变量(continuous numerical variable).</li>
</ul></li>
<li>条形图适用于名义数据(nominal data).</li>
<li>学习直方图(histogram)时一定要记住这一点,因为条形图(bar plot)和直方图(histogram)一开始看上去可能
很相似,但使用场景不同.</li>
<li>另外要注意,这些柱子的百分比相加并不等于 100%,因为参与者可能会从多个来源获取新闻.这点会在后面学习
饼图(pie chart)以及直方图百分比归一化时做对比.</li>
<li>一些关于用条形图可视化数据的补充(更通用的)说明:
<ul class="org-ul">
<li>坐标轴必须有标签,明确告诉读者所展示的内容;</li>
<li>每个刻度(坐标轴上的小刻线)都有对应的标签,尤其是 x 轴;</li>
<li>标签可以旋转以适应较长的文字,或出于美观的考虑;</li>
<li>x 轴上的刻度距离相等,让图形看起来整齐美观;</li>
<li>x 轴上类别的顺序是任意的.例如,"报纸" 并不一定非要排在"互联网"左边;类别可以按多种方式排序,如按
新闻来源发展历史的先后,或按其受欢迎程度(即按 y 轴数值排序).这都是我们人为的选择,而不是数据本
身固有的属性;</li>
<li>y 轴的范围应当与数据范围匹配(想象一下,如果 y 轴画成 -500% 到 +6000%的区间会多荒谬).</li>
<li>有些数据是天然有界的(如身高不可能为负),有些则没有固定边界.</li>
<li>坐标轴范围一般略微超过数据范围,即在最小值的下方和最大值的上方略微延伸;</li>
<li>标题要简短且信息充足;</li>
</ul></li>
<li>图中没有颜色.本书的实体版采用灰度印刷以降低印刷成本.如果你绘制的图表会有人打印成纸质版,尽量避免
使用颜色;</li>
<li>但如果你的图表只会在电子设备上查看,或确定会彩色打印,那么颜色可以使图表更美观,也更易于区分图形元
素(参见 3.12 节).</li>
<li>使用python创建一个条形图(Bar Plot)非常简单
<ul class="org-ul">
<li><p>
下面的代码就能创建图3-4的例子
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #531ab6;">import</span> matplotlib.pyplot <span style="color: #531ab6;">as</span> plt

<span style="color: #005e8b;">Y</span> = <span style="color: #000000;">[</span>1, 4, 3, 9<span style="color: #000000;">]</span>  <span style="color: #7f0000;"># </span><span style="color: #7f0000;">bar heights
</span><span style="color: #005e8b;">X</span> = <span style="color: #000000;">[</span>0, 1, 3, 4<span style="color: #000000;">]</span>  <span style="color: #7f0000;"># </span><span style="color: #7f0000;">bar locations
</span>
plt.figure<span style="color: #000000;">(</span>figsize=<span style="color: #dd22dd;">(</span>5, 2.5<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
plt.bar<span style="color: #000000;">(</span>X, Y, color=<span style="color: #3548cf;">"k"</span><span style="color: #000000;">)</span>
plt.xlabel<span style="color: #000000;">(</span><span style="color: #3548cf;">"X"</span><span style="color: #000000;">)</span>
plt.ylabel<span style="color: #000000;">(</span><span style="color: #3548cf;">"Y"</span><span style="color: #000000;">)</span>
plt.savefig<span style="color: #000000;">(</span><span style="color: #3548cf;">"vis_barplot_basic.png"</span><span style="color: #000000;">)</span>
plt.show<span style="color: #000000;">()</span>
</pre>
</div></li>

<li><p>
图3-4
</p>

<div id="org78bade4" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-4.png" alt="3-4.png" />
</p>
<p><span class="figure-number">Figure 11: </span>ms/3-4.png</p>
</div></li>
</ul></li>
</ul>
</div>
<div id="outline-container-org07a2e5f" class="outline-4">
<h4 id="org07a2e5f"><span class="section-number-4">3.3.1.</span> Bar plots for grouped data</h4>
<div class="outline-text-4" id="text-3-3-1">
<ul class="org-ul">
<li>我们继续使用之前的"新闻来源调查"例子.现在假设我们调查了两组人:
<ul class="org-ul">
<li>千禧一代(millennials)</li>
<li>婴儿潮一代(boomers).</li>
</ul></li>
<li>于是,我们就可以根据年轻人和年长者来对回答结果进行区分.</li>
<li>那么,应该如何可视化这些数据呢?其实有两种方式来对条形图进行分组:
<ul class="org-ul">
<li>按年龄组分组</li>
<li>按新闻来源分组</li>
</ul></li>
<li>图 3.5 展示了这两种方式.
<ul class="org-ul">
<li>如图</li>
<li>重要的是:两幅图的数据完全相同,但结果的解读是不同的.</li>
<li>我将在下一段讨论这两幅图的不同之处;</li>
</ul></li>
<li>在阅读之前,请先仔细观察这张图,思考一下每种分组方式在讲述怎样的故事(narrative).</li>
<li>在图 3.5 的 A 面板(panel A)中,叙事的重点是年龄组之间的差异.例如,你可以清楚地看到:
<ul class="org-ul">
<li>千禧一代相比婴儿潮一代,从互联网获取新闻的比例更高;</li>
<li>而婴儿潮一代从电视获取新闻的比例更高³.</li>
</ul></li>
<li>在 B 面板(panel B)中,叙事的重点则是每个年龄组内部的新闻获取方式.例如,你可以看到:
<ul class="org-ul">
<li>千禧一代从口口相传获取新闻的比例高于从报纸获取新闻的比例.</li>
<li>这一信息在 A 面板中同样存在,但不太显眼.</li>
</ul></li>
<li>关键是:两种可视化方式都不是"对"或"错".你需要根据研究问题,研究目的,以及你想传达给受众的发现,来
选择数据分组的方式.</li>
<li>我在这里不会给出绘制分组条形图(grouped bar plot)的代码,因为这是本章末尾的一个练习.</li>
<li>不过我会从概念上解释一下分组条形图是如何从一个数字矩阵创建出来的.
<ul class="org-ul">
<li>在 Python 中,plt.bar() 会按照行(row)进行分组,</li>
<li>同一行(row)中的列(column)会显示为相邻的柱形,</li>
<li>不同行(row)之间会在 x 轴上用空白分隔.</li>
<li>因此,如果想用另一种方式分组,就需要转置(transpose)这个数据矩阵,以交换行和列</li>
<li><p>
如图3-6
</p>

<div id="org44d3b6f" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-6.png" alt="3-6.png" />
</p>
<p><span class="figure-number">Figure 12: </span>ms/3-6.png</p>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org2825b7a" class="outline-4">
<h4 id="org2825b7a"><span class="section-number-4">3.3.2.</span> Error bars</h4>
<div class="outline-text-4" id="text-3-3-2">
<ul class="org-ul">
<li>误差线(Error bars) 是条形图的一种自然扩展.</li>
<li>误差线图(error bar plot)看起来像普通的条形图,只不过&#x2013;它带有误差线.</li>
<li>误差线是在每个柱子的中心位置画出的竖直线,它的顶部和底部通常会有水平的小刻度线(hash marks),以便你看清误差线的两端位置.
<ul class="org-ul">
<li><p>
参见图 3.7 示例.
</p>

<div id="org492d4a8" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-7.png" alt="3-7.png" />
</p>
<p><span class="figure-number">Figure 13: </span>ms/3-7.png</p>
</div></li>
</ul></li>
<li>误差线可以表示多种不同的统计量,例如:
<ul class="org-ul">
<li>标准差(standard deviation)</li>
<li>标准误(standard error)</li>
<li>置信区间(confidence interval)</li>
</ul></li>
<li>从一个角度来看,任何由误差线表示的统计量都会与数据的变异性有关;</li>
<li>但从另一个角度来看,不同的统计量会导致不同的解读&#x2013;这意味着误差线图可能会让人感到困惑或被误解.</li>
<li>解决办法很简单:务必明确说明误差线代表的数据特性.</li>
<li>误差线是通过数学公式计算得出的,而数学公式并不总是受到现实的约束.举个例子:
<ul class="org-ul">
<li>假设图 3.7 中的柱子代表不同地理区域内建筑物的高度.如果 "类别 0" 和 "类别 1" 的误差线向下延伸到负
数,这在物理上毫无意义(建筑物高度不可能为负),但却可能是公式计算的结果.</li>
</ul></li>
<li>关键结论:
<ul class="org-ul">
<li>误差线确实有用,可以提供信息,</li>
<li>但如果没有明确解释,它们同样会引起困惑.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org0ee0b32" class="outline-3">
<h3 id="org0ee0b32"><span class="section-number-3">3.4.</span> Pie charts</h3>
<div class="outline-text-3" id="text-3-4">
<ul class="org-ul">
<li>饼图(Pie chart) 看起来像一个被切成楔形块的圆&#x2013;换句话说,就像一个派(pie).</li>
<li>饼图中的每一块对应数据中的一个类别,它所占的面积与该类别占整体的百分比成比例.</li>
<li>"占整体的百分比" 是判断什么时候该使用饼图的关键.只有当数据可以转换成总和为 100% 的百分比时,饼图才
是可解释的.
<ul class="org-ul">
<li>例如,上一节中展示新闻来源类别的条形图,不能用饼图表示,因为这些类别的百分比相加超过了 100%.</li>
<li>但是,如果修改调查问卷,数据就可能适合用饼图:假设调查中有一个单一问题,问人们大部分新闻是从哪里
获得的.这时,每位受访者只能选择一个答案,那么,各类新闻来源的百分比加起来就会正好等于 100%.</li>
</ul></li>
<li><p>
图 3.8 展示了一个饼图的示例.
</p>

<div id="org359c111" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-8.png" alt="3-8.png" />
</p>
<p><span class="figure-number">Figure 14: </span>ms/3-8.png</p>
</div></li>
<li>饼图适用于分类数据(categorical data),这些数据可以是有序的(ordinal)或无序的(nominal).</li>
<li>使用饼图的主要限制是:数据必须可以转换为占整体百分比.</li>
</ul>
</div>
</div>
<div id="outline-container-orgb2d1cdc" class="outline-3">
<h3 id="orgb2d1cdc"><span class="section-number-3">3.5.</span> Box plots</h3>
<div class="outline-text-3" id="text-3-5">
<ul class="org-ul">
<li>箱线图(Box plots),传统上也叫箱形-须状图(box-and-whiskers plots),和条形图有些相似:
<ul class="org-ul">
<li>它们的 x 轴 也是用于显示分类数据,</li>
<li>而 y 轴 用于显示连续数据.</li>
</ul></li>
<li>不过,与条形图只展示数据的一个特征(平均值)不同,箱线图能够展示更多与数据分布相关的特征,从而帮助
更好地解释数据.</li>
<li>图 3.9 展示了我模拟的随机数据所绘制的一个箱线图.图中的字母对应了下面要介绍的重要组成部分(其中一些
术语你可能是第一次看到,我会在下一章更详细解释):
<ul class="org-ul">
<li><p>
如图
</p>

<div id="org584f207" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-9.png" alt="3-9.png" />
</p>
<p><span class="figure-number">Figure 15: </span>ms/3-9.png</p>
</div></li>
<li>a) 中间的横线表示数据的中位数(median).中位数将数据一分为二:50% 的数据值小于中位数,50% 的数据
值大于中位数.</li>
<li>b) 箱体的下边界表示数据的第 25 百分位(下四分位数).换句话说,25% 的数据值小于箱体的下边缘.</li>
<li>c) 同 b,但对应第 75 百分位(上四分位数).换句话说,25% 的数据值大于箱体的上边缘.</li>
<li>综合来看:箱体包围了数据集中间的 50% 数值,25% 的数据在箱体下方,25% 的数据在箱体上方.箱体所代表
的中间 50% 数据范围叫做 四分位距(IQR: interquartile range).</li>
<li>d) "非极端最小值":下方的水平短线是最小的数据值,但它不算离群点或异常极值.</li>
<li>e) 同 d,但对应非极端最大值.</li>
<li>f) 离群值(outliers):非常大,非常小或不具代表性的数据值.离群值的精确定义和合理处理方式并不总是
简单的,你将在 第 4 章 和 第 7 章 学到更多.</li>
</ul></li>
<li>箱线图的用途:
<ul class="org-ul">
<li>箱线图非常有用,因为它能反映数据的分布形态,并帮助识别离群值或潜在问题数据.</li>
<li>它还可以帮助比较分类变量之间的差异模式.</li>
</ul></li>
<li>例如,图 3.10 展示了两个类别的箱线图.假设这些数据表示同一职业在不同国家的月薪:
<ul class="org-ul">
<li>虽然两个国家的薪资中位数相同,但国家 A 的薪资分布更宽,</li>
<li>这可能意味着薪资增长的潜力更大.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgcba3b4b" class="outline-3">
<h3 id="orgcba3b4b"><span class="section-number-3">3.6.</span> Histograms</h3>
<div class="outline-text-3" id="text-3-6">
<ul class="org-ul">
<li>直方图(Histogram) 乍一看和 条形图(Bar plot) 很像,但它们在几个重要方面有明显区别.最重要的区别是:
<ul class="org-ul">
<li>条形图 用于分类数据(categorical data);</li>
<li>直方图 用于数值型数据(numerical data).</li>
</ul></li>
<li><p>
让我们先从一个小整数数据集的简单例子开始:
</p>
<pre class="example" id="org6b232d8">
X = [1, 2, 2, 3, 3, 4, 5, 5, 5, 5, 6, 7, 7, 7, 8, 8, 9]
</pre></li>
<li>在这个数据集中:
<ul class="org-ul">
<li>有一个 "1"</li>
<li>有两个 "2"</li>
<li>有两个 "3"</li>
<li>以此类推.</li>
</ul></li>
<li>直方图就是将 x 轴表示的数值元素 与 y 轴表示的出现次数进行可视化.
<ul class="org-ul">
<li><p>
见图 3.11
</p>

<div id="orga4b4ca1" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-11.png" alt="3-11.png" />
</p>
<p><span class="figure-number">Figure 16: </span>ms/3-11.png</p>
</div></li>
<li>请花点时间观察一下x 轴的刻度位置和标签.它们出现的位置是不是和你预期的不一样?</li>
<li>我猜你的答案是 "是的,不一样" &#x2013; 你可能希望刻度正好位于柱子的中间(参考图 3.7 或 3.3).其实,这
种刻度位置的呈现方式对应的是条形图,而不是直方图.</li>
</ul></li>
<li>这是一个细微但非常重要的区别,理解并区分它对于正确解读和绘制直方图至关重要.
<ul class="org-ul">
<li><p>
现在,请看图 3.12.
</p>

<div id="orged6a8cf" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-12.png" alt="3-12.png" />
</p>
<p><span class="figure-number">Figure 17: </span>ms/3-12.png</p>
</div></li>
<li>你会注意到:柱子的高度与图 3.11 完全相同;但刻度线却位于每个柱子的中心位置,也就是你原本可能会预
期的位置.</li>
</ul></li>
<li>在解释这两幅图的区别之前,我们先来看看混淆从哪里产生的:
<ul class="org-ul">
<li>你看到数据后,自然而然会想到去数一数每个数值(1,2,3&#x2026;&#x2026;)各出现了多少次,</li>
<li>然后预期直方图会直接显示这些计数值.</li>
<li>实际上,你这是在期待直方图变成条形图的呈现方式.但是,直方图并不是这样工作的.</li>
</ul></li>
<li>直方图通过定义数值边界,然后计算落在这些边界之间的数据值数量来工作.
<ul class="org-ul">
<li>图 3.11 和图 3.12 看起来不同,是因为我使用了不同的分箱(binning)边界.</li>
<li><p>
图 3.13 展示了这两组边界.(图3.11的间距是0.9,所以导致中心一直在变. 但是图3.12的间距是1.0,所以中心
一直不变)
</p>

<div id="org741555d" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-13.png" alt="3-13.png" />
</p>
<p><span class="figure-number">Figure 18: </span>ms/3-13.png</p>
</div></li>
<li>因此,当你查看数据集 X 时,你是数了有多少个 1;</li>
<li>但图 3.11 中的直方图是通过计算 1 到 1.9 之间的数值个数,1.9 到 2.8 之间的数值个数,依此类推绘制出来的.</li>
</ul></li>
<li>图 3.11 和图 3.12 的另一个不同点是:
<ul class="org-ul">
<li>前者的 x 轴刻度 位于 边界 上,</li>
<li>而后者的 x 轴刻度 位于 区间中点 上.</li>
</ul></li>
<li>这种差异在可视化多个直方图时非常重要,你很快就会学到原因.</li>
<li>我知道这个讨论可能有点让人困惑,但如果你花时间理解它,那么关于直方图的其他内容都会变得清晰易懂.这
是非常值得的,因为许多数据分析,假设和解释都是建立在直方图形状的基础上的.</li>
<li>并不是所有数据集都只包含少量整数(所以不适用于条形图).假设我们有更多的数据,而且精度更高呢?例如,
<ul class="org-ul">
<li>想象一个包含 500 只巴比伦獴 长度的数据集.</li>
<li>如果我们以 毫米 为精度来测量,那么数据集中很可能 没有任何两个完全相同的数值.因此,我们不能像处理
整数数据集那样,直接用条形图来绘制.</li>
<li><p>
我们唯一的办法,就是定义边界,然后统计数据点落在每个边界之间的数量,也就是直方图的做法.你可以在
图 3.14 中看到这种情况.
</p>

<div id="org654d3e4" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-14.png" alt="3-14.png" />
</p>
<p><span class="figure-number">Figure 19: </span>ms/3-14.png</p>
</div></li>
</ul></li>
<li>仅仅通过观察直方图,我们是无法知道这些獴的精确长度的,只能知道在每个柱状条对应的边界区间内,有多少个数据点.
<ul class="org-ul">
<li>这意味着我们在可视化过程中丢失了部分信息,不过这种信息损失被可解释性的提升所平衡.</li>
<li>事实上,数据可视化往往会涉及一定的信息损失,而数据可视化的艺术就在于知道应该牺牲哪些信息,以及牺
牲多少信息,才能换取更强的可解释性和模式发现能力.</li>
</ul></li>
<li>那么,为什么我选择了这些边界,而不是其他的边界呢?
<ul class="org-ul">
<li>我本可以添加更多的分箱(也就是增加更多的柱形条),这样可以减少信息损失.</li>
<li>确实,直方图使用的分箱数越多,保留的信息就越多.</li>
<li>当然,分箱数量的选择带有一定的任意性,这一点我稍后还会讨论.</li>
</ul></li>

<li>不过,咱们先达成一个共识:
<ul class="org-ul">
<li>分箱太少 → 信息损失太大</li>
<li>分箱太多 → 可能过于复杂,也不易解读</li>
</ul></li>
<li><p>
看看图 3.15,你会发现它展示了同一组数据,在分箱数量少与分箱数量多的情况下,直方图的不同表现.
</p>

<div id="org2b6bfae" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-15.png" alt="3-15.png" />
</p>
<p><span class="figure-number">Figure 20: </span>ms/3-15.png</p>
</div></li>
<li>显然,有一些极端的分箱数量选项并不太有用,但这仍然留下了一个很宽的可接受分箱范围.那么,该如何选择合适的分箱数量呢?
<ul class="org-ul">
<li>有几种指南可以用于计算分箱数量,这些指南是基于数据集的某些特征来确定的,你会在下一章中学习到,因
此我们会在后面回到这个话题.</li>
</ul></li>
<li>目前而言,根据视觉检查随意选择一个分箱数量是可以的.在很多数据集中,30–40 个分箱效果往往不错(在图
3.14 中,我使用了 30 个分箱).</li>
<li>Distribution tails: 你必须掌握的一个术语是:在用直方图表示分布时,分布的两端被称为 "尾部(tails)":
<ul class="org-ul">
<li>左边的尾部称为 "左尾(left tail)"</li>
<li>右边的尾部称为 "右尾(right tail)"</li>
<li>见图 3.16</li>
</ul></li>
<li>如果数据包含负值和正值,那么它们也可能分别被称为:
<ul class="org-ul">
<li>负尾(negative tail)- 负值</li>
<li>正尾(positive tail)- 正值</li>
</ul></li>
<li>这些术语(指"尾部")其实是比较宽泛的,因为对于所有分布类型和所有分析方法来说,并不存在一个精确统一
的边界来严格区分分布的尾部(tail)与分布主体(body).
<ul class="org-ul">
<li>有时,尾部是通过定性描述来界定的;</li>
<li>而在另一些情况下,尾部会被精确地定义,</li>
</ul></li>
<li>比如使用:
<ul class="org-ul">
<li>占整个分布面积一定百分比的区域</li>
<li>或者一个特定的数值边界</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org71edcf1" class="outline-4">
<h4 id="org71edcf1"><span class="section-number-4">3.6.1.</span> Histogram vs. bar plot</h4>
<div class="outline-text-4" id="text-3-6-1">
<ul class="org-ul">
<li>条形图(bar plot)和直方图(histogram)的关键区别在于:
<ul class="org-ul">
<li>条形图 的 x 轴 上是类别(categories)</li>
<li>直方图 的 x 轴 上是数值区间(numerical ranges)</li>
</ul></li>
<li>要判断你看到的是条形图还是直方图,可以问自己这样一个问题:x 轴上的柱子能否互换位置?</li>
<li>回想一下我们在"新闻来源研究"(news sources study)和"獴(mongoose)研究"中的例子:
<ul class="org-ul">
<li>在条形图中,比如 Internet(互联网) 这一栏并不一定要放在 Newspaper(报纸) 这一栏的右边,如果把它
们互换位置,图表的意义依然不变.</li>
<li>但在直方图中,柱子的位置是不能乱换的,比如 40–41 cm 这一组(bin)必须在 41–42 cm 这一组的左边,互
换顺序会让图失去原本意义.</li>
</ul></li>
<li>在直方图中,你不能随意改变分组(bin)的顺序,否则会破坏图表的可解释性.</li>
<li>这种区别并不 trivial(不是无关紧要的):
<ul class="org-ul">
<li>直方图的形状可以用来判断数据是否来自某种特定分布,以及某些统计方法是否可以应用于该数据.</li>
<li>分布都是是具有某种图形的(比如正太分布像一个钟,钟的最高点在平均值那里),所以只有直方图能表示正态分
布,而能把中间最高值随意调换的条形图显然不行</li>
</ul></li>
<li>为了避免遗漏,我再强调一次这个重要的观点:
<ul class="org-ul">
<li>直方图:x 轴的顺序有意义,因为它反映了数据内在的性质,这些数据被"分组"(bin)时是根据连续的数值边
界划分的.</li>
<li>条形图:x 轴的顺序可以改变,虽然某些排序可能会让解释更方便,但排序本身是由数据分析者决定的,而不
是由数据的内在性质来决定的.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb622b41" class="outline-4">
<h4 id="orgb622b41"><span class="section-number-4">3.6.2.</span> Counts vs. proprotions</h4>
<div class="outline-text-4" id="text-3-6-2">
<ul class="org-ul">
<li>到目前为止,我展示的直方图(histogram)和条形图(bar plot)的 y 轴 都表示原始计数(raw counts).</li>
<li>原始计数的优点是:
<ul class="org-ul">
<li>你可以清楚地看到每个分组(bin)中确切的样本数量.</li>
</ul></li>
<li>但原始计数也有缺点,特别是在比较 <b>样本量不同</b> 的数据集时.
<ul class="org-ul">
<li>一种替代方法是将计数转换为比例(proportion)或百分比(percentage).</li>
<li><p>
比如 图 3.17 显示了同一份数据:
</p>

<div id="org1954c0f" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-17.png" alt="3-17.png" />
</p>
<p><span class="figure-number">Figure 21: </span>ms/3-17.png</p>
</div></li>
<li>左侧是原始计数形式</li>
<li>右侧是转换为比例后的形式</li>
</ul></li>
<li>需要注意的是:
<ul class="org-ul">
<li>在转换为比例后,分布的形状以及柱子的相对高度并没有变化</li>
<li>唯一不同的是 y 轴上的数值</li>
</ul></li>
<li>这两种方式没有哪一种绝对更好,只是提供了不同的信息表达方式. <b>但是,如果不同样本数比较的话,就必须得用比例显示的形式</b></li>
<li><p>
图 3.18 对比了计数和比例的优缺点,以及何时更适合使用哪种方式.
</p>

<div id="orgca13a78" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-18.png" alt="3-18.png" />
</p>
<p><span class="figure-number">Figure 22: </span>ms/3-18.png</p>
</div></li>
<li>如何将计数转换为比例?
<ul class="org-ul">
<li>公式很简单:将每个分箱(bin)的值除以所有分箱值的总和,即:</li>
<li>如果要将比例转换为百分比:只要将比例乘以 100,然后在数字后加上 % 符号即可</li>
</ul></li>
<li>比例化(Normalization) 在直方图中的优势示例:以獴(mongoose)的体长为例,假设我们要比较非洲獴与亚
洲獴:
<ul class="org-ul">
<li>我去毛里求斯(Mauritius)测量了 100 只獴的体长</li>
<li>你去了爪哇(Java)并测量了 500 只獴的体长(好吧,我承认我在旅途中没有你努力 😅)</li>
<li><p>
图 3.19 显示了来自这两个岛屿的体长直方图:
</p>

<div id="org835682a" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-19.png" alt="3-19.png" />
</p>
<p><span class="figure-number">Figure 23: </span>ms/3-19.png</p>
</div></li>
</ul></li>
<li>在原始计数直方图中,由于样本量差别很大,柱子的高度也必然不同,因此不能直接比较两张图的高度</li>
<li>但如果我们的研究问题是"非洲獴和亚洲獴,谁更长",那么我们并不关心 y 轴的绝对数值,通过将两张直方图归
一化为百分比,我们就可以定量地比较它们的分布形状与柱子高度.</li>
<li>如果要统计比较这两个分布,可以使用 t 检验(t-test) 或 KL 检验(Kullback–Leibler test),但这超出当前讨论范围.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org4a74b94" class="outline-3">
<h3 id="org4a74b94"><span class="section-number-3">3.7.</span> Lines vs. bars in a histogram</h3>
<div class="outline-text-3" id="text-3-7">
<ul class="org-ul">
<li>直方图使用的是条形而不是线条,因为所有落在某一范围(区间)内的数据点都归入同一个"箱子"(bin)中;</li>
<li>对数据进行离散化处理时会导致一定的信息损失,我们将无法知道如果改变分界点数据会呈现什么样的形态.这
意味着你应该使用条形(或者点,或者其他任何离散的标记),并在相邻条形之间使用竖直的间隔.</li>
<li>为什么不适合用线连接各个"箱子"呢?
<ul class="org-ul">
<li>问题在于,线条暗示了各点之间的平滑过渡,就好像你知道在改变分界点后数据会呈现的形状(见图 3.20).</li>
<li>一方面,如果样本量很大,分布又比较平滑,那么这种隐含的平滑过渡假设通常是合理的.</li>
<li>但另一方面,我们应尽量保持准确性,只有在收益超过代价时才允许牺牲准确性.</li>
</ul></li>
<li>话虽如此,也确实存在一些情况,使用线条能够实现柱形图无法达到的解释效果和视觉美感.你将在练习 5 中对
此进行探索.</li>
<li>结论是:
<ul class="org-ul">
<li>直方图用柱形展示最为准确,</li>
<li>但在解释性上的收益大于可能造成的"离散化之间平滑连续"这种错误印象的风险时,也可以使用线条.值得庆
幸的是,直方图中的柱形(bin)越多,线条就越能准确地反映直方图的形状</li>
</ul></li>
<li><b>Lines in bar plots</b> 很显然,在条形图(bar plot, 不是histogram)中使用线条是完全不合适的.
<ul class="org-ul">
<li>回到"新闻来源"的例子:在"互联网"和"口耳相传"之间画一条线,这意味着什么呢?</li>
<li>在这些类别之间想象一个插值数据点是没有意义的,尤其是考虑到这些类别的排序本身是任意的.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org1809512" class="outline-3">
<h3 id="org1809512"><span class="section-number-3">3.8.</span> Violin plots</h3>
<div class="outline-text-3" id="text-3-8">
<ul class="org-ul">
<li>使用线条而不是柱形的一个优势是,它让你能够将数据以"小提琴图"的形式可视化,而小提琴图既优秀又美观.</li>
<li>小提琴图的制作方法是:用线条绘制直方图,并交换 x 轴与 y 轴的位置(见图 3.21).</li>
<li>你经常会看到直方图沿垂直方向镜像,从而形成小提琴图的对称外观.如果你有两个数据集,你可以创建一个非
对称的小提琴图,让每个数据集占据图形的一侧(图 3.21D).在第 7 个练习中你将学习如何绘制这种图.</li>
<li>另一种小提琴图:
<ul class="org-ul">
<li>当两个数据集包含相关且取值范围相当的数据时,非对称小提琴图是合适的.
<ul class="org-ul">
<li>例如,可以想象一个非对称小提琴图,用于展示成年男性和女性的身高.在这种情况下,数据特征是相同的
(身高),并且两个群体(男性和女性)的数据范围相当.</li>
<li>反例则是将"年薪(单位:美元/年)"和"身高(单位:英尺)"放在同一个非对称小提琴图中.这两个特征完
全不同,且数值范围差别巨大,这么做只会造成混乱.</li>
</ul></li>
</ul></li>
<li>最后补充一点:在图 3.21 中对应于每个单独数据值的点,沿 x 轴增加一些微小的随机偏移有助于可视化;否则
这些点会彼此重叠.
<ul class="org-ul">
<li>这里要解释一下,在上图C中, 如果不进行"x轴微小的随机偏移",那么这些点就会全部在x轴位0,的这条垂直竖线
上面紧密的排列,并且很大概率会重叠.</li>
<li>换个句话说,如果我们以一个矩形来从上到下的框起来这些点,就能得到直方图里面的数量(这也是为什么中间的
点密集,而边缘的点稀少的原因)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org5a8af80" class="outline-3">
<h3 id="org5a8af80"><span class="section-number-3">3.9.</span> Linear vs. logarithmic axis scaling</h3>
<div class="outline-text-3" id="text-3-9">
<ul class="org-ul">
<li>许多图表的 y 轴可以采用线性刻度(linear scaling)或对数刻度(logarithmic scaling)显示.
<ul class="org-ul">
<li><p>
图 3.22 展示了相同的数据在线性刻度(A 面板)与对数刻度(B,C 面板)下的效果.
</p>

<div id="org3847453" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-22.png" alt="3-22.png" />
</p>
<p><span class="figure-number">Figure 24: </span>ms/3-22.png</p>
</div></li>
<li>B 和 C 面板的曲线和刻度缩放完全相同,但 y 轴的刻度标签格式不同.</li>
</ul></li>
<li>线性刻度与对数刻度的区别,本质区别在于 y 轴刻度之间的间距:
<ul class="org-ul">
<li>在线性刻度中,刻度的间距是按照加法来的;例如,在图 3.22A 中,每个 y 轴刻度是在前一个刻度的基础上
加 2000;</li>
<li>在对数刻度中,刻度的间距是按照乘法来的.而在图 3.22B-C 中,每个刻度是在前一个刻度的基础上 乘以10.</li>
</ul></li>
<li>这种差异的结果是:较大的数据值会在图中显得被压缩.</li>
<li>当然,数据的数值本身没变,只是可视化方式不同.</li>
<li>什么时候应该使用对数刻度而不是线性刻度?
<ul class="org-ul">
<li>当数据表现出指数增长或衰减时(如人口动态,细菌生长,股票增长,辐射衰减等),采用对数刻度可以让数
据特征更清晰.</li>
<li>在生物学和物理学中,对数刻度的应用尤其广泛.</li>
</ul></li>
<li>但需要注意的是:
<ul class="org-ul">
<li>大多数没有科学背景的人更容易理解线性刻度的图表.因此,如果希望图表对更多人来说易于理解,除非有充
分理由,否则最好使用线性刻度.</li>
</ul></li>
<li>关于对数刻度的几点补充说明
<ul class="org-ul">
<li>图 3.22 中 y 轴刻度是 10 的倍数,但在某些场景下其他倍数可能更适用,例如倍增(刻度为 2,4,8,16,
32&#x2026;&#x2026;).</li>
<li>对数刻度通常只能用于正值数据,因为乘以零以及某些数据变换下的负数会导致数学上的问题.</li>
<li>对数刻度也可以用在 x 轴;如果 x,y 两个轴都是对数刻度,这种图称为 "双对数图(log-log plot)".</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org9230b76" class="outline-3">
<h3 id="org9230b76"><span class="section-number-3">3.10.</span> Discretizing continuous data</h3>
<div class="outline-text-3" id="text-3-10">
<ul class="org-ul">
<li>将连续数据离散化有它的"阴暗面".我会在这里简单介绍这个问题,因为它与直方图相关.稍后在本书中我还会
继续讨论这个话题,
<ul class="org-ul">
<li>因为离散化在决定是否使用方差分析(ANOVA)或回归分析,以及在回归分析中进行数据可视化时,都是一个需
要考虑的因素.</li>
</ul></li>
<li>开头先看图 3.23A,它形象地展示了直方图是如何生成的:
<ul class="org-ul">
<li><p>
如图
</p>

<div id="org88627e8" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-23.png" alt="3-23.png" />
</p>
<p><span class="figure-number">Figure 25: </span>ms/3-23.png</p>
</div></li>
<li>左图:数据值的范围被分成等大小的区间(bin);</li>
<li>右图:统计落在每个区间内的数据点数量,由此画出直方图.</li>
<li>现在想象一下,只用两个区间来画直方图.这称为 <b>二值化(binarizing</b> 数据,因为数据从连续型被转换成
了两类(见 图 3.23B).</li>
</ul></li>
<li>虽然在每个区间内,许多数据值确实比较接近,但也会出现另一些情况:跨区间的两个数据点可能比区间内的某
些点更接近.
<ul class="org-ul">
<li>例如:方块(square)和上三角形(up-facing triangle)的数值很接近,但因为刚好落在不同的区间,它们
被分到不同的直方图柱里;</li>
<li>而方块和菱形(diamond)之间数值差距更大,却因为落在同一区间而出现在同一个柱子里(上,下三角形的情
况也是一样).</li>
</ul></li>
<li>因此,二值化数据的风险在于:
<ul class="org-ul">
<li>有些属于不同类别(bin)的数据点,实际上比同一类别中的数据更接近.</li>
<li>这违反了在解读直方图时的一个不成文假设&#x2013;即假设同一区间内的数据是足够同质化的,从而可以用一根柱子
去表示它们.</li>
</ul></li>
<li>你可能会觉得这种情况有点极端,甚至不现实&#x2013;毕竟,谁会用只有两个区间的直方图呢?
<ul class="org-ul">
<li>但这并非只是理论探讨;在实际中对连续数据进行二值化是很常见的,比如通过 <b>中位数拆分(median split</b>
来将数据分成两类.</li>
</ul></li>
<li>结论:
<ul class="org-ul">
<li>离散化连续数据有时是合理的,也能简化数据分析,</li>
<li>但必须谨慎执行,并充分考虑这种离散化对分析结果可能造成的影响.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org735b15a" class="outline-3">
<h3 id="org735b15a"><span class="section-number-3">3.11.</span> Radial plots</h3>
<div class="outline-text-3" id="text-3-11">
<ul class="org-ul">
<li>雷达图&#x2013;也称为极坐标图(polar plots)或蜘蛛图(spider plots)&#x2013;是一种用来可视化循环性或周期性数据的
方法.</li>
<li>"循环性"意味着这些数据会不断地首尾相接,比如一天的 24 小时,一周的 7 天.</li>
<li>要创建一个雷达图,首先将数据类别等距地分布在一个圆周上,然后径向轴(从圆心出发的距离)表示数据的大
小,相当于柱状图中的 y 轴高度.</li>
<li>图 3.24 显示了一个极坐标图的例子,
<ul class="org-ul">
<li><p>
如图
</p>

<div id="orgba48dad" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-24.png" alt="3-24.png" />
</p>
<p><span class="figure-number">Figure 26: </span>ms/3-24.png</p>
</div></li>
<li>内容是某一年各月的平均温度.这显然是一个合理使用雷达图的例子,因为数据序列的末尾(12 月)自然衔接
到数据序列的开头(1 月).</li>
</ul></li>
<li>雷达图不应用于非循环数据.</li>
<li>此外,雷达图不能用于包含负值的数据,因为从原点的距离是非负数.</li>
<li>图 3.25 展示了一个不恰当使用雷达图的例子:
<ul class="org-ul">
<li><p>
如图
</p>

<div id="orga6ba1aa" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/3-25.png" alt="3-25.png" />
</p>
<p><span class="figure-number">Figure 27: </span>ms/3-25.png</p>
</div></li>
<li>圆周轴上是电影类型(类别),径向轴显示我对这些类型的喜好程度(1 到 10 分).</li>
</ul></li>
<li>雷达图不应用于非循环数据的原因有几个:
<ul class="org-ul">
<li>第一个原因,和为什么在柱状图的类别型 x 轴上使用折线图是不合适的一样:
<ol class="org-ol">
<li>折线会给人一种印象&#x2013;在"浪漫喜剧(Romcom)"和"恐怖片(Horror)"之间存在某个插值数据点,而这其实是
没有意义的.(当然,一部电影可能同时包含浪漫喜剧和恐怖元素,但类别数据并不能像数值数据那样进行数学计算和插值.)</li>
<li>这可以和图 3.24 进行对比:在 9 月和 10 月之间的插值是有意义的,比如9 月第三周的预计最高温度可能是 18°C.</li>
</ol></li>
<li>第二个原因,这些类别本身并不是循环的,就如同柱状图中类别的排序是任意的
<ol class="org-ol">
<li>在数据中没有任何内在逻辑要求我们必须"循环"地从"浪漫喜剧"到"动漫",再回到"浪漫喜剧".类别间并不
存在这样的必然衔接性或周期性.</li>
<li>相比之下,图 3.24 中月份的顺序是自然且周期性的,这是数据的内在属性.</li>
</ol></li>
</ul></li>
<li>尽管有上述理由,在你数据科学的旅途中,仍会遇到很多不恰当的雷达图.</li>
<li>正如我之前所写的,人类文明要担心的事情已经够多了,所以我们可以原谅这种统计礼仪上的"小违例".</li>
<li>但是,请不要自己继续这种不当使用雷达图的坏习惯.</li>
</ul>
</div>
</div>
<div id="outline-container-org10b45a7" class="outline-3">
<h3 id="org10b45a7"><span class="section-number-3">3.12.</span> Color</h3>
<div class="outline-text-3" id="text-3-12">
<ul class="org-ul">
<li>颜色在图形可视化中是一种简单而强大&#x2013;但也具有潜在风险&#x2013;的方式,可以让你的可视化更有吸引力,更具信息性.</li>
<li><b>Adding color is simple</b> 在 Python 或 R 中添加颜色非常简单,比如:
<ul class="org-ul">
<li>指定调色板(palette)直接指定绘图对象的颜色(你可以通过快速的网页搜索找到各种调色板的可视化列表)</li>
<li>在大多数 matplotlib 函数中,颜色通过三位参数指定:color=(R,G,B),其中 R,G,B 是 0~1 之间的浮点数.
在本书的代码中,你会看到我几乎总是让这三个值相等,从而生成灰度线条.</li>
</ul></li>

<li><b>Color is powerful</b> 在大脑中,有几个机制在意识之前就会对颜色产生反应:
<ul class="org-ul">
<li><b>上丘(superior colliculus)</b> 是视网膜信号到达的第一批神经目标之一,它会对颜色变化作出反应,并能
直接控制眼睛转向某个颜色的区域.</li>
<li>大脑中还有专门区域负责处理颜色.</li>
<li>换句话说,我们的大脑就是天生会关注意力到颜色,所以给图表加颜色能自动触发注意力机制.</li>
</ul></li>
<li>另外,颜色还可以为图表增加一维信息.例如:
<ul class="org-ul">
<li>热力图(heat maps):用于展示多维数据(如相关性矩阵)</li>
<li>区分同一坐标轴上的多条线</li>
</ul></li>
<li><b>Color is risky</b> 添加颜色的两个风险:
<ul class="org-ul">
<li>你无法控制观众看到的实际颜色质量</li>
<li>如果在现场演示中投屏,色彩可能会被投影仪或屏幕失真,如果放到网上,观众可能在小手机屏幕或强光环境中
观看,颜色效果会打折,即便在同一屏幕,也不能保证每个人看到的颜色相同</li>
</ul></li>
<li>色盲约占男性的 8%,女性的 0.5%(据 Google 搜索)很多人难以区分红色或绿色的细微色差</li>
<li><b>注意</b> 我并不是要劝你不要用颜色,颜色是很棒的可视化增强手段,应该在可能的情况下多用.但建议&#x2013;颜色最
好是一个辅助特征,而不是唯一依赖的信息.
<ul class="org-ul">
<li>例如,可以让颜色在亮度或线型上有冗余:浅绿色实线 vs 深红色虚线,即便在灰度条件下也能区分,因为亮
度和线型不同.</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org3c98801" class="outline-4">
<h4 id="org3c98801"><span class="section-number-4">3.12.1.</span> 选择哪些颜色?</h4>
<div class="outline-text-4" id="text-3-12-1">
<ul class="org-ul">
<li>颜色理论和数据可视化中的配色选择是一个有争议的话题&#x2013;一些"数据可视化纯粹主义者"会对哪些颜色或调色板
能用/不能用发表强烈意见.</li>
<li>"Jet colormap"就是一个经典争议例子(网上搜索 Should I use the jet colormap? 就能看到很多讨论).</li>
<li>我对此没有强烈观点,也不打算卷入长期的争论.颜色与人类视觉感知相关的理论和实验非常多,如果这个话题
你感兴趣,非常支持你去深入研究;否则,你可以直接用 Python 或 R 自带的调色板,也可以去一些调色网站找
颜色.</li>
<li>我自己常用的两个网站是:
<ul class="org-ul">
<li>colorhunt.co</li>
<li>color.adobe.com</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org660caae" class="outline-2">
<h2 id="org660caae"><span class="section-number-2">4.</span> Chapter 04: Descriptive Statistics</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-orgae3cde0" class="outline-3">
<h3 id="orgae3cde0"><span class="section-number-3">4.1.</span> Descriptive vs. inferential statistics</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>让人略感无奈的是,"statistics"(统计)这个词是个多义词 &#x2013; 在不同的语境下,它的含义并不完全相同.这
里我想将它分成两种主要的区别:
<ul class="org-ul">
<li>描述性统计(Descriptive statistics): 用来刻画某个数据集特征的数字.你会在本章学习很多描述性统计
量;你可能已经听过一些术语,比如均值(mean),中位数(median),方差(variance),偏度(skew),
谱(spectrum)和协方差(covariance).</li>
<li>推断性统计(Inferential statistics): 一类算法,用于对一个或多个样本数据集进行检验,以判断这些样
本的描述性统计量是否很可能可以推广到其他数据集.你会在本书后续章节(但不是本章)学习推断性统计;
你可能已听过一些术语,比如F 值(F-value),t 值(t-value),方差分析(ANOVA)以及回归(regression).</li>
</ul></li>
<li>为什么这个区别很重要呢?
<ul class="org-ul">
<li>在描述性统计中,我们不关心样本数据与其母体(总体)之间的关系,也不关心是否可以从一个样本推广到其
它样本或总体;我们也不会去比较两个数据集的特征差异.描述性统计的目的,仅仅是获得某一个特定数据集
的数值化表示.</li>
<li>推断性统计恰好相反,它完全是关于:如何使用一个特定的数据集,来了解这个样本所来自的总体特征.换句
话说,推断性统计是利用我们已有的数据去对未知数据作出推测</li>
</ul></li>

<li>举个例子:假设我们测量了一组男性和一组女性的身高,发现男性的平均身高是 178 cm,女性是 171 cm.
<ul class="org-ul">
<li>这就是两个样本的描述性统计,并且在这两个样本中,男性平均比女性高正好 7 cm &#x2013; 这是确凿的,没有概率
和不确定性的问题.</li>
<li>但是,我们能否推断 "男性在总体上比女性高 7 cm",即包括那些没有被测量到的男性和女性呢?这是描述性
统计无法回答的问题.要想回答,我们需要使用推断性统计,而答案会取决于样本是如何收集的,样本大小,
以及每组内部的方差等因素.</li>
</ul></li>
<li>我希望这个区别已经很清晰了.本章完全聚焦于描述性统计,接下来的几个章节也是如此.你会在第 8 章开始学
习推断性统计.</li>
<li>因为大多数人提到"statistics"(统计)时,想到的其实是推断性统计,所以:
<ul class="org-ul">
<li>我会在提到描述性统计时加上限定词"descriptive"(描述性),</li>
<li>而当我直接写 statistics 时,则是指推断性统计.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb08896d" class="outline-3">
<h3 id="orgb08896d"><span class="section-number-3">4.2.</span> Data distributons</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>让我们先来想一想一个实验.假设我找到 300 个人,并且向每个人提出同样的问题:
<ul class="org-ul">
<li>"你在 YouTube 上看过几次《江南 Style》音乐视频?"</li>
</ul></li>
<li>关于这些数据,有两个重要的观察点:
<ul class="org-ul">
<li>首先,观看视频的次数并不一定是整数;你可以在中途停下来,那么你可以报告说你看了 0.5 次 的江南
Style 视频.</li>
<li>其次,零是有意义的数值:看了 0 次意味着完全没有看过这个视频.</li>
</ul></li>
<li>这也意味着,看 10 次确实是看 5 次的两倍.而且,不可能出现看 -2 次 这样的情况.因此,这类数据的类型
是 Ratio(比率数据)- 参见本书第 49 页表 2.8</li>
<li>回到数据分布问题:我们该如何可视化这些数据呢?
<ul class="org-ul">
<li><p>
一种方法是,直接绘制一个包含两列的表格&#x2013;第一列是参与者 ID,第二列是观看次数&#x2013;每个受访者占一行(见
图 4.1).但是,这并不是一种好的可视化方法,而且对于大型数据集来说也不适用.
</p>

<div id="orge497f0b" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-1.png" alt="4-1.png" />
</p>
<p><span class="figure-number">Figure 28: </span>ms/4-1.png</p>
</div></li>
<li><p>
另一种方法是,画一张图,把调查对象放在横轴(x 轴),观看次数放在纵轴(y 轴)(见图 4.2A).这种方
法比单纯的数字表格好一些,但它是可视化这些数据的最佳方式吗?
</p>

<div id="orgdf54b2c" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-2.png" alt="4-2.png" />
</p>
<p><span class="figure-number">Figure 29: </span>ms/4-2.png</p>
</div></li>
</ul></li>
<li>其实并不是,因为在横轴上,数据的排序是没有意义的&#x2013;我们并不认为相邻的样本之间有必然关系(前提是我们
是从总体中随机抽样的).</li>
<li>你可能已经在想:**直方图(histogram)**可能是更好的可视化方法.
<ul class="org-ul">
<li>我同意. 图 4.2B 展示了这个数据的直方图(我们后面会看到直方图不是完全好过当前的scatter plot,只不过
是各有千秋)</li>
</ul></li>
<li>正如我在上一章说过的,直方图是一种有损可视化&#x2013;它会损失一部分信息.不过,我们所损失的信息,往往会被
它在理解数据分布特征方面的优势所弥补.</li>
<li>在直方图中最明显的特征是向下的斜率;这既不是一个高斯分布,也不是对称分布:绝大多数受访者看过该视频
的次数很少,而随着观看次数的增加,人数稳定减少.</li>
<li>不过,这种减少并非完全单调;在数字 18 附近,人数有一个小上升:也就是说,比周围区间更多的人看过该视
频大约 18 次.</li>
<li>至于这种"凸起"是否会在另一组样本中重现,是另一个问题;关键是,这种特征在原始数据表格中完全看不出来.</li>
<li>另一方面,图 4.2A 的散点图显示,在大约 35 次 的位置,有一个明显的高值,可能是一个离群点.虽然这个点
在直方图中也存在,但它对应的柱高只有 1,不仔细看很容易被忽略(也许你直到刚才才注意到它!).</li>
<li>结论是:
<ul class="org-ul">
<li>没有哪一种可视化能够同时呈现数据的所有特征;在理解特定数据特性时,有些图比其它图更有效.</li>
<li>即便是简单的数据集,查看多种可视化形式也能帮助你更全面地理解数据.</li>
</ul></li>
<li>通过这些定性观察,我们发现了仅凭数字表格很难甚至无法看出的数据特征.在本章后面,你将学习如何将这些
"定性观察"量化为数据.</li>
<li>这里有两个重要的概念:
<ul class="org-ul">
<li>我们用两种方法(散点图和直方图)可视化了数据&#x2013;每种方法揭示了数据的不同特征;</li>
<li>我们观察了直方图的形状,这为我们提供了关于数据分布的定性信息.</li>
</ul></li>
<li>因为我们关注的是分布,所以无论是用 <b>总数(count)还是换算成比例(proportion)</b> 来绘图都没关系&#x2013;这种
转换不会改变分布的形状,也不会影响你将在本章学习的描述性统计特征.</li>
</ul>
</div>
<div id="outline-container-org6146251" class="outline-4">
<h4 id="org6146251"><span class="section-number-4">4.2.1.</span> Empirical vs. analytical distributions</h4>
<div class="outline-text-4" id="text-4-2-1">
<ul class="org-ul">
<li>数据分布来自两种来源之一:测量得到的数据,或数学公式.它们也分别被称为
<ul class="org-ul">
<li>经验分布(empirical distribution)</li>
<li>解析分布(analytical distribution).</li>
</ul></li>
<li>经验分布会生成类似你在图 4.2B 中看到的直方图.
<ul class="org-ul">
<li><p>
图4-2
</p>

<div id="orge3e5050" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-2.png" alt="4-2.png" />
</p>
<p><span class="figure-number">Figure 30: </span>ms/4-2.png</p>
</div></li>
<li>之所以叫"经验分布",是因为它基于经验数据(empirical data),也就是从现实世界测量获得的数据(虽然
在我们的例子中,这些数据是虚构的,但它们模拟了真实世界中的情况).</li>
</ul></li>
<li>另外,即使从同一个总体(population)中抽样,每个数据集的经验分布也会有所不同.
<ul class="org-ul">
<li>换句话说,如果我们的实验是随机抽样,那么每次抽样得到的新数据集都会生成不同的直方图,</li>
</ul></li>
<li>当然,我们会期望它们的整体形状是相似的.</li>
<li>图 4.3 展示了这个概念:
<ul class="org-ul">
<li><p>
图4-3
</p>

<div id="org32753da" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-3.png" alt="4-3.png" />
</p>
<p><span class="figure-number">Figure 31: </span>ms/4-3.png</p>
</div></li>
<li>为了绘制这两个直方图,我用计算机的随机数生成器随机抽样.</li>
<li>由于随机抽样的缘故,这两个直方图并不完全相同,但我相信你会同意它们的整体定性特征是相似的.</li>
</ul></li>
<li>我在前一章解释过,尽管直方图应该用条形来绘制(因为它们通过分箱 binning 创建),但当分箱的精度足够高
并且折线有助于理解时,也可以用线条来表示.</li>
<li>你可以想象,当样本量越来越大&#x2013;趋近于无限大,且分箱尺寸越来越小&#x2013;趋近于无限小的时候,用条形和用折线
就没有区别了.</li>
<li>这就引出了 解析分布(analytical distribution):
<ul class="org-ul">
<li>它并不是通过对测量数据进行分箱得到的,而是通过计算一个数学公式得到的.</li>
<li><p>
例如,考虑一个公式:
</p>
\begin{equation}
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{x^2}{2\sigma^2}} \tag{4.1}
\end{equation}</li>

<li>这个公式叫做高斯函数(Gaussian function),也称为正态函数(normal function).暂时不用担心它是什
么意思,它从哪里来,或者公式中的参数 σ 代表什么;你会在本书的后续内容中更多地学习高斯公式的概念.</li>
<li><p>
目前关键是要理解:它是变量 x 的一个数学函数,它不是从经验数据计算出来的,我们可以直接计算这个公式
并画出类似图 4.4 的曲线.
</p>

<div id="org16bc84f" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-4.png" alt="4-4.png" />
</p>
<p><span class="figure-number">Figure 32: </span>ms/4-4.png</p>
</div></li>
</ul></li>
<li>解析分布的 y 轴可以按不同方式归一化,这会影响它的含义.</li>
<li>在很多情况下,分布会被归一化为概率,即分布在所有取值上的总和等于 1(对于连续分布,这就是从 −∞ 到 +∞
的积分等于 1).</li>
<li>这意味着什么呢?
<ul class="org-ul">
<li>这意味着解析分布告诉我们某些数值范围出现的概率.</li>
<li>例如,在高斯分布中,数值越接近 x = 0,出现的概率越大;而越远离 0,则概率越小. (注意y值最高在x轴
为0的时候)</li>
</ul></li>
<li>在统计学中有无数种解析分布;你会在本章的后续部分看到几个例子,在本书的其余部分,你可能会学习到大约
六种常用的解析分布.</li>
</ul>
</div>
</div>
<div id="outline-container-org8fde5f4" class="outline-4">
<h4 id="org8fde5f4"><span class="section-number-4">4.2.2.</span> The usese of data distributions</h4>
<div class="outline-text-4" id="text-4-2-2">
<ul class="org-ul">
<li>数据分布(data distribution)有多种应用,包括:
<ul class="org-ul">
<li>他们提供了数据的可视化概览和定性的(qualitative)概览 &#x2013; 它们可以让我们直观地了解数据的特征.</li>
<li>统计分析过程通常基于关于总体分布的假设 &#x2013; 样本是从这个总体中抽取的,因此理解分布有助于判断哪种统
计方法适合某个数据集.</li>
<li>统计推断(Statistical inference) ,即判断一个样本统计量是否只是偶然产生的,是通过将 <b>观测到的统计值</b>
与 <b>如果结果确实只是随机产生</b> 时所期望的统计分布进行比较来实现的.</li>
<li>计算机模拟 &#x2013; 在生物学,物理学和计算机科学中,很多模拟是基于从某种分布中进行采样的.例如,能生成
音乐,文本和人脸的生成式 AI 模型,就是通过从特定分布中随机采样实现的.</li>
<li>统计建模 &#x2013; 在统计建模中,我们会检查 <b>残差(residuals)</b> 的分布(即实际观测值与模型预测值之间的差
异)来评估模型的质量.</li>
<li>分布假设是统计学基本定律的基础 &#x2013; 比如大数定律和中心极限定理.因此,理解分布有助于你掌握统计学的
核心概念.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org4282a47" class="outline-4">
<h4 id="org4282a47"><span class="section-number-4">4.2.3.</span> Examples of distributions</h4>
<div class="outline-text-4" id="text-4-2-3">
<ul class="org-ul">
<li>你将在本书后面学习各种不同的分布,但我希望现在先让你感受一下几种解析分布(analytical distributions)
的样子.</li>
<li>图 4.5 展示了四种分布及其部分参数.
<ul class="org-ul">
<li><p>
图4-5
</p>

<div id="org665d6ab" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-5.png" alt="4-5.png" />
</p>
<p><span class="figure-number">Figure 33: </span>ms/4-5.png</p>
</div></li>
<li>请暂时不要担心这些不同分布的含义,来源,用途,或者"df"是什么意思&#x2013;我保证这些内容会在本书后续逐渐
变得清晰.</li>
<li>现在,请专注于这些分布的定性特征(qualitative features).</li>
</ul></li>
<li>我会在图下方提供一些总体评论,但在阅读之前,希望你先根据图上的信息自己做一些观察.以下是我对这些分
布的定性观察:
<ul class="org-ul">
<li>有些分布的定义域包括正数和负数,而有些分布只适用于正数.</li>
<li>大多数分布在一个"波峰"两侧逐渐趋近于 0.</li>
<li>有些分布是对称的,而有些分布在"波峰"一侧比另一侧更快地下降到 0.</li>
<li>有些参数值对分布的形状影响较小,而另一些参数值则可能导致分布形状发生定性的巨大变化.</li>
</ul></li>
<li>接下来看看一些 <b>经验分布(empirical distributions)</b> 的例子.
<ul class="org-ul">
<li>经验分布可能比解析分布更加多样,因为它们基于测量获得的数据&#x2013;而在实际测量中,样本量有限或者设备问
题可能会产生外形异常的分布.</li>
<li><p>
图 4.6 中的数据是通过随机数生成的,但它们包含了真实数据中可能出现的特征.
</p>

<div id="org2c3c04c" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-6.png" alt="4-6.png" />
</p>
<p><span class="figure-number">Figure 34: </span>ms/4-6.png</p>
</div></li>
</ul></li>
<li>与解析分布的例子一样,我希望你在阅读我的评论之前,先自己观察这些直方图并作出定性判断.</li>
<li>这是我的观察:
<ul class="org-ul">
<li>面板 A(Panel A):这个分布看起来有点像正态分布,但它是线性下降而不是非线性下降,并且不会降到零.
一个分布下降到零的速率被称为峰度(kurtosis),你将在本章后面学习如何量化和解释峰度.</li>
<li>面板 B(Panel B):这是从均匀分布(uniform distribution)中抽样得到的数据,这意味着在下限和上限之
间(此处是 0 到 1),每个值出现的概率相同.它看起来好像某些值比其他值更常见,但其实这是一种由抽样
变异性造成的假象.</li>
<li>面板 C(Panel C):这是一个幂律分布(power-law distribution),其中事件出现的次数与事件大小成反比.
幂律分布被认为是无尺度(scale-free)结构的证据.很多有趣的物理和生物系统都呈现这种分布形状,例如
地震震级和大脑活动.在分布的右尾部有一个小"突起",这是我模拟设备问题的尝试&#x2013;当较大的数据值被设备
截断并统一为一个最大值时,就会出现这种情况.这种现象是数据中的一个伪影(artifact),需要进行处理,
并且研究人员必须具备相关领域的知识来判断是否以及如何处理它.</li>
<li>面板 D(Panel D):有些分布会有多个波峰,意味着数值分布中存在多个高概率的聚集区.这个例子有两个峰,
因此叫做双峰分布(bimodal);更一般地,有多个峰的分布叫做多峰分布(multimodal).如果分布只有一个
峰,则称为单峰分布(unimodal).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org923a48d" class="outline-4">
<h4 id="org923a48d"><span class="section-number-4">4.2.4.</span> Quantifying qualitative characteristics</h4>
<div class="outline-text-4" id="text-4-2-4">
<ul class="org-ul">
<li>在进入本章其他内容之前,再看一个分布的例子:
<ul class="org-ul">
<li><p>
请观察图 4.7 中的三个解析分布(analytical distributions),试着比较它们的不同之处和相同之处.
</p>

<div id="org7510ecd" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-7.png" alt="4-7.png" />
</p>
<p><span class="figure-number">Figure 35: </span>ms/4-7.png</p>
</div></li>
<li>首先,你会发现,这三个分布的形状大致相同:它们都围绕着单个峰值呈对称分布,并且在两侧逐渐趋向于
零(事实上,它们都是高斯分布).</li>
<li>其中,有两个分布在相同的 x 轴位置达到峰值,而第三个则向左平移.</li>
<li>在那两个峰值位置相同的分布中,其中一个比另一个更宽("更胖").</li>
</ul></li>
<li>这些都是关于分布集中趋势(central tendency)和离散程度(dispersion)的定性观察.</li>
<li>定性观察很重要,但我们还需要通过赋予这些特征数值来量化这些特性.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org5e0e545" class="outline-3">
<h3 id="org5e0e545"><span class="section-number-3">4.3.</span> Central tendency</h3>
<div class="outline-text-3" id="text-4-3">
<ul class="org-ul">
<li>集中趋势(Central tendency)是一个统称,一般指数据集中"典型的"或最有可能出现的值(见图 4.8).</li>
<li>集中趋势是最重要的描述性统计量之一,并且经常用于推论统计中.事实上,许多数据集仅使用集中趋势就能进
行描述.</li>
<li>根据数据的类型和分布特性,有多种方法可以量化数据集的集中趋势.Wikipedia 上列出了 18 种衡量集中趋势
的方法.其中最常见的三种是:
<ul class="org-ul">
<li>均值(mean)</li>
<li>中位数(median)</li>
<li>众数(mode).</li>
</ul></li>
<li>在接下来的内容中,均值(又称算术平均数或平均数)占据最大篇幅,因为它是最重要且应用最广泛的集中趋势
度量方法.</li>
</ul>
</div>
<div id="outline-container-org32e9574" class="outline-4">
<h4 id="org32e9574"><span class="section-number-4">4.3.1.</span> Mean</h4>
<div class="outline-text-4" id="text-4-3-1">
<ul class="org-ul">
<li>要计算均值(mean),只需将所有数据值相加,然后除以数据值的数量.
<ul class="org-ul">
<li><p>
公式如下:
</p>
\begin{equation}
\overline{x} = n^{-1} \sum_{i=1}^{n} x_i
\end{equation}</li>
</ul></li>
<li>在讲公式之前,我想花点时间解释一下数学符号的含义,以确保你能理解它:
<ul class="org-ul">
<li>x 表示我们关心的变量;</li>
<li>上面加横线的 \(\overline{x}\) 是表示"x 的平均值"的常用符号.</li>
<li>在正式的统计学符号中: \(\mu\) 或者 $&mu; x$表示 总体均值(population mean), $\overline{x}$表示样本
均值(sample mean);</li>
<li>在其他情况下,你也可能会看到 μ 被用于任何一组数字的平均值,而不论它们来自总体还是样本.</li>
<li>n 表示样本数量,因此 1/n 就是除以样本大小(原文中的 n⁻¹ 是指数文错误,实际是 1/n 的意思).</li>
<li>Σ(求和符号) 表示将变量 x 的所有取值相加.</li>
</ul></li>
<li>计算以下数据集的均值:[2,1,3,2,2]步骤:
<ul class="org-ul">
<li>将所有数相加(2+1+3+2+2=10)</li>
<li>除以数据个数(5) → (10 2 5 = 2),所以均值= 2</li>
</ul></li>
<li><p>
均值可以在直方图中用一条竖线表示(见图 4.9).
</p>

<div id="org45fb1de" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-9.png" alt="4-9.png" />
</p>
<p><span class="figure-number">Figure 36: </span>ms/4-9.png</p>
</div></li>
<li>注意事项:
<ul class="org-ul">
<li>均值可以针对任何数值型数据集计算,但并不是所有情况下它都有直观的解释意义.</li>
<li>均值作为数据集的描述性统计量,只有在数据分布 <b>(1) 大致对称</b> ,*(2) 单峰(unimodal)*时才是有用的.</li>
</ul></li>
<li>为什么是这样?(先自己想一想):
<ul class="org-ul">
<li><p>
例 1(图 4.10 左):
</p>

<div id="org54db387" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-10.png" alt="4-10.png" />
</p>
<p><span class="figure-number">Figure 37: </span>ms/4-10.png</p>
</div></li>
<li>该分布符合幂律分布(power-law),值越大出现的概率越小.</li>
<li>在这种情况下,均值并不能反映数据的集中位置,也不能代表典型值,因此没太大意义.</li>
<li>这种分布有时叫无尺度分布(scale-free),因为它没有明确的特征尺度或数值.</li>
<li>例 2(图 4.10 右):</li>
<li>数据呈现双峰分布(bimodal),恰好左右对称,所以均值位于两个峰的正中间.</li>
<li>但实际上,很少有数据落在均值附近,因此它并不能很好代表"中心位置".</li>
</ul></li>
<li>结论:
<ul class="org-ul">
<li>虽然均值计算简单,可以用于任何数值型数据集,</li>
<li>但只有当分布是单峰且大致对称时,均值才是集中趋势的直观体现.</li>
</ul></li>
<li>关于均值的补充说明:
<ul class="org-ul">
<li>均值不一定是数据中实际存在的值.例如,1,3,4 的均值是 2.67,这既不是中位数,也不是众数(后面会学
到).</li>
<li>均值适用于区间数据(interval data)或比率数据(ratio data)在某些情况下也可以对其他数据类型求均值
<ol class="org-ol">
<li>例如,上一章提到的"产品评分"可以计算均值.</li>
<li>但有时解释会很奇怪,比如美国平均每个家庭有 1.9 个孩子(那是不是说第二个孩子缺了 10%?😂).</li>
<li>对 <b>名义数据(nominal data)</b> 求均值几乎没有意义:若你把巧克力冰淇淋标为 1,香草为 2,草莓为
3,那么"(巧克力 + 香草)÷ 2 = 草莓"显然是无意义的.</li>
</ol></li>
<li>均值不等于期望值(expected value)在某些假设条件下它们可能相等,但并不总是一样的.如果你还不熟悉
期望值,先不用担心,这将在第 8 章详细讲.这里提到它只是为了避免你混淆概念.</li>
<li>均值对极端值(outliers)非常敏感样本量越小,这个问题越严重.你将在练习 6 中具体看到.</li>
<li>即使分布大致对称且单峰,当方差较小时,均值对集中趋势的描述才更有意义.这会引出一个相关的描述性统
计量&#x2013;变异系数(coefficient of variation),后面会学到.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org8c585a0" class="outline-4">
<h4 id="org8c585a0"><span class="section-number-4">4.3.2.</span> Median</h4>
<div class="outline-text-4" id="text-4-3-2">
<ul class="org-ul">
<li>中位数(median)是能够把数据集分成两等份的数据值.</li>
<li>想象一下,将数据从最小(最负)排到最大(最正),那么中位数就是恰好位于中间的那个数.
<ul class="org-ul">
<li>换句话说,50% 的数据小于中位数,50% 的数据大于中位数.</li>
</ul></li>
<li>目前并没有统一公认的中位数符号,有时人们会写成 med(x),但最常见的方式就是直接写成 <b>"数据的中位数(the
median of the data)"</b>.
<ul class="org-ul">
<li><p>
计算中位数的公式如下:
</p>
\begin{equation}
med(x) = \tilde{x}_i, \quad i = \frac{n+1}{2} \tag{4.4}
\end{equation}</li>
<li>~{x} 表示排好序的数据;</li>
<li>i 表示中间那个值的索引(下面会解释公式中的 "+1").</li>
</ul></li>
<li>我们找个例子练个手,
<ul class="org-ul">
<li><p>
比如下面的数据集
</p>
<pre class="example" id="orgef9c14b">
x=[0,1,1,2,2]
</pre></li>
<li>计算步骤:
<ol class="org-ol">
<li>将数据排序(这里本身已排好序);</li>
<li>取中间的那个值.</li>
<li>中间的那个值是 1(已加粗).</li>
</ol></li>
<li>在这个例子中,(n=5),</li>
<li>公式中的索引 (i=(5+1)/2=3),意思是"排序后第 3 个值"就是中位数.在该中位数左边有 2 个数更小,右边
有 2 个数更大.</li>
<li>该数据集的均值(mean)是 2,这个数字并不在数据中.</li>
</ul></li>

<li>我们再找个数据集练手
<ul class="org-ul">
<li><p>
数据集:
</p>
<pre class="example" id="orgd3267c8">
y=[10,0,4,1,−2,7]
</pre></li>
<li><p>
排序后:
</p>
<pre class="example" id="org361d073">
y=[−2,0,1,4,7,10]
</pre></li>
<li>这里数据个数是偶数,因此没有单个值能完美分割数据集.那怎么办呢?会有两个中位数吗?还是根本就没有
中位数?</li>
<li>答案是:中位数为 2.5.它是中间两个数字 1 和 4 的平均数:(1+4)/2=2.5</li>
<li>虽然 2.5 不在原始数据集中,但它能将数据分成两半,因此满足中位数的定义.</li>
<li>当然,有无数个值都能将数据分成两半(例如 1.000001,π,3.999999&#x2026;),但是通过明确规定偶数个数据时取
中间两个值的平均数,就保证了中位数对于给定数据集是唯一的.</li>
</ul></li>
<li>结论:
<ul class="org-ul">
<li>如果数据集元素个数是奇数,中位数就是排序后最中间的那个值;</li>
<li>如果数据集元素个数是偶数,中位数就是中间两个值的平均数.</li>
</ul></li>
<li><p>
图 4.11 展示了一个分布的例子,其中中位数和均值几乎相同.实际上,在对称分布中,中位数和均值是相等或
非常接近的.
</p>

<div id="org733323c" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-11.png" alt="4-11.png" />
</p>
<p><span class="figure-number">Figure 38: </span>ms/4-11.png</p>
</div></li>
<li>中位数是否总是适用于所有数值型数据集?
<ul class="org-ul">
<li><p>
让我们回到图 4.10,不过这次不仅标出均值,也标出中位数(见图 4.12).
</p>

<div id="orge3c559f" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-12.png" alt="4-12.png" />
</p>
<p><span class="figure-number">Figure 39: </span>ms/4-12.png</p>
</div></li>
</ul></li>
<li>请先观察图形并自己得出一些结论.作者的评论
<ul class="org-ul">
<li>左侧面板展示了一个隐含但重要的区别:你之前学到,均值不能代表该数据集的典型取值;而中位数能做到&#x2013;
它准确地把数据分为两半.</li>
<li>但问题是,人们通常把中位数当成分布的中心位置来理解,如果没有看到分布形状,就容易以为中位数就是"最
典型值",这可能会产生误导.</li>
<li>右侧面板的情况类似:中位数虽然把数据切成了两半,但它并不比均值更能代表典型值.</li>
</ul></li>
<li>总结:
<ul class="org-ul">
<li>均值和中位数还有很多重要的特性需要了解,尤其是在样本量和离群值(outliers)方面的表现差异.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org27910c5" class="outline-4">
<h4 id="org27910c5"><span class="section-number-4">4.3.3.</span> Mode</h4>
<div class="outline-text-4" id="text-4-3-3">
<ul class="org-ul">
<li>先补充一点关于术语的说明:
<ul class="org-ul">
<li>作为一种集中趋势的度量,众数(mode)和描述分布峰数量的单峰(unimodal),双峰(bimodal),多峰
(multimodal)等词不是同一个概念.</li>
<li>这两个概念容易混淆,需要注意区分.</li>
</ul></li>
<li>众数是数据集中出现次数最多的值.
<ul class="org-ul">
<li>计算众数的方法很简单:只需统计每个唯一取值的出现次数,</li>
<li>出现次数最多的那个值就是众数(modal value).</li>
</ul></li>
<li>众数最适合用于分类数据(categorical data).事实上,它是唯一适合名义数据(nominal data) 的集中趋势
度量,例如电影类型,冰淇淋口味等.</li>
<li>对于数值型数据,如果数据是离散的(整数或分箱后的值),也可以计算众数,但通常均值会更合适.因为对于
任意精度的连续数据,出现完全相同数值的几率几乎为零.举个例子:
<ul class="org-ul">
<li>假设用精度达到 微克(microgram,一百万分之一克) 的秤来测量企鹅的体重,那么两只企鹅测得的重量完全相同(精确到微克)的概率几乎不可能发生.</li>
</ul></li>
<li>例 1:
<ul class="org-ul">
<li>计算下面整数数据集的众数:[10,0,5,1,5,7]</li>
<li>众数是 5,因为它出现了两次,而其他数值只出现一次.</li>
</ul></li>
<li>例 2
<ul class="org-ul">
<li>数据集:[0,0,1,1,2,7]</li>
<li>这个数据集有两个众数:0 和 1.它们的出现频率相同,并且比其他值出现得更频繁.</li>
</ul></li>
<li>需要注意:
<ul class="org-ul">
<li>均值和中位数对于一个数值型数据集来说是唯一的;但众数可能有多个.</li>
</ul></li>
<li>拥有多个众数并不一定难以解读.例如:
<ul class="org-ul">
<li><p>
图 4.13(示意图)展示了某个虚拟数据集的洗衣日(laundry day)分布:周三 和 周日 出现频率最高.
</p>

<div id="org5445183" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-13.png" alt="4-13.png" />
</p>
<p><span class="figure-number">Figure 40: </span>ms/4-13.png</p>
</div></li>
<li>解释很简单:人们最可能在周三和周日洗衣服.</li>
</ul></li>
<li>另一方面,对这类数据集计算均值是毫无意义的:"平均洗衣日"大约在周四半天,显然没有任何现实上的解释价值.</li>
<li>可视化上,众数对应于柱状图中最高的柱子.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org33d260b" class="outline-3">
<h3 id="org33d260b"><span class="section-number-3">4.4.</span> Measures of dispersion</h3>
<div class="outline-text-3" id="text-4-4">
<ul class="org-ul">
<li>离散程度(dispersion)指的是一个分布的宽度.
<ul class="org-ul">
<li><p>
举个例子:想象两种收入分布情况&#x2013;(见图 4.14).
</p>

<div id="org07e75b4" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-14.png" alt="4-14.png" />
</p>
<p><span class="figure-number">Figure 41: </span>ms/4-14.png</p>
</div></li>
<li>数据集 A:1000 位拥有相同职位,在同一行业工作的人的工资分布(比如某连锁餐厅的所有经理的工资);</li>
<li>数据集 B:1000 位来自任何岗位,任何行业的人的工资分布</li>
<li>在餐厅经理这个群体中,工资不会完全相同,但会相对接近;</li>
<li>而在随机抽取的全行业人员中,工资分布会涵盖收入水平的低端和高端.</li>
<li>我在这里创建了均值相同,但离散程度不同的两个数据集.</li>
<li>衡量数据集离散程度的方法有很多,在本节我会一一介绍.不过,我希望图 4.14 先能帮你直观理解这个概念.</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orgf462a52" class="outline-4">
<h4 id="orgf462a52"><span class="section-number-4">4.4.1.</span> Variance</h4>
<div class="outline-text-4" id="text-4-4-1">
<ul class="org-ul">
<li>总体方差的符号是 σ²,样本方差的符号是 s².</li>
<li>有时你还会看到记作 σ²ₓ 或 var(x),表示变量
<ul class="org-ul">
<li><p>
方差的公式见方程(4.5).
</p>
\begin{equation}
s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2 \tag{4.5}
\end{equation}</li>
<li>用文字解释公式:计算方差的步骤:
<ol class="org-ol">
<li>从每个数据值中减去该数据的均值;</li>
<li>对差值平方;</li>
<li>把所有平方后的差值加总;</li>
<li>再除以样本大小减 1(n - 1).</li>
</ol></li>
<li>换句话说,方差就是数据与均值的偏差平方的平均值.</li>
<li>可能你对公式中某些部分有疑问,我会在后面逐一解释.但先来看几个例子.</li>
</ul></li>
<li>例 1
<ul class="org-ul">
<li>数据集:P=[8,0,4,1,−2,7]</li>
<li>均值是 3.</li>
<li>方差计算结果是 16.</li>
</ul></li>
<li>例 2
<ul class="org-ul">
<li>数据集:Q=[2,3,4,3,4,4]</li>
<li>方差是 2/3.</li>
</ul></li>
<li>这两个结果意味着什么呢?
<ul class="org-ul">
<li>var(P) 比 var(Q) 大 24 倍,说明 P 的数据更分散;</li>
<li><p>
从图 4.15 的可视化中,你能直观看到 P 的数据比 Q 更分布分散.
</p>

<div id="orgcff9a08" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-15.png" alt="4-15.png" />
</p>
<p><span class="figure-number">Figure 42: </span>ms/4-15.png</p>
</div></li>
<li>它看起来有 24 倍那么分散吗?可能并没有,但这是因为方差涉及平方操作,数值差异会被放大.</li>
</ul></li>
<li>方差可用于任何数值型或有序型(ordinal)数据.</li>
<li>你可能会觉得,既然方差公式里用到均值,那只有在均值有意义的分布下它才有用.
<ul class="org-ul">
<li>但事实上,即使数据分布不是单峰或对称的,方差也能提供有价值的比较指标&#x2013;尤其是在分布形状相似的不同
数据集之间比较时.</li>
</ul></li>
<li>关于方差公式(方程 4.5)的几个重要问题</li>
<li><b>为什么要"中心化"(mean-center)?</b>
<ul class="org-ul">
<li>我们希望方差反映数据内部的离散程度,而不是受数据整体位置的影响.比如,以下两个数据集的方差都相同(4/5):
<ol class="org-ol">
<li>[d1 =[1,2,3,3,2,1]]</li>
<li>[d2 =[101,102,103,103,102,101]]</li>
</ol></li>
<li>它们的均值不同,但"分散程度"一样,通过中心化可以得出相同的方差.</li>
</ul></li>

<li><b>为什么要平方差值?</b>
<ul class="org-ul">
<li>平方的作用是给每个差值一个正数的"距离"度量.</li>
<li><p>
如果不平方(直接求和),对于任何经过均值中心化的数据集,差值的总和恒为 0.例如:
</p>
\begin{equation}
\frac{1}{5} \sum_{i=1}^{n} (x_i - 2)
= \frac{-1 + 0 + 1 + 1 + 0 - 1}{5}
= 0 \notag
\end{equation}</li>
<li>如果不平方,结果永远是零,起不到衡量离散程度的作用.</li>
</ul></li>
<li><b>为什么不用绝对值代替平方?</b>
<ul class="org-ul">
<li><p>
实际上是可以的,这种方法叫平均绝对偏差(MAD, mean absolute difference):
</p>
\begin{equation}
MAD = \frac{1}{n-1} \sum_{i=1}^{n} \left| x_i - \overline{x} \right| \tag{4.8}
\end{equation}</li>
<li>它的优点是不太受离群值影响,在机器学习和优化算法中也有应用.</li>
</ul></li>
<li>为什么统计学中更多用方差而不是 MAD?</li>
<li>方差相比 MAD 有很多数学和统计学上的优势:
<ul class="org-ul">
<li>更强调大的偏差,有助于发现离群值(异常值),适用于如金融风险评估等领域;</li>
<li>连续且可微分,便于数学优化;</li>
<li>与欧几里得距离密切相关,有良好的几何解释;</li>
<li>是分布的二阶矩(second moment)(后面章节会讲统计矩的概念);</li>
<li>与最重要的 <b>最小二乘法(least squares)</b> 拟合算法紧密相关.</li>
</ul></li>
<li><b>为什么分母是 n-1 而不是 n?</b>
<ul class="org-ul">
<li>这是很多人困惑的点.</li>
<li>首先,我们必须通过样本大小来缩放方差,否则数据量增加,方差会无意义地增大.</li>
<li>当计算的是**总体方差(σ²)时,可以除以 (n);</li>
<li>当计算的是样本方差(s²)**时,需要除以 (n−1).原因是:已知样本均值后,数据集中最后一个值是可以被
确定的,所以数据的**自由度(degrees of freedom)**是 (n−1) 而不是 (n).</li>
</ul></li>
<li>举个例子:
<ul class="org-ul">
<li>公平的 6 面骰子,1~6 的均值是 3.5;</li>
<li>如果掷 4 次骰子,已知均值是 3,且前三次是 1,2,4,那么第四次的值一定是 5.</li>
<li>即,一旦样本均值确定了,数据中只有 (n−1) 个值可以自由变化.因此,除以 n-1 就是为了正确归一化样本方差.</li>
</ul></li>
<li>补充:
<ul class="org-ul">
<li>方差对于只有一个数据值的样本是未定义的(n=1 时自由度为零),</li>
<li>但均值在 n=1 的情况下是可以定义的.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org15a1a45" class="outline-4">
<h4 id="org15a1a45"><span class="section-number-4">4.4.2.</span> Standard deviation</h4>
<div class="outline-text-4" id="text-4-4-2">
<ul class="org-ul">
<li>从数学上说,本节的核心内容很简单:标准差就是方差的平方根.</li>
<li>换句话说,如果方差是 \(s^2\), 那么标准差就是 \(s\)</li>
<li><p>
有时标准差也会写成 std(x) 或 STD.
</p>
\begin{equation}
s = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2} \tag{4.9}
\end{equation}</li>
<li>那么问题来了:既然标准差只是方差的平方根,为什么我们还要区分这两个概念?(或者反过来说:既然方差只
是标准差的平方,为什么不只用一个呢?)</li>
<li>实际上,这两者在数学和概念上密切相关&#x2013;它们都反映了数据围绕均值的离散程度.区别在于解释性:
<ul class="org-ul">
<li>标准差的单位与原始数据相同,因此通常更容易解释;</li>
<li>方差的单位是原单位的平方,这在直观理解上更困难.</li>
</ul></li>
<li>例如:
<ul class="org-ul">
<li>如果测量的是身高(单位:英尺),那么标准差的单位也是英尺,容易理解;</li>
<li>但方差的单位是平方英尺(feet²),在概念上就不太直观.</li>
<li>在实际统计中,有些场景会用方差,有些会用标准差;</li>
</ul></li>
<li>在后续的学习中(包括本书后面的章节),你会遇到各种使用二者的情况.</li>
<li>举个例子:
<ul class="org-ul">
<li>一种常见的数据归一化方法叫 z-score 标准化(z-scoring),它将数据转换为以标准差为单位的形式;</li>
<li>回归分析等统计结果也可以转换为标准差单位,以方便解释和比较.</li>
</ul></li>
<li>当前你只需要记住两点:
<ul class="org-ul">
<li>标准差和方差通过平方/平方根关系互相关联;</li>
<li>在不同情境中,会因为解释性或数学计算便利性的原因,选择用其中一个而不是另一个.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org715d54e" class="outline-4">
<h4 id="org715d54e"><span class="section-number-4">4.4.3.</span> Heteroscedasticity and Homoscedasticity</h4>
<div class="outline-text-4" id="text-4-4-3">
<ul class="org-ul">
<li>同方差性(homoscedasticity) 指的是一个变量在所有取值上都具有相同的方差
<ul class="org-ul">
<li><p>
(见图 4.16A).
</p>

<div id="org05e32e7" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-16.png" alt="4-16.png" />
</p>
<p><span class="figure-number">Figure 43: </span>ms/4-16.png</p>
</div></li>
</ul></li>
<li>它的相反概念是异方差性(heteroscedasticity),意思是某个变量的方差会随变量取值的变化而变化.
<ul class="org-ul">
<li>例如,图 4.16B 展示了一个方差随 x 轴数值增加而增大的例子.</li>
</ul></li>
<li>比这两个词稍微没那么有趣,但同样准确的术语是:
<ul class="org-ul">
<li>方差齐性(homogeneity of variance)</li>
<li>方差异质性(heterogeneity of variance)</li>
</ul></li>
<li>异方差性的重要性:它在许多分析(如相关性分析,回归分析)中非常关键.</li>
<li>当数据存在异方差性时:
<ul class="org-ul">
<li>相关系数的解释性会下降;</li>
<li>回归系数的标准误可能被放大;</li>
<li>显著性检验的结果可能不可靠.</li>
</ul></li>
<li>现实中的例子财富与消费支出是一个经典案例:随着财富增加,消费支出的波动性也会增加;
<ul class="org-ul">
<li>低收入家庭的支出水平相对接近,因为支出的下限由基本生活需求(食品,房租等)决定,而且大额消费的可
能性有限;</li>
<li>高收入家庭可支配收入更多,因此既能购买低价物品,也会购买高价商品,比如昂贵的电子产品,家电,汽车
度假等.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga8dd0b6" class="outline-4">
<h4 id="orga8dd0b6"><span class="section-number-4">4.4.4.</span> Full width at half maximum (FWHM)</h4>
<div class="outline-text-4" id="text-4-4-4">
<ul class="org-ul">
<li>FWHM 是衡量高斯函数(Gaussian function)宽度的一种方法.</li>
<li>它既可以通过解析方法(analytically)计算,也可以通过经验方法(empirically)计算,并且对于任何近似高
斯形状的分布都是有意义的.</li>
<li>"Full Width at Half Maximum" 这个短语全称很长(直译:半最大值处的全宽),它到底是什么意思呢?
<ul class="org-ul">
<li>想象一个高斯函数,经过归一化处理之后峰值为 1(或 100%),两边逐渐下降至 0.</li>
<li>这意味着在峰值两侧,会各有一个数据点对应的高度为 0.5(或 50%) &#x2013; 这就是 half maximum(半最大值).</li>
<li>在 x 轴上,这两个半最大值点之间的距离,就是 FWHM.</li>
<li><p>
图 4.17 给出了这个概念的示意图.
</p>

<div id="org58878fd" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-17.png" alt="4-17.png" />
</p>
<p><span class="figure-number">Figure 44: </span>ms/4-17.png</p>
</div></li>
</ul></li>
<li>对于精确的高斯函数,FWHM 可以通过解析公式直接计算.
<ul class="org-ul">
<li><p>
高斯函数
</p>
\begin{equation}
g(x) = \exp\left( \frac{-x^2}{2\sigma^2} \right) \tag{4.10}
\end{equation}</li>
<li><p>
高斯函数的FWHM
</p>
\begin{equation}
FWHM\left( g(x) \right) = 2\sigma \sqrt{2 \ln 2} \tag{4.11}
\end{equation}</li>
</ul></li>
<li>但对于经验分布(empirical distribution),FWHM 不能直接得到.在这种情况下,你可以使用一个算法从数据
分布中求得 FWHM.我会在练习 10中解释这个算法;</li>
</ul>
</div>
</div>
<div id="outline-container-org19bad29" class="outline-4">
<h4 id="org19bad29"><span class="section-number-4">4.4.5.</span> Fano factor and CV</h4>
<div class="outline-text-4" id="text-4-4-5">
<ul class="org-ul">
<li>我在本章前面提到过,即使是对称的单峰分布,均值的可解释性也在一定程度上取决于方差:
<ul class="org-ul">
<li>对于近似正态分布来说,方差越小,数据值就越集中在均值附近.</li>
</ul></li>
<li>Fano因子(Fano factor: FF)和 变异系数(Coefficient of Variation: CV)就是用于刻画这种"均值与方差关系"
的指标.
<ul class="org-ul">
<li>它们的公式很相似</li>
<li><p>
FF公式
</p>
\begin{equation}
FF = \frac{s^2}{\overline{x}} \tag{4.12}
\end{equation}</li>
<li><p>
CV公式
</p>
\begin{equation}
CV = \frac{s}{\overline{x}} \tag{4.13}
\end{equation}</li>
</ul></li>
<li>这两个量只在均值为正的数据集(通常是数值严格为正的数据集)中才有意义.原因很容易理解:
<ul class="org-ul">
<li>看分母部分,如果数据值围绕 0 对称分布,那么均值为 0 或接近 0;</li>
<li>这样一来,FF 或 CV 的值可能会无法定义,会趋向于 ±∞,甚至可能为负数;这些情况都没有实际解释意义.</li>
</ul></li>
<li>解释:
<ul class="org-ul">
<li>当 FF 或 CV 趋近于 0 时,说明方差相对均值非常小,数据很集中;</li>
<li>当 FF 或 CV 变大时,说明分散程度远大于均值(见图 4.18);</li>
</ul></li>
<li>因此,Fano 因子和 CV 可以视为信噪比的倒数测度(inverted signal-to-noise ratio).</li>
<li>它们在计算科学领域(物理学,生物学,神经科学等)有广泛应用.</li>
<li>主要区别:
<ul class="org-ul">
<li>Fano 因子在计算中使用方差,因此保留数据的单位;</li>
<li>CV 在计算中使用标准差,因此是无单位的比值.</li>
</ul></li>
<li>例子:假设你测量计算机运行一个算法所需的时间(单位:毫秒 ms):
<ul class="org-ul">
<li>Fano 因子的单位是 \(\frac{{ms}^2}{ms}=ms\)</li>
<li>CV 的单位是\(\frac{ms}{ms}=1\)</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb882094" class="outline-3">
<h3 id="orgb882094"><span class="section-number-3">4.5.</span> Interquartile range(IQR)</h3>
<div class="outline-text-3" id="text-4-5">
<ul class="org-ul">
<li>IQR 是另一种衡量数据集 离散程度 的方法,我在 第 3.5 节(箱线图部分)中曾简要介绍过它.</li>
<li>IQR 表示数据中 第 25% 分位数 和 第 75% 分位数 之间的数值距离
<ul class="org-ul">
<li><p>
(见图 4.19,该图来自前一章).
</p>

<div id="org061f813" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-19.png" alt="4-19.png" />
</p>
<p><span class="figure-number">Figure 45: </span>ms/4-19.png</p>
</div></li>
<li>具体来说,IQR 的计算步骤如下
<ol class="org-ol">
<li>计算数据的 中位数,记作 第二四分位数(Q2).</li>
<li>取出所有 小于 Q2 的数据(在直方图中位于中位数左侧的数据),然后计算它们的中位数,记作 第一四分
位数(Q1).</li>
<li>取出所有 大于 Q2 的数据(直方图中位于中位数右侧的数据),然后计算它们的中位数,记作 第三四分位
数(Q3).</li>
<li>用 Q3 - Q1 的结果,就是 IQR.</li>
</ol></li>
<li><p>
具体步骤如(见图 4.20):
</p>

<div id="org5266752" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-20.png" alt="4-20.png" />
</p>
<p><span class="figure-number">Figure 46: </span>ms/4-20.png</p>
</div></li>
<li>(题外话:我觉得"四分位数 1-3"的命名不太直观,因为"四分位数"指的是数据区段,而中位数是这些区段的
边界 &#x2013; 事实上,四个区段有五条边界.我更倾向于使用"p25","p50","p75"这样的叫法,其中 p 表示 "百
分位数".不过,一旦术语习惯被确立,很难再去改变.就像莎士比亚说的:"不论名字叫什么,玫瑰依然是玫瑰.")</li>
</ul></li>
<li>IQR 的解释:
<ul class="org-ul">
<li>较小的 IQR 表示数据分布比较集中;</li>
<li>较大的 IQR 表示数据分布比较分散.</li>
</ul></li>
<li>IQR 具有和数据相同的 单位,因此在数据集内部解释很方便,但在跨数据集比较时,如果单位不同,就不太容易
直接比较.</li>
<li>IQR 是一种 非参数的变异性度量方法,因为它是基于 中位数 而不是 均值.正因如此,IQR 对 异常值(outliers)
不敏感.</li>
</ul>
</div>
</div>
<div id="outline-container-orgb249cb2" class="outline-3">
<h3 id="orgb249cb2"><span class="section-number-3">4.6.</span> QQ plots</h3>
<div class="outline-text-3" id="text-4-6">
<ul class="org-ul">
<li>QQ 图最棒的地方就是它的名字&#x2013;把它说出口的感觉就很好玩.除此之外嘛&#x2026;&#x2026;嗯,我们只能说,QQ 图需要有一定
经验才能用得很顺手.</li>
<li>第一个 "Q" 代表 quantile(分位数),这个词指的是把数据分成等大小的区间(比如 IQR 就是把数据分成四个
分位数).</li>
<li>第二个 "Q" 也是 quantile.因此,QQ 图的全称就是 quantile-quantile plot(分位数对分位数图).</li>
<li>QQ 图用来展示如下两者之间的关系.
<ul class="org-ul">
<li>一个经验数据分布</li>
<li>一个理论高斯(正态)分布</li>
</ul></li>
<li>它的目的是 定性地 评估一个给定分布是否看起来像高斯分布.因此,QQ 图常被用于判断数据是否适合统计方法
(比如 ANOVA 方差分析,回归分析),以及用来发现数据集中可能存在的问题.</li>
<li>用一个例子来引入 QQ 图
<ul class="org-ul">
<li>假设我给你两个数据集,并且问你:这些数据是不是来自于一个正态分布的总体?你会怎么判断呢?</li>
<li>当然,你会先画出这些数据的 直方图(如果这不是你想到的,也没关系,就假装这是你的主意 😄).</li>
<li>为了帮助你判断这些分布是否 "像" 高斯分布,你可能会在直方图上叠加一个已经归一化的理论高斯曲线.</li>
<li><p>
在 图 4.21 中,
</p>

<div id="org10729c8" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-21.png" alt="4-21.png" />
</p>
<p><span class="figure-number">Figure 47: </span>ms/4-21.png</p>
</div></li>
<li>你可以看到:
<ol class="org-ol">
<li>面板 A 中的直方图看上去很像理论高斯分布</li>
<li>而面板 B 则明显不同具体来说,在面板 B 中,直方图左侧的概率值陡然下降,而不是平滑地收尾;而右尾
部的概率值比高斯分布的要大.</li>
</ol></li>
</ul></li>
<li>QQ 图的思路
<ul class="org-ul">
<li>QQ 图的想法是提供一种更有效的图形方式,把 "观测到的分布" 与 "理论高斯分布" 作比较.</li>
<li>在图 4.21A 和 4.21B 中,我们有两条曲线:
<ol class="org-ol">
<li>一条是经验数据的直方图曲线</li>
<li>一条是理论高斯分布曲线</li>
</ol></li>
<li>QQ 图会把这两条曲线的信息组合到一张图中:</li>
</ul></li>
<li>在 QQ 图中:
<ul class="org-ul">
<li>x 轴 是理论高斯分布的分位数</li>
<li>y 轴 是样本数据的分位数</li>
<li>如果数据确实来自一个完全的高斯过程,那么数据点会全部落在对角线(y = x)上.</li>
<li>相反,如果数据不是高斯分布,数据点就会明显偏离对角线.</li>
</ul></li>
<li>看一个例子
<ul class="org-ul">
<li>在图 4.21 的左列:数据是从正态分布中采样得到的(对应面板 A 的直方图)所以在 QQ 图(面板 C)中,
我们预期数据点会落在对角线上当然,由于采样变异和一些噪声,它们不会完全在对角线上,但会很接近</li>
<li>在图 4.21 的右列:数据来自一个幂分布(对应面板 B 的直方图)它显然不是高斯分布在对应的 QQ 图(面板
D)中,我们看到数据点明显没有落在对角线上,而且这些偏离不是由随机噪声造成的,而是有系统性差异这就
是明显的 "数据不服从正态分布" 的信号.</li>
</ul></li>
<li>如何解读偏离现在说到难点:如何精确解读这些偏离?</li>
<li>举个例子:
<ul class="org-ul">
<li>在图 4.21D 中,假设我们画一条垂直线穿过 x = 3,它会在大约 y = 6 的地方与数据点相交.这表示:在右
尾部,数据比高斯分布的预期更大(右尾 "拉长" 了).</li>
<li>再画一条垂直线穿过 x = -3,它会在大约 y = 0 的地方与数据交点&#x2013;这说明数据的最小值比高斯分布的预期
更大.实际上,这组数据的最小值都大于零,即数据没有比正态分布中心更小的值.</li>
</ul></li>
<li>总结:
<ul class="org-ul">
<li>我希望上面的解释有帮助.不过正如我一开始说的,QQ 图需要一定练习才能熟练使用.</li>
<li>一般来说,我们通常不需要去深究 QQ 图每个区域的复杂含义&#x2013;主要是判断 数据是否大致沿对角线分布:
<ol class="org-ol">
<li>接近对角线 → 数据可能符合正态分布</li>
<li>远离对角线 → 数据可能不符合正态分布</li>
</ol></li>
<li>理解 QQ 图的构建机制,需要掌握 概率函数,这部分会在第 8 章详细介绍.</li>
<li>目前,你只需要理解:QQ 图是在比较 实际数据(y 轴) 与 理论正态分布(x 轴) 的分位数关系.</li>
</ul></li>
<li>最后补充一句:
<ul class="org-ul">
<li>QQ 图中 x 轴上的理论分布不一定必须是正态分布&#x2013;只是因为正态分布在统计学上的重要性,我们大多数情况
下都会用它,但理论上可以用任何分布来做 QQ 图.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgf2738a7" class="outline-3">
<h3 id="orgf2738a7"><span class="section-number-3">4.7.</span> Statistical "moments"</h3>
<div class="outline-text-3" id="text-4-7">
<ul class="org-ul">
<li>统计学中的 "矩(moments)" 是用来描述分布形状的一组数值.</li>
<li>每个分布都有第一矩,第二矩,第三矩,依此类推.</li>
<li>在实际应用中,最常用的是 前两个矩,而超过第四矩的情况很少被报告.</li>
<li>事实上,你已经知道第一矩和第二矩了&#x2013;它们就是我们平时说的 均值(mean) 和 方差(variance).</li>
<li>因此,我想你会觉得,把这些特征推广到"矩"的概念既直观,又能带来一些新的认知.</li>
</ul>
</div>
<div id="outline-container-orge94dbbc" class="outline-4">
<h4 id="orge94dbbc"><span class="section-number-4">4.7.1.</span> Unstandardized and standardized moments</h4>
<div class="outline-text-4" id="text-4-7-1">
<ul class="org-ul">
<li>我把这一部分内容按前四阶矩的顺序分成了几个小节,不过在讨论每个矩的具体细节和解释之前,我想先给你展
示两条公式和一张表,用于提供一个整体的概览.</li>
<li>我们先来看 "非标准化"矩(unstandardized moments) 的通用公式.
<ul class="org-ul">
<li><p>
对于数据集 X其 第 k 阶矩(kth moment) 定义如下:
</p>
\begin{equation}
m_k = \frac{1}{N} \sum_{i=1}^{N} (X_i - \overline{X})^{k} \tag{4.14}
\end{equation}</li>
<li>其中,N 是样本的规模(数量),</li>
<li>i 用来索引数据集中每一个样本点.</li>
<li>你可以看到,所有统计矩 都可以由同一个公式定义,只是求和之前的差值会被提升到更高次幂.我将在接下来
的小节中讨论这个特点的重要意义.</li>
<li>除以 N 的作用,是为了防止样本数量较大的数据集在数值上"天然"拥有更大的矩值</li>
<li>然而,即使如此,数据的量纲(单位)仍会影响 $m_k$的值.例如,同一组数据如果用 毫米 和 米 两种单位
来表示,得到的 \(m_k\) 数值会不同.</li>
<li>正因如此,非标准化矩(unstandardized moments) 的数值往往不容易直接解释&#x2013;更无法在量纲不同的数据集
之间比较.</li>
</ul></li>
<li>为了解决这个问题,我们通常会用 标准差(standard deviation) 的 k 次幂 对矩进行 归一化.
<ul class="org-ul">
<li><p>
这一归一化步骤能够去除数据的量纲影响.
</p>
\begin{equation}
m_k = \frac{1}{N\sigma^k} \sum_{i=1}^{N} (X_i - \overline{X})^{k}
\end{equation}</li>
<li>上面的公式assume标准差 \(\sigma\) 已知,在实践当中,通常以s(也就是sample的标准差)来预估全体(population)
的标准差 \(\sigma\).</li>
<li>同时注意, <b>这里不同k用的标准差的不同次方</b>, k=1的时候是标准差,k=2的时候就是方差,k=3的时候是标准差
的三次方</li>
<li><p>
图 4.22 给出了前四阶矩的概览,它们的统计学术语,以及各自的公式.请在阅读本节剩余内容时,参考这张
表来理解
</p>

<div id="org2e27d40" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-22.png" alt="4-22.png" />
</p>
<p><span class="figure-number">Figure 48: </span>ms/4-22.png</p>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org2f808c9" class="outline-4">
<h4 id="org2f808c9"><span class="section-number-4">4.7.2.</span> First moment: mean</h4>
<div class="outline-text-4" id="text-4-7-2">
<ul class="org-ul">
<li>一个分布的第一阶矩就是均值(mean)&#x2013;也就是平均值.</li>
<li>这来自于在公式 4.14 中取 k=1的情况</li>
<li>不过,当 (k=1) 时,实际上我们会得到 \(m_1 = 0\) ,这个显然的结果,而这个结果与数据的均值无关.因为,从
形式上来说,每个分布的第一阶矩都是 0.</li>
<li><p>
正因为如此,在实际处理中,我们会去掉均值中心化项(mean-centering term),并重新定义第一阶矩为:
</p>
\begin{equation}
m_1 =\frac{1}{N} \sum_{i=1}^{N} X_i \tag{4.16}
\end{equation}</li>
<li>当然,你一定一眼就认出来了&#x2013;这正是均值的公式.</li>
<li>你也知道,它的意义就是分布的集中趋势(central tendency):</li>
<li>它是数据的质心(center of mass),也是直方图平衡时的支点位置.</li>
</ul>
</div>
</div>
<div id="outline-container-org0288765" class="outline-4">
<h4 id="org0288765"><span class="section-number-4">4.7.3.</span> Second moment: variance</h4>
<div class="outline-text-4" id="text-4-7-3">
<ul class="org-ul">
<li>你其实也已经知道了分布的第二阶矩:那就是方差(variance),它表示数据围绕均值的离散程度.</li>
<li>当你把公式 4.15 在 k=2与公式 4.5 进行比较时,你会发现,它们是一样的.
<ul class="org-ul">
<li>既然第二阶矩就是方差,那么很容易看出,标准化的第二阶矩 恒等于 1&#x2013;因为它是方差除以方差的结果.</li>
<li>因此,在实际应用中,我们使用的通常是非标准化的第二阶矩.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org57f5bf2" class="outline-4">
<h4 id="org57f5bf2"><span class="section-number-4">4.7.4.</span> Third moment: skew</h4>
<div class="outline-text-4" id="text-4-7-4">
<ul class="org-ul">
<li>分布的第三阶统计矩叫做 "偏"(skew),而分布中偏的程度称为 "偏度"(skewness).</li>
<li>偏度与均值两侧方差分布的不对称性有关.</li>
<li>第三阶矩通常使用标准化公式来计算,它不会像前面的一些情况那样,在所有数据集里都得到相同的固定值.
<ul class="org-ul">
<li>一个 <b>纯高斯分布(正态分布)</b> 的偏度为 0,这是合理的,因为高斯分布是完全对称的.</li>
<li><p>
图4-23
</p>

<div id="org374c896" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-23.png" alt="4-23.png" />
</p>
<p><span class="figure-number">Figure 49: </span>ms/4-23.png</p>
</div></li>
<li>在直方图中,如果一个分布被拉向均值左侧,我们称之为负偏(left-skew,也叫左偏)(见图 4.23A);</li>
<li>而如果分布被拉向均值右侧,则称为正偏(right-skew,也叫右偏)(见图 4.23B).</li>
</ul></li>
<li>为什么第三阶矩能够反映分布的倾斜程度呢?</li>
<li>考虑一下:均值中心化,用 \((X_i - \overline{X})\) 后,均值左边的数据是负数,均值右边的数据是正数.
<ul class="org-ul">
<li>如果将数值提升到偶数次幂,负数会变成正数: \((-3)^2=9\) 和 \(3^2=9\) 一样</li>
<li>但如果将数值提升到奇数次幂,符号会被保留: \((-2)^3=-8\) 和 \(2^3=8\) 则不一样</li>
</ul></li>
<li>因此,如果均值左边出现的极端值比右边多,那么均值中心化后的立方值的平均就会是负数,偏度也就为负.</li>
<li>同理,如果右边的极端值更多,则偏度为正.</li>
<li>所以,偏度的大小与符号,本质上是由分布在均值左右的不对称性(left-right asymmetry)决定的.</li>
</ul>
</div>
</div>
<div id="outline-container-orgd432049" class="outline-4">
<h4 id="orgd432049"><span class="section-number-4">4.7.5.</span> Fourth moment: kurtosis</h4>
<div class="outline-text-4" id="text-4-7-5">
<ul class="org-ul">
<li>第四阶矩称为 峰度(kurtosis),通常计算并报告的是它的标准化版本.
<ul class="org-ul">
<li><p>
在继续阅读下文之前,不妨先试着通过 图 4.24 中的分布推测一下峰度的含义.
</p>

<div id="org2ca170c" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-24.png" alt="4-24.png" />
</p>
<p><span class="figure-number">Figure 50: </span>ms/4-24.png</p>
</div></li>
<li>在那张图中,三个分布的均值是相同的,但有一个分布的尾部比另一个分布更快地收敛到零.</li>
<li>峰度用来衡量的就是这种差异.</li>
</ul></li>
<li>峰度的解释:
<ul class="org-ul">
<li>峰度衡量的是分布尾部的"肥胖程度"(fatness of the tails).</li>
<li>判断峰度时要问的问题是:这个分布的尾部,比纯高斯分布(图 4.24 中的灰色实线)更快还是更慢地趋近于零?</li>
</ul></li>
<li>纯高斯分布的峰度是 3,因此报告峰度时,人们往往会用 计算值减去 3,得到所谓的 "过度峰度"(excess
kurtosis).具体来说:
<ul class="org-ul">
<li>正的过度峰度(或者未修正值 &gt; 3)意味着分布的尾部比高斯分布更陡峭地趋向于零: 尾部是 "瘦" 的(thin tails);</li>
<li>负的过度峰度(或者未修正值 &lt; 3)意味着尾部比高斯分布收敛得更慢: 尾部是 "肥" 的(fat tails).</li>
</ul></li>
<li>常见误区
<ul class="org-ul">
<li>从 公式 4.14 看,你可能会以为峰度就是方差的平方,但事实并非如此.</li>
<li>原因是 k 次幂在求和符号内部,而不是外部.这与"方差并不是均值的平方"同理.</li>
<li>实际上,峰度和方差是彼此独立的,你可以改变其中一个而不改变另一个.</li>
</ul></li>
<li>峰度的应用特点
<ul class="org-ul">
<li>因为偏离均值的差异会被提升到 四次幂,峰度对极端值(outliers)非常敏感.</li>
<li>这使得峰度成为衡量数据中极端事件的一个有效工具.</li>
</ul></li>

<li>例如:
<ul class="org-ul">
<li>在 金融数据 中,峰度被用于评估风险(极端波动的可能性);</li>
<li>在 多变量数据分析 中,峰度可用于通过 独立成分分析(Independent Components Analysis, ICA) 区分信
号和噪声.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgde422fb" class="outline-4">
<h4 id="orgde422fb"><span class="section-number-4">4.7.6.</span> What to memorize</h4>
<div class="outline-text-4" id="text-4-7-6">
<ul class="org-ul">
<li>关于统计矩,其实还有很多有趣的细节可以深入探讨.</li>
<li>但是,在应用统计中要学的东西实在太多&#x2013;而我们的时间又太有限.
<ul class="org-ul">
<li><p>
图 4.26 展示了我关于"统计矩"应该记住哪些重点内容的建议
</p>

<div id="orgb4f19b5" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-26.png" alt="4-26.png" />
</p>
<p><span class="figure-number">Figure 51: </span>ms/4-26.png</p>
</div></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org208cc56" class="outline-3">
<h3 id="org208cc56"><span class="section-number-3">4.8.</span> Histograms part 2: Number of bins</h3>
<div class="outline-text-3" id="text-4-8">
<ul class="org-ul">
<li>现在,你已经积累了一些绘制和使用直方图的经验,我们可以回到之前提到过的一个问题:绘制直方图时应该使
用多少个箱(bins).(这实际上等同于箱子的宽度问题,因为箱子的数量决定了箱子的宽度,反之亦然.)</li>
<li>我之前曾定性地说过,箱子太少或太多都不利于解释数据;而在本节中,我会向你介绍几种常用的确定直方图箱
数的指南.(人们有时称它们为"规则",但科学家可不是那种盲目信任权威的人,所以我们更愿意把它们看作建
议.)</li>
<li>先定义变量关系
<ul class="org-ul">
<li><p>
我们从分箱数量(变量 k)与分箱宽度(变量 w)的关系开始:
</p>
\begin{equation}
k = \left\lceil \frac{\max(x) - \min(x)}{w} \right\rceil \tag{4.17}
\end{equation}</li>
<li>公式里的不完整括号表示上取整函数(ceiling function),即向上舍入到下一个更大的整数.</li>
</ul></li>

<li>三种常用的方法
<ul class="org-ul">
<li><p>
确定分箱数量的方法有很多,这里我会重点介绍三种常见方法(见图 4.27,稍后会详细说明):
</p>

<div id="org377e86d" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-27.png" alt="4-27.png" />
</p>
<p><span class="figure-number">Figure 52: </span>ms/4-27.png</p>
</div></li>
</ul></li>
<li>任意指定法(Arbitrary)
<ul class="org-ul">
<li>简单地把分箱数设为 40(或任何你觉得合适的数).</li>
<li>方法简单直观,容易重复;</li>
<li>当样本量在几百或更多时,这个方法通常表现不错.</li>
</ul></li>
<li>Sturges 规则
<ul class="org-ul">
<li>根据数据量计算箱数的公式化方法;</li>
<li>箱的数量会随数据量变化.</li>
</ul></li>
<li>Freedman-Diaconis 规则(简称 FD 或 F-D)
<ul class="org-ul">
<li>不直接计算箱数,而是先算箱宽 w,再计算箱数 k同时考虑了样本量 和 数据的变异性;</li>
<li>很多人认为 FD 规则是最好的.</li>
</ul></li>
<li>注意:
<ul class="org-ul">
<li>Arbitrary 和 Sturges 规则直接确定的是 (k)(分箱数);</li>
<li>FD 规则确定的是 (w)(箱宽),然后再计算 (k).</li>
</ul></li>
<li>实际中怎么用
<ul class="org-ul">
<li>在实际工作中,你不需要自己去实现这些规则.</li>
<li><p>
Python自动帮你计算,比如:
</p>
<div class="org-src-container">
<pre class="src src-python">plt.hist<span style="color: #000000;">(</span>data, bins=<span style="color: #3548cf;">'fd'</span><span style="color: #000000;">)</span>
</pre>
</div></li>
<li><p>
R也能自动帮你计算,比如
</p>
<pre class="example" id="org67208b0">
hist(data, breaks='FD')
</pre></li>
</ul></li>
<li>关于可变箱宽
<ul class="org-ul">
<li>目前介绍的方法都是统一箱宽的情况.</li>
<li><p>
理论上,可以让箱宽随数据分布情况变化(可变箱宽),这样看起来很漂亮(见图 4.28),但实际上并不好:
</p>

<div id="org0c101d5" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/4-28.png" alt="4-28.png" />
</p>
<p><span class="figure-number">Figure 53: </span>ms/4-28.png</p>
</div></li>
<li>可变箱宽的直方图更难解释</li>
<li>在数据集之间进行直方图的比较会比较困难</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org8c04b92" class="outline-4">
<h4 id="org8c04b92"><span class="section-number-4">4.8.1.</span> Other descriptive stats</h4>
<div class="outline-text-4" id="text-4-8-1">
<ul class="org-ul">
<li>本章绝不是对所有描述性统计量的穷尽列举.
<ul class="org-ul">
<li>生物和物理观测数据可以用分布特征(如赫斯特指数 Hurst exponent)来刻画;</li>
<li>时间序列数据有谱特性和自相关等特征;</li>
<li>多变量数据集包含协方差;</li>
<li>数据矩阵可以用秩(rank),条件数(condition number),**奇异值谱(singular value spectrum)**等量来描述,等等.</li>
</ul></li>
<li>本章没有介绍的那些描述性统计量,往往是特定学科领域专用的.</li>
<li>好消息是,那些不太常用的描述性统计量,都是基于本章讲过的概念和方法扩展出来的.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org13d2f88" class="outline-2">
<h2 id="org13d2f88"><span class="section-number-2">5.</span> Chapter 05: Simulating Data</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org71eb223" class="outline-3">
<h3 id="org71eb223"><span class="section-number-3">5.1.</span> Why simulate data?</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>为什么在学习统计学时要使用模拟数据(simulated data)?</li>
<li>来数数它的好处吧:</li>
<li><b>Validate ana</b> 验证分析方法
<ul class="org-ul">
<li>现有的统计分析方法种类繁多,而且每种方法都有可能影响结果的参数.</li>
<li>通过模拟数据,你可以把统计检验的结果与数据中已知的"真实模式"进行比较.</li>
<li>这种基准测试(benchmarking)能帮助你评估分析方法的有效性,尤其是在模拟数据具有与真实数据相似特征
时,效果更佳.</li>
</ul></li>
<li><b>Understand advantages and limitations of analysis methods</b> 理解分析方法的优缺点
<ul class="org-ul">
<li>模拟数据允许你调整效应量和噪声特征,这是在真实数据中几乎无法做到的.</li>
<li>这可以让你理解分析方法在什么边界条件下会失效或不再产生有意义,准确的结果.</li>
</ul></li>
<li><b>Understand how analysis methods work</b> 理解分析方法的工作原理
<ul class="org-ul">
<li>要理解一种统计分析方法,阅读解释和公式当然有帮助.</li>
<li>但如果能配合使用模拟数据,你的理解会变得更深入和直观.</li>
</ul></li>
<li><b>Understand your  data better H</b> 更好地理解真实数据
<ul class="org-ul">
<li>模拟数据的一个重要目标是创建与真实数据具有相似特征的虚拟数据集.</li>
<li>因此,制作模拟数据的过程能帮助你分析并解释真实数据.</li>
</ul></li>
<li><b>Think more carefully and critially about data</b> 更加仔细,批判性地思考数据
<ul class="org-ul">
<li>现代统计软件让几乎所有人都可以只靠复制粘贴几行代码就完成一次分析.</li>
<li>但如果你想从初学者晋升为专家型数据建模师,就需要对数据有深刻的理解:知道该从数据中期待什么,如何
处理数据,如何选择合适的分析方法,以及如何可视化和解释结果.</li>
<li>专家型数据分析师通常是主动去理解数据,而初学者往往是被动的,只会胡乱把算法套到数据上,然后看看哪
个"有效".</li>
<li>使用模拟数据测试模型,可以帮助你建立直觉,知道在什么类型的数据和什么分布下,哪些统计分析方法更可
能成功.</li>
</ul></li>
<li><b>Computational statistics</b> 计算统计学(Computational Statistics)
<ul class="org-ul">
<li>有一类统计方法,叫做 计算统计学 或 经验统计学,它们依赖于模拟,抽样或打乱数据来计算统计显著性.</li>
<li>这些方法包括:
<ol class="org-ol">
<li>自助法(bootstrapping)</li>
<li>置信区间(confidence intervals)</li>
<li>置换检验(permutation testing)</li>
<li>统计检验效能估计(power estimation)</li>
</ol></li>
<li>后面的章节你会学到这些方法.</li>
<li>重点是,当数据违反了参数统计(parametric statistics)的前提假设时,模拟数据有时是必需的.</li>
</ul></li>
<li><b>Improve thinking skills</b> 提升思维能力
<ul class="org-ul">
<li>精确且合理地模拟数据需要批判性思维,战略规划,适应能力和创造性.</li>
<li>因此,模拟数据的过程是锻炼你宏观思维能力,同时兼顾细节的好机会.</li>
</ul></li>
<li><b>Improve programming skills</b> 提升编程技能
<ul class="org-ul">
<li>模拟数据需要用代码来生成,因此学会模拟数据会提升你的编程能力.</li>
</ul></li>
<li><b>Convenience</b> 方便快捷
<ul class="org-ul">
<li>模拟数据让你可以尝试新的统计算法,而无需做实验或花费数小时在网上寻找数据集.</li>
<li>更不用担心找到的数据文档不足,或者需要大量清理与格式转换.</li>
</ul></li>
<li><b>Fun!</b> 有趣!
<ul class="org-ul">
<li>如果你从来没试过模拟数据,那么你会发现它其实挺好玩~ 我想你会喜欢的.</li>
</ul></li>
<li>伦理提醒
<ul class="org-ul">
<li>模拟数据是很棒且完全合乎伦理的事情.</li>
<li>但如果有意误导你的受众,让他们误以为虚构数据是真实的&#x2013;无论是直接声称数据真实,还是没有说明数据是假的&#x2013;都是不道德的行为.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orge775dff" class="outline-3">
<h3 id="orge775dff"><span class="section-number-3">5.2.</span> Random data from distributions</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>在上一章中,我介绍了分布,并用根据数据绘制的直方图来说明.核心思想是:
<ul class="org-ul">
<li>你从一个具有未知分布的数据集开始,通过绘制直方图来直观地观察和计算描述性统计量,以发现该分布的特性.</li>
</ul></li>
<li>而模拟数据则反过来了:
<ul class="org-ul">
<li><p>
你不是从数据开始,而是从分布开始(见图 5.1).
</p>

<div id="org206402b" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/5-1.png" alt="5-1.png" />
</p>
<p><span class="figure-number">Figure 54: </span>ms/5-1.png</p>
</div></li>
<li>你可以先选择分布的形状和描述性统计特征,然后生成符合这些特征的随机数据.</li>
</ul></li>
<li>这种方法为理解和探索统计提供了强大的工具,这也是我认为学习如何模拟数据应该成为统计训练核心内容的原因.</li>
<li>本节的目的就是向你展示如何从多种分布中生成随机数据.这里介绍的所有概念和公式都会在本章末的练习中实现.</li>
</ul>
</div>
<div id="outline-container-orgbd007d7" class="outline-4">
<h4 id="orgbd007d7"><span class="section-number-4">5.2.1.</span> Normally distributed random data</h4>
<div class="outline-text-4" id="text-5-2-1">
<ul class="org-ul">
<li>你已经很熟悉正态分布(Normal 或 Gaussian 分布)数据了.
<ul class="org-ul">
<li><p>
从数学表示法来看,从正态分布中抽取数据的写法如下:
</p>
\begin{equation}
X \sim \mathcal{N}(\mu, \sigma^2) \tag{5.1}
\end{equation}</li>
<li>这个公式表示,数据集 X 来自一个均值为 μ,方差为 σ² 的正态分布 N.</li>
<li>请注意,这里使用的是波浪号 ∼ 而不是等号,它的含义是"服从某种分布".</li>
</ul></li>
<li>公式 (5.1) 并不能告诉你数据集的全部信息,比如:数据点的数量,可能的异常值,偏度(skew),峰度(kurtosis)
等.不过,除非另有说明,否则你可以假设这些特征在数学上与理论正态分布一致.</li>
<li>在其他书籍或课程中,你可能会看到用标准差而不是方差来表示分布,
<ul class="org-ul">
<li><p>
例如:
</p>
\begin{equation}
X \sim \mathcal{N}(\mu, \sigma) \notag
\end{equation}</li>
<li>这种写法比较少见,因此当你看到第二个参数时,通常假设它表示的是方差.</li>
</ul></li>
<li>当 μ = 0 并且 σ² = 1 时,我们称其为标准正态分布.
<ul class="org-ul">
<li><p>
图 5.2 展示了四个不同参数的正态分布数据集示例.
</p>

<div id="org50b1b48" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/5-2.png" alt="5-2.png" />
</p>
<p><span class="figure-number">Figure 55: </span>ms/5-2.png</p>
</div></li>
<li>这些分布是我使用 NumPy 的 random 模块生成随机数据并绘制直方图得到的</li>
<li>图中的标题标明了我在生成数据时设定的均值和方差(也就是 \(\mu, \sigma\), 他们作为np.random.normal()的
参数),以及我从实际随机数据中计算得到的均值和方差(也就是 \(\overline{X}, std(X)\) ), 它们是从生成出
来的随机样本,算出来的</li>
<li>为什么我计算出来的经验统计量和在 Python 函数参数中指定的值不完全一致呢?</li>
<li>这是因为抽样变异性(sampling variability).这也是我们需要推断统计学(ferential statistics)的主要
原因之一.你将在本章的练习中进一步探索这种差异,而我会在第 8 章和第 9 章中更详细地讨论这个问题.</li>
</ul></li>
<li>目前你只需要知道,抽样变异性意味着:
<ul class="org-ul">
<li>随机抽取的数据特征不可能完全等于总体分布的特征参数.</li>
</ul></li>
<li>举个极端的例子:
<ul class="org-ul">
<li>假设你从一个均值为 0 的正态分布中随机抽取一个数,它可能是 0.42.而此时 N=1 的数据集的平均值显然不
是零.</li>
</ul></li>
<li><b>Shifting and stretching(平移与拉伸)</b> 在模拟数据时,我会把均值(mean)和标准差(standard deviation)
这两个参数理解为"平移"和"拉伸".</li>
<li>这些并不是正式的统计学术语,但我觉得用它们来理解操纵前两个统计矩(mean 和 standard deviation)对分
布产生的影响很有帮助:
<ul class="org-ul">
<li>均值 会将分布整体向左或向右平移,而不会改变它的形状;</li>
<li>标准差 则会让分布"拉伸"或"压缩",而不会改变它的中心位置.</li>
</ul></li>
<li>在 NumPy 中,这两个概念分别被称为 location(位置,即平移) 和 scaling(缩放,即拉伸).</li>
<li>我相信还有更多人会用不同的词来描述它们.理想情况下,我们应该用一套统一的术语,但我很怀疑全球的统计
学家们会在这些术语上达成一致&#x2013;毕竟,友好的分歧也是科学进步的重要推动力.</li>
</ul>
</div>
</div>
<div id="outline-container-org4737b6e" class="outline-4">
<h4 id="org4737b6e"><span class="section-number-4">5.2.2.</span> Uniformly distributed data</h4>
<div class="outline-text-4" id="text-5-2-2">
<ul class="org-ul">
<li><p>
从均匀分布中随机抽取数据的数学表达式如下:
</p>
\begin{equation}
X \sim \mathcal{N}(a, b) \tag{5.2}
\end{equation}</li>
<li>你可能会觉得它和公式 5.1(正态分布的表达)很像,但这里的两个参数并不是均值和方差,也不是其他统计矩.
<ul class="org-ul">
<li>这里,a 和 b 是分布的下限和上限.常见的默认参数是 a = 0 和 b = 1,</li>
<li><p>
也就是说,生成在 0 到 1 之间均匀分布的随机数(见图 5.3).
</p>

<div id="org73a96b5" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/5-3.png" alt="5-3.png" />
</p>
<p><span class="figure-number">Figure 56: </span>ms/5-3.png</p>
</div></li>
<li>其中 U(0,1) 被称为 <b>标准均匀分布</b>.</li>
</ul></li>
<li>事实证明,任何均匀分布都可以通过对标准均匀分布进行 <b>平移</b> 和 <b>缩放</b> 得到.
<ul class="org-ul">
<li><p>
例如,来看公式 5-3和5-4
</p>
\begin{equation}
X \sim \mathcall{U}(0, 1) \tag{5.3}
\end{equation}</li>
<li><p>
5-4中数据集 Y 的上下界是多少:
</p>
\begin{equation}
Y = 2\pi X - \pi \tag{5.4}
\end{equation}</li>
</ul></li>
<li>思路是:
<ul class="org-ul">
<li><p>
用边界 [0, 1] 代替 X.在上面的例子里:
</p>
\begin{equation}
2\pi [0, 1] - \pi = [0, 2\pi] - \pi = [-\pi, \pi]  \notag
\end{equation}</li>
<li>换句话说,我们创建了一个随机数在 [-π, π] 区间内均匀分布的数据集,可以用来模拟相位角.这类数据在
<b>信号处理</b>,*计算几何* 和 <b>复分析</b> 中都会用到.</li>
</ul></li>

<li>更一般情况,如果要创建一个任意上下界 a 和 b(假设 a &lt; b)的均匀分布,
<ul class="org-ul">
<li><p>
可以从标准均匀分布开始,应用以下公式:
</p>
\begin{equation}
Y = a + (b-a)U \tag{5.5}
\end{equation}</li>
<li><p>
以及如下公式
</p>
\begin{equation}
U \sim \mathcal{U}(0,1) \tag{5.6}
\end{equation}</li>
<li><p>
图 5.4 给出了一些示例.
</p>

<div id="org50833f1" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/5-4.png" alt="5-4.png" />
</p>
<p><span class="figure-number">Figure 57: </span>ms/5-4.png</p>
</div></li>
<li>对于均匀分布而言,经验边界(从数据计算的最小值和最大值)和指定边界通常会比正态分布的均值,标准差匹配
得更接近.不过,由于有限样本的抽样变异,经验边界可能不会与设定的 a 和 b 完全一致.下一章你会学习的
<b>min-max scaling(最小-最大缩放)</b> 方法,可以确保完美匹配.</li>
</ul></li>
<li>公式 5.2 并没有直接给出分布的均值或方差.如果你想创建一个 <b>均匀分布且已知均值和方差</b> 的数据集,该怎么
办呢?
<ul class="org-ul">
<li><p>
来看均匀分布已知上下界 a 和 b 条件下的期望均值公式
</p>
\begin{equation}
\mu = \frac{a + b}{2}  \tag{5.7}
\end{equation}</li>
<li><p>
和方差公式:
</p>
\begin{equation}
\sigma^2 = \frac{(a - b)^2}{12}  \tag{5.8}
\end{equation}</li>
<li>- 公式 5.7 很直观:如果数值在 a 和 b 之间均匀分布,那么平均值就是边界的平均数.</li>
<li>公式 5.8 看起来奇怪:为什么是边界之差的平方除以 12?其实它们来自期望值和统计矩的定义,通过微积分和代
数推导得出.我们会在第 8 章详细解释,你会在练习 2 中用数据验证这些公式的正确性.</li>
</ul></li>
<li>假设你要创建一个均匀分布并且它有 <b>指定的均值和标准差</b> ,
<ul class="org-ul">
<li><p>
你需要的就是公式 5.9:
</p>
\begin{equation}
Y = \mu + \sqrt{3}\,\sigma\, (2U - 1) \tag{5.9}
\end{equation}</li>
<li><p>
公式5-10
</p>
\begin{equation}
U \sim \mathcal{U}(0, 1) \tag{5.10}
\end{equation}</li>
<li>(2U - 1) 把标准均匀分布平移/缩放到以 0 为中心,边界是 [-1, 1]</li>
<li>乘上 \((\sqrt{3}\sigma)\) 拉伸到边界为 [-√3σ, √3σ]</li>
<li>再加上均值 μ,把分布平移到指定中心</li>
</ul></li>
<li>公式 5.9 可以通过在公式 5.7 和 5.8 中解出 a 和 b(并转化为标准差公式),再代入公式 5.5 得到.</li>
</ul>
</div>
</div>
<div id="outline-container-org5cc9cf8" class="outline-4">
<h4 id="org5cc9cf8"><span class="section-number-4">5.2.3.</span> Random data from other distributions</h4>
<div class="outline-text-4" id="text-5-2-3">
<ul class="org-ul">
<li>有无数种分布可以用来生成随机数据.</li>
<li>下面我会给出几个例子,但总体来说,如果你想创建一种 <b>不是正态分布或均匀分布</b> 的数据集,有两种方法可
以实现:
<ul class="org-ul">
<li>找到一个 Python 或 R 的函数,它可以直接生成你需要的分布的随机数;</li>
<li>从正态分布或均匀分布出发,通过某种数学变换得到.</li>
</ul></li>
<li><b>第一种方法示例</b> :
<ul class="org-ul">
<li><p>
使用 Python 函数 `np.random.weibull` 可以从威布尔分布生成随机数据(见图 5.5).
</p>

<div id="org360f31b" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/5-5.png" alt="5-5.png" />
</p>
<p><span class="figure-number">Figure 58: </span>ms/5-5.png</p>
</div></li>
<li>我不会深入介绍威布尔分布的性质,定义或应用,因为本书并不使用这种分布.</li>
<li>这里的重点是演示如何使用一个 Python 函数来生成某个特定分布的随机数.</li>
</ul></li>
<li><b>第二种方法示例</b>
<ul class="org-ul">
<li>将服从正态分布或均匀分布的数列,经过一定的数学变换,得到新的分布.</li>
<li>之前的例子:通过对标准均匀分布进行拉伸和平移,得到范围为 <b><b>[−π, π]</b></b> 的均匀分布.</li>
<li>另一个例子是 <b>对数正态分布(log-normal distribution)</b>.  对数正态分布是通过将服从正态分布的随机
数,传递给自然指数函数(natural exponential function)来生成的.</li>
<li><p>
公式5-11如下:
</p>
\begin{equation}
Y = \exp(X\sigma + \mu) \tag{5.11}
\end{equation}</li>
<li><p>
公式5-12如下:
</p>
\begin{equation}
  X \sim \mathcal{N}(0,1) \tag{5.12}
\end{equation}</li>
<li><p>
图 5.6 展示了一个对数正态分布的例子
</p>

<div id="org8b425c0" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/5-6.png" alt="5-6.png" />
</p>
<p><span class="figure-number">Figure 59: </span>ms/5-6.png</p>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgfe133de" class="outline-4">
<h4 id="orgfe133de"><span class="section-number-4">5.2.4.</span> Random integers</h4>
<div class="outline-text-4" id="text-5-2-4">
<ul class="org-ul">
<li>随机整数有着无数的应用场景,从算法,到电子游戏中的随机事件,再到密码学.</li>
<li>在本书中,我们将经常使用随机整数作为索引,从数据集中进行选择,并模拟带标签的数据.</li>
<li>你可以在 Python 中使用 NumPy 的 random.randint 函数生成均匀分布的整数数据集
<ul class="org-ul">
<li>这个函数的用法与生成均匀分布数据类似&#x2013;你需要指定一个下界,一个上界,以及样本大小,函数会返回符合
这些参数的数据集.</li>
<li><p>
需要注意的是,上界是不包含的.例如,代码:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np

<span style="color: #8f0075;">print</span><span style="color: #000000;">(</span>np.random.randint<span style="color: #dd22dd;">(</span>1, 5<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>

<span style="color: #7f0000;"># </span><span style="color: #7f0000;">&lt;====================OUTPUT====================&gt;
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">2</span>
</pre>
</div></li>
<li>会随机返回一个整数,取自集合 (1, 2, 3, 4).</li>
<li>该函数生成的整数是均匀分布的.</li>
<li><p>
如果你想生成具有其他分布的整数数据,可以先生成对应分布的非整数数据,然后将结果四舍五入为最近的整数.
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np

<span style="color: #7f0000;"># </span><span style="color: #7f0000;">&#35774;&#23450;&#27491;&#24577;&#20998;&#24067;&#30340;&#21442;&#25968;:&#22343;&#20540;(mean)&#20026; 50,&#26631;&#20934;&#24046;(std)&#20026; 15
</span><span style="color: #005e8b;">mean</span> = 50
<span style="color: #005e8b;">std</span> = 15
<span style="color: #005e8b;">size</span> = 10  <span style="color: #7f0000;"># </span><span style="color: #7f0000;">&#29983;&#25104;10&#20010;&#25968;
</span>
<span style="color: #7f0000;"># </span><span style="color: #7f0000;">1. &#29983;&#25104;&#27491;&#24577;&#20998;&#24067;&#30340;&#28014;&#28857;&#25968;
</span><span style="color: #005e8b;">normal_floats</span> = np.random.normal<span style="color: #000000;">(</span>mean, std, size<span style="color: #000000;">)</span>
<span style="color: #8f0075;">print</span><span style="color: #000000;">(</span><span style="color: #3548cf;">"&#27491;&#24577;&#20998;&#24067;&#28014;&#28857;&#25968;:"</span>, normal_floats<span style="color: #000000;">)</span>

<span style="color: #7f0000;"># </span><span style="color: #7f0000;">2. &#23558;&#28014;&#28857;&#25968;&#22235;&#33293;&#20116;&#20837;&#21040;&#26368;&#36817;&#30340;&#25972;&#25968;
</span><span style="color: #005e8b;">normal_ints</span> = np.<span style="color: #8f0075;">round</span><span style="color: #000000;">(</span>normal_floats<span style="color: #000000;">)</span>.astype<span style="color: #000000;">(</span><span style="color: #8f0075;">int</span><span style="color: #000000;">)</span>  <span style="color: #7f0000;"># </span><span style="color: #7f0000;">&#20351;&#29992;.astype(int)&#30830;&#20445;&#36716;&#25442;&#20026;&#25972;&#25968;&#31867;&#22411;
</span><span style="color: #8f0075;">print</span><span style="color: #000000;">(</span><span style="color: #3548cf;">"&#27491;&#24577;&#20998;&#24067;&#25972;&#25968;:"</span>, normal_ints<span style="color: #000000;">)</span>

<span style="color: #7f0000;"># </span><span style="color: #7f0000;">&lt;====================OUTPUT====================&gt;
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">&#27491;&#24577;&#20998;&#24067;&#28014;&#28857;&#25968;: [32.31366131 34.02574255 85.55818388 26.68463221 42.301085 56.959998 49.00103983 57.95171271 68.3780933  58.05922764]
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">&#27491;&#24577;&#20998;&#24067;&#25972;&#25968;: [32 34 86 27 42 57 49 58 68 58]</span>
</pre>
</div></li>
<li><p>
我再通过下面的例子再理解下np.random.normal这个函数,其返回值真的是随机从正态分布里面取的数值,没有
顺序
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #531ab6;">import</span> matplotlib.pyplot <span style="color: #531ab6;">as</span> plt
<span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np

<span style="color: #7f0000;"># </span><span style="color: #7f0000;">&#29983;&#25104;10000&#20010;&#22343;&#20540;&#20026;100,&#26631;&#20934;&#24046;&#20026;20&#30340;&#38543;&#26426;&#25968;
</span><span style="color: #005e8b;">large_sample</span> = np.random.normal<span style="color: #000000;">(</span>100, 20, 10000<span style="color: #000000;">)</span>


<span style="color: #7f0000;"># </span><span style="color: #7f0000;">&#27880;&#24847;&#36825;&#20010;&#20989;&#25968;plt.hist&#35774;&#32622;&#20102;bin&#20026;50&#20043;&#21518;,&#20250;&#33258;&#21160;&#21435;&#36825;&#20010;large_sample&#30340;10000&#20010;&#25968;&#32452;&#37324;&#38754;&#21435;&#21462;,&#20998;&#25104;&#20116;
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">&#21313;&#20221;,&#27599;&#20221;&#21306;&#38388;&#37324;&#38754;&#26377;&#22810;&#23569;&#20010;&#20250;&#33258;&#21160;&#35745;&#31639;,&#24212;&#35813;&#26469;&#35828;mean&#25152;&#22312;&#30340;&#20221;&#37324;&#38754;&#30340;&#25968;&#37327;&#26368;&#22810;&#36825;&#20063;&#24847;&#21619;&#30528;large_sample
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">&#21482;&#26159;&#19968;&#20010;&#22823;&#25968;&#32452;&#32780;&#24050;,&#19981;&#26159;&#35828;large_sample&#30340;&#31532;5000&#20010;&#25968;&#25454;&#26368;&#22823;&#30340;
</span>plt.hist<span style="color: #000000;">(</span>large_sample, bins=50, edgecolor=<span style="color: #3548cf;">"black"</span>, alpha=0.7<span style="color: #000000;">)</span>
plt.title<span style="color: #000000;">(</span><span style="color: #3548cf;">"(&#956;=100, &#963;=20)"</span><span style="color: #000000;">)</span>
plt.xlabel<span style="color: #000000;">(</span><span style="color: #3548cf;">"value"</span><span style="color: #000000;">)</span>
plt.ylabel<span style="color: #000000;">(</span><span style="color: #3548cf;">"freq"</span><span style="color: #000000;">)</span>
plt.grid<span style="color: #000000;">(</span><span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>
plt.show<span style="color: #000000;">()</span>
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org8ea8336" class="outline-3">
<h3 id="org8ea8336"><span class="section-number-3">5.3.</span> Random elements of a set</h3>
<div class="outline-text-3" id="text-5-3">
<ul class="org-ul">
<li>在本节中,我将向你展示如何从一个已有的数据集中随机选择数据.
<ul class="org-ul">
<li>例如,假设你想从集合 (1, 2, 3, 6, 7, 8) 中随机选择一个元素.</li>
<li>随机从集合中选择元素有很多应用,包括在基于置换的统计和自助法(bootstrapping)中基于真实数据创建替
代数据集.你会在本书的后续章节学到这些技术,但我想让你先知道,你将在这里学到的方法在现代计算统计
中有重要应用.</li>
<li><p>
我们先从集合的随机选择开始吧.来看下面的 Python 代码,并猜猜第二行可能会输出什么:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #005e8b;">s</span> = <span style="color: #000000;">[</span>1, 2, np.pi, 10<span style="color: #000000;">]</span>
np.random.choice<span style="color: #000000;">(</span>s, 1<span style="color: #000000;">)</span>
</pre>
</div></li>
<li>我第一次运行这段代码时,得到的结果是 2.</li>
<li>接着我再次运行,结果变成了 10.如果你一遍又一遍运行,你会发现结果是不可预测的&#x2013;不过它一定会是列表
中的一个数字,而且只返回一个数字,因为第二个参数指定了要返回的元素数量.</li>
</ul></li>

<li>顺便说一下,Python 的 choice 和 R 的 sample 函数可以从任何集合(列表,元组或向量)中随机选择元素,
而不仅限于数字.
<ul class="org-ul">
<li><p>
例如,以下代码会从列表 t 中随机选取一个元素:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #005e8b;">t</span> = <span style="color: #000000;">[</span><span style="color: #3548cf;">"a"</span>, <span style="color: #3548cf;">"b"</span>, <span style="color: #3548cf;">"hello"</span><span style="color: #000000;">]</span>
np.random.choice<span style="color: #000000;">(</span>t, 1<span style="color: #000000;">)</span>
</pre>
</div></li>
</ul></li>
<li>我们回到之前的数字例子:如果我们把第二个参数设置为 4 会怎样呢?
<ul class="org-ul">
<li>列表中只有 4 个元素,所以你可能会期待函数返回全部 4 个元素,或许是一个随机的顺序.</li>

<li><p>
来看我运行的结果:
</p>
<div class="org-src-container">
<pre class="src src-python">np.random.choice<span style="color: #000000;">(</span>s, 4<span style="color: #000000;">)</span>
<span style="color: #7f0000;"># </span><span style="color: #7f0000;">&lt;====================OUTPUT====================&gt;
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">array([10, 1, 1, 3.14159])</span>
</pre>
</div></li>
<li>这里发生了什么?为什么数字 2 不见了,而数字 1 出现了两次?</li>
<li>这涉及到随机采样中的一个重要参数 &#x2013; replacement(是否放回).
<ol class="org-ol">
<li>有放回采样:每次选中的元素在下次采样前都会被放回集合中,因此可能会多次选中同一个元素.</li>
<li>无放回采样:每次选中一个元素后,就会从集合中移除它,直到不再参与后续采样.</li>
</ol></li>
<li><p>
这个区别在图 5.7 中进行了说明.
</p>

<div id="orgd6278c8" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/5-7.png" alt="5-7.png" />
</p>
<p><span class="figure-number">Figure 60: </span>ms/5-7.png</p>
</div></li>
</ul></li>
<li>这个区别并不简单:
<ul class="org-ul">
<li>有放回采样意味着你可以生成比原始数据集更大的新数据集,因为某些元素可能会被重复采样.这也意味着,
当你从一个 N 元素的数据集中随机采样 N 次时,得到的新数据集的描述性统计量可能与原来的数据集不同.
你已经在上面的例子中看到这一点:数字 1 被选中了两次,而数字 2 没被选中.</li>
<li><p>
如果是无放回采样,那么新数据集会与原数据集的元素完全相同,不同的地方只是元素的排列顺序可能会改变.
事实上,无放回的随机采样就是 <b>随机排列(permutation)</b> 的一种机制,这也是你将在下一节学习的内容.
</p>
<pre class="example" id="org7de9be5">
Random sampling without replacement is a mechanism of random permutations
</pre></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org8da8c38" class="outline-3">
<h3 id="org8da8c38"><span class="section-number-3">5.4.</span> Random permutations</h3>
<div class="outline-text-3" id="text-5-4">
<ul class="org-ul">
<li><b>置换(Permuting)或洗牌(shuffling)</b> 是一种随机化列表或数组中元素顺序的方法,
<ul class="org-ul">
<li>它不会改变这些元素的值,也不会像上一节一样做子采样(subsampling)或过采样(oversampling).</li>
<li><p>
这里有一个简单的代码示例:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np
<span style="color: #005e8b;">l</span> = np.arange<span style="color: #000000;">(</span>5<span style="color: #000000;">)</span>
<span style="color: #8f0075;">print</span><span style="color: #000000;">(</span>l<span style="color: #000000;">)</span>
<span style="color: #8f0075;">print</span><span style="color: #000000;">(</span>np.random.permutation<span style="color: #dd22dd;">(</span>l<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>

<span style="color: #7f0000;"># </span><span style="color: #7f0000;">&lt;====================OUTPUT====================&gt;
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">[0 1 2 3 4]
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">[4 0 1 2 3]</span>
</pre>
</div></li>
</ul></li>
<li>置换的一个应用是随机重新排序(re-sort)顺序型数据.
<ul class="org-ul">
<li><p>
例如,我们生成一个从 -3 到 +3 的整数序列,然后将它们立方,接着随机重新排序:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np

<span style="color: #005e8b;">theData</span> = np.arange<span style="color: #000000;">(</span>-3, 4<span style="color: #000000;">)</span> ** 3
<span style="color: #005e8b;">newIdx</span> = np.random.permutation<span style="color: #000000;">(</span><span style="color: #8f0075;">len</span><span style="color: #dd22dd;">(</span>theData<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
<span style="color: #005e8b;">shufData</span> = theData<span style="color: #000000;">[</span>newIdx<span style="color: #000000;">]</span>
<span style="color: #8f0075;">print</span><span style="color: #000000;">(</span>theData<span style="color: #000000;">)</span>
<span style="color: #8f0075;">print</span><span style="color: #000000;">(</span>newIdx<span style="color: #000000;">)</span>
<span style="color: #8f0075;">print</span><span style="color: #000000;">(</span>shufData<span style="color: #000000;">)</span>

<span style="color: #7f0000;"># </span><span style="color: #7f0000;">&lt;====================OUTPUT====================&gt;
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">[-27  -8  -1   0   1   8  27] # &#21407;&#25968;&#25454;
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">[3 0 1 4 2 6 5]               # &#38543;&#26426;&#32034;&#24341;(newIdx)
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">[  0 -27  -8   1  -1  27   8] # &#37325;&#26032;&#25490;&#24207;&#21518;&#30340;&#25968;&#25454;</span>
</pre>
</div></li>
<li>这里变量 newIdx 包含整数 0 到 6(在 R 中是 1 到 7),它们是变量 theData 的索引位置.</li>
<li>请注意:数字 -27 是 theData 的第一个元素,但在 shufData 中它变成了第 6 个元素(Python 中索引 0 对
应 -27).</li>
<li>你一定要理解 数据值 和 数据索引 之间的区别,这是一个重要的概念.</li>
<li>置换后的索引(变量 newIdx)本身并不是数据,而是用来创建一个替代数据集(surrogate dataset)的工具.</li>
</ul></li>
<li>置换的一个应用是随机化配对数据样本.
<ul class="org-ul">
<li>比如,假设我们有一个包含 50 个人的身高和体重的数据集.你会预期这两项特征是相关的.</li>
<li>但如果我们随机置换了身高(只改变顺序),却没有置换体重,那么你还会期望这两个变量有很强的相关性吗?</li>
<li>答案当然是 不会,因为身高与体重之间的映射关系已经被随机化了.</li>
<li>因此,如果我们在洗牌数据(shuffled data)中计算相关性,就可以测量纯粹由随机机会造成的相关性.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org83497a9" class="outline-3">
<h3 id="org83497a9"><span class="section-number-3">5.5.</span> Reproducing randomness</h3>
<div class="outline-text-3" id="text-5-5">
<ul class="org-ul">
<li>每次调用一个随机化函数时,你都会得到一组不同的数字.
<ul class="org-ul">
<li>这当然是合理的 &#x2013; 如果每次调用函数返回的数字都是相同的,那它就不算随机了,对吧?</li>
</ul></li>
<li>不过,其实有很好的理由去让随机结果可重复.
<ul class="org-ul">
<li>例如,如果能够复现完全一样的随机序列,就可以保证别人能够精确地重复你的研究结果.</li>
</ul></li>
<li>我们先来演示一下,反复生成随机数会得到不同的结果.
<ul class="org-ul">
<li><p>
如果你有一个 Python 或 R 会话在运行,请试试下面的代码:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np

<span style="color: #005e8b;">ret</span> = np.random.randn<span style="color: #000000;">(</span>3, 3<span style="color: #000000;">)</span>
<span style="color: #8f0075;">print</span><span style="color: #000000;">(</span><span style="color: #3548cf;">"""|ret|=&gt;"""</span>, ret<span style="color: #000000;">)</span>
<span style="color: #8f0075;">print</span><span style="color: #000000;">(</span><span style="color: #3548cf;">"""|ret|=&gt;"""</span>, ret<span style="color: #000000;">)</span>

<span style="color: #7f0000;"># </span><span style="color: #7f0000;">&lt;====================OUTPUT====================&gt;
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">|ret|=&gt; [[ 0.01056085  2.03669496 -1.37248536]
</span><span style="color: #7f0000;">#  </span><span style="color: #7f0000;">[ 0.25169481  0.96166009 -0.40347195]
</span><span style="color: #7f0000;">#  </span><span style="color: #7f0000;">[ 0.12881357 -0.81958542  0.59849039]]
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">|ret|=&gt; [[ 0.01056085  2.03669496 -1.37248536]
</span><span style="color: #7f0000;">#  </span><span style="color: #7f0000;">[ 0.25169481  0.96166009 -0.40347195]
</span><span style="color: #7f0000;">#  </span><span style="color: #7f0000;">[ 0.12881357 -0.81958542  0.59849039]]</span>
</pre>
</div></li>
<li>这会生成一个 3×3 的矩阵,其中的数字是从正态分布中随机抽取的.</li>
<li>连续运行几次,你会发现这个矩阵的内容每次都与上一次不同,虽然代码并没有改变.</li>
</ul></li>

<li>接下来试一下这个:
<ul class="org-ul">
<li><p>
代码如下
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np

<span style="color: #005e8b;">rs</span> = np.random.RandomState<span style="color: #000000;">(</span>17<span style="color: #000000;">)</span>
<span style="color: #005e8b;">ret</span> = rs.randn<span style="color: #000000;">(</span>3, 3<span style="color: #000000;">)</span>
<span style="color: #8f0075;">print</span><span style="color: #000000;">(</span><span style="color: #3548cf;">"""|ret|=&gt;"""</span>, ret<span style="color: #000000;">)</span>
<span style="color: #8f0075;">print</span><span style="color: #000000;">(</span><span style="color: #3548cf;">"""|ret|=&gt;"""</span>, ret<span style="color: #000000;">)</span>

<span style="color: #7f0000;"># </span><span style="color: #7f0000;">&lt;====================OUTPUT====================&gt;
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">|ret|=&gt; [[ 0.27626589 -1.85462808  0.62390111]
</span><span style="color: #7f0000;">#  </span><span style="color: #7f0000;">[ 1.14531129  1.03719047  1.88663893]
</span><span style="color: #7f0000;">#  </span><span style="color: #7f0000;">[-0.11169829 -0.36210134  0.14867505]]
</span><span style="color: #7f0000;"># </span><span style="color: #7f0000;">|ret|=&gt; [[ 0.27626589 -1.85462808  0.62390111]
</span><span style="color: #7f0000;">#  </span><span style="color: #7f0000;">[ 1.14531129  1.03719047  1.88663893]
</span><span style="color: #7f0000;">#  </span><span style="color: #7f0000;">[-0.11169829 -0.36210134  0.14867505]]</span>
</pre>
</div></li>
<li>我非常确定,你会得到与我上面打印出的完全相同的结果.</li>
<li>我为什么如此确定?因为 Python 的 RandomState 函数会初始化一个随机数种子(seed),有了这个种子,所
有随机数就会按照同样的序列生成.</li>
<li>正因为如此,带种子的随机数(seeded random numbers)被称为伪随机数(pseudorandom numbers)或伪随机
数序列.</li>
</ul></li>
<li>需要注意的是,种子只能在你使用了rs时保证可重复性.</li>
<li>如果你创建了rs(定义了一个随机种子)却不使用,如果随后直接调用 np.random.randn(但是没有使用rs),它依
然会生成新的,不可预测的随机数集.</li>
<li>随机数设定种子的主要优势是:
<ul class="org-ul">
<li>它能让你在任何电脑,任何时间精确复现某一个结果.</li>
<li>如果我在本书中每次生成随机数时都设置了随机种子,那么你将能够精确地重现我的每一个结果和图形 &#x2013; 不
仅是定性上的,连具体的数值都能相同.</li>
</ul></li>
<li>那么,我们是不是应该一直设置种子呢?
<ul class="org-ul">
<li>你可能会觉得这是学习的最佳方式,因为这样可以精确复现书中的每一个结果和模拟.这种观点是有一定道理
的,我也理解这种想法.</li>
<li>但是,我并不认为在学习统计时总是设定随机种子是一种好方法.原因是:
<ol class="org-ol">
<li>在统计学(以及更广泛的经验科学)中,一个重要的课程是数据样本总是包含多种变异性与噪声,并且重复
同样的实验可能会得到不同的结果.</li>
<li>了解某些结果或算法对这种变异性的鲁棒性,是非常重要的.</li>
<li>如果只从一个特定的例子来学习,反而可能会有风险,因为那个结果可能只是某一个随机样本的"偶然产物",
并不能反映普遍规律.</li>
</ol></li>
</ul></li>
<li>在经验科学中也是如此,这就是为什么大样本量和实验重复对科学进步如此关键的原因.</li>
<li>基于这些原因,我在书中选择不使用随机种子.不过,知道种子是怎么工作的非常重要,</li>
<li>当然,你可以不赞同我的理由,并在你的学习,研究,应用和教学中自由使用随机种子.</li>
</ul>
</div>
</div>
<div id="outline-container-org77d9db9" class="outline-3">
<h3 id="org77d9db9"><span class="section-number-3">5.6.</span> Runningexperiments with random numbers</h3>
<div class="outline-text-3" id="text-5-6">
<ul class="org-ul">
<li>使用模拟数据来理解统计方法的主要途径之一,就是进行实验.在本节中,我会先说明这个观点的理由,然后解
释如何用随机数据进行实验.</li>
<li>我们先来问一个基础问题:实验的意义是什么?
<ul class="org-ul">
<li>当你想了解某件你目前还不了解的事情时,你就会进行实验.</li>
</ul></li>
<li>科学实验的过程一般是:
<ul class="org-ul">
<li>先提出一个研究问题,</li>
<li>然后将其转化为假设,根据假设设计实验,收集数据,</li>
<li>最后对这些经验数据进行处理和分析.</li>
</ul></li>
<li>用模拟数据进行的统计实验与此类似:
<ul class="org-ul">
<li>你希望理解某种当前还不太理解的分析方法.那么你会先提出一个研究问题(例如:"一个变量的标准差如何影
响它与另一个变量的相关性?"),</li>
<li>接着设计实验(例如,决定如何改变标准差和相关性的强度),然后编写代码来运行实验并可视化结果,</li>
<li>最后分析和解释这些发现,从而加深你对统计的认识.</li>
</ul></li>
<li>重要的是,通过使用虚拟数据进行统计实验,所获得的洞察力往往超过单纯观察公式或代码所能得到的.
<ul class="org-ul">
<li>例如,你可能会好奇,在方差分析(ANOVA)中的交互项显著性检验时,方差不均衡会产生怎样的影响.</li>
<li>当然,你可以去问一位资深的统计教授,但他可能会回答:"嗯,这不是理想情况,但除非方差差异特别夸张,
否则 ANOVA 往往还是比较稳健."</li>
<li>如果你只是盯着 ANOVA 的公式看,可能很难获得深入理解(如果不信,你可以翻到第 14 章试试,看你是否能
仅凭公式回答这个问题).</li>
<li>我建议你设计,编写并运行一个实验,在其中系统地操纵一个 ANOVA 单元的方差,同时保持其他单元的方差不
变.这样,你就会得到一个能被可视化,能被理解,并且能够向别人解释的答案.</li>
</ul></li>

<li>正如我在本章开头写的,
<ul class="org-ul">
<li>使用随机虚拟数据进行实验的优势在于,你可以完全控制数据的特性,并以现实实验中几乎不可能的方式进行
操控.</li>
<li>它的主要缺点是,要模拟真实数据的特性&#x2013;也就是制造一个数据"拼贴"(datapastiche)&#x2013;有时会很困难,甚
至不可能.</li>
</ul></li>
<li>另一方面,用模拟数据进行实验的目的&#x2013;至少在本书中&#x2013;是帮助你理解统计方法,而不是取代真实的经验实验.</li>
</ul>
</div>
<div id="outline-container-orga5226b0" class="outline-4">
<h4 id="orga5226b0"><span class="section-number-4">5.6.1.</span> Experiment: Impact of standard deviation on mean</h4>
<div class="outline-text-4" id="text-5-6-1">
<ul class="org-ul">
<li>为了让这个概念更具体,我们来运行一个实验.本实验的目标是:
<ul class="org-ul">
<li>确定标准差对正态分布数据的均值估计的影响.</li>
</ul></li>
<li>事情是这样的:从理论上讲,标准差对均值没有影响,因为标准差的定义是围绕均值的离散程度;它不会偏移均
值本身,至少在正态分布的情况下是如此.</li>
<li>但理论并不总是等同于实践,尤其是在样本量较小且存在噪声时.所以,我们将运行一个实验,帮助我们理解标
准差对均值的影响.</li>
<li>既然关键问题是关于标准差,那么 <b>它就是我们要操控的变量</b>.</li>
<li>其它变量&#x2013;例如预期均值,样本大小,分布形状&#x2013;也可以被操控,但为了简洁和直接,我们将在这里 <b>保持它们固定不变</b> .</li>
<li>具体做法如下:
<ul class="org-ul">
<li>我创建了一个包含 100 个随机数的数据集,这些数据来自于一个期望总体均值为 0,标准差为 0.01 的正态分布.</li>
<li>该数据集的经验均值(empirical mean)为 0.0453&#x2013;不完全是零,但很接近.</li>
<li>然后,我又创建了一个样本量相同,期望总体均值相同,但标准差为 5 的数据集.这个数据集的经验均值是
0.6231&#x2013;比零远得多!</li>
</ul></li>
<li>这只是两个随机样本.实验的核心思想是:重复数据生成与分析的过程(在这里,分析就是计算均值),并在此
过程中系统地改变标准差.</li>
<li>在代码实现中,这个过程是用一个 for 循环完成的:
<ul class="org-ul">
<li>在循环的每一次迭代中,我都会创建一个新的数据集(保持相同的预期均值和样本大小),</li>
<li>但标准差不同.标准差的取值范围从 0.01 到 10,共有 40 个不同的数值.</li>
<li>每一次迭代中,我都会计算经验均值(empirical mean)并将其存储到一个向量中,然后绘制经验均值随标准差
变化的图像.</li>
<li><p>
在阅读下面的文字之前,请先花几分钟看看 图 5.8,并试着去观察,总结标准差与均值估计之间的关系.
</p>

<div id="orgb1c09c0" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/5-8.png" alt="5-8.png" />
</p>
<p><span class="figure-number">Figure 61: </span>ms/5-8.png</p>
</div></li>
</ul></li>
<li>你对这些结果感到惊讶吗?所有这些数据都来自均值为零的理论分布,
<ul class="org-ul">
<li>那么我们又是如何得到像 大于 1 或小于 -1 这样极端的经验均值的呢?</li>
</ul></li>
<li>这种差异来源于抽样的随机性和随机噪声.
<ul class="org-ul">
<li>在真实数据中,这是一个严重的问题,也是我们需要推断统计(inferential statistics)的主要原因之一.</li>
</ul></li>

<li>还有两个额外的观察结果:
<ul class="org-ul">
<li>随着标准差的增加,经验均值离零的距离变得更大.这看起来符合直觉,但数学(以及生活中的很多事情)中
的直觉并不总是正确的.</li>
<li>经验均值既可能大于预期均值,也可能小于它,这表明标准差引入的是一种非系统性偏差(non-systematic bias),
而不是始终朝一个方向偏移的系统性偏差(systematic bias).在对数正态分布中,你会在练习 10中看到,
这种系统性偏差是可能发生的.</li>
</ul></li>
<li>我对这个实验的结果很感兴趣,所以我设计了一个后续实验,来探究样本量(sample size)对标准差与均值估计
关系的影响.为运行这个实验,我修改了代码,生成了两个数据集:
<ul class="org-ul">
<li>一个样本量为 100</li>
<li>一个样本量为 10,000</li>
</ul></li>
<li>除此之外,其他参数保持完全相同.
<ul class="org-ul">
<li><p>
实验结果见图 5.9.
</p>

<div id="orgfb3dbd2" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/5-9.png" alt="5-9.png" />
</p>
<p><span class="figure-number">Figure 62: </span>ms/5-9.png</p>
</div></li>
<li>那么,你对这些结果有什么观察呢?</li>
</ul></li>
<li><b>A myriad of possible experiments</b> 稍加思考,你会意识到,即便是这样一个小小的实验,也可以变得非常复
杂,非常精细.例如,你可以进一步 系统地更改:
<ul class="org-ul">
<li>预期均值(expected mean)</li>
<li>样本量(不仅是两个样本量,而是更多不同规模的样本,覆盖更广范围)</li>
<li>数据分布类型(例如:正态分布,均匀分布,对数正态分布,幂律分布等)</li>
<li>你还可以改变衡量的方式,例如计算经验均值与预期均值之间的距离.</li>
</ul></li>
<li>在本节里,我只是对结果做了定性的解释;我们还可以用统计分析方法,对结果进行定量比较,并在不同数据分
布间进行对照.</li>
<li>我们还可以重复整个实验多次,量化每个标准差值下经验均值的波动程度.</li>
<li>这些只是我在写这一段时想到的一些点子,也许你还有更多方法来扩展这个实验.</li>
<li>不过,我还是要提醒你:
<ul class="org-ul">
<li>保持实验简单,专注非常重要.</li>
<li>实验越复杂,越难解释结果(这对模拟实验和真实世界的实验都成立).</li>
</ul></li>
<li>本节的主要目的,是向你介绍一种思路:
<ul class="org-ul">
<li>在模拟数据中进行实验时,通过系统地操控一个因素,同时保持其他因素不变,可以探索统计关系.</li>
</ul></li>
<li>我希望你觉得这一节有用.
<ul class="org-ul">
<li>我对统计学的大量理解都来自于亲自运行数据模拟实验,也希望你在实践中有类似的收获</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org24d59fc" class="outline-3">
<h3 id="org24d59fc"><span class="section-number-3">5.7.</span> The amazing world of data-simulations</h3>
<div class="outline-text-3" id="text-5-7">
<ul class="org-ul">
<li>在本章中,我介绍的只是模拟数据方法的一小部分,实际上还有更多方式可以进行数据模拟.
<ul class="org-ul">
<li>例如,研究人员会模拟时间序列信号,金融数据,气候与天气模式,图像,生物与物理过程,交通流量&#x2026;&#x2026;这样
的例子不胜枚举.</li>
</ul></li>
<li>生成式深度学习模型在制造虚拟数据方面正在取得令人惊叹的(在某些情况下甚至令人恐惧的)进展,
<ul class="org-ul">
<li>例如生成完全不存在的人的照片,</li>
<li>以及所谓的"深度伪造"视频&#x2013;这些视频看似展示了知名人士在做或者说一些他们从未做过或说过的事情.</li>
</ul></li>
<li>本章介绍的方法已经足够支撑本书的内容(当然在后续章节我还会引入一些新的数据生成方法).其他的数据生
成技术往往具有特定的领域性,
<ul class="org-ul">
<li>比如如果你想学习如何模拟气候数据,那你需要参加计算气候科学相关的课程.</li>
</ul></li>
<li>好消息是,几乎所有的数据生成方法都建立在你在本章学到的原理之上,包括:
<ul class="org-ul">
<li>从某种分布中随机抽取数字</li>
<li>对数据进行拉伸或平移(stretch &amp; shift)</li>
<li>施加非线性变换</li>
<li>随机打乱已有数据</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgda2ef02" class="outline-3">
<h3 id="orgda2ef02"><span class="section-number-3">5.8.</span> Finding publicly available real datasets</h3>
<div class="outline-text-3" id="text-5-8">
<ul class="org-ul">
<li>现在,公开可获取的数据量正在不断增加,你可以将这些数据用于教育,科研以及商业目的.</li>
<li>但请注意,世界上并不存在所谓的 "唯一数据集"(The Data);网上存在着数不清的数据集,它们具有不同的特
征,样本量和质量.</li>
<li>同样地,也不存在存放所有数据的单一仓库.实际上,许多数据仓库是特定于某个学科领域的.</li>
<li>这意味着,如果你想寻找某种特定类型的数据集,首先你需要明确自己想要的数据类型,然后在互联网上使用相
关关键词进行搜索.
<ul class="org-ul">
<li>一些数据仓库是免费且开放的,你只需点击链接即可下载数据;</li>
<li>另外一些仓库虽然免费,但需要你注册并提供一些个人信息;</li>
<li>还有一些数据会发布在个人网站或像 GitHub 这样的代码仓库上.</li>
<li>有些数据集无法直接在网上找到,只能通过向研究报告的作者个人请求来获得.</li>
</ul></li>
<li>目前没有通用标准来规定数据该如何开放,也没有统一的数据格式;如果你想在网上获取数据,你需要做好耐心
与持续尝试的准备.</li>
<li>在这里,我提到两个非常受欢迎的获取数据的网站:
<ul class="org-ul">
<li>UCI 机器学习数据仓库(UCI Machine Learning Repository)</li>
<li>Kaggle</li>
</ul></li>
<li>在本书中,我会多次使用 UCI 数据仓库中的数据集.</li>
<li>需要注意的是:能下载数据并不意味着数据一定有用.
<ul class="org-ul">
<li>很多数据集很遗憾且令人沮丧地&#x2013;不完整,缺乏文档,甚至已损坏.</li>
<li>这并不是理想状态,但却是现实,你必须做好心理准备.</li>
</ul></li>
<li>好消息是,较受欢迎的数据仓库中的数据通常更有可能是高质量,可用的.</li>
<li>虽然本书主要关注的是模拟数据,但我也有不少练习和示例使用真实数据&#x2013;这些通常作为每一章最后的练习出现.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgf9b4e6a" class="outline-2">
<h2 id="orgf9b4e6a"><span class="section-number-2">6.</span> Chapter 06: Transformations</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-org30400b2" class="outline-3">
<h3 id="org30400b2"><span class="section-number-3">6.1.</span> Wht, why, and how of data transformations</h3>
<div class="outline-text-3" id="text-6-1">
</div>
<div id="outline-container-orge8a51d7" class="outline-4">
<h4 id="orge8a51d7"><span class="section-number-4">6.1.1.</span> What are data transformations?</h4>
<div class="outline-text-4" id="text-6-1-1">
<ul class="org-ul">
<li>数据变换(Data transformation)指的是对数据应用某种数学运算,或者一系列运算.
<ul class="org-ul">
<li><p>
举一个简单的例子,下面这个数据集及其变换版本:
</p>
<pre class="example" id="org441c40a">
X = [1, 3, 4, 6, 7]
X* = [0, 2, 3, 5, 6]
</pre></li>
<li>这里从 X 得到 X* 所用的变换,就是将所有值都减去 1.</li>
<li>我并没有声称这是一个有趣或复杂的变换,但它确实是一个有效的数据变换.</li>
</ul></li>
<li>很多数据变换都很简单,可以用一个或几个数学或算法表达式来表示.</li>
<li>当然,简单并不是数据变换的本质,因为也有非常复杂的变换,这些变换需要用非常严密的数学技术文档来描述
并且可能需要高性能计算机花费数小时甚至数天才能完成.</li>
<li>不过,这些复杂的变换往往是特定学科和特定数据才会用到的;而相对简单的变换在很多场景中都具有普遍的用途.</li>
</ul>
</div>
</div>
<div id="outline-container-org9463e7e" class="outline-4">
<h4 id="org9463e7e"><span class="section-number-4">6.1.2.</span> Why transform data?</h4>
<div class="outline-text-4" id="text-6-1-2">
<ul class="org-ul">
<li>数据转换的原因有很多,但它们都源于同一个核心动机:
<ul class="org-ul">
<li>解决(或者至少缓解)数据中的问题.</li>
</ul></li>
<li>下面介绍的是一个并不完整的,可以通过数据转换改善的问题列表:
<ul class="org-ul">
<li><b>不同尺度的数据集需要比较,但它们的量纲不一致(例如距离和质量)</b> .使用标准化(z-scoring)或最小
-最大缩放(min-max scaling)等转换方法,可以将数据放入相同的数值范围,从而实现直接比较.</li>
<li><b>数据的分布并非正态(normal distribution)</b> 但统计分析要求数据服从正态分布, 一些转换方法,如对数
变换(log),平方根变换(square root)和 Fisher-z 变换,可以将非高斯分布(non-Gaussian)转换成高
斯分布(Gaussian).</li>
<li>与上一个问题相关:*有些分析要求数据必须位于特定的数值范围内* .例如,一些图像处理技术期望数据位于
[0,1] 范围,而原始图像数据通常是介于 0 到 255 之间的整数.</li>
<li><b>数据中存在极端值(outliers),会对结果产生负面影响</b> 通过转换可以减少甚至消除这些极端值的影响.对
数变换,平方根变换以及秩转换(tied rank transform)在这种情况下都很有用.</li>
<li><b>数据尺度上的统计参数难以解释,而通过转换可以让其更易于理解</b> 这种情况在回归分析(详见第 15 章)中
很常见.Z-scoring 是一种常用的转换方法,只要有一定的统计训练(包括读完本章节的你!)就能轻松使用.</li>
<li><b>有时数据的转换并非出于统计原因,例如为了便于数字存储或传输</b></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgbbab3f9" class="outline-4">
<h4 id="orgbbab3f9"><span class="section-number-4">6.1.3.</span> How to transform data?</h4>
<div class="outline-text-4" id="text-6-1-3">
<ul class="org-ul">
<li>正如我之前所写,数据转换就是对数据应用某种数学运算或算法.不过这只是一个抽象的说法,下面我会把它具
体化.</li>
<li><b>调用 Python 或 R 函数</b>
像 numpy 和 scipy.stats 这样的 Python 库,以及 R 的基础函数和诸如 caret 的库,都提供了实现大多数常
用数据转换的方法.在使用这些库提供的函数时,要注意不同函数在实现的直观程度和文档的可读性方面可能有
所不同.</li>
<li><b>下载别人编写的函数</b>
互联网上充满了人们发布的代码,例如在 GitHub 或个人网站上.不过,在使用非专业开发者编写的代码时请务
必小心.代码出现在 GitHub 并不意味着它一定是正确的.在使用前务必仔细检查代码,并通过模拟数据来验证
其准确性.</li>
<li><b>自己写代码</b>
本书中有许多练习会引导你从零开始编写数据转换,分析和算法.很多情况下,这些算法在 numpy,scipy 或 R
等库中已经实现了.但自己编写数据转换代码在学习上有很高的教育价值,并且在实际应用中可能在可用性或速
度方面有优势.另外,有些数据转换是现有库中没有实现的,这时就需要你自己编写.</li>
<li>无论代码来自哪里,请记住,并不是所有转换都适合所有数据集.</li>
<li>大多数转换对数据类型和数值范围有一定的假设(例如你不能对负数取平方根).</li>
<li>有些假设可能并不明显,除非你查看具体的公式,或者花时间去理解这些方法(比如通过阅读本章节!).</li>
<li>运行一些代码来转换数据是很容易的,但知道什么时候用哪种转换,以及如何解释结果,则需要统计学训练与批
判性思维.永远不要对数据应用你不理解或无法合理解释的转换方法.</li>
</ul>
</div>
</div>
<div id="outline-container-org44802b6" class="outline-4">
<h4 id="org44802b6"><span class="section-number-4">6.1.4.</span> What kinds of transformations are there?</h4>
<div class="outline-text-4" id="text-6-1-4">
<ul class="org-ul">
<li>目前并没有一个被广泛认可的数据转换分类体系,而我也不喜欢人为强加的分类体系,因为它们往往会错误地表
现分类之间的区别和联系.</li>
<li>不过,把数据转换按照一些共同点和差异进行分组,有时还是有帮助的.例如:
<ul class="org-ul">
<li>有 <b>线性(linear)和非线性(nonlinear)</b> 转换;</li>
<li>有 <b>有损(lossy)和无损(lossless)</b> 转换("有损"指的是在应用转换后会丢失部分信息);</li>
<li>有 *迭代(iterative)和非迭代(non-iterative) * 转换("迭代"指的是算法的某些步骤会重复执行,直到
满足某个条件为止).</li>
</ul></li>
<li>本章的目标是帮助你全面理解各种数据转换,它们的特性,以及它们的影响.</li>
<li>我希望在读完本章之后,你能掌握应用数据转换所需的工具;在读完本书之后,你能具备在实践中做出何时以及
如何进行数据转换的明智决策的知识.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org98ec065" class="outline-3">
<h3 id="org98ec065"><span class="section-number-3">6.2.</span> Z-score standardization</h3>
<div class="outline-text-3" id="text-6-2">
<ul class="org-ul">
<li>Z-标准化(Z-scoring) 可能是整个统计学中最常见,也最重要的转换方法之一.</li>
<li>我们先从一个 Z-scoring 能解决的问题开始.
<ul class="org-ul">
<li>假设我们想知道,一个人在给定身高的情况下,体重算多还是少.</li>
<li>这里的问题是,身高和体重的单位完全不同:身高用厘米(或者英寸,甚至光年&#x2013;为了方便我们用厘米),而
体重用千克.</li>
<li>这两个量纲没有直接可比性,比如说"177 厘米比 70 千克大"就完全没有意义.</li>
</ul></li>
<li>解决办法是:把思路从绝对值转变为相对值.
<ul class="org-ul">
<li>一方面,177 厘米是一个有意义,可解释的测量值,但我们也可以这样看:在亚美尼亚成年男性人群中,177
厘米比平均身高高出 5.5 厘米;而 70 千克则比平均体重轻了 4.6 千克.</li>
<li>因此,如果我们假设此人是一位亚美尼亚成年男性,我们可以说,他的身高高于平均水平,但体重低于平均水平.</li>
<li><p>
图 6.1 说明了这一概念:
</p>

<div id="org269ee3e" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/6-1.png" alt="6-1.png" />
</p>
<p><span class="figure-number">Figure 63: </span>ms/6-1.png</p>
</div></li>
<li>面板 A1−2 显示了该个体的身高和体重数据.由于量纲完全不同,我们无法直接比较它们.</li>
<li>面板 B1−2 显示了同一个人的数据,叠加到其他成年亚美尼亚男性的样本直方图上.虽然量纲仍然不可比,但
你可以看到,这个人的数据可以被理解为相对于分布的某个位置.</li>
<li>面板 C1−2 显示了将数据标准化为 z-score(Z 分数) 后的结果.此时,个体的数据被重新编码为距离平均值
多少个标准差.例如,此人的身高大约比平均值高 1.5 个标准差,而体重则比平均值轻 1 个标准差.</li>
</ul></li>
<li>这种将单位转化成"相对值"的好处是,我们现在可以直接比较身高和体重:相对于亚美尼亚成年男性的人群分布,
这个人既高又轻.</li>
</ul>
</div>
<div id="outline-container-org569f2e9" class="outline-4">
<h4 id="org569f2e9"><span class="section-number-4">6.2.1.</span> Z-score math</h4>
<div class="outline-text-4" id="text-6-2-1">
<ul class="org-ul">
<li>Z评分的数学很简单,但它背后隐藏的假设可能在数据违反这些假设时导致解释上的困难</li>
<li>在标题为"Hard and soft assumptions"的小节中,我将描述一些 Z 评分的局限性和潜在尴尬特征;在此之前,
我希望你思考一些在应用和解释 Z 评分时可能出现的问题.</li>

<li>Z 评分涉及两个简单的变换:
<ol class="org-ol">
<li><b>平移数据(去均值)</b>:
<ul class="org-ul">
<li>将数据按它的均值平移,使变换后的数据平均值为 \(\overline{x}=0\) .这称为 <b>均值中心化</b> 或 <b>去均值</b>.</li>
<li>具体做法是从每个数据点中减去该特征的均值.</li>
</ul></li>
<li><b>按标准差缩放数据</b>:
<ul class="org-ul">
<li>缩放数据,使得变换后数据的标准差为 s = 1.</li>
<li>具体做法是将每个数据点除以数据集的标准差.</li>
</ul></li>
</ol></li>
<li><p>
公式如下
</p>
\begin{equation}
z_i = \frac{x_i - \bar{x}}{s} \tag{6.1}
\end{equation}</li>
<li>这会变换每个数据点 <b>i</b>.</li>
<li>注意,这里的均值和标准差是来自整个数据集(sample)的.
<ul class="org-ul">
<li>如果知道总体特征或可以假设总体特征,则应使用: \(\frac{x_i - \mu}{\sigma}\)</li>
<li>换句话说就是使用population的均值和方差</li>
</ul></li>
<li>下面我们通过一个示例来说明:
<ul class="org-ul">
<li>X = [ 1, 4, -5, 2, 1 ]</li>
<li>数据均值 \(( \overline{x} \approx 0.6 )\)</li>
<li>标准差 \(( s \approx 3.36 )\)</li>
<li>Z 变换后: \(X_z \approx [ 0.12, 1.01, -1.66, 0.41, 0.12 ]\)</li>
</ul></li>
<li>这里我使用了 ≈ 符号,因为我截断了数值.Z 变换后的数字通常不太"好看"成整数.虽然 Z 变换算法很简单,但不
太适合手工实现(坦白说:我用 Python 得到上述数值).</li>
<li>下面的图标中对比了"原始的均值方差"和"转换后的均值和方差"
<ul class="org-ul">
<li><p>
图标如下
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Original</th>
<th scope="col" class="org-right">z-transformed</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Mean</td>
<td class="org-right">0.6</td>
<td class="org-right">0.00</td>
</tr>

<tr>
<td class="org-left">stdev</td>
<td class="org-right">3.36</td>
<td class="org-right">1.00</td>
</tr>
</tbody>
</table></li>
<li>Z 变换后数据的均值和标准差按定义总是 0 和 1.(顺便提一下,Z 变换后数据的方差也是 1,因为 \(1^2=1\)</li>
<li>Z 变换不会改变分布的形状,因此不会改变偏度(skew)或峰度(kurtosis).我稍后会展示相关示例,但这
一点从公式上应该已经很明显了.</li>
<li>不改变skew和kurtosis,其实很抽象,因为样子看起来就已经完全改变了(只是数值型的数值skew, kurtosis没改
变,但是形状完全变了),如下图</li>
<li><p>
对应的python代码如下
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #531ab6;">import</span> numpy <span style="color: #531ab6;">as</span> np
<span style="color: #531ab6;">import</span> matplotlib.pyplot <span style="color: #531ab6;">as</span> plt
<span style="color: #531ab6;">from</span> scipy.stats <span style="color: #531ab6;">import</span> norm

<span style="color: #7f0000;"># </span><span style="color: #7f0000;">Create original Gaussian data
</span>np.random.seed<span style="color: #000000;">(</span>42<span style="color: #000000;">)</span>
<span style="color: #7f0000;"># </span><span style="color: #7f0000;">mean=5, std=2
</span><span style="color: #005e8b;">data</span> = np.random.normal<span style="color: #000000;">(</span>loc=5, scale=2, size=1000<span style="color: #000000;">)</span>

<span style="color: #7f0000;"># </span><span style="color: #7f0000;">Z-score Standardization
</span><span style="color: #005e8b;">mean_data</span> = np.mean<span style="color: #000000;">(</span>data<span style="color: #000000;">)</span>
<span style="color: #005e8b;">std_data</span> = np.std<span style="color: #000000;">(</span>data<span style="color: #000000;">)</span>
<span style="color: #005e8b;">z_data</span> = <span style="color: #000000;">(</span>data - mean_data<span style="color: #000000;">)</span> / std_data

<span style="color: #7f0000;"># </span><span style="color: #7f0000;">Prepare x ranges for plotting smooth curves
</span><span style="color: #005e8b;">x_original</span> = np.linspace<span style="color: #000000;">(</span><span style="color: #8f0075;">min</span><span style="color: #dd22dd;">(</span>data<span style="color: #dd22dd;">)</span>, <span style="color: #8f0075;">max</span><span style="color: #dd22dd;">(</span>data<span style="color: #dd22dd;">)</span>, 200<span style="color: #000000;">)</span>
<span style="color: #005e8b;">pdf_original</span> = norm.pdf<span style="color: #000000;">(</span>x_original, loc=mean_data, scale=std_data<span style="color: #000000;">)</span>

<span style="color: #005e8b;">x_z</span> = np.linspace<span style="color: #000000;">(</span><span style="color: #8f0075;">min</span><span style="color: #dd22dd;">(</span>z_data<span style="color: #dd22dd;">)</span>, <span style="color: #8f0075;">max</span><span style="color: #dd22dd;">(</span>z_data<span style="color: #dd22dd;">)</span>, 200<span style="color: #000000;">)</span>
<span style="color: #005e8b;">pdf_z</span> = norm.pdf<span style="color: #000000;">(</span>x_z, loc=np.mean<span style="color: #dd22dd;">(</span>z_data<span style="color: #dd22dd;">)</span>, scale=np.std<span style="color: #dd22dd;">(</span>z_data<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>

<span style="color: #7f0000;"># </span><span style="color: #7f0000;">Plot both in the same coordinate system
</span>plt.figure<span style="color: #000000;">(</span>figsize=<span style="color: #dd22dd;">(</span>8, 5<span style="color: #dd22dd;">)</span><span style="color: #000000;">)</span>
plt.plot<span style="color: #000000;">(</span>
    x_original,
    pdf_original,
    label=f<span style="color: #3548cf;">"Original (mean=</span>{mean_data:.2f}<span style="color: #3548cf;">, std=</span>{std_data:.2f}<span style="color: #3548cf;">)"</span>,
    color=<span style="color: #3548cf;">"blue"</span>,
    linewidth=2,
<span style="color: #000000;">)</span>
plt.plot<span style="color: #000000;">(</span>
    x_z,
    pdf_z,
    label=f<span style="color: #3548cf;">"Z-scored (mean=</span>{np.mean(z_data):.2f}<span style="color: #3548cf;">, std=</span>{np.std(z_data):.2f}<span style="color: #3548cf;">)"</span>,
    color=<span style="color: #3548cf;">"red"</span>,
    linestyle=<span style="color: #3548cf;">"--"</span>,
    linewidth=2,
<span style="color: #000000;">)</span>

plt.title<span style="color: #000000;">(</span><span style="color: #3548cf;">"Gaussian Distribution Before and After Z-Score Standardization"</span><span style="color: #000000;">)</span>
plt.xlabel<span style="color: #000000;">(</span><span style="color: #3548cf;">"Value"</span><span style="color: #000000;">)</span>
plt.ylabel<span style="color: #000000;">(</span><span style="color: #3548cf;">"Probability Density"</span><span style="color: #000000;">)</span>
plt.legend<span style="color: #000000;">()</span>
plt.grid<span style="color: #000000;">(</span><span style="color: #0000b0;">True</span><span style="color: #000000;">)</span>
plt.show<span style="color: #000000;">()</span>
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org85e646f" class="outline-4">
<h4 id="org85e646f"><span class="section-number-4">6.2.2.</span> Interpretation</h4>
<div class="outline-text-4" id="text-6-2-2">
<ul class="org-ul">
<li>Z 分数(Z-score)的解释是:
<ul class="org-ul">
<li>每个数据点被解释为到分布中心的标准化距离.</li>
<li>更具体地说,每个数据点的数值表示它距离均值有多少个标准差,其中均值和标准差对应于某个分布(通常是
该数据点所属的分布).</li>
</ul></li>
<li>这种解释的原因是 Z 分数的单位就是"标准差".</li>
<li>注意一下公式 6.1 中的单位变化:分子和分母的数据单位是相同的(例如英尺,秒,克,计数),在相除时会相
互抵消,留下一个无单位的度量.</li>
<li>这很方便,因为它允许我们比较来自不同测量的数据值,例如身高和体重.
<ul class="org-ul">
<li>这也是我们用标准差而不是方差来缩放的原因:如果用方差缩放,Z 分数的单位将会是数据单位的倒数,例如
数据以英尺为单位时,Z 分数的单位会变成 1/英尺.</li>
</ul></li>
<li>重要的是,Z 转换会平移并拉伸数据,但不会改变分布的形状.也就是说,
<ul class="org-ul">
<li>数据的单位变化了,但它们的相对值并没有改变.</li>
</ul></li>
<li><p>
我们在 x 轴上平移分布,在 y 轴上拉伸它,但成员相互之间的对比并没有变化(but there is no warping or
difference in how the transformation impacts some data values compared to others),如图6-2
</p>

<div id="org3f4ff6b" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/6-2.png" alt="6-2.png" />
</p>
<p><span class="figure-number">Figure 64: </span>ms/6-2.png</p>
</div></li>
<li>换句话说,就是不会对不同的数据点造成不同程度的变形(即不产生扭曲).这一点可以和本章后面会介绍的非线
性变换进行对比.</li>
</ul>
</div>
</div>
<div id="outline-container-orgff11754" class="outline-4">
<h4 id="orgff11754"><span class="section-number-4">6.2.3.</span> Hard and soft assumptions</h4>
<div class="outline-text-4" id="text-6-2-3">
<ul class="org-ul">
<li>所谓"硬性假设"(hard assumptions)与"软性假设"(soft assumptions),分别指的是 数学层面的问题 和
解释层面的问题.</li>
<li>硬性假设方面,z-score(标准分)最主要的数学假设是
<ul class="org-ul">
<li>标准差必须不为零.</li>
<li>如果 σ = 0,那么转换过程中就会出现除以零的情况.</li>
</ul></li>
<li>什么时候会出现标准差为零呢?当数据中没有任何变异性,所有数据点的数值都完全相同时,就会发生这种情况.
如果你的全部数据值都相同,那么很可能是数据收集或处理过程中出现了错误.</li>
<li>接着说软性假设.
<ul class="org-ul">
<li>这些假设并不是 z-score 计算的数学必需条件,但它们可以让 z 值的解释更自然易懂.</li>
<li>违反这些假设不会让数学推导失效.</li>
</ul></li>
<li>关键的软性假设是:
<ul class="org-ul">
<li>均值和标准差能够很好地描述数据的分布特征.</li>
<li>这在接近高斯分布(Gaussian-like)的数据中通常成立&#x2013;也就是说,数据值在分布中更倾向于集中在中间位置,
并且大致围绕均值对称.</li>
</ul></li>
<li>让我们看一个反例.
<ul class="org-ul">
<li><p>
图 6.3 展示了一组服从指数分布的正值数据.
</p>

<div id="org8e3426a" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/6-3.png" alt="6-3.png" />
</p>
<p><span class="figure-number">Figure 65: </span>ms/6-3.png</p>
</div></li>
<li>理论上,我们可以计算该数据的均值和标准差,但均值并不能真正反映数据的中心趋势&#x2013;即随机抽取一个数据
点,它的期望值并不是均值.</li>
<li>这使得 z 值的解释变得尴尬.比如在高斯分布中,z-score 离零点越远意味着数值越极端,出现的概率越低;</li>
<li>但在指数分布中,一个 z = -1 的数据点比均值(或 z = 1)更有可能被随机抽中(见图 6.3B).</li>
<li>另一个尴尬的地方是,在一个只有正数的数据集中,却会出现负的 z 值.</li>
</ul></li>
<li>那么,违反这种软性假设是否是致命问题呢?
<ul class="org-ul">
<li>答案是不是.比如在下一章,你会了解到如何用 z-score 来识别数据中的异常值;</li>
<li>在这种情况下,我们只是把 z-score 当作数据清理的工具,而不是在统计分析中解释它们.</li>
<li>这里的核心是:解释 z-score 时要留意数据的分布形态.</li>
</ul></li>
<li>最后,因为 z-score 是用某个数据点与一个分布进行比较的,所以只有当每个数据点在质量上与该分布的总体样
本相似时,z 值才容易解释.
<ul class="org-ul">
<li>举个例子,在本节开头我提到用某人的身高和体重与亚美尼亚成年男性的总体进行比较.但如果这个人是来自
秘鲁的 10 岁女孩,她的身高体重相对于那个总体会低好几个标准差,尽管她可能是一个非常普通的秘鲁 10
岁女孩.这种情况下数学上没有问题,但解释会令人困惑甚至产生误导.</li>
</ul></li>
<li>另一方面,有些情况确实需要用不同的数据来定义均值和标准差.这在时间序列分析中很常见,
<ul class="org-ul">
<li>比如分析某个时间段的数据时,会拿它去和另一个时间段的分布比较.</li>
<li>另一个例子是儿童的生长曲线:当你 5 岁去看儿科医生时,医生用的身高体重参考表里的均值和标准差,是基
于一组并不包含你自己的儿童数据统计出来的.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgbe74e30" class="outline-4">
<h4 id="orgbe74e30"><span class="section-number-4">6.2.4.</span> The modified z-score method</h4>
<div class="outline-text-4" id="text-6-2-4">
<ul class="org-ul">
<li>如果你的数据分布与高斯分布(Gaussian)差别很大,你可以考虑使用改进的 z-score 方法.
<ul class="org-ul">
<li>在本节的剩余部分,我会把基于均值和标准差的那种方法称为"普通 z-score"方法."普通"并不是它的官方名
称,只是为了与改进版进行对比方便而这样叫.</li>
<li>在本书剩下的内容 &#x2013; 以及你之后所有的统计学习中,z-score 一般都会指这种基于均值/标准差的版本.</li>
</ul></li>
<li>改进的 z-score 方法在概念上与"普通 z-score"很相似,但它不依赖那些只在类似高斯分布的数据中才容易解释
的描述性统计量.具体来说,
<ul class="org-ul">
<li>普通 z-score 是用数据点减去均值,再除以标准差;</li>
<li>而改进版是用数据点减去中位数,再除以中位数绝对差(median absolute deviation,MAD).</li>
</ul></li>
<li>不过核心思想是一样的:
<ul class="org-ul">
<li>先用一个衡量中心趋势的指标(比如均值或中位数)去减掉每个数据点;</li>
<li>再用一个衡量数据离散程度的指标(比如标准差或中位数绝对差)去进行除法,完成标准化.</li>
</ul></li>
<li>让我们先来看公式:
<ul class="org-ul">
<li><p>
公式6-6
</p>
\begin{equation}
( M_i = \frac{x_i - \tilde{x}}{1.4826 \times MAD} ) \tag{6.6}
\end{equation}</li>
<li>分子部分表示每个数据点 i 都是经过 <b>中位数中心化</b> 的(可对比 z-score 中的 <b>均值中心化</b> ).</li>
<li><p>
分子部门的 \(\tlide{x}\) 表示中位数
</p>
\begin{equation}
\tilde{x} = median(x) \tag{6.7}
\end{equation}</li>
<li>分母部分是 MAD,即 <b>中位数绝对差</b> (median absolute difference),它是基于中位数的,用来替代标准
差的离散度量.</li>
</ul></li>
<li>在讨论那个奇怪的归一化常数 1.4826 之前,我想先解释 MAD.
<ul class="org-ul">
<li><p>
理解 MAD 的公式 (6.8) 时,可以回忆或参考标准差公式(见第 4.9 式,第 125 页):
</p>
\begin{equation}
MAD = median(|x_i - \tilde{x}|) \tag{6.8}
\end{equation}</li>
<li>标准差是计算每个数据点到数据均值距离的平方的平均值的平方根.</li>
<li>而 MAD 则是每个数据点到数据中位数的距离的中位数.</li>
<li>如你所知,对于对称分布,均值和中位数几乎相同,这意味着标准差和 MAD 的区别主要在于是否对距离进行平方.</li>
<li><p>
平方根函数 <b>先平方再开根号,虽然有根号运算,但是在平反步骤放大了偏差权重</b>,看下例, 结果是标准差通常
比 MAD 大.
</p>
<pre class="example" id="org411d81b">
例如,对于数据集{1, 3, 5, 7, 9}:
均值是5,偏差分别为-4,-2,0,2,4
标准差 = √[(16+4+0+4+16)/5] = √8 ≈ 2.83
MAD = 中位数{|1-5|,|3-5|,|5-5|,|7-5|,|9-5|} = 中位数{4,2,0,2,4} = 2
</pre></li>
<li><b>分母更大,分数值就更小</b>,因此"普通" z 值通常比改进版的值更小.</li>
<li>为了弥补这种数值大小的差异,在改进版 z-score 的公式中,MAD 会乘以一个常数进行放大.这个放大效果实际
上是让 \((M_i)\) 的值变小,使它们在正态分布下更接近普通 z 值.</li>
</ul></li>
<li>那么,为什么是 1.4826 呢?这个数字从哪里来?它对应的是正态分布的第三四分位数(third quartile).
<ul class="org-ul">
<li>更严格地写法是 \((\Phi(3/4))\) ,而 1.4826 是这个四分位值的近似值.你会在第 8 章更深入地理解 \((\Phi)\)
的含义.</li>
</ul></li>
<li>图 6.4 比较了"普通" z-score 和改进版 z-score 在同一批数据上的变换效果.
<ul class="org-ul">
<li><p>
图6-4
</p>

<div id="orga4d330a" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/6-4.png" alt="6-4.png" />
</p>
<p><span class="figure-number">Figure 66: </span>ms/6-4.png</p>
</div></li>
<li>你可以看到两种方法总体上产生了类似的直方图(见图 6.4B),只是改进版稍微更宽一些.</li>
<li>图 6.4C 进一步强调了这种差异:如果两种转换完全相同,数值会落在虚线(斜率为 1)上;</li>
<li>实际中,改进版的数值在分布边缘部分会向虚线的上下方向延伸.由于均值和标准差较中位数和 MAD 更易受到分
布形状的影响,这种偏离虚线的现象很好地体现了两种变换的关系,而具体的对应关系取决于数据的特性.</li>
<li>不过,图 6.4 还揭示了一个更重要的事实:普通 z-score 和改进版 z-score <b><b>通常非常相似</b></b>.换句话说,你不
太可能因为使用某种方法而得出质上完全不同的数据结论.这也不足为奇,因为两种方法在概念上是完全一致的
&#x2013;都是用一个中心趋势指标进行平移,再用一个离散程度指标进行缩放.</li>
</ul></li>

<li>什么时候使用改进版?你可以在以下两种情况下使用改进版 z-score:
<ul class="org-ul">
<li>数据分布明显非高斯分布;</li>
<li>数据中存在较大的异常值.</li>
</ul></li>
<li>第二点的原因是:中位数相比均值更不受异常值影响,同样地,MAD 相比标准差更不受异常值影响.</li>
<li>这也解释了图 6.4 中改进版略多拉伸的现象.</li>
<li>你可能会想:"那我是不是应该一直用改进版 z-score 呢?"这种说法是有道理的:毕竟两种方法很相似,而改进版在
面对非正态分布和异常值时更稳健.</li>
<li>然而,均值和标准差在数学上有一些有用的性质,比如:
<ul class="org-ul">
<li>明确定义的导数;</li>
<li>与统计矩的链式关系;</li>
<li>方便作为标准差单位解释;</li>
<li>等等.</li>
</ul></li>
<li>因此, <b>普通 z-score 方法应尽量使用,在必要时再用改进版</b> 在实践中,
<ul class="org-ul">
<li>改进版 z-score 主要用于数据清理过程中的异常值检测;</li>
<li>而普通 z-score 则更常用于将数据标准化到标准差单位,从而方便模型拟合和解释.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgc68e23e" class="outline-3">
<h3 id="orgc68e23e"><span class="section-number-3">6.3.</span> Min-max normalization</h3>
<div class="outline-text-3" id="text-6-3">
<ul class="org-ul">
<li>Min-max 归一化的目的,是将数据重新缩放到你指定的最小值和最大值范围内.
<ul class="org-ul">
<li>通常新的范围是 <b>[0, 1]</b>,</li>
<li>但也可以是 <b>[-1, 1]</b> 或 <b>[0, 2π]</b> 等其他区间.</li>
</ul></li>
<li>我会先展示如何把任意数据集缩放到 [0, 1] 区间,然后再扩展到其他区间的方法.
<ul class="org-ul">
<li><p>
公式如下
</p>
\begin{equation}
\tilde{x}_i = \frac{x_i - min(x)}{max(x) - min(x)} \tag{6.9}
\end{equation}</li>
<li><b>min(x)</b> :数据中的最小值(这里的"最小"是数值上最小,不是最接近零的值).</li>
<li><b>max(x)</b> :数据中的最大值.</li>
<li><b>分母</b>:数据的总范围,即 `max(x) - min(x)`.</li>
</ul></li>
<li>理解方式:
<ul class="org-ul">
<li><b>分子</b> 部分:把数据整体平移,使得最小值变成 0.</li>
<li>接着看 <b>分母</b> 部分:此时 `min(x) = 0`,我们相当于把数据的最大值除以自己,使其最大值变成 1,从而完
成归一化.</li>
</ul></li>
<li>图 6.5 展示了一个经过 min-max 缩放的数据集;
<ul class="org-ul">
<li><p>
如图
</p>

<div id="orgb63e99b" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/6-5.png" alt="6-5.png" />
</p>
<p><span class="figure-number">Figure 67: </span>ms/6-5.png</p>
</div></li>
<li>在面板 C 中可以看到,这种变换并不会改变数据值之间的相对关系.</li>
</ul></li>
<li>公式 (6.9) 把数据缩放到了单位区间 <b><b>[0, 1]</b></b>.
<ul class="org-ul">
<li><p>
如果你希望数据落在其他范围,可以在归一化结果上进行线性变换:
</p>
\begin{equation}
x^{*} = a + (b-a)\tilde{x_i} \tag{6.10}
\end{equation}</li>
<li>`a` 为新范围的下限</li>
<li>`b` 为新范围的上限</li>
</ul></li>
<li>我希望你觉得公式 (6.10) 看起来有点眼熟;如果没有,请回顾公式 (5.5),它讲的是如何将一个标准均匀分布
转换成具有任意下限和上限的数据集.</li>
<li>作为一种线性变换,min-max 缩放不会改变分布的形状,也不仅仅适用于均匀分布的数据.不过,均匀分布和
min-max 缩放都有一个共同的关键描述特征:都有明确的下限和上限.</li>
</ul>
</div>
<div id="outline-container-org7b1adfb" class="outline-4">
<h4 id="org7b1adfb"><span class="section-number-4">6.3.1.</span> Interpretation</h4>
<div class="outline-text-4" id="text-6-3-1">
<ul class="org-ul">
<li>就像 z-score 标准化一样,min-max 缩放不会改变数据分布的形状,也不会影响数据集中各数值之间的相对距离;
<ul class="org-ul">
<li>它只是对数据进行平移和缩放.</li>
</ul></li>
<li>这种方法在图像处理中很常用,用来将像素强度值调整到 [0, 1] 范围内.</li>
<li>在信号处理中,它也被用来缩放滤波器,使得数值能够反映滤波器的增益或加权特性.</li>
<li>图 6.6 展示了一个数据示例,
<ul class="org-ul">
<li><p>
如图
</p>

<div id="org48712fb" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/6-6.png" alt="6-6.png" />
</p>
<p><span class="figure-number">Figure 68: </span>ms/6-6.png</p>
</div></li>
<li>其数值范围从低于 -40 到超过 20;</li>
<li>经过 min-max 缩放后,分布被限制在 0 和 1 之间.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org2097ae9" class="outline-3">
<h3 id="org2097ae9"><span class="section-number-3">6.4.</span> Z-scoring vs. min-max scaling</h3>
<div class="outline-text-3" id="text-6-4">
<ul class="org-ul">
<li>什么时候应该使用 z-score 标准化,什么时候应该使用 min-max 缩放?</li>
<li>有些情况下,这两种方法都可以:如果目标只是限制数据的数值范围,例如:
<ul class="org-ul">
<li>为了方便对不同测量结果进行比较,</li>
<li>或者为了将数据缩放到适合机器学习模型使用的范围,那么两种方法都可能同样适用.</li>
</ul></li>
<li>但在某些情形下,一种方法会优于另一种.例如
<ul class="org-ul">
<li>z-score 标准化 会生成均值和标准差已知的数据,</li>
<li>而 min-max 缩放 除了规定上下限外,对数据的统计特性没有要求.</li>
</ul></li>
<li>同样地,z-score 标准化后的数据单位(标准差)是可解释且具有通用性的;</li>
<li>相比之下,min-max 缩放的数据单位是任意的,并不直接与数据分布的形状,矩特征或其他统计特性相关.</li>
<li>Min-max 缩放 最适合用于分布大致均匀的数据,也就是说数据分布没有很长的尾部.
<ul class="org-ul">
<li>原因是:少量极端值可能会压缩绝大部分数据的有效范围.</li>
<li>例如,假设一个数据集是高峰度(高 kurtosis)的正态分布,在进行 min-max 缩放后,大多数数据点可能都
会集中接近 0.5.如果这种数值压缩很严重,可能会导致计算机的精度误差.</li>
</ul></li>
<li>Min-max 缩放通常用于分析要求数据在特定范围内的场景,这在机器学习分类算法(如人工神经网络)中最常见.</li>
<li><b>Nomenclature</b> 在机器学习领域,
<ul class="org-ul">
<li>"normalization" 通常指 min-max 缩放,</li>
<li>而 "standardization" 通常指 z-score 标准化.</li>
</ul></li>
<li>不过,你也不用对这些术语过于严格&#x2013;它们是可以互换的.例如,
<ul class="org-ul">
<li>有时"normalizing"会被解释为将数据转换成服从正态分布的过程,这恰好与"standardizing"的定义相符.</li>
<li>事实上,我在大学里学习统计学很多年后才第一次遇到这种特意区分的用法.</li>
</ul></li>
<li>在统计学的核心文献中,并没有对标准化(standardization)和归一化(normalization)作出统一规定.你甚
至会看到 "z-normalization" 或 "z-score normalization" 这样的说法.</li>
</ul>
</div>
</div>
<div id="outline-container-org9d6acc9" class="outline-3">
<h3 id="org9d6acc9"><span class="section-number-3">6.5.</span> Percent change</h3>
<div class="outline-text-3" id="text-6-5">
<ul class="org-ul">
<li>百分比变化是你一定听说过的概念;它广泛用于购物促销和民意调查&#x2013;这些都是现代社会的标志.</li>
<li>百分比变化用于量化一个变量的变化.这种变化需要两个测量值:从某个数值变化到另一个数值.这个"某个数值"
可以反映多种情境的变化:
<ul class="org-ul">
<li>随时间变化(例如:某种医疗治疗前 vs 治疗后)</li>
<li>不同分析方法之间的变化(例如:删除损坏数据后,数据平均值的变化)</li>
<li>产品或服务价格的变化(例如:促销或通货膨胀导致的变化)</li>
</ul></li>
<li>百分比变化公式
<ul class="org-ul">
<li><p>
如下
</p>
\begin{equation}
\text{pctchng} = 100 \times \frac{\text{ref} - \text{new}}{\text{ref}} \tag{6.11}
\end{equation}</li>
<li>`ref` 表示参考值.</li>
<li>乘以 100 得到百分数(每百份).</li>
</ul></li>
<li><b>百分比变化的特性</b> 由于百分比变化是一种相对转换,其结果取决于参考值.</li>
<li>我觉得商店经常利用这个技巧:
<ul class="org-ul">
<li>提高原价,然后提供更高的打折百分比,</li>
<li>让产品保持相同价格,但看起来更有吸引力,因为打折幅度"很大".</li>
</ul></li>
<li>百分比变化通常用于一次只比较两个数据点的情况.这与 z-score 转换不同:
<ul class="org-ul">
<li>对仅有两个数据点进行 z-score 没有意义.</li>
</ul></li>
<li>百分比变化的优点:一个优点是它去除了数据原始测量单位,这有助于比较不同规模或不同单位的数据.</li>
<li>百分比变化的适用范围和限制
<ul class="org-ul">
<li>百分比变化适用于参考值非零的数值型数据.</li>
<li>如果参考值非常接近零,可能会产生数值不准确或计算机的舍入错误.</li>
<li>百分比变化适用于正值数据,例如价格,重量,疾病严重程度等.</li>
<li>负值数据的百分比变化可能难以解释:
<ol class="org-ol">
<li>从 5 到 4 的变化是直观的 -20%</li>
<li>但从 -5 到 +4 的百分比变化是 -180%,很多人会觉得困惑,因为从负数增加到正数却得到一个负百分比.</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org7bccb24" class="outline-3">
<h3 id="org7bccb24"><span class="section-number-3">6.6.</span> Nonlinear data transformations</h3>
<div class="outline-text-3" id="text-6-6">
<ul class="org-ul">
<li>所有非线性变换都源自同一个核心动机:*改变分布的形状*.</li>
<li>这是与线性变换的一个关键区别&#x2013;线性变换不会改变分布的形状.这意味着,
<ul class="org-ul">
<li>线性变换对任何数据值的作用都是相同的,而非线性变换的效果则取决于具体的数据值.</li>
</ul></li>
<li><b>非线性变换的选择依据</b> 不同的非线性变换会将数据扭曲成不同的分布形状,因此,选择哪种合适的非线性变换,
取决于:
<ul class="org-ul">
<li>对数据的假设</li>
<li>期望的分布特性</li>
</ul></li>
<li>正因为如此,非线性变换的方法很多,我会在本节中重点介绍其中的五种.</li>

<li><b>单调关系的保持</b> 在我这里讨论的所有变换中,都会保持数据点之间的*单调关系*.这意味着,
<ul class="org-ul">
<li>如果 \((x_1 < x_2)\),那么 \((\tilde{x}_1 < \tilde{x}_2)\) (其中 \((x)\) 是原始数据,\((\tilde{x})\) 是
变换后的数据).</li>
</ul></li>
<li>保持单调关系有助于解释统计分析的结果,例如:
<ul class="org-ul">
<li>t 检验(t-tests)</li>
<li>相关分析(correlation)</li>
<li>回归分析(regression)</li>
</ul></li>
<li>一个非单调变换的例子是正弦函数:\(\tilde{x} = \sin(x) \notag\) 在这种情况下,可能出现:
<ul class="org-ul">
<li>\((x_1 < x_2)\)</li>
<li>但 \((\tilde{x}_1 > \tilde{x}_2)\)</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org669e1c6" class="outline-4">
<h4 id="org669e1c6"><span class="section-number-4">6.6.1.</span> Rank-transform</h4>
<div class="outline-text-4" id="text-6-6-1">
<ul class="org-ul">
<li>排序变换会将带有某种有意义量纲的数值数据(例如:英寸,欧元,幸福感评分等)转换为 <b>序数位置</b>
<ul class="org-ul">
<li><p>
举一个简单的例子,考虑下面的数据集 \((X)\)
</p>
\begin{equation}
X = [\, 1,\ 2,\ 3,\ 9348753945,\ 2.01 \,]  \tag{6.12}
\end{equation}</li>
<li><p>
及其排序变换后的数据 \((\tilde{X})\):
</p>
\begin{equation}
\tilde{X} = [\, 1,\ 2,\ 4,\ 5,\ 3 \,]  \tag{6.13}
\end{equation}</li>
</ul></li>
<li>排序变换的过程是将数值转换为其*序数位置*.  该过程的原理:
<ol class="org-ol">
<li>对数据值进行排序</li>
<li>将排序的索引作为排序变换后的数据</li>
</ol></li>
<li>例如,在这个数据集中,
<ul class="org-ul">
<li>数值 <b><b>2.01</b></b> 是第三大的数,因此它对应的排序值是 <b><b>3</b></b>.</li>
<li>注意:**2.01** 变成了 <b><b>3</b></b>,而 <b><b>9348753945</b></b> 变成了 <b><b>5</b></b>.</li>
<li>换句话说,相邻数值之间的**数值差异**不会影响它们的排名结果;</li>
<li>所有经过排序变换后的数据点与最近邻的距离都是 <b>1 单位</b>.</li>
</ul></li>
<li><b>有损变换特性</b> 排序变换是**有损变换**,意味着变换后的数据包含的信息少于原始数据. 一旦应用这种变换,
就**无法恢复到原始数据**.所以我们建议:
<ul class="org-ul">
<li>将排序变换后的数据保存到一个新的变量中</li>
<li>不要覆盖原始数据</li>
</ul></li>

<li>数据含义差异
<ul class="org-ul">
<li>数据集 \((X)\):数值代表对世界的一种测量(例子中的数字是虚构的,但在真实数据集中,它们反映了自然界
可测量的事物)</li>
<li>排序后的 \((\tilde{X})\):数值为索引,编码了相对大小的位置,并去除了数字之间距离的信息</li>
</ul></li>
<li>在上述例子中:
<ul class="org-ul">
<li><b>2.01与2的差值</b> 和 <b>3 与9348753945的差值</b> ,相差巨大,达到了11 个数量级</li>
<li>然而,它们的排序距离都是 <b>1</b></li>
</ul></li>

<li><b>注意事项</b> 不要混淆 X 中的前两个元素与 \((\tilde{X})\) 中的前两个元素.它们看上去好像相同("1" 和
"2"),但其实非常不同:
<ul class="org-ul">
<li>X 中的 "1" 和 "2" 是原始数据值</li>
<li>\(\tilde{X}\) 中的 "1" 和 "2" 是索引</li>
</ul></li>
<li>我们来看另一个例子.
<ul class="org-ul">
<li><p>
请先花点时间对下面的数据集进行排序变换
</p>
\begin{equation}
X = [10, 2, 4, 5, 5] \tag{6.14}
\end{equation}</li>
<li><p>
把它们分配为相同的排序索引是合理的 &#x2013; 毕竟两个相同的数字无法区分顺序.所以我们可以这样设置:
</p>
\begin{equation}
\tilde{X} = [4, 1, 2, 3, 3] \tag{6.15}
\end{equation}</li>
</ul></li>
<li>事实上,之前的结果并不正确.排序变换有两个额外的约束条件:
<ol class="org-ol">
<li>最大索引值应对应集合大小
<ul class="org-ul">
<li>也就是说,\((\tilde{X})\) 中的最大排名应该是 5,因为 \((X)\) 中有 5 个元素.</li>
</ul></li>
<li>所有索引之和应等于从 1 到 \((N)\) 的整数和
<ul class="org-ul">
<li>在这个例子中,\((1+2+3+4+5 = 15)\).</li>
</ul></li>
</ol></li>
<li>解决方法:分数排名(Fractional Ranking)对于并列的数字,分配它们在能够区分排序时对应排名的*平均值*.
<ul class="org-ul">
<li><p>
换句话说:
</p>
\begin{equation}
\tilde{X} = [\, 5,\ 1,\ 2,\ 3.5,\ 3.5 \,] \tag{6.16}
\end{equation}</li>
</ul></li>
<li>这样更改之后
<ul class="org-ul">
<li>最高排名等于数据集大小</li>
<li>所有排名之和等于 1 到 N 的总和</li>
<li>这种变换称为 <b>tiedrank</b> (并列排名)</li>
<li>在处理并列数字时默认采用平均值,因此很多人为了方便,直接称为 <b>rank transform</b>,而不是"fractional
rank transform"或"tied-rank transform"</li>
</ul></li>

<li>*应用场景*排序变换在许多非参数推断统计分析中都有使用,包括:
<ul class="org-ul">
<li><b><b>威尔科克森检验(Wilcoxon test)</b></b>:一种非参数 t 检验替代方法</li>
<li><b><b>斯皮尔曼相关(Spearman correlation)</b></b>:一种非参数相关系数</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org72902b6" class="outline-4">
<h4 id="org72902b6"><span class="section-number-4">6.6.2.</span> Logarithm and square root transformation</h4>
<div class="outline-text-4" id="text-6-6-2">
<ul class="org-ul">
<li>Logarithm transformation 和 square root transformation 是两种不同的变换,但它们足够相似,因此我将它
们归为同一部分.</li>
<li>对数变换(Logarithm Transform)
<ul class="org-ul">
<li>对数变换(通常简称 log-transform)很简单:对数据取对数.通常使用自然对数(ln),但使用任何其他底
数也会有类似的效果.</li>
</ul></li>

<li>log 变换的影响是:
<ul class="org-ul">
<li>较大的正值会被压缩</li>
<li>0 到 1 之间的数值会变成负值,并被拉伸</li>
</ul></li>
<li>因此,异常大的正值影响会减弱,高度非正态分布的数据也可能变得更接近正态分布.
<ul class="org-ul">
<li><p>
见图 6.7
</p>

<div id="org7491b01" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/6-7.png" alt="6-7.png" />
</p>
<p><span class="figure-number">Figure 69: </span>ms/6-7.png</p>
</div></li>
<li>在这个例子中,数据经过 log 变换后并不能完全符合高斯分布(见图 6.7D 中的负偏斜),但它们确实比原始
数据更接近高斯分布.</li>
</ul></li>
<li>log 变换常用于:
<ul class="org-ul">
<li>幂律分布(power-law distributed data)</li>
<li>数据的方差与数值大小成比例的情况,也就是说,数据值越大,波动越大.在第 4 章中我称这种现象为 异方
差性(heteroscedasticity),它会违反某些统计分析(如回归分析)的假设.</li>
</ul></li>
<li>log 变换只适用于正值数据,因为:
<ul class="org-ul">
<li>log(0) 是未定义的</li>
<li>log(负数) 是复数</li>
</ul></li>
<li>如果你有负值数据并希望使用 log 变换,可以在变换之前加一个常数来平移数据.</li>

<li><p>
平方根变换(Square Root Transform)平方根变换更简单:直接对原始数据取平方根:
</p>
\begin{equation}
y = \sqrt{x}
\end{equation}</li>
<li>平方根变换适用于非负数据,并且结果也一定是非负的.它的效果与 log 变换相似,都是对较大数值压缩得更多,
对较小数值压缩得更少,但最终的分布形状与 log 变换不同.
<ul class="org-ul">
<li><p>
你可以在图 6.8 中看到平方根变换的例子,以及它与 log 变换的对比;这部分你还会在练习 2 中进一步探索.
</p>

<div id="orgac86fab" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/6-8.png" alt="6-8.png" />
</p>
<p><span class="figure-number">Figure 70: </span>ms/6-8.png</p>
</div></li>
<li>与 log 变换类似,平方根变换同样可以减少异方差性.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org069d664" class="outline-4">
<h4 id="org069d664"><span class="section-number-4">6.6.3.</span> Fisher-Z</h4>
<div class="outline-text-4" id="text-6-6-3">
<ul class="org-ul">
<li>Fisher-z 变换用于将一个范围在 (-1, +1) 的均匀分布扭曲成一个近似正态分布
<ul class="org-ul">
<li><p>
见图 6.9A-B
</p>

<div id="org3b2b922" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/6-9.png" alt="6-9.png" />
</p>
<p><span class="figure-number">Figure 71: </span>ms/6-9.png</p>
</div></li>
<li>其方法是通过在数值接近 -1 或 +1 时将它们"拉伸"来实现的(见图 6.9C).</li>
<li><p>
Fisher-z的"拉伸"公式如下
</p>
\begin{equation}
x_z = \frac{1}{2} \ln\left( \frac{1 + x}{1 - x} \right) \tag{6.17}
\end{equation}</li>
<li>注意自然对数函数,这意味着这是一个非线性变换.</li>
<li>当 \(x > 1\) 或 $x &lt; -1$即 $|x| &gt; 1$时,该函数是未定义的,因为此时会出现负分数,log 函数无
法计算负数的对数.</li>
<li>当 \(x = -1\) 或 \(x = +1\) 时,公式同样未定义,因为分母为零或 log(0) 是未定义的.</li>
<li>通过以上分析可以看出,此变换的定义域为 \(x \in (-1, 1)\) 的开区间</li>
</ul></li>
<li>现在想想当 x接近零时会发生什么:
<ul class="org-ul">
<li>当 (x→0) 时,log 所在分式的值趋近于 1,而 (log(1)=0).</li>
<li>当 (x→+1) 时,分子趋近于 2,分母趋近于 0,这意味着分式的值会变得非常大且为正数.</li>
<li>最后,当 (x→−1) 时,分子趋近于零,而分母趋近于 2,这意味着分式趋近于零,其对数值为负数.</li>
</ul></li>
<li>希望这个解释对你有帮助;
<ul class="org-ul">
<li>当你遇到一个令人困惑的数学表达式时,可以先通过考虑边界条件以及变量趋近这些边界时函数的变化行为,
来帮助理解它.</li>
<li>另一种理解复杂公式的好方法是&#x2013;使用模拟数据去探索和可视化它.</li>
</ul></li>
<li>事实证明,公式 (6.17) 其实就是三角学中的反双曲正切函数(inverse hyperbolic tangent),因此在实践中,
<ul class="org-ul">
<li>Fisher‑z 变换可以用 Python 的 np.arctanh 或 R 的 atanh 来实现.</li>
</ul></li>
<li>Fisher‑z 变换常用于将数据转换为更适合那些依赖正态分布假设的统计分析场景.你会在第 12 章看到它的一个
应用(用于相关系数的计算,而相关系数的取值范围固定在 [-1,1]</li>
</ul>
</div>
</div>
<div id="outline-container-org2f2cdee" class="outline-4">
<h4 id="org2f2cdee"><span class="section-number-4">6.6.4.</span> Transform any distribution to Gaussian</h4>
<div class="outline-text-4" id="text-6-6-4">
<ul class="org-ul">
<li>事实证明,任何形状的分布都可以通过非线性变换转换为高斯分布.这个算法分为三个步骤:
<ol class="org-ol">
<li>排序变换(Rank-transform) 数据.</li>
<li>最小-最大缩放(Min-max scale) 数据到范围 [−0.999,0.999]</li>
<li>Fisher‑z 变换 已经 min-max 缩放的数据(注意:Fisher-z 变换在数据值恰好等于-1或者+1时是无效的).</li>
</ol></li>
<li>你可以在图 6.10 中看到一个例子.
<ul class="org-ul">
<li><p>
如图
</p>

<div id="orga9c0eef" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/6-10.png" alt="6-10.png" />
</p>
<p><span class="figure-number">Figure 72: </span>ms/6-10.png</p>
</div></li>
<li>经过变换后,数据呈现高斯分布,并且保持了数据点之间的单调关系.</li>
<li>另一方面,这种方法是有损的(lossy),并且不可逆(non-invertible).</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org88595a5" class="outline-3">
<h3 id="org88595a5"><span class="section-number-3">6.7.</span> Interpreting transformed data</h3>
<div class="outline-text-3" id="text-6-7">
<ul class="org-ul">
<li>许多数据变换会改变数据的单位.这本身并无"好坏"之分,但可能需要我们调整对结果的解释方式.</li>
<li>Z 变换(Z-transformation) 可能是最容易理解的一种,同时也有助于解释像回归分析这样的统计模型.
<ul class="org-ul">
<li>例如,当数据经过 z 标准化后,回归参数可以这样解释:"每当变量 A 增加 1 个标准差,变量 B 就增加 0.4
个标准差."</li>
</ul></li>
<li>其他归一化方法虽然去除了数据的量纲,但本身并没有容易理解的指标.
<ul class="org-ul">
<li>例如,使用 min-max 缩放 时,变换后的数据范围是固定的,但它们的均值和方差并不固定.对 min‑max 缩放
后的数据进行回归时,最好把结果解释为:一个变量的变化与另一个变量的变化相关,而不是用具体单位来衡
量变化的大小.</li>
</ul></li>
<li>非线性变换 在解释方面具有最大的挑战.例如,
<ul class="org-ul">
<li>假设你对数据进行了对数变换,然后执行了回归分析.该分析可能显示一个变量对另一个变量的线性作用,</li>
<li><p>
但这个线性作用是基于已经经过非线性变换的数据的(见图 6.11),因此这个作用在原始数据中实际上是非线
性的.
</p>

<div id="org3fbb32c" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/6-11.png" alt="6-11.png" />
</p>
<p><span class="figure-number">Figure 73: </span>ms/6-11.png</p>
</div></li>
</ul></li>
<li>这并不是缺陷,也不是严重的限制,只是需要牢记这一点.如果不同的变量使用了不同的变换方式,解释会进一
步变得复杂.</li>
<li>正如我在 min‑max 缩放部分写到的那样,对于归一化数据,最容易且最安全的解释(虽然不一定是最精确的)是:
<ul class="org-ul">
<li>一个变量的变化与另一个变量的变化有关.</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org362663a" class="outline-4">
<h4 id="org362663a"><span class="section-number-4">6.7.1.</span> When to transform your  data</h4>
<div class="outline-text-4" id="text-6-7-1">
<ul class="org-ul">
<li>总体来说,除非必要,尽量避免对数据进行变换.
<ul class="org-ul">
<li>多数情况下,数据在其原始量纲下最容易理解,</li>
<li>也最能有意义地与它们所测量的系统建立联系.</li>
</ul></li>
<li>当然,数据变换在某些情况下是有效且有用的:
<ul class="org-ul">
<li>它们有时能够提升可解释性(例如用 z 分数标准化来比较不同量纲的变量)</li>
<li>可以使数据适用于参数化统计分析(例如对幂律分布的数据进行对数变换)</li>
<li>当参数化分析的假设被违反时,也会在非参数分析中使用(例如排序变换).</li>
</ul></li>
<li>关键在于:你应该在有明确理由的情况下才对数据进行变换,而不是作为一种默认的数据处理步骤,随意或无理由地应用.</li>
<li>结论:
<ul class="org-ul">
<li>在可能的情况下尽量少对数据进行变换,但在必要时要足够变换.</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orgc3d033e" class="outline-2">
<h2 id="orgc3d033e"><span class="section-number-2">7.</span> Chapter 07: Assess And Improve Data Quality</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-orgdfd730c" class="outline-3">
<h3 id="orgdfd730c"><span class="section-number-3">7.1.</span> Data quality matters</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>数据质量指的就是数据的质量.</li>
<li>高质量的数据结构合理,几乎没有噪声,伪影或缺失值.低质量的数据可能会导致管理不善,混乱,错误以及错误的解释.</li>
<li>本章的目的是向你介绍一些方法,用来评估和提高数据的质量.</li>
<li>由于数据的形式极其多样,来源和应用也各不相同,因此不存在一个单一的全球统一衡量标准来评价数据质量;
<ul class="org-ul">
<li>相反,你需要学习多种质量衡量方法,并将它们灵活组合,</li>
<li>甚至可能需要开发新的量化方式,以便为你的特定应用和数据集定制数据清洗流程.</li>
</ul></li>
<li>幸运的是,数据质量在一定程度上是可以通过各种转换,算法和过滤技术来提升的.</li>
<li>在本章中,你将学习几种常见的提高数据质量的方法.</li>
<li>但对于你的数据,如何评估,清理和处理,最终还是需要你自己作出判断,以确保在进行统计分析之前,数据保持尽可能高的质量.</li>
<li>数据质量控制并不是处理数据时最有趣或最有启发性的部分,但它很重要.</li>
<li>在收集和清洗数据时,尽量做到细致,严谨,耐心且不偏不倚.</li>
<li>有效进行数据清洗需要统计学知识,实践经验,对研究主题的深入理解,以及对数据特殊特性的熟悉.</li>
</ul>
</div>
<div id="outline-container-org66f097a" class="outline-4">
<h4 id="org66f097a"><span class="section-number-4">7.1.1.</span> Data quality influences data-driven decisions</h4>
<div class="outline-text-4" id="text-7-1-1">
<ul class="org-ul">
<li>为什么拥有高质量的数据很重要?
<ul class="org-ul">
<li>原因是所有的决策都是基于数据.</li>
</ul></li>
<li>你所做的每一个决定,都是依赖数据作出的.
<ul class="org-ul">
<li>也许你并不同意这个观点.你可能觉得有些决策是基于数据,而另一些决策则是基于经验,情感或直觉.</li>
<li>但是,那些直觉是从哪里来的呢?它们同样源于数据.这些数据可能是你过去的经历,别人告诉你的故事,或
者你在书籍,博客中读到的内容,甚至是在 TikTok 视频里看到的事情.</li>
</ul></li>
<li>问题是,
<ul class="org-ul">
<li>这类数据往往只是轶事或小样本观察,它们本身并不一定是高质量的;</li>
<li>或者,即使在特定情境中有效,也未必能推广到其他情境中去&#x2013;一次性的经历,在某种情况下合适,换到另一
个情境里可能就不合适了.</li>
</ul></li>
<li>关键在于:
<ul class="org-ul">
<li>我们一直都在基于数据做决策,即便我们自己没意识到这一点.</li>
<li>因此,如果我们想做出好的决策,就必须依赖好的数据.</li>
</ul></li>
<li>回到统计学的实际问题上来:统计学的目的,就是帮助你基于数据做出决策.这些决策可能包括:
<ul class="org-ul">
<li>在一项学术科学研究中支持某个假设</li>
<li>优化一个金融投资组合</li>
<li>评估工厂中的安全隐患</li>
<li>选择一项可能影响数百万人生活的经济或政治策略</li>
</ul></li>
<li>无论决策的性质和影响如何,这个决策的质量在很大程度上取决于数据的质量.</li>

<li><b>GarbageIn, Garbage out(GIGO)</b> 垃圾进,垃圾出&#x2013;如果你一开始输入的是糟糕的数据,你得到的结果也会是糟
糕的.没有任何炫酷,复杂,高级的数据分析方法能够弥补真正糟糕的数据带来的问题.</li>
<li>另一方面,GIGO 的反面并不一定成立:拥有出色的高质量数据,并不保证就一定能得到出色的结果.因为优秀的
结果取决于很多因素,包括:
<ul class="org-ul">
<li>明确且合理的假设</li>
<li>良好开展的实验</li>
<li>准确的测量</li>
<li>以及适当的统计分析.</li>
</ul></li>
<li>因此,高质量数据是必要条件,但不是高质量决策的充分条件.</li>
<li>不要因为数据质量低或数据不足而感到尴尬.</li>
<li>数据就是数据,你应该接纳并面对数据不完美的现实.</li>
<li>事实上,最重要的数据驱动决策之一就是:"在做决定之前,我需要更多的数据."</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orga81fd01" class="outline-3">
<h3 id="orga81fd01"><span class="section-number-3">7.2.</span> Data cleaning phases</h3>
<div class="outline-text-3" id="text-7-2">
<ul class="org-ul">
<li>你有四个不同的时间窗口可以用来控制和提高数据质量.
<ul class="org-ul">
<li><p>
图 7.1 给出了这些时间窗口的概念性概览,下面的文字将提供更多解释.
</p>

<div id="org1416bc0" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/7-1.png" alt="7-1.png" />
</p>
<p><span class="figure-number">Figure 74: </span>ms/7-1.png</p>
</div></li>
<li>需要牢记的核心信息是:越早投入精力去生成高质量数据,最终在数据质量上的回报就越大.</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org04fa1d6" class="outline-4">
<h4 id="org04fa1d6"><span class="section-number-4">7.2.1.</span> 获取数据之前</h4>
<div class="outline-text-4" id="text-7-2-1">
<ul class="org-ul">
<li>获得高质量数据的最佳方式就是收集高质量数据,这意味着在数据获取开始之前就要采取预防性策略.</li>
<li>这只有在你参与数据获取或数据生成过程时才可能做到.基本目标是:
<ul class="org-ul">
<li>提前预判在数据处理和分析阶段可能出现的问题,并在数据获取阶段进行调整,以尽量降低这些问题的风险.</li>
</ul></li>
<li>随着你在数据处理方面的经验增加,你会逐渐具备预测分析中可能出现问题的能力.但刚开始时你可以先参考以下两个建议:
<ul class="org-ul">
<li>熟悉已有经验:阅读科学论文,技术报告,博客文章,YouTube 视频等,这些内容中有在相似类型的数据上做过
类似分析的案例.思考他们遇到的限制或困难,并确定你能做什么来避免这些限制.</li>
<li>咨询有经验的人:和曾处理过相关类型数据或分析的经验人士交流,询问他们会采取什么不同的做法,或者他们
在数据采集之前希望自己知道哪些信息.</li>
</ul></li>
<li>确保实验高质量且能产出高质量数据的一个好方法是开展试点研究(pilot study),先收集并分析一小部分数据.
希望在试点研究过程中能发现预料之外的问题,并据此优化后续的研究设计.</li>
</ul>
</div>
</div>
<div id="outline-container-org72e4931" class="outline-4">
<h4 id="org72e4931"><span class="section-number-4">7.2.2.</span> 数据收集期间</h4>
<div class="outline-text-4" id="text-7-2-2">
<ul class="org-ul">
<li>数据获取的方式千差万别,但某些实验允许研究者在数据收集的过程中实时监控数据.</li>
<li>如果你有这个条件,应当实时检查数据是否存在伪影(artifacts),传感器校准错误或其他可能降低数据质量的问题.</li>
<li>如果可能,暂停数据收集并尝试修复问题.</li>
<li>永远不要假设后续的数据清理或分析策略能"神奇地"修复低质量数据.</li>
</ul>
</div>
</div>
<div id="outline-container-org53b21ab" class="outline-4">
<h4 id="org53b21ab"><span class="section-number-4">7.2.3.</span> 数据收集之后</h4>
<div class="outline-text-4" id="text-7-2-3">
<ul class="org-ul">
<li>数据收集完成后的清理主要有两种策略:
<ul class="org-ul">
<li>数据转换</li>
<li>数据剔除.</li>
</ul></li>
<li>上一章你已经学习了数据转换.
<ul class="org-ul">
<li>数据剔除是指移除那些可能对最终结果产生负面影响的数据,比如数据噪声过大,或者包含极端值(离群值).</li>
<li>虽然某些统计方法对离群值相对鲁棒,但许多常用的分析方法即便只受到一个极端数据点的影响,也可能产生严重偏差.</li>
</ul></li>
<li>你将在本章的后面学习离群值的识别与剔除策略.</li>
</ul>
</div>
</div>
<div id="outline-container-orgc9411eb" class="outline-4">
<h4 id="orgc9411eb"><span class="section-number-4">7.2.4.</span> 数据分析期间</h4>
<div class="outline-text-4" id="text-7-2-4">
<ul class="org-ul">
<li>在数据分析阶段进行数据质量控制是最糟糕的时机,因为这会带来最大的结果偏倚风险.</li>
<li>这种偏倚可能是有意或无意引入的,例如以某种方式选择或修改数据,从而增加获得某个统计结果的机会&#x2013;即使
这种效果在数据中并不存在.</li>
<li>这种情况是如何发生的,以及如何避免这种偏倚,将在第 18 章详细讨论.</li>
<li>当然,在统计分析过程中有时进行数据清理是必要且合理的,但应尽量避免在分析过程中进行数据转换或数据选择.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org6255f8c" class="outline-3">
<h3 id="org6255f8c"><span class="section-number-3">7.3.</span> Assessing data quality</h3>
<div class="outline-text-3" id="text-7-3">
<ul class="org-ul">
<li>有两大类方法可以用来评估数据质量.在这里我将它们分开讨论是为了说明方便,但实际上,你并不是只能选择
其中一种方法&#x2013;它们是互补的,最好在可能的情况下都使用.</li>
</ul>
</div>
<div id="outline-container-orga3a046f" class="outline-4">
<h4 id="orga3a046f"><span class="section-number-4">7.3.1.</span> 定性质量评估(Qualitative quality assessments)</h4>
<div class="outline-text-4" id="text-7-3-1">
<ul class="org-ul">
<li>定性检查数据,就是视觉检查,也就是说你要用一种或多种方式将数据可视化,然后花点时间盯着屏幕仔细观察.</li>
<li><p>
在可视化数据进行质量评估时,你需要关注什么呢?这里有一些建议:
</p>

<div id="org404dab3" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/7-2.png" alt="7-2.png" />
</p>
<p><span class="figure-number">Figure 75: </span>ms/7-2.png</p>
</div></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org3c8368f"></a>数据范围(图 7.2A)<br />
<div class="outline-text-5" id="text-7-3-1-1">
<ul class="org-ul">
<li><p>
检查数值是否符合预期.图 7.2A 中的前 20 个数据点数值可疑地过于相似,这可能是设备问题导致的.
</p>

<div id="org60aa4a9" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/7-2.png" alt="7-2.png" />
</p>
<p><span class="figure-number">Figure 76: </span>ms/7-2.png</p>
</div></li>
<li>另一个例子:如果你的数据集是共享单车公司的租用时长,你会期望数据在几十分钟的范围内.</li>
<li>如果数值小于 1,就可能是错误(或数据是按小时而非分钟编码),这也很重要,需要确认.</li>
</ul>
</div>
</li>
<li><a id="org02535eb"></a>分布形状(图 7.2B)<br />
<div class="outline-text-5" id="text-7-3-1-2">
<ul class="org-ul">
<li>分布的形状影响可适用的统计分析类型,以及你可能需要进行的变换.</li>
<li>如果分布看起来不像任何典型的分析分布,可能存在数据截断(clipping)或噪声问题.</li>
</ul>
</div>
</li>
<li><a id="orga175e49"></a>单组或多组数据(图 7.2C)<br />
<div class="outline-text-5" id="text-7-3-1-3">
<ul class="org-ul">
<li>如果数据来自同质系统,数据应该呈现出单一组形态.</li>
<li>如果存在多个分组或簇(cluster),可能说明数据实际上包含了几个不同的群组.</li>
<li>本书中你已经见过一些多峰分布的例子,这类分布需要进一步调查,有时甚至可以将数据拆分成几个组单独分析.</li>
</ul>
</div>
</li>
<li><a id="org2328b2b"></a>离群值(图 7.2D)<br />
<div class="outline-text-5" id="text-7-3-1-4">
<ul class="org-ul">
<li>关于离群值本章稍后会详细讨论,但相对于整体分布值显得异常的数据,通常在可视化中很容易识别出来.</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org3cc07c7" class="outline-4">
<h4 id="org3cc07c7"><span class="section-number-4">7.3.2.</span> 定量质量评估(Quantitative quality assessments)</h4>
<div class="outline-text-4" id="text-7-3-2">
<ul class="org-ul">
<li>有一些定量的衡量方法可以评估数据质量,其中最有用的指标往往是针对具体应用甚至具体数据集定制的.</li>
<li>下面的不完全列表,可以让你了解可能的质量评估类型.</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org3d28f97"></a>样本量(Sample sizes)<br />
<div class="outline-text-5" id="text-7-3-2-1">
<ul class="org-ul">
<li>较大的样本量通常能提供更可信的统计结果,原因包括:
<ul class="org-ul">
<li>只有较大样本才能检测到一些微小的效应;</li>
<li>噪声影响被降低;</li>
<li>群体参数估计更准确;</li>
<li>更容易进行分组和个体差异分析.</li>
</ul></li>
<li>请注意,样本大并不意味着数据质量一定高,但总体来说,样本量越大,统计分析的结果就越可靠.</li>
<li>同时也有心理层面的因素:较大的样本通常需要更多的资金,时间,精力和资源去获取,这可能(希望如此)意
味着研究在开始前花了更多时间进行严谨思考.</li>
<li>什么样才算"大样本量"?是 N=30?N=300?还是百万级?这是个复杂的问题,后续将在第 17 章中讨论.</li>
</ul>
</div>
</li>
<li><a id="orgf07cd45"></a>错误率(Error rates)<br />
<div class="outline-text-5" id="text-7-3-2-2">
<ul class="org-ul">
<li>指缺失或被剔除的数据比例.剔除的数据越多,预清理阶段的数据质量就越低.</li>
<li>高错误率也可能引发人们对实验设计,设备,以及清理后的数据质量的质疑.</li>
</ul>
</div>
</li>
<li><a id="org8f263cb"></a>数据范围(Data range)<br />
<div class="outline-text-5" id="text-7-3-2-3">
<ul class="org-ul">
<li>指数据值的最小值与最大值,这个特征可能揭示数据中存在的问题.
<ul class="org-ul">
<li>例如,如果你有一个不同网络广告的点击率数据集,而数据中出现了 100% 的点击率,那很可能有什么问题
(我很怀疑即便是最吸引人的广告也能达到 100% 点击率).</li>
</ul></li>
</ul>
</div>
</li>
<li><a id="orge4173f1"></a>方差(Variance)<br />
<div class="outline-text-5" id="text-7-3-2-4">
<ul class="org-ul">
<li>方差就像"金发姑娘"法则(Goldilocks principle)&#x2013;太小或太大都不好,刚刚好才最佳.</li>
<li>统计分析需要一定的方差,否则就没有可解释的内容;但过大的方差会降低参数估计的精度,并可能掩盖真实效应.</li>
<li>正如第 4 章所讨论的,"原始"方差在不同单位的变量之间难以直接比较,但对于单位相同的变量,如果方差差异
过大,可能说明存在离群值或数据损坏.</li>
<li>此外,还有其他定量数据质量评估方法,比如相关矩阵(correlation matrices),在本书后面会回来讨论.</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-orgbf4432e" class="outline-3">
<h3 id="orgbf4432e"><span class="section-number-3">7.4.</span> Improving data quality through  transformations</h3>
<div class="outline-text-3" id="text-7-4">
<ul class="org-ul">
<li>在某些情况下,数据转换可以用来将数据转化为更适合分析的格式.
<ul class="org-ul">
<li>例如,强烈右偏的(right-tailed)数据分布,在进行**对数转换(log transform)**后,往往会变得更接近
高斯分布(Gaussian).</li>
</ul></li>
<li>正如你在前一章中学到的,数据转换方法有很多.</li>
<li>尤其是非线性转换,有时很难判断哪种转换适合哪种情境.</li>
<li>在第 11 章,你将看到一个例子,展示数据转换对统计分析结果可能产生的深远影响.</li>
<li>在决定是否以及如何转换数据时,领域特定知识(domain-specific knowledge)非常有用.</li>
<li>无论使用何种数据转换方式,都必须在转换前后检查数据的分布,以确保数据质量确实得到了提升.</li>
</ul>
</div>
</div>
<div id="outline-container-org4a7d4d4" class="outline-3">
<h3 id="org4a7d4d4"><span class="section-number-3">7.5.</span> What are outliers?</h3>
<div class="outline-text-3" id="text-7-5">
<ul class="org-ul">
<li>离群值(outlier) 是指具有"相对异常值"的数据点.</li>
<li>那么,数据点具有相对异常值是什么意思呢?
<ul class="org-ul">
<li>相对于什么?</li>
<li>"异常"又是什么意思?</li>
<li>又是谁来决定这个值是否异常?</li>
</ul></li>
<li>将一个数据值标记为"离群值"包含了一些主观和随意的判断,不同的人即使面对同一个数据集,也可能对哪些算
作离群值有不同意见.</li>
<li>在本章接下来的内容中,我们会重点讨论识别,解释和处理离群值的各种方面.</li>
<li>这些"讨厌的小家伙"既可能是统计学家梦魇的来源,也可能成为一条新的科学探索之路的灵感.</li>
</ul>
</div>
<div id="outline-container-orgb0eed98" class="outline-4">
<h4 id="orgb0eed98"><span class="section-number-4">7.5.1.</span> How to think about outliers</h4>
<div class="outline-text-4" id="text-7-5-1">
<ul class="org-ul">
<li>我们不要懒惰地陷入一个陷阱:
<ul class="org-ul">
<li>把所有离群值都当作邪恶,糟糕,必须毫不犹豫也毫无歉意地删除的"怪物".</li>
<li>事实上,离群值是一个需要更深入探讨的话题,远比许多网站以及入门统计学书籍或课程中给予的处理更为复杂.</li>
</ul></li>
<li>"离群值"其实是一个总称(umbrella term),包含了几种不同类型,且在质上有明显差异的现象.</li>
<li>你如何理解和处理离群值,取决于你对它们来源和意义的假设.</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orgaf4c531"></a>The outlier is an invalid data point<br />
<div class="outline-text-5" id="text-7-5-1-1">
<ul class="org-ul">
<li>无效数据可能来自于噪声,错误,传感器故障或其他失误.
<ul class="org-ul">
<li>例如:假设一位研究人员正在收集一片森林不同区域的瓢虫重量数据.根据一些 Google 搜索结果,瓢虫的重
量大约为 0.02 克.</li>
<li>如果研究人员在输入某一只瓢虫的重量时忘了按下"."小数点键,那么该样本可能被记录成 254 克,而不是
0.0254 克.这个数据显然是无效的,不应该包含在统计分析中.</li>
</ul></li>
<li>其他无效数据也可能来自人为行为(例如,有人在线调查问卷中填写自己的出生年份为 1852;有些人觉得这种行
为很有趣).</li>
<li>对于这类离群值,应该被识别出来并删除.在部分情况下,可能可以修正错误(比如瓢虫重量的例子),</li>
<li>但更多时候,无效数据点应直接标记为剔除.</li>
</ul>
</div>
</li>
<li><a id="org6df3b6d"></a>The outlier is valid but unusual<br />
<div class="outline-text-5" id="text-7-5-1-2">
<ul class="org-ul">
<li>这是完全不同的情形:离群值不是由噪声或错误造成的,而是它的数值与其他数据点差别非常明显.
<ul class="org-ul">
<li>例如:假设我们收集美国所有名字叫 "Jeff" 的成年男性净资产数据.大多数 Jeff 的净资产在数万到数十万
美元之间.</li>
<li>但其中有一位 Jeff Bezos,他的净资产在本文写作时超过了一千亿美元.这个值与其他 Jeff 的净资产极为不
同,但它既不是无效数据,也不是录入错误.</li>
</ul></li>
<li>还有许多更平常的例子:
<ul class="org-ul">
<li>德克萨斯州沿海地区的洪水保险费用比犹他州高得多;</li>
<li>大多数国家的恐怖袭击很低,但个别国家很高;</li>
<li>一家公司可能大部分销售额都来自一个产品,其余产品销量很少;</li>
<li>大多数流行音乐时长在 2-3 分钟之间,但有些流行歌曲超过 10 分钟;</li>
<li>礼品包装纸在一年中销售量都很低,圣诞节前一周却极度攀升;</li>
<li>大多数信用卡交易金额较小且发生在持卡人所在地区,但一次金额异常大的海外交易可能是欺诈行为.</li>
</ul></li>
<li>这类数据通常被称为非代表性(non-representative),异常(anomalous),极端(extreme)或偏离(deviant).</li>
<li>它们不寻常,但是真实反映了世界的情况.</li>
<li>有效但不寻常的离群值更难处理
<ul class="org-ul">
<li>如果将它们包含在统计分析中,可能会导致偏差(例如,把 Bezos 的净资产纳入平均值,会让人误以为其他
Jeff 都富得多).</li>
<li>但另一方面,有效数据不能仅因为"不方便"就剔除.这样做本身会引入偏差,甚至接近数据操纵或直接作弊.</li>
</ul></li>
<li>离群值的处理方式取决于决策</li>
<li>不寻常的数据并不一定是"坏"的.每一个离群值背后都有一个故事.可能这个故事很简单,
<ul class="org-ul">
<li>比如数据录入错误或设备故障;也可能非常有意义,比如财富不平等,或气候变化对当地经济的影响.</li>
</ul></li>

<li>你需要了解这个故事,才能做出正确的决策.处理离群值最终会走向两种结果:
<ul class="org-ul">
<li>删除离群值:假设离群值是无效数据,它不能反映你关注的系统.</li>
<li>保留离群值:假设离群值是有效但不寻常的数据.虽然它们可能对统计描述和分析产生负面影响,但可以选择以
下三种方法处理:
<ol class="org-ol">
<li>数据转换(Data Transformation):压缩离群值的影响,例如对数据进行对数变换(log transform)来缩小大数值.</li>
<li>使用对离群值稳健的分析方法(Robust Analysis):比如非参数统计,稳健回归(加权回归).</li>
<li>子群分析(Subgroups Analysis):将数据按照相似特征分组,并分别进行统计分析.</li>
</ol></li>
</ul></li>
<li>我想,这一节的核心结论是:
<ul class="org-ul">
<li>在处理离群值时要谨慎思考.很多情况下,删除数据是有正当理由的,但永远不要机械地,没有经过深思熟虑
地删除数据.</li>
</ul></li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-org3e6f188" class="outline-3">
<h3 id="org3e6f188"><span class="section-number-3">7.6.</span> Identifying outliers</h3>
<div class="outline-text-3" id="text-7-6">
<ul class="org-ul">
<li>就像我在 第 7.3 节 中写到的,你可以通过可视化数据来进行定性数据检视,并观察其中是否存在异常的模式或
数据点.</li>
<li>在具体处理离群值时,要寻找一个或几个落在其他数据范围之外的点.
<ul class="org-ul">
<li><p>
图 7.3 给出了一个示例.
</p>

<div id="orgb35f9a3" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/7-3.png" alt="7-3.png" />
</p>
<p><span class="figure-number">Figure 77: </span>ms/7-3.png</p>
</div></li>
</ul></li>
<li>虽然通过可视化进行定性数据检视是熟悉数据的一种极佳方式,但这种方法在识别离群值时并不总是最佳选择,
原因有以下几点:
<ul class="org-ul">
<li>基于视觉检视的离群值评估不可扩展.视觉检视适用于小型或单变量数据集,但无法扩展到大型,多样化或多变
量数据集.</li>
<li>定性识别离群值增加主观偏差的风险:你对数据的背景知识可能会影响你是否将某个数据点标记为离群值.
<ol class="org-ol">
<li>例如:如果你的假设预测该数据集的平均值应该较小,你可能更倾向于将某个数据点标记为离群值.</li>
<li>又或者,你当前的压力,饥饿感,疲惫,对博士研究的挫败感,甚至距离午休的时间,都可能影响你是否将
某个数据点标记为离群值.</li>
</ol></li>
<li>与前一点相关,离群值识别的决策可能不可复现:也就是说,你认为的离群值,可能与别人认为的不同.这意味
着即使不同的人处理同一份数据,也可能得到不同的结果.</li>
</ul></li>
<li>需要强调的是,这些问题都不是致命缺陷,我也不会建议永远不要用定性方法来识别离群值.</li>
<li>事实上,在我的研究实验室里,我们会使用可视化方法来识别多通道神经时间序列数据中的伪迹(artifacts).</li>
<li>但定性评估是一个棘手的过程,通常需要一些特定领域的训练;除非有明确的理由,否则一般应该优先使用算法
化或半算法化的离群值识别策略来代替纯粹的定性方法.</li>
</ul>
</div>
<div id="outline-container-orgf3a33f3" class="outline-4">
<h4 id="orgf3a33f3"><span class="section-number-4">7.6.1.</span> Absolute threshold detection</h4>
<div class="outline-text-4" id="text-7-6-1">
<ul class="org-ul">
<li>这个方法的思路很简单:
<ul class="org-ul">
<li>我们预期数据值会落在某个特定范围内,而超出该范围的数据就被视为离群值.</li>
</ul></li>
<li>回到瓢虫重量的例子:我们并不认为瓢虫会超过 0.05 克,因此任何大于这个值的数据点都应被标记为离群值.</li>
<li>这种方法的优点是可以针对具体的数据集量身定制,研究人员能够利用领域特定的知识来优化离群值检测.</li>
<li>但使用绝对阈值也有两个缺点:
<ul class="org-ul">
<li>对实验方法不熟悉的人,可能很难选择合适的阈值.</li>
<li>数值型数据可能依赖于仪器的校准或设置,这意味着合适的阈值可能是设备特定或数据集特定的.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org6d5ac83" class="outline-4">
<h4 id="org6d5ac83"><span class="section-number-4">7.6.2.</span> The z-score method</h4>
<div class="outline-text-4" id="text-7-6-2">
<ul class="org-ul">
<li>绝对阈值检测方法的主要缺点,可以通过使用相对阈值来克服.</li>
<li>这也是z 分数(z-score)离群值检测方法的动机所在.</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org7f19871"></a>Z-scores and data proportions<br />
<div class="outline-text-5" id="text-7-6-2-1">
<ul class="org-ul">
<li>在介绍 z 分数法 来识别离群值之前,我先说明一下 z 分数在不同范围内所包含的数据比例.
<ul class="org-ul">
<li><p>
图 7.4 展示了标准正态分布,其中坐标单位为标准差(即 z 分数).
</p>

<div id="org355ed3a" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/7-4.png" alt="7-4.png" />
</p>
<p><span class="figure-number">Figure 78: </span>ms/7-4.png</p>
</div></li>
</ul></li>
<li>对于符合该分布的数据集来说:
<ul class="org-ul">
<li>68.3% 的数据落在离均值一个标准差范围内(即从均值下方一个标准差到均值上方一个标准差,总共覆盖两个
标准差范围);</li>
<li>95.5% 的数据落在离均值两个标准差范围内(即两侧各两个标准差,总共覆盖四个标准差范围);</li>
<li>99.7% 的数据落在离均值三个标准差范围内.</li>
</ul></li>
<li>我建议大家把这些 z 分数对应的数据比例记忆下来,这是非常有用的基础知识.</li>
<li>掌握了这些知识,我们就可以用 z 分数来识别相对较大的数据值,并将它们标记为离群值.</li>
</ul>
</div>
</li>
<li><a id="org550ee1f"></a>The z-score method<br />
<div class="outline-text-5" id="text-7-6-2-2">
<ul class="org-ul">
<li>z 分数法非常常见&#x2013;甚至可能是最常用的离群值识别方法,而且它非常简单:
<ul class="org-ul">
<li>只需将数据转换成 z 分数,并认为任何 z 分数超过某个阈值的数据点就是一个离群值.</li>
</ul></li>
<li>这种方法的优点是:
<ul class="org-ul">
<li>无论数据的原始单位是克,米,牛顿,计数,欧元&#x2026;&#x2026;都可以使用.</li>
<li>只要数据可以转换成 z 分数(也就是数值型数据),就可以用这个方法检测离群值.</li>
<li><p>
例如,在图 7.5 中,数据已被转换为 z 分数,任何 z &gt; 3.29 的数据点都被视为离群值.
</p>

<div id="org99a35c8" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/7-5.png" alt="7-5.png" />
</p>
<p><span class="figure-number">Figure 79: </span>ms/7-5.png</p>
</div></li>
<li>为什么选择 z = 3.29 这个阈值?因为它对应的概率 p &lt; 0.001(具体原因会在第 8 章中介绍).目前只需要
知道,很多应用中常见的阈值大约是 3.</li>
<li>这个图也很好地体现了离群值识别的主观性:例如,索引大约在 130 的数据点刚好低于阈值.如果我选择稍低
一点的阈值,或者该数据值稍微高一些,那么它就会被标记为离群值.</li>
</ul></li>
<li>需要说明的是,上述描述针对的是正向离群值(即分布右尾的极值).实际上,离群值也可能出现在分布左边的
尾部.因此,在实际应用中,识别离群值的阈值是双尾的,我们可以用公式表示为:|z| &gt; 3.29</li>
<li>换句话说,当数据点高于或低于均值 三倍标准差时,就被认为是离群值.</li>
<li>关于 z 分数法的一些评论
<ul class="org-ul">
<li>精确且可复现:只要给定相同的数据和相同的阈值,不同的人都会得到相同的结果.</li>
<li>相对性:因为依赖于 z 标准化,某个相同的原始数据值,在不同的数据集中,可能会被标记为离群值,也可能
不会,这取决于其他数据的分布情况.</li>
<li>阈值的主观性:该方法只有一个参数&#x2013;阈值,但选择多少存在主观判断.常见的阈值有 2.3,3 和 3.29.如果
你预期数据波动较大,可能需要更高的阈值,例如 5 或 8.对非正态分布的数据,可能也需要更极端的阈值,或者可以使用改进版 z 分数法(modified-z method).</li>
</ul></li>
</ul>
</div>
</li>
<li><a id="orgdb4c3c7"></a>Modified-z<br />
<div class="outline-text-5" id="text-7-6-2-3">
<ul class="org-ul">
<li>在上一章中你已经学过,如果数据的**中位数(median)和中位数绝对离差(MAD)**比均值和标准差更合适来衡
量数据的中心位置和分散程度时,就可以用改进版的 z 分数来替代普通 z 分数.</li>
<li>同样的思想也可以用于识别离群值.</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org46d96a8" class="outline-4">
<h4 id="org46d96a8"><span class="section-number-4">7.6.3.</span> Iterative z-score method</h4>
<div class="outline-text-4" id="text-7-6-3">
<ul class="org-ul">
<li>在使用相对转换(relative transformation)时,一个问题是:
<ul class="org-ul">
<li>转换后的数值依赖于数据集中其他值.</li>
</ul></li>
<li>想象一下图 7.6 所示的情景:
<ul class="org-ul">
<li><p>
如图
</p>

<div id="orgfd04b2f" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/7-6.png" alt="7-6.png" />
</p>
<p><span class="figure-number">Figure 80: </span>ms/7-6.png</p>
</div></li>
<li>如果数据中存在少量离群值,它们会增加标准差,从而压缩其他数据点的 z 分数.</li>
<li>如果去除这些离群值,重新计算的 z 分数会变得更大.</li>
</ul></li>
<li>这个现象启发了迭代 z 分数法(iterative z-score method).其工作原理如下:
<ul class="org-ul">
<li>执行普通 z 分数法(如前所述),将 z 值超过阈值的数据标记为离群值.</li>
<li>去除已标记的离群值,重新计算剩余数据的 z 分数.将新的 z 值超过阈值的数据再次标记为离群值.</li>
<li>重复步骤 2,直到没有新的离群值出现为止.</li>
<li><p>
图 7.7 展示了该过程的一个示例,实现这一方法并生成该图的代码在练习 1中.
</p>

<div id="org1392dee" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/7-7.png" alt="7-7.png" />
</p>
<p><span class="figure-number">Figure 81: </span>ms/7-7.png</p>
</div></li>
<li>你对索引为 17 的数据点怎么看?在第 0 次迭代中,它没有被标记为离群值,但在第 1 次迭代中,它的 z 值
超过了阈值而被移除.</li>
<li>对我来说,这个数据点并不显得特别大.当然,这里我设定的阈值是 z = 2,这个阈值相当宽松.</li>
<li>不过,这里需要记住两个关键点:
<ol class="org-ol">
<li>带有任意阈值的迭代方法可能会删除比你认为合理的数据还要多.</li>
<li>盲目地信任任何算法都是有风险且不推荐的&#x2013;一定要亲自检查你的数据.</li>
</ol></li>
</ul></li>
<li>这种方法适用于同时存在不同尺度离群值的数据集,尤其是较小的离群值和极大的离群值同时存在的情况.极大
的离群值会偏移标准差,从而掩盖那些相对较小的离群值.</li>
<li>该方法的结果往往更加宽松,因为它可能会删除一些接近边缘但不一定极端的值.</li>
<li>迭代 z 分数法并不是特别常见,但它值得了解,也非常适合作为有挑战性的编程练习 😉.</li>
</ul>
</div>
</div>
<div id="outline-container-orgeee4681" class="outline-4">
<h4 id="orgeee4681"><span class="section-number-4">7.6.4.</span> Removing data by trimming</h4>
<div class="outline-text-4" id="text-7-6-4">
<ul class="org-ul">
<li>另一种清理数据并移除离群值的方法叫做 "截尾法"(trimming).</li>
<li>它的动机与阈值法相同&#x2013;移除数值过大的数据&#x2013;但在实现方式上有一个重要区别:
<ul class="org-ul">
<li>不是删除所有超过某个阈值的点,而是直接删除最极端的 k 个数据点,</li>
<li>或者删除分布两端各占 k% 的数据.</li>
</ul></li>
<li>例如,我们可以设定 k = 2 对每个尾部进行截尾,这意味着删除最大和最小的两个数据点(即最大的两个正值和
最小的两个负值)(见图 7.8).</li>
<li>你应该已经可以想到这种方法的风险:
<ul class="org-ul">
<li>最大的两个数据点可能并不是离群值,</li>
<li>或者实际上有三个离群值,但因为只删除了两个,其中一个就被遗漏了.</li>
</ul></li>
<li>老实说,我并不喜欢这种方法,原因正是上面提到的那些问题.</li>
<li>不过,了解它仍然是有益的,而且你可能会发现它在你的数据中是合适的.</li>
<li>截尾法的优点在于:
<ul class="org-ul">
<li>它快速且易于实现;</li>
<li>只要数据和 k 值相同,任何人都能重复得到相同的结果.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orged3aa11" class="outline-4">
<h4 id="orged3aa11"><span class="section-number-4">7.6.5.</span> Manual, automatic, and semi-automatic cleaning</h4>
<div class="outline-text-4" id="text-7-6-5">
<ul class="org-ul">
<li>自动化离群值移除方法是指依赖算法自动替你删除离群值;而人工方法则是基于可视化检视,由研究人员手动识
别离群值.
<ul class="org-ul">
<li>自动化方法的优势在于简单且可重复,</li>
<li>而人工方法的优势在于研究人员可以利用专业领域知识进行判断.</li>
</ul></li>
<li>半自动化方法结合了这两种方法的优点:
<ul class="org-ul">
<li>其思路是&#x2013;使用算法来识别可能的离群值(putative outliers),但并不直接将它们从数据中删除;</li>
<li>随后由专家对这些可疑数据进行检查,并决定是否接受或拒绝算法的建议.</li>
</ul></li>
<li>半自动化方法的另一大优点是缓解可扩展性问题:
<ul class="org-ul">
<li>当数据量非常庞大时,人工逐一检查会耗费大量时间;</li>
<li>但如果算法能够标记出可疑数据,那么专家就可以只关注已被标记的数据,而无需检查整个数据集.</li>
</ul></li>
<li>这种方法在商业和金融数据处理中非常常见.
<ul class="org-ul">
<li>例如:当算法标记出你银行账户中的一笔大额现金存入时,银行的人工专员会对该交易进行进一步调查.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org8d77941" class="outline-4">
<h4 id="org8d77941"><span class="section-number-4">7.6.6.</span> What happends to rejected outliers?</h4>
<div class="outline-text-4" id="text-7-6-6">
<ul class="org-ul">
<li>在某些情况下,直接移除离群值是可行的,就好像它从未存在过一样.
<ul class="org-ul">
<li>例如,一个包含 100 个样本的数据集,如果有两个离群值,被删除后就变成一个 98 样本的数据集.</li>
</ul></li>
<li>但是在很多数据集中,完全移除整行数据并不是最佳做法.
<ul class="org-ul">
<li>这是因为离群值可能只出现在某一个特征(feature)中,而其他特征并没有问题,</li>
<li>仅仅因为某一列的离群值而删除整行数据未免可惜.</li>
</ul></li>
<li>在其他情况下,数据是配对的(paired data),必须与其他列或其他数据集中的对应行保持匹配.</li>
<li>因此,在实际应用中,被判定为不合格的数据通常会替换为 NaN.
<ul class="org-ul">
<li>NaN 是 "not a number"(不是一个数字)的缩写;</li>
<li>在数学上,NaN 是如除以零等无效运算的结果,但在统计程序中,常用于表示在计算中需要忽略的数据点.</li>
</ul></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orgc08eebf"></a>Hybrid methods<br />
<div class="outline-text-5" id="text-7-6-6-1">
<ul class="org-ul">
<li>多种离群值检测策略是可以组合使用的.
<ul class="org-ul">
<li>例如,你可以先使用绝对阈值法去除那些明显极端且肯定是错误的数据,</li>
<li>然后再应用迭代法来识别并非录入错误的离群值,</li>
<li>但因为数据是非正态分布,你可以使用 <b>改进版 z 分数法(modified-z)</b> 来替代普通 z 分数法.</li>
</ul></li>
<li>更一般地说,最佳离群值检测方法通常是针对具体应用的;
<ul class="org-ul">
<li>你应当把离群值检测方法的集合视作一个工具箱,</li>
<li>从中选择最适合当前任务的工具来使用.</li>
</ul></li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-org1b4f9fc" class="outline-3">
<h3 id="org1b4f9fc"><span class="section-number-3">7.7.</span> Analysis-based solutions to outliers</h3>
<div class="outline-text-3" id="text-7-7">
<ul class="org-ul">
<li>把离群值当作你神奇数据汤里的有毒成分并一刀切地剔除,有时可能会消灭其中有意义的细微差异.</li>
<li>有时候,非代表性的数据值其实是有效的数据点;
<ul class="org-ul">
<li>你希望保留它们在数据集中,但又不希望它们严重影响你的分析结果.</li>
</ul></li>
<li>在这种情况下,你可以采用一些既能包含极端数据值,又能最大限度减少它们影响的分析方法.</li>
</ul>
</div>
<div id="outline-container-orgcd41681" class="outline-4">
<h4 id="orgcd41681"><span class="section-number-4">7.7.1.</span> 非参数分析(Nonparametric analyses)</h4>
<div class="outline-text-4" id="text-7-7-1">
<ul class="org-ul">
<li>许多统计方法(如 t 检验,相关分析,方差分析 ANOVA)都有非参数替代方法,
<ul class="org-ul">
<li>这些替代方法要么对数据进行秩转换(rank-transform),</li>
<li>要么比较**中位数(median)**而不是均值(mean).</li>
</ul></li>
<li>非参数方法对离群值具有稳健性(robustness),因为离群值对中位数的影响很小.</li>
<li>不过,非参数分析的敏感性可能会下降,
<ul class="org-ul">
<li>部分原因是秩转换时会丢失一些有意义的变异性,</li>
<li>从而不易检测到较微弱的效应.</li>
</ul></li>
<li>你会在后续章节中学习这些方法.</li>
</ul>
</div>
</div>
<div id="outline-container-org51cd2d7" class="outline-4">
<h4 id="org51cd2d7"><span class="section-number-4">7.7.2.</span> 基于置换的检验(Permutation-based tests)</h4>
<div class="outline-text-4" id="text-7-7-2">
<ul class="org-ul">
<li>基于置换的统计方法,是通过随机重新分配数据点到其他组或实验条件,构造经验的零假设分布(empirical
null-hypothesis distribution).</li>
<li>在这个过程中,某组的离群值会被随机分配到其他组内,这意味着离群值被自然地纳入了统计推断过程.</li>
<li>你将在第 16 章学习基于置换的统计方法.</li>
</ul>
</div>
</div>
<div id="outline-container-org22f1e85" class="outline-4">
<h4 id="org22f1e85"><span class="section-number-4">7.7.3.</span> 加权分析(Weighted analyses)</h4>
<div class="outline-text-4" id="text-7-7-3">
<ul class="org-ul">
<li>如果离群值的问题在于它们对结果的影响过大,那么&#x2013;为什么不 <b>降低它们的权重(de-weight)</b> 呢?</li>
<li>"降权"离群值意味着根据它的离群程度(outlier badness),将它乘以一个介于 0 到 1 之间的数值.
<ul class="org-ul">
<li>例如,在数据集 [1, 2, 3, 40] 中,我们可以使用权重向量 [1, 1, 1, 0.1].</li>
<li>这样,原本的 40 在分析中就变成了 4.</li>
</ul></li>
<li>理论上,加权数据值是个好主意,但实现起来可能很棘手&#x2013;比如,如何具体定义权重?</li>
<li>我会在第 15 章对此进行更详细的讨论.</li>
</ul>
</div>
</div>
<div id="outline-container-org46dd478" class="outline-4">
<h4 id="org46dd478"><span class="section-number-4">7.7.4.</span> 子群分析(Subgroups analysis)</h4>
<div class="outline-text-4" id="text-7-7-4">
<ul class="org-ul">
<li>如果数据集中有很多值都可能被标记为离群值,</li>
<li>那么这些值有可能其实代表了一个与大部分数据质上不同的独立群体.例如,假设有一个包含 100 辆汽车油耗
(mpg, miles per gallon)的数据集:
<ul class="org-ul">
<li>其中 80 辆车的油耗在 18-25 mpg 之间,</li>
<li>而另外 20 辆车的油耗在 40-60 mpg 之间.</li>
<li>这 20 辆车看起来像是离群值,但实际上,它们可能是油电混合动力车(electric-gas hybrids).</li>
<li>在这种情况下,后续的统计分析就可以分别对两组数据进行.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org7762099" class="outline-3">
<h3 id="org7762099"><span class="section-number-3">7.8.</span> Missing data</h3>
<div class="outline-text-3" id="text-7-8">
<ul class="org-ul">
<li>缺失数据在统计学中可能带来很大的挑战,并可能对研究结果产生显著影响.图 7.9 展示了一个带有缺失数据的数据集示例.</li>
<li>数据缺失可能由多种原因造成:
<ul class="org-ul">
<li>退出研究:在跟踪同一群体一段时间的研究中(称为纵向研究),参与者可能拒绝继续参与研究,搬到其他地方,
甚至去世(在医学研究中,这种情况确实会发生).这些参与者可能只提供部分数据,这些数据在某些分析中有用,但在其他分析中无法使用.</li>
<li>设备故障:技术问题可能导致部分数据不可恢复,损坏或无法使用.</li>
<li>人为错误:有时,数据缺失仅仅是因为无心的错误.</li>
</ul></li>
<li>那么,面对缺失数据该怎么办呢?让我们逐一来看(在下面的讨论中,记住数据表是列代表特征,行代表观测值):</li>
</ul>
</div>
<div id="outline-container-orga0ebccf" class="outline-4">
<h4 id="orga0ebccf"><span class="section-number-4">7.8.1.</span> 按行删除(Row-wise removal)</h4>
<div class="outline-text-4" id="text-7-8-1">
<ul class="org-ul">
<li>如果某一行中包含至少一个缺失值,就将整行删除.</li>
<li>这种策略适用于配对数据(例如,测量运动员在一周训练课程前后的表现).假设是:一旦有数据缺失,那么该
个体的所有数据都不可使用.</li>
</ul>
</div>
</div>
<div id="outline-container-orgeb50e14" class="outline-4">
<h4 id="orgeb50e14"><span class="section-number-4">7.8.2.</span> 针对特定分析的行删除(Analysis-specific row removal)</h4>
<div class="outline-text-4" id="text-7-8-2">
<ul class="org-ul">
<li>按行删除的缺点是:行中有些非缺失数据可能是有效且可用的.</li>
<li>因此另一种方法是保留这行数据,但在某些需要该数据点的统计分析中,将其排除.</li>
<li>该方法在数据量较少或进行多种分析时特别有用&#x2013;因为至少有一部分分析不包含带有缺失值的特征.</li>
<li>为了保留数据集的大小和顺序,这种情况下会用 NaN 替换缺失值.</li>
</ul>
</div>
</div>
<div id="outline-container-org7e9eb3c" class="outline-4">
<h4 id="org7e9eb3c"><span class="section-number-4">7.8.3.</span> 插补(Imputation)</h4>
<div class="outline-text-4" id="text-7-8-3">
<ul class="org-ul">
<li>这种方法是用估计值替换缺失数据,估计值基于已有数据计算.例如,常用该特征的现有数据值的平均值来替换缺失值.
<ul class="org-ul">
<li><p>
例如,在图 7.9 中,缺失的数值会被 6.67 代替(这是该特征在时间点 2 的平均值).
</p>

<div id="orgfdcfe43" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/7-9.png" alt="7-9.png" />
</p>
<p><span class="figure-number">Figure 82: </span>ms/7-9.png</p>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org931d770" class="outline-4">
<h4 id="org931d770"><span class="section-number-4">7.8.4.</span> 预测建模(Predictive modeling)</h4>
<div class="outline-text-4" id="text-7-8-4">
<ul class="org-ul">
<li>这种方法在概念上与插补相似,不过使用更详细的统计模型来预测缺失值,而不是简单地用平均值替换.</li>
<li>预测模型的复杂度可以从简单的回归到深度学习神经网络不等.</li>
<li>这种方法的好处是能提供更合理的缺失值估计,但是构建模型参数需要相对较多的数据,因此这种方法主要适用
于大型数据集.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org0891341" class="outline-2">
<h2 id="org0891341"><span class="section-number-2">8.</span> Chapter 08: Probability Theory</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-org2894f02" class="outline-3">
<h3 id="org2894f02"><span class="section-number-3">8.1.</span> From descriptive to inferential statistics</h3>
<div class="outline-text-3" id="text-8-1">
<ul class="org-ul">
<li>本章是本书的一个转折点.到目前为止,本书一直专注于描述性统计&#x2013;即用数值特征来帮助你理解你的数据.
<ul class="org-ul">
<li>现在我们要过渡到推论统计,它的目标是利用你的数据去对数据集之外的世界做出有依据的推测.</li>
</ul></li>
<li>这里有一个比喻:想象你有一副眼镜,
<ul class="org-ul">
<li>描述性统计就是去了解这副眼镜&#x2013;它的大小,形状,颜色,折射特性等等;</li>
<li>而推论统计则是透过这副眼镜去看世界.眼镜(数据)是一种工具,帮助你更准确地感知周围的环境.</li>
</ul></li>
<li>那么,为什么从描述性统计向推论统计过渡时要先从概率开始呢?</li>
<li>为什么不直接进入像 t 检验,相关分析或回归分析这样的统计方法?</li>
<li>原因是:
<ul class="org-ul">
<li>推论统计的核心目的,是帮助你在一个不确定的世界中做出决策,</li>
<li>而概率论正是用来量化不确定性的.事实上,推论统计基本上可以看作是应用概率论.</li>
</ul></li>
<li><b>"纯"概率 vs "计算机"概率</b> 概率论是数学中的一个重要领域,历史悠久,可以追溯到几百年前.传统的,数学
导向的概率论涉及大量的微积分,特别是对概率分布进行积分的定义与计算,以及从基本原理推导概率定理的证明.</li>
<li>如果没有严谨的数学分析基础,统计学将只是一堆看起来合理的算法集合.</li>
<li>但是另一方面,我认为应用统计是可以在不进行所有繁琐证明和推导的前提下被学习,理解和正确运用的
<ul class="org-ul">
<li>我也不打算让微积分成为本书的必修前置条件.因此,本章会与数学内容较重的统计书籍中的同类章节有所不同.</li>
</ul></li>
<li>我会将
<ul class="org-ul">
<li>传统的,数学内容较重的概率论称为**"纯","分析型"或"理论型"概率;</li>
<li>而将基于计算机应用的概率论称为"计算机型"或"经验型"概率.</li>
</ul></li>
<li>在本书中,我们重点关注后者,你会看到
<ul class="org-ul">
<li>求和代替积分</li>
<li>分布代替密度.</li>
</ul></li>
<li>"计算机型"概率还有一些"纯"概率中没有的考虑因素,例如:
<ul class="org-ul">
<li>数字化可能引入数值误差,</li>
<li>舍入误差</li>
<li>以及域范围限制(你在练习 4.2 中已经看到过一个例子:高斯分布的离散积分).</li>
</ul></li>
<li>好消息是,本章中的所有概念都可以直接用 Python 或 R 实现,并且可以在不需要两学期大学微积分的情况下理解.</li>
</ul>
</div>
</div>
<div id="outline-container-org718422a" class="outline-3">
<h3 id="org718422a"><span class="section-number-3">8.2.</span> What is probability?</h3>
<div class="outline-text-3" id="text-8-2">
<ul class="org-ul">
<li>从自然界的最微观尺度,到人类生活中的死亡和税收,再到银河尺度上黑洞的寿命&#x2013;没有什么是绝对确定的.我
们无法确切知道未来会发生什么;我们只能为各种事件发生的可能性赋予一个概率.</li>
<li>从数学上来说,我们可以讨论"绝对确定性",但在现实中,世界是概率性的.</li>
<li>概率是用数值来描述事件发生的可能性.它的取值范围在 0 到 1 之间:
<ul class="org-ul">
<li>0 表示绝对不可能</li>
<li>1 表示绝对确定</li>
</ul></li>
<li>本章将为你介绍概率论.概率论本身就是一个庞大的领域,它是物理学,工程,金融,赌博,体育,保险,医学
人工智能,天气预测,以及无数其他学科(包括统计学)的核心.</li>
<li>还有许多与概率论密切相关的数学领域,例如集合论,它们通常会包含在一本专门介绍概率论的书中.</li>
<li>在本章结束时,你将具备足够的概率论知识来理解本书中的统计概念,并为你在其他领域学习概率论的应用打下
坚实的基础.</li>
</ul>
</div>
<div id="outline-container-org1d24925" class="outline-4">
<h4 id="org1d24925"><span class="section-number-4">8.2.1.</span> The problem with  probability</h4>
<div class="outline-text-4" id="text-8-2-1">
<ul class="org-ul">
<li>概率的问题在于,它并不是很直观,尤其是当你第一次接触它的时候,有时甚至会让人觉得违背直觉.
<ul class="org-ul">
<li>例如,互联网上说今天在日本大阪(我正在写这一章的地方)下雨的概率是 40%.</li>
<li>这是什么意思呢?城市并不是处于 "40% 下雨,60% 不下雨" 的叠加状态&#x2013;至少我们不是这样感知和体验它的.</li>
<li>到了今天午夜,我可以回顾这一天,并确定今天是下雨了,还是没下雨.这是一个要么发生,要么不发生的二元事件.</li>
</ul></li>
<li>那么,"下雨的概率是 40%"到底是什么意思呢?下面我给出几种解释,其中只有一种是正确的,其他都是错的.
在往下看我的答案之前,我希望你先自己思考一下哪个是对的,以及为什么其他是错的:
<ol class="org-ol">
<li>今天会下雨 40% 的时间(也就是 0.4 × 24 = 9.6 小时).</li>
<li>今天有 40% 的城市地区会下雨,而 60% 的城市地区是干的.</li>
<li>今天在大阪的某个时间,某个地方下雨的概率是 2/5.</li>
<li>气象预报员对今天会下雨的信心是 40%.</li>
</ol></li>
<li>希望你真的花时间想了想这些说法,如果没有,你还有一次机会认真思考.下面是我的答案:
<ol class="org-ol">
<li>错误.概率并不表示某件事情发生的持续时间.即使只下了 5 分钟,甚至连续下了 5 小时,它也可能是 40%
的概率.</li>
<li>错误.理由与上面相同:概率并不表示事件发生的空间范围.</li>
<li>正确.在一天开始之前,就存在一个下雨的可能性.一些数学公式会根据多种因素(大多数与过去几天的天气
状况以及往年同日期该地区的历史气象数据相关)计算出今天下雨的概率.</li>
<li>错误.信心(Confidence)与概率是不同的:
<ul class="org-ul">
<li>我们可以以 99% 的信心认为今天下雨的概率是 40%.在这种情况下,40% 是我们的参数估计值,</li>
<li>而置信区间可能是 [39%, 41%].</li>
<li>第 13 章会专门讲解置信区间,如果你现在对它和概率的区别还不清楚,希望在读到那一章时能明白.</li>
</ul></li>
</ol></li>
<li><b>Example of probabilities</b> 我相信你在日常生活中已经很熟悉概率的概念.这里有几个例子帮你进入状态:
<ul class="org-ul">
<li>抛硬币:一枚硬币有两面,在抛出后,每一面朝上的概率都是 1/2(50%).</li>
<li>掷骰子(注意,"die"是"dice"的单数形式)
<ol class="org-ol">
<li>一个标准的六面骰子,每一面出现的概率是 1/6(16.667%).</li>
<li>如果是一个十二面骰子,每一面出现的概率是 1/12(8.333%).</li>
</ol></li>
<li>扑克牌:一副标准扑克牌有 52 张牌.
<ol class="org-ol">
<li>随机抽到一张特定牌(比如红心皇后)的概率是 1/52(1.92%)</li>
<li>随机抽到一张红牌的概率是 26/52(50%)</li>
<li>随机抽到任意花色的皇后的概率是 4/52(7.69%)</li>
</ol></li>
</ul></li>
<li>你是否在这些例子中看出了规律呢?我想指出两点:
<ol class="org-ol">
<li>事件的概率等于该事件可能发生的次数(例如,骰子掷出"2"的次数)除以所有可能事件的总数(例如,骰子
的 6 个面).
<ul class="org-ul">
<li>这种计算非常简单,但前提是所有事件发生的可能性相等&#x2013;而这并不总是成立,</li>
<li>例如在沙漠中下雨的概率并不是 50%,尽管结果只有"下雨"和"不下雨"两种.</li>
</ul></li>
<li>所有事件的概率之和等于 1(或 100%).
<ul class="org-ul">
<li>抛硬币时,正面概率 + 反面概率 = 100%;掷骰子时,从"1"到"6"的所有概率之和也 = 100%.</li>
<li>这不是我例子里的巧合,而是概率定义的一部分.</li>
</ul></li>
</ol></li>
<li>实际上,一个概率集合中的所有概率之和必须是 100%.
<ul class="org-ul">
<li>这在直觉上也很好理解:总得发生一些事情&#x2013;如果硬币没有落在正面,就一定落在反面.</li>
<li>即便考虑极小概率的"硬币立起来",那么它就是三个事件,它们的概率之和也必须是 100%.</li>
</ul></li>
<li>这是理论上的情况.但当你在计算机上实现概率时,可能会发现所有概率的总和小于 100%.
<ul class="org-ul">
<li>这样的结果在理论上是错误的,不过它反映的是数字化实现上的不完整性.你将在本章后面看到一些这种情况
的例子.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org1d7f295" class="outline-4">
<h4 id="org1d7f295"><span class="section-number-4">8.2.2.</span> When do we need probabilities?</h4>
<div class="outline-text-4" id="text-8-2-2">
<ul class="org-ul">
<li>我在本书前面提到过,你并不总是需要统计学.同样,你也并不总是需要概率.</li>
<li>在实践中,只有当一个事件的结果存在较大的不确定性时,我们才需要考虑概率.比如:
<ul class="org-ul">
<li>光的传播速度比声音快吗?&#x2013;这种情况不需要概率.</li>
<li>如果你朋友的姐姐的宠物沙鼠把一个土豆推下桌子,它会掉到地上吗?&#x2013;也不用概率.</li>
<li>一位意大利厨师会在披萨上放菠萝吗?(对于许多意大利人而言,披萨是其文化遗产的一部分,任何偏离传统的
创新,尤其是加入菠萝这种带有甜味的水果,常常被看作是对经典的破坏.网上甚至流传着这样一种说法:"如
何快速激怒一个意大利人?往他的披萨上放菠萝!)</li>
</ul></li>
<li>有些事情的发生概率太高或太低,去计算它的概率完全是浪费时间.</li>
<li>再来几个问题:
<ul class="org-ul">
<li>11 月的河内会下雨吗?</li>
<li>MRI 扫描中的白色区域是否意味着病人患有癌症?</li>
<li>在超市佩戴医用口罩能防止新冠病毒传染吗?</li>
</ul></li>
<li>这些问题都涉及不确定性,而我们需要用概率来帮助自己做出更有依据的决策.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb8582ab" class="outline-3">
<h3 id="orgb8582ab"><span class="section-number-3">8.3.</span> Probability vs. proportion</h3>
<div class="outline-text-3" id="text-8-3">
<ul class="org-ul">
<li>概率(Probability)和比例(Proportion)是相关但不同的概念.它们容易被混淆,因为有时它们的数值是相同的.</li>
<li>我先用一个例子说明,然后给出定义.
<ul class="org-ul">
<li>想象你抛 10 枚硬币并记录结果.</li>
<li>每枚硬币正面朝上的概率是 50%,因此合理的预期是会有 5 枚硬币正面朝上.</li>
<li><p>
但实验结果却是 10 枚硬币里有 6 枚是正面(见图 8.2).
</p>

<div id="orgcc935aa" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/8-2.png" alt="8-2.png" />
</p>
<p><span class="figure-number">Figure 83: </span>ms/8-2.png</p>
</div></li>
</ul></li>
<li>在这个例子中,50% 是正面朝上的概率,而 60% 是正面出现的比例.</li>
<li>定义:
<ul class="org-ul">
<li>概率(Probability): 某事件发生的可能性.</li>
<li>比例(Proportion): 整体中某部分所占的分数(或百分比).</li>
</ul></li>
<li>这里有一个概率与比例数值相同但含义不同的例子:
<ul class="org-ul">
<li>我每天总共花 5.1 分钟刷牙,而我一天的清醒时间是 17 小时(1020 分钟).</li>
<li>刷牙所占的清醒时间比例 = 5.1 / 1020 = 0.005(即 0.5%).</li>
<li>随机选择我清醒时间的某一分钟,这一分钟在刷牙的概率也是 0.5%.</li>
</ul></li>
<li>一个概率的微妙之处在于,问题的提问方式不同,答案也会不同.例如:
<ul class="org-ul">
<li>我一天一定会刷牙,那么"我在某一天刷牙"的概率是 1(100%).</li>
<li>如果我每周五都会吃巧克力,那么在随机选择某一天的情况下,吃巧克力的概率是 1/7;但选择星期一这个特
定的日子,吃巧克力的概率是 0%,而选择星期五,则概率是 100%.</li>
<li>一周中吃巧克力的天数比例是 1/7,这个数值恰好等于随机选一天吃巧克力的概率.</li>
</ul></li>
<li>回到抛硬币的例子:
<ul class="org-ul">
<li>每枚硬币正面朝上的概率是 50%,而这一轮实验中正面的比例是 60%.</li>
<li>现在硬币都已经抛完,如果随机选一枚硬币,它是正面的概率就是 60%.</li>
</ul></li>
<li>总结:
<ul class="org-ul">
<li>概率涉及事件发生的不确定性.</li>
<li>比例是对现有数据的描述统计,它不涉及不确定性,因为它只是计数已经发生的结果.</li>
</ul></li>
<li>我们可以讨论未来事件的概率,但不能知道未来事件的比例,直到它们发生(不过我们可以根据概率对未来的比例做预测).</li>
</ul>
</div>
</div>
<div id="outline-container-orgc0cc022" class="outline-3">
<h3 id="orgc0cc022"><span class="section-number-3">8.4.</span> Computing probabilities</h3>
<div class="outline-text-3" id="text-8-4">
<ul class="org-ul">
<li>让我们稍微退一步,先问一个简单的问题:我们怎么才能知道一个事件的概率呢?换句话说,概率是从哪里来的?</li>
<li>概率的来源主要有三个:</li>
<li><b>基于语义知识的直觉(Intuition from semantic knowledge)</b></li>
<li>语义知识是你对世界的已知信息.比如:
<ul class="org-ul">
<li>你知道一个标准骰子有 6 个面,而且在掷骰子时每一面出现的可能性是相等的.</li>
<li>你知道伦敦下雨的概率比开罗高,尽管你可能不知道确切的数值概率.</li>
<li>你知道吸烟会增加患肺癌和其他疾病的风险,尽管你可能不知道具体的概率数值.</li>
</ul></li>
<li>这种你"自然知道"的概率在日常生活中很有用,但在正式的统计分析中没什么价值.</li>
<li>事实上,人们相信的许多"概率"都是错误的.</li>
<li><b>数学公式(A mathematical formula)</b></li>
<li>在推论统计中最重要的概率,通常是通过数学公式计算出来的.接下来几章你会学到:
<ul class="org-ul">
<li>判断统计显著性的核心机制,就是计算一个样本的描述性统计量反映已知参数总体特征的概率是多少.</li>
</ul></li>
<li>这个概率来自基于数学原理推导出来的公式,而不是直觉或经验数据.
<ul class="org-ul">
<li>不要害怕&#x2013;有些公式非常简单,你甚至可以在脑海里推导并计算;</li>
<li>另一些公式较复杂,那就交给计算机来帮我们计算.</li>
</ul></li>
<li><b>经验测量(Empirical measurements)</b></li>
<li>真实世界非常复杂.
<ul class="org-ul">
<li>如果能有一个数学公式精确告诉我今年心脏病发作的概率,那当然很好.</li>
<li>不过,这里存在一个有趣的哲学争论:自然界是否真的能被如此精确地数学化描述?现实是,我们通常无法仅
靠纯数学来推导出这种具体的概率.</li>
</ul></li>
<li>因此,我们必须通过经验测量来估计这种概率.
<ul class="org-ul">
<li>例如,假设在 100,000 位与我同龄的男性中,有 100 人在过去一年经历了心脏病发作.</li>
<li>那么,我今年心脏病发作的概率就是 0.1%.</li>
</ul></li>
<li>这个例子其实是极度简化了的,因为它没有考虑无数的遗传因素和生活方式因素,这些都会影响我的个人概率.</li>
<li>但重点是:
<ul class="org-ul">
<li>在很多情况下&#x2013;尤其是在研究与生物学相关的内容(包括心理学和医学)时&#x2013;我们必须依靠收集经验数据来估计概率.</li>
</ul></li>
<li>为了方便起见,我会进行如下的命名:
<ul class="org-ul">
<li>分析型概率(Analytical probabilities):指不依赖数据,仅通过数学公式推导出来的概率.</li>
<li>经验型概率(Empirical probabilities):指必须基于数据才能计算出来的概率,因为它不能纯粹依靠数学原
理推导.</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org17554be" class="outline-4">
<h4 id="org17554be"><span class="section-number-4">8.4.1.</span> Computing analytical probabilities</h4>
<div class="outline-text-4" id="text-8-4-1">
<ul class="org-ul">
<li>分析型概率(Analytical probabilities)是通过数学公式推导出来的.</li>
<li>我在本章前面已经给过一些简单的例子:
<ul class="org-ul">
<li>抛硬币得到正面的概率是 50%</li>
<li>掷骰子掷出"6"的概率是 1/6</li>
</ul></li>
<li>在这些情况下,概率的计算过程非常直接:
<ul class="org-ul">
<li>事件的概率 = 感兴趣事件的次数 ÷ 事件集合中的所有可能事件数</li>
<li>然后,如果需要,可以乘以 100 转换成百分比.</li>
</ul></li>
<li>我认为这样的描述本身已经很清楚了,但为了完整性&#x2013;并且为了连接到本章后面介绍的 softmax 函数&#x2013;
<ul class="org-ul">
<li><p>
我会把它用数学形式写出来:
</p>
\begin{equation}
p(x_i) = \frac{N(X=x_i)}{N(x)} \tag{8.1}
\end{equation}</li>
<li>p(xᵢ) 是事件 xᵢ 发生的概率(例如,骰子掷出数字"4"的概率).</li>
<li>大写 X 表示所有可能事件的集合(在此例中,集合是骰子的所有结果:"1","2","3","4","5","6").</li>
<li>分子 N(X = xᵢ) 是事件 xᵢ 可能发生的次数.</li>
<li>分母 N(X) 是集合中元素的数量,在这个例子中是 6.</li>
</ul></li>
<li>两个例子:
<ul class="org-ul">
<li>掷骰子得到偶数的概率 = 骰子上所有偶数的数量 除以 骰子面总数. 结果是 3/6 = 50%.</li>
<li>掷骰子得到小于 3 的数字的概率 = 2/6 = 33.3%,因为有 2 个结果满足我们的小于 3 的条件,而骰子共有
6 种可能结果.</li>
</ul></li>
<li><b>A more involved example</b></li>
<li>统计学老师特别喜欢用装满弹珠的瓶子做示例.假设有一个瓶子,里面装着 90 颗弹珠:
<ul class="org-ul">
<li>40 颗蓝色</li>
<li>30 颗黄色</li>
<li>20 颗橙色</li>
</ul></li>
<li>我们假设这些弹珠完全混合均匀,这样随机选出某个颜色的概率只取决于该颜色弹珠的数量(而不是因为取样偏
差,比如所有蓝色弹珠都在瓶底).</li>
<li>问题是: 随机抽取每种颜色的弹珠的概率是多少?
<ul class="org-ul">
<li>换句话说,伸手进入瓶子,抽到蓝色弹珠的概率是多少?黄色的呢?橙色的呢?</li>
</ul></li>
<li>我会在下面展示并解释计算过程,但在继续阅读之前,我希望你先自己动手算一算,然后再对照我的计算和推理.
<ul class="org-ul">
<li>不过,在做任何计算之前,你需要先确定这些数据是否适合用于概率计算.</li>
</ul></li>
<li>注意,这个问题可以不需要收集任何经验数据就能解决.
<ul class="org-ul">
<li>也就是说,我们不用真的去商店买弹珠和瓶子,然后大量地随机抽取弹珠再统计结果.</li>
<li>换句话说,我们可以直接推导出这些结果的分析型概率(Analytical probabilities).</li>
</ul></li>
<li>在练习 7中,你将做一个实验,看看这些分析型概率与用实验数据计算出来的经验型概率(Empirical
probabilities)有多接近.</li>
<li>随机抽取到每种颜色的概率,其实就是:该颜色弹珠的数量 ÷ 弹珠总数.
<ul class="org-ul">
<li><p>
蓝色概率
</p>
\begin{equation}
  p(blue) = 100 \frac{40}{90} = 44.4\% \tag{8.2}
\end{equation}</li>
<li><p>
黄色概率
</p>
\begin{equation}
  p(yello) = 100 \frac{30}{90} = 33.3\% \tag{8.3}
\end{equation}</li>
<li><p>
橙色概率
</p>
\begin{equation}
  p(orange) = 100 \frac{20}{90} = 22.2\% \tag{8.4}
\end{equation}</li>
</ul></li>
<li>注意,这里的分母是弹珠的总数量,而不是颜色的数量.</li>
<li>也就是说,每一颗弹珠都被视为一个"事件",所以在公式 8.1 里,N(X) 的值是 90,而不是 3.</li>
<li>在数学中,一个常用的好习惯是对结果做合理性检查(sanity check),意思是用你的答案去验证它是否合理.</li>
<li><p>
在这里,我们可以做的一个合理性检查就是:把所有颜色的概率加起来,看看是否等于 100%.
</p>
<pre class="example" id="org390555c">
44.4 + 33.3 + 22.2 = 99.9
</pre></li>
<li>啊哦!这些概率的总和没有等于 100%.</li>
<li>是我算错了吗?是,也不是.</li>
<li>计算过程是对的,但我在除法运算时做了截断处理.
<ul class="org-ul">
<li>实际上,p(蓝色) 并不是 44.4%,而是 40/90 = 44.444444&#x2026;(这个 4 是无限循环的).</li>
<li>另外两种颜色的概率也是同样的情况.</li>
</ul></li>
<li>换句话说,我引入了精度误差(precision errors),导致总和非常接近 100%,但并不完全等于 100%.</li>
<li>这种程度的误差在计算机执行数学运算时非常常见,而且这是你必须接受的一种容差(tolerance).</li>
<li><b>More complicated analytical probabilities</b></li>
<li>有些统计学教材坚持用一些可以在脑子里算出来的简单例子.
<ul class="org-ul">
<li>虽然我理解这种做法的初衷,但我觉得这对那些想将概率论应用到数据科学的人来说帮助不大.</li>
</ul></li>
<li>换句话说,像"弹珠瓶"这样的例子,其实只是高中水平的概率论.
<ul class="org-ul">
<li>现实一点来说:你的雇主绝不会要求你手算"从一个瓶子里随机抽到蓝色弹珠的概率".</li>
</ul></li>
<li>他们更可能要求你计算的是:
<ul class="org-ul">
<li>基于经验数据计算出来的某个检验统计量,在给定样本量和变异性的情况下,有多大的概率只是由于随机性产
生的结果.</li>
</ul></li>
<li>这种概率是基于数学公式的,而且通常无法手算,因此你需要依赖像 Python 或 R 这样的数值计算软件来得出答案.</li>
<li>在本章后面以及接下来的章节中,我会继续讲解这类分析型概率,并配合例子和可视化来说明.</li>
<li>但首先,我想先讨论如何从经验数据集计算概率.</li>
</ul>
</div>
</div>
<div id="outline-container-org3e44330" class="outline-4">
<h4 id="org3e44330"><span class="section-number-4">8.4.2.</span> Computing expirical probabilities</h4>
<div class="outline-text-4" id="text-8-4-2">
<ul class="org-ul">
<li>实际上,计算经验型概率(computing empirical probabilities) 用的公式与 分析型概率(analytical
probabilities) 相同(见公式 8.1).</li>
<li>唯一的区别是:
<ul class="org-ul">
<li>集合 X 来自数据,而不是来自我们对世界的先验知识(例如:骰子有六个面),</li>
<li>或者不是来自题目描述(例如:瓶子里有 40 颗蓝色弹珠).</li>
</ul></li>
<li>不过,在计算经验型概率时,还需要额外考虑两个方面:
<ul class="org-ul">
<li>数据类型(data type)</li>
<li>概率的解释(interpretation)</li>
</ul></li>
<li><b>Requirements on data to compute probabilities</b></li>
<li>要从 <b>经验数据(empirical data)</b> 计算概率,有两个必要条件:</li>
<li>第一个条件:数据必须是数值-离散型,有序型(ordinal)或类别型(categorical);我们不能直接在 区间型
(interval)或比率型(ratio)据上计算概率.</li>
<li>为了理解这个条件,假设我们想知道一只企鹅体重的经验概率.
<ul class="org-ul">
<li>因为我们没有企鹅体重的完美数学模型,所以我们收集了 100 只企鹅的体重数据.</li>
<li>体重是数值-比率型数据,我们可以问:"随机选择一只企鹅,它的体重恰好是 3.43721410851 千克的概率是多少?"</li>
</ul></li>
<li>这个问题显然没有意义.实际上,在这种精度下概率几乎为零:
<ul class="org-ul">
<li>如果我们找到一只企鹅的体重是 3.43721410852 千克,</li>
<li>严格来说,这与我们指定的体重是不同的数字.</li>
</ul></li>
<li>那该怎么处理这种问题呢?</li>
<li>你可能已经猜到了:
<ul class="org-ul">
<li>将体重数据分组(分箱,binning).</li>
<li>与其问一个精度荒谬的问题,不如考虑一个体重区间,比如:</li>
</ul></li>
<li>随机选择一只企鹅,它的体重在 3.0 千克到 3.1 千克之间 的概率是多少?</li>
<li>这个问题如果我们有数据,就可以计算.</li>
<li>关键在于:区间型或比率型数据需要转换成离散区间,就像你在制作直方图时会做的那样.</li>
<li>稍微正式一点的解释是:
<ul class="org-ul">
<li>比率型和区间型数据的精度可以是任意的(arbitrary precision),因此它们是不可数的(uncountable),</li>
<li>这意味着我们无法计算公式 8.1 的分子.</li>
<li>计算概率必须依靠对事件的计数.</li>
</ul></li>
<li>第二个条件:数据必须有互斥(mutually exclusive)的标签或分箱.</li>
<li>原因是:所有事件的概率之和必须等于 100%(或在不转换为百分比时等于 1).</li>
<li>这个条件在我之前举的所有例子中都是成立的:抛硬币,掷骰子,抽牌,企鹅体重.
<ul class="org-ul">
<li>如果骰子掷出"4",它不可能同时是"2";</li>
<li>如果企鹅体重在 3.0–3.1 千克,它不可能同时在 2.8–2.9 千克.</li>
</ul></li>
<li>反例:
<ul class="org-ul">
<li>想一想第 3 章里那个虚构的实验&#x2013;调查人们从哪里获取新闻.</li>
<li>我们能从那个数据表中计算概率吗?答案是不行,因为标签不是互斥的:</li>
<li>一个人既可以从互联网获取新闻,也可以从电视获取新闻,</li>
<li>所有概率的总和会大于 100%.</li>
</ul></li>
<li>当然,我们可以重新设计这个实验,使其可以计算概率:
<ul class="org-ul">
<li>例如,调查问卷只要求受访者报告一个主要新闻来源,</li>
<li>这样答案就是互斥的,就可以计算概率了.</li>
</ul></li>
<li><b>Interpreting empirical probabilities</b></li>
<li>从某种意义上说,解释**经验型概率(empirical probabilities)很简单,它和解释分析型概率(analytical
probabilities)**是一样的:</li>
<li>一个事件发生的概率,就是该事件发生的可能性.</li>
<li>但是,解释经验型概率有一些分析型概率没有的困难:</li>
<li>第一个困难:
<ul class="org-ul">
<li>经验型概率是基于**样本(samples)的,不同样本可能因抽样变异性(sampling variability)**而得到不同
的经验型概率.</li>
<li>我将在下一章更详细地讨论这个概念,但它的核心思想是:</li>
<li>从同一总体随机抽取的不同样本,很可能至少有一些差别.</li>
<li>因此,经验型概率在不同样本之间总会有一定差异.</li>
</ul></li>
<li>解决这个问题的方法包括:
<ul class="org-ul">
<li>采集更大的样本</li>
<li>计算置信区间(confidence intervals)</li>
<li>但事实是,经验型概率几乎总是带有一定的不确定性.</li>
</ul></li>
<li>第二个困难:
<ul class="org-ul">
<li>经验型概率在数值上往往与**比例(proportion)**相同.</li>
</ul></li>
<li>正如我在本章前面讨论过的:
<ul class="org-ul">
<li>概率是对世界中事件发生不确定性的度量</li>
<li>比例是对已经发生的事情进行计数</li>
<li>当用同样的公式在同一组数据上计算比例和经验型概率时,容易产生混淆,因为它们实际上代表了不同的概念.</li>
</ul></li>
<li>因此,结论是:
<ul class="org-ul">
<li>在可以计算分析型概率的情况下,优先使用分析型概率</li>
<li>在无法直接得到分析型概率时,再使用经验型概率</li>
</ul></li>
<li>我们将在第 13 章(置信区间)和第 16 章(基于置换的统计)中再次回到分析型 vs 经验型概率的区别.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org0a74a57" class="outline-3">
<h3 id="org0a74a57"><span class="section-number-3">8.5.</span> Probability functions, mass, and density</h3>
<div class="outline-text-3" id="text-8-5">
<ul class="org-ul">
<li><p>
我们要先区分一下如下的几个概念:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Probability</th>
<th scope="col" class="org-left">The chance that an event will occur</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Probability function</td>
<td class="org-left">A mathematical expression that relates each element in a set to</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">a numerical probability value</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">Probability mass</td>
<td class="org-left">A function that describes probabilities for a set of exclusive</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">discrete events</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">Probability density</td>
<td class="org-left">A function that describes probabilities for exclusive continuous</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">event</td>
</tr>
</tbody>
</table></li>
<li>概率函数将一个集合(即所考虑的所有可能结果)中的每个元素与其对应的概率联系起来.为了让这个定义更精确,
<ul class="org-ul">
<li><p>
我将提供一个数学定义.
</p>
\begin{equation}
f(x)=p(X=x) \tag{8.5}
\end{equation}</li>
<li>请注意大写 X 和小写 x 的区别:前者是变量所有取值的集合,而后者是该集合中的一个特定取值(另见公式 8.1).</li>
<li>我知道使用 X 和 x 容易让人困惑,但数学符号有时就是这样令人困惑,这只能去适应.</li>
<li>无论如何,这个公式定义了一个函数 f,它将事件的概率映射到分配给该事件的数值.</li>
</ul></li>
<li>对于每个独特的取值  \(x_{i}\)  来概念化这个函数可能会有所帮助.这也有助于理解 f 成为概率函数所需满足
的要求(在下面的公式中,s.t. 是"满足"的缩写).
<ul class="org-ul">
<li><p>
公式 8.6 是概率函数在单个取值层面上的定义,其后两个陈述适用于此定义.
</p>
\begin{equation}
f(x_{i})=p(X=x_{i}) \tag{8.6}
\end{equation}</li>
<li><p>
公式 8.7 说明概率值是非负的;
</p>
\begin{equation}
s.t.\,p(X=x_{i})\geq 0 \tag{8.7}
\end{equation}</li>
<li><p>
公式 8.8 说明集合 X 内的事件必须是互斥的(在  f(x<sub>i</sub>)  处,除  x<sub>i</sub>  之外的任何其他事件的概率值为零);
</p>
\begin{equation}
s.t.\,p(X\neq x_{i})=0 \tag{8.8}
\end{equation}</li>
<li><p>
最后,公式 8.9 说明所有概率的总和必须等于 1.
</p>
\begin{equation}
s.t.\,\sum_{i=1}^{n}p(X=x_{i})=1 \tag{8.9}
\end{equation}</li>
<li>总之,这些公式共同形式化地表达了到目前为止我希望您通过本章学习已经能够直观理解的内容.</li>
</ul></li>
<li>概率质量(Probability Mass)与概率密度(Probability Density)是不同类型的概率函数.两者之间的区别很简单:
<ul class="org-ul">
<li>质量针对离散事件,</li>
<li>而密度针对连续事件.</li>
</ul></li>
<li>您可以从数据类型的角度来理解:
<ul class="org-ul">
<li>概率质量针对分类数据,</li>
<li>而概率密度针对数值数据.</li>
</ul></li>
<li>概率质量函数用于描述离散事件,例如抛硬币,掷骰子,考试通过/不通过或疾病结果等.</li>
<li>相反,概率密度函数用于描述连续事件,例如身高,财富,距离和温度.</li>
<li>概率质量和概率密度的可视化方式也不同:概率质量函数采用条形图进行可视化,</li>
<li>而概率密度函数则采用折线图进行可视化.</li>
<li>图8.4展示了一些示例(均为模拟数据).
<ul class="org-ul">
<li><p>
如图
</p>

<div id="org08dd292" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/8-4.png" alt="8-4.png" />
</p>
<p><span class="figure-number">Figure 84: </span>ms/8-4.png</p>
</div></li>
<li>图A显示了不同类型汽车的概率函数.汽车类型是分类数据且无内在排序关系,因此其概率函数采用柱状图进行可视化.</li>
<li>图B展示了来自100人样本的实证智商数据.虽然智商存在已知的解析分布(N(100, 152)),但有限样本量的数
据集经过离散化处理,使得该概率函数表现为概率质量&#x2013;不过我们可以将此质量视为概率密度的经验估计.</li>
<li>最后,图C显示了总体智商的理论分布,该分布源于解析函数,因此属于概率密度函数</li>
</ul></li>
<li>你之所以将图B中的直方图称为概率函数而非比例,体现了这两个概念在实证数据中容易混淆的特性.</li>
<li>需要明确的是,该直方图确实展示了抽样人群的分箱IQ比例(若为真实数据).
<ul class="org-ul">
<li>但当你提问"从该群体中随机抽取一人,其IQ介于98至102的概率是多少?"时,你可将此图视作概率函数使用.</li>
<li>反之,若你提问"该群体中IQ介于98至102之间的人数有多少?",则该图可被解读为展示比例</li>
</ul></li>
<li>此处存在一个微妙但关键的区别:
<ul class="org-ul">
<li>当你将图B解释为比例时,你仅是在描述这个特定数据样本的特征,无法据此推断总体人群的其他特征.</li>
<li>但若你将其视为概率质量函数,你便能推断样本外人群(假设该样本是随机且能代表总体)拥有特定IQ值的概率</li>
</ul></li>
<li>我将在后续两章中更详细地阐述这一区别.</li>
<li>回到质量函数与密度函数:在计算机应用统计中,
<ul class="org-ul">
<li>连续变量通常通过分箱进行离散化处理.</li>
<li>这意味着在实际应用中,概率函数&#x2013;尤其是经验推导得到的&#x2013;在技术上是以质量而非密度的形式呈现,即使该
变量在纯数学领域是连续的.</li>
<li>将概率质量理解为概率密度的估计值通常很有帮助</li>
</ul></li>
<li>这里有一个例子:
<ul class="org-ul">
<li>在我的想象中,我前往南极洲称量我能找到的最可爱的企鹅.我买了一台能测量到 10 −16千克的超高精度秤.</li>
<li>现在我想计算描述企鹅体重的经验概率函数.问题是:测量到一只体重恰好为 34.5368513085760812 千克的企
鹅的概率基本为零.</li>
<li>事实上,以如此精度测量到任何体重的任何企鹅的概率都是零.</li>
<li><p>
这显然不太有用.因此,我转而将数据离散化为宽度为 0.25 千克的分箱(图 8.5).
</p>

<div id="orgd95823f" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/8-5.png" alt="8-5.png" />
</p>
<p><span class="figure-number">Figure 85: </span>ms/8-5.png</p>
</div></li>
<li>我所做的是将现实世界中的比率数据转换为数字化的离散数值数据.我将要计算的概率函数在技术上是一个概
率质量,但它是对密度函数的经验估计.</li>
<li>通过分箱我们失去了一些精度,但概率质量函数将更易于解释和处理.</li>
</ul></li>
<li>在纯数学中的对应概念是:
<ul class="org-ul">
<li>你无法在单一点上计算连续函数的离散积分,只能在一个区间上进行积分.</li>
<li>因此,要计算一个离散积分,你需要选择一个合理的,较小的数值范围进行积分,而这个范围就对应于一个直
方图分箱的边界.</li>
</ul></li>
<li>本节的结论是:
<ul class="org-ul">
<li>概率密度是连续的数学函数.</li>
<li>在第 10 章,你将学习如何在使用推断统计分析时运用概率密度函数来评估统计显著性.</li>
<li>当处理经验数据时,你使用的是概率质量函数,
<ol class="org-ol">
<li>这要么是因为数据本身就是分类数据,</li>
<li>要么是因为你将连续数据进行了分箱处理.对于后一种情况,概率质量函数是对概率密度函数的一种估计</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orged92777" class="outline-3">
<h3 id="orged92777"><span class="section-number-3">8.6.</span> Cumulative distribution function (cdf)</h3>
<div class="outline-text-3" id="text-8-6">
<ul class="org-ul">
<li>累积分布函数是概率质量函数或概率密度函数的累积和(对于密度函数,求和替换为积分).</li>
<li>这意味着 cdf 中的每个点表示获取小于或等于该值的概率.
<ul class="org-ul">
<li><p>
换句话说:
</p>
\begin{equation}
F(x)=p(X\leq x) \tag{8.10}
\end{equation}</li>
<li>请注意公式 8.5 与 8.10 之间的微妙区别:它们几乎完全相同,唯一的差异在于 pdf 中的等号在 cdf 中变为
小于等于号.这一符号的差异产生重要影响:
<ol class="org-ol">
<li>pdf 表示随机选取的数据点等于某一特定值的概率,</li>
<li>而 cdf 表示随机选取的数据点的值小于等于某一特定值的概率.</li>
</ol></li>
<li><p>
图 8.6 展示了正态分布的 pdf 与 cdf 之间的区别与联系.
</p>

<div id="org1325a67" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/8-6.png" alt="8-6.png" />
</p>
<p><span class="figure-number">Figure 86: </span>ms/8-6.png</p>
</div></li>
<li>注:图 8.6 中的虚线并非严格意义上的 pdf;其 y 轴数值经过缩放以便与 cdf 进行直观比较.其分布形状是
准确的,但 y 轴上的数值被放大.</li>
</ul></li>
<li>将其转换为概率密度函数只需按x轴网格间距(在解析统计学中即为dx)进行缩放.您将在练习1中对此进行探索.
<ul class="org-ul">
<li><p>
图8.7展示了一些概率密度函数及其对应累积分布函数的示例.
</p>

<div id="orgd112232" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/8-7.png" alt="8-7.png" />
</p>
<p><span class="figure-number">Figure 87: </span>ms/8-7.png</p>
</div></li>
<li>注意,所有累积分布函数都具有从0开始,到1结束,并在此范围内单调递增的共同特征.</li>
<li>事实上,累积分布函数永远不会下降,因为它反映的是非负值的累积求和.</li>
</ul></li>
<li>累积分布函数的用途是什么? 在两种情况下会使用累积分布函数:
<ul class="org-ul">
<li>第一种是当您想确定样本小于等于(或大于)某个特定值的概率时.例如:随机选择的一只可爱企鹅体重至少
为5公斤的概率是多少?</li>
<li>但在推断统计中,累积分布函数更重要的用途是计算p值.p值是现代统计评估与报告的核心关键.所以,是的,
累积分布函数非常重要.</li>
</ul></li>
<li>本节的最后一点:
<ul class="org-ul">
<li>由于累积分布函数中的每个点都定义为左侧概率的总和,因此累积分布函数仅对能够进行有意义的数值排序的
数据可解释.</li>
<li>对于名义数据(不可排序的类别)计算累积分布函数是没有意义的(例如,在计算电影类型的累积和时,哪种
类型算在"科幻片"的左侧呢?).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga20b75b" class="outline-3">
<h3 id="orga20b75b"><span class="section-number-3">8.7.</span> Expected value</h3>
<div class="outline-text-3" id="text-8-7">
<ul class="org-ul">
<li>在本节中,我将定义*期望值*,解释如何推导和理解它,以及它与平均值和方差等量之间的关系.</li>
<li>请看看下面的平均值和期望值公式(在公式中,E[X] 表示变量 X 的期望值,p_i 表示取到数据点 x_i 的概率).
请花一点时间检查这两个公式,以确定:
<ol class="org-ol">
<li>它们的区别;</li>
<li>它们在什么情况下会收敛为相同的值.</li>
</ol></li>
<li><p>
公式8-11:
</p>
\begin{equation}
\overline{X} = \sum_{i=1}^n \frac{x_i}{n} \quad (8.11)
\end{equation}</li>
<li><p>
公式8-12:
</p>
\begin{equation}
E[X] = \sum_{i=1}^n x_i p_i \quad (8.12)
\end{equation}</li>
<li>我先从它们的区别开始:
<ul class="org-ul">
<li>平均值公式涉及用样本总数来做除法,而期望值公式涉及将数值乘以概率.</li>
<li>事实上,可以将期望值视为一种 <b>加权平均</b> ,其中权重由每个值出现的概率决定.</li>
</ul></li>
<li>这种观察引出了两个公式相等的条件:
<ul class="org-ul">
<li>当 \(( p_i = 1/n )\) 时,即所有数据值被选中的概率相等,两者会相等.</li>
<li>当 \(( p_i \neq 1/n )\) 时,如果分布是对称的,使得均值左右的概率总和相同,平均值与期望值也会相等.</li>
</ul></li>
<li>但还有一个更重要的区别,这个区别在公式中并不明显:
<ul class="org-ul">
<li>期望值是根据分布进行的理论计算,而平均值是根据数据样本得到的经验性描述统计量.</li>
<li>计算期望值不需要实际数据,只需要概率分布;而计算平均值则必须依赖数据样本.</li>
</ul></li>
<li>示例:加权骰子的期望值与平均值
<ul class="org-ul">
<li><p>
假设我们有一个加权骰子,不同的面出现的概率如图 8.8 所示.
</p>

<div id="orgc908a4a" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/8-8.png" alt="8-8.png" />
</p>
<p><span class="figure-number">Figure 88: </span>ms/8-8.png</p>
</div></li>
<li>我们可以很快验证 "Prob" 列的值满足概率质量函数的要求:所有值在 0 到 1 之间并且总和为 1,并且所有
事件相互排斥.</li>
<li><p>
我们可以用公式 (8.12) 计算它的期望值:
</p>
\begin{equation}
E[x] = \frac{1}{4} + \frac{2}{4} + \frac{3}{8} + \frac{4}{8} + \frac{5}{8} + \frac{6}{8} = 3
\end{equation}</li>
<li>分子是每个面的数值,分母是对应概率</li>
<li>该骰子的期望值为 3,而无权骰子的期望值为 3.5.</li>
</ul></li>
<li>我们也可以通过实际掷骰子若干次来计算平均值.
<ul class="org-ul">
<li><p>
<b>假设</b> 我们掷了 8 次,结果是:
</p>
\begin{equation}
x = [1, 3, 4, 4, 4, 3, 2, 5]
\end{equation}</li>
<li>那么平均值为:\(\overline{x} = 3.25\)</li>
<li>到了本书的这一部分,我相信你已经不再惊讶于样本描述性统计量并不等同于期望值这一事实</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orgd9cc4fa" class="outline-4">
<h4 id="orgd9cc4fa"><span class="section-number-4">8.7.1.</span> Computing expected value</h4>
<div class="outline-text-4" id="text-8-7-1">
<ul class="org-ul">
<li>计算一个分布的期望值最精确的方法是进行**解析计算**.
<ul class="org-ul">
<li>对于那些在数学上有明确特征描述的分布,这是可行的.</li>
</ul></li>
<li>作为例子,这里推导 <b>均匀分布</b> 的期望值.
<ul class="org-ul">
<li>解析形式的均匀分布是 <b>连续</b> 的,因此我们使用 <b>积分</b> 而不是求和.</li>
<li><p>
推导过程如下
</p>
\begin{align*}
E[X] &= \int_a^b \frac{x}{b-a} \, dx \tag{8.13}\\
     &= \frac{1}{b-a} \int_a^b x \, dx \tag{8.14}\\
     &= \left( \frac{1}{b-a} \cdot \frac{x^2}{2} \right) \Bigg|_{a}^{b} \tag{8.15}\\
     &= \left( \frac{1}{b-a} \right) \cdot \frac{b^2 - a^2}{2} \tag{8.16}\\
     &= \frac{1}{b-a} \cdot \frac{(b-a)(b+a)}{2} \tag{8.17}\\
     &= \frac{b + a}{2} \tag{8.18}
\end{align*}</li>
<li><b>结论</b>:公式 (8.18) 表明,均匀分布的期望值就是其区间边界的平均值.</li>
<li>这很巧妙,因为我们刚刚严谨地证明了在第 5 章中凭直觉得到的一个结果.</li>
</ul></li>
<li><b>Estimating expected value</b> 在现实世界中,许多数据分布及其概率密度函数(pdf)是未知的.</li>
<li>如果你想要从数据中估计期望值,有两种方法可选:
<ul class="org-ul">
<li>你可以查看数据的直方图,并对其分布做出合理猜测,然后计算该解析分布的期望值.(例如,许多经验分布
看起来像高斯分布,因此你可以使用与经验数据具有相同均值和标准差的解析高斯分布来计算期望值.)</li>
<li>你可以计算数据的加权平均值,其中权重来自各个数值出现的比例(使用经验比例作为概率的估计).</li>
</ul></li>
<li>需要记住的是,经验平均值的精确度不如解析计算的期望值.</li>
<li>实际上,可以将期望值看作样本量为无穷大的数据集的平均值;任何有限样本量得到的结果,都只是一个估计值.</li>
</ul>
</div>
</div>
<div id="outline-container-org898ac73" class="outline-4">
<h4 id="org898ac73"><span class="section-number-4">8.7.2.</span> Expected value and statistical moments</h4>
<div class="outline-text-4" id="text-8-7-2">
<ul class="org-ul">
<li>分布的统计矩可以表示为数据的期望值,其幂次为矩的阶数. 例如:
<ul class="org-ul">
<li>第一阶矩是 \(( E[X] )\)</li>
<li>第二阶矩是 \(( E[X^2] )\)</li>
<li>更一般地,第 \(( k )\) 阶矩是 \(( E[X^k] )\)</li>
</ul></li>
<li>这些称为 <b>非标准化矩(unstandardized moments)</b></li>
<li><p>
我们可以通过分布的**以均值为中心的第二阶矩**来计算它的期望方差:
</p>
\begin{align*}
\sigma_X^2 &= E\left[ \left( X - E[X] \right)^2 \right] \tag{8.19} \\
           &= E\left[ X^2 \right] - \left( E[X] \right)^2 \tag{8.20}
\end{align*}</li>
<li>我将以均匀分布为例来讲解这个公式.
<ul class="org-ul">
<li>我们已经推导出均匀分布的期望值(第一阶矩),因此公式 8.20 中的减法项就是公式 8.18 的平方.</li>
<li>接下来我们只需要对第一项进行积分来求其值.</li>
</ul></li>
<li><p>
我们首先计算均匀分布的二阶矩(即 \((E[X^2])\)):
</p>
\begin{align*}
E[X^2] &= \int_a^b \frac{x^2}{b-a} \, dx \tag{8.21} \\
       &= \left( \frac{1}{b-a} \cdot \frac{x^3}{3} \right) \Bigg|_{a}^{b} \tag{8.22} \\
       &= \frac{1}{b-a} \cdot \frac{b^3 - a^3}{3} \tag{8.23}
\end{align*}</li>
<li><p>
现在结合一阶矩与二阶矩来计算方差:
</p>
\begin{align*}
\sigma_X^2 &= \frac{b^3 - a^3}{3(b-a)} - \left( \frac{b + a}{2} \right)^2 \tag{8.24} \\
           &= \frac{b^3 - a^3}{3(b-a)} - \frac{b^2 + a^2 + 2ab}{4} \tag{8.25} \\
           &= \frac{4(b^3 - a^3)}{12(b-a)} - \frac{3(b-a)(b^2 + a^2 + 2ab)}{12(b-a)} \tag{8.26} \\
           &= \text{thick algbra here...} \tag{8.27} \\
           &= \frac{b^2 - a^2}{12} \tag{8.28}
\end{align*}</li>
<li><b>说明*</b>
<ul class="org-ul">
<li>我省略了部分代数运算步骤,因为在进行因式分解,化简和约分时会变得复杂.</li>
<li>分母中出现的 <b><b>12</b></b> 来源于组合两个分母分别是 3 和 4 的分数.</li>
<li>如果你喜欢这种深入公式的推导,可以进一步探索概率论的数学处理方法;</li>
<li>如果你觉得枯燥或难懂,也不用担心&#x2013;本书其余部分更注重直觉和实际应用.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org03b2234" class="outline-3">
<h3 id="org03b2234"><span class="section-number-3">8.8.</span> Softmax</h3>
<div class="outline-text-3" id="text-8-8">
<ul class="org-ul">
<li>Softmax 是一种将一组数值转换为概率质量(probability mass)的**数学变换**.</li>
<li>它在机器学习中被广泛使用,而在统计学中使用较少. 但是 Softmax 运算非常重要,值得花时间去理解.
<ul class="org-ul">
<li>通过 Softmax 运算得到的概率质量具有**非标准化的解释**,因为它并不直接映射到某事件发生的概率.</li>
<li>相反,Softmax 函数会将表示预测的数值转换为一组概率分布,从而表示某个决策被作出的概率.</li>
</ul></li>
<li>注意:
<ul class="org-ul">
<li>向量 \((x)\) 中的数值不必是计数(counts)或比例(proportions),甚至可以是负数.</li>
<li><p>
Softmax 方程如下:
</p>
\begin{equation}
\sigma(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} \tag{8.29}
\end{equation}</li>
<li>这里 Softmax 方程的意义是:</li>
<li>每个数据值 \((x_i)\) 经过 Softmax "变换"后,等于它的自然指数值与所有数据自然指数值之和的比值.</li>
<li>希腊字母 \((\sigma)\) 常用于表示 Softmax 函数 &#x2013; 不要与标准差符号 \((\sigma_x)\) 混淆.</li>
</ul></li>
<li>Softmax 的特性
<ul class="org-ul">
<li>原始数值增加是线性的,但 Softmax 变换后的值是指数级变化的.</li>
<li>Softmax 变换后的所有输出值之和等于 1,这满足概率分布的一个基本要求.</li>
<li><p>
图 8.9 展示了一个数值示例,验证了以上性质.
</p>

<div id="org7e7b7d7" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/8-9.png" alt="8-9.png" />
</p>
<p><span class="figure-number">Figure 89: </span>ms/8-9.png</p>
</div></li>
<li><p>
图 8.10 展示了在更大范围数值上的 Softmax 变换.
</p>

<div id="orgcea7847" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/8-10.png" alt="8-10.png" />
</p>
<p><span class="figure-number">Figure 90: </span>ms/8-10.png</p>
</div></li>
<li>对于小于 2 的数值,该变换看起来趋于平坦,但实际上这些数值并非为零,而是较小的正数.</li>
</ul></li>
<li>现在我已经展示了 Softmax 的公式和一些示例,接下来让我回答你可能会对 Softmax 函数产生的两个问题.</li>
</ul>
</div>
<div id="outline-container-org914a240" class="outline-4">
<h4 id="org914a240"><span class="section-number-4">8.8.1.</span> Does \(\sigma(x)\) really produce a probability mass function</h4>
<div class="outline-text-4" id="text-8-8-1">
<ul class="org-ul">
<li>是的,因为它满足概率分布的要求:
<ul class="org-ul">
<li>事件是互斥的(mutually exclusive).</li>
<li>每个 \((\sigma(x_i))\) 的值介于 0 和 1 之间(自然指数函数的值为正,且除以总和使最大值为 1).</li>
<li>所有 \((\sigma)\) 值的总和等于 1.</li>
</ul></li>
<li><p>
最后这个结论可以通过对所有 \((\sigma)\) 值求和来证明:
</p>
\begin{equation}
\sum_{i=1}^{n} \sigma(x_i) =
\sum_{i=1}^{n} \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} =
\frac{\sum_{i=1}^{n} e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} \tag{8.30}
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-org80e84c7" class="outline-4">
<h4 id="org80e84c7"><span class="section-number-4">8.8.2.</span> What is softmax used for?</h4>
<div class="outline-text-4" id="text-8-8-2">
<ul class="org-ul">
<li>Softmax 常用于机器学习中的分类任务(classification tasks).
<ul class="org-ul">
<li>思想是分类器会将样本数据转换为每个类别的数值(例如将一组图像像素强度值转换为标签,如"猫 vs 狗 vs
大象").</li>
<li>然后这些数值会通过 Softmax 函数转化为概率质量,最后根据概率值随机选择一个标签.</li>
</ul></li>
<li>示例:医学影像分类假设一个分类器使用医学影像数据来预测患者是否:
<ul class="org-ul">
<li>没有肿瘤</li>
<li>良性肿瘤</li>
<li>恶性肿瘤</li>
</ul></li>
<li>对于某位患者,
<ul class="org-ul">
<li><p>
分类器产生了图 8.9 所示的数值输出
</p>

<div id="org6edccc5" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/8-9.png" alt="8-9.png" />
</p>
<p><span class="figure-number">Figure 91: </span>ms/8-9.png</p>
</div></li>
<li>这些输出经过 Softmax 转换后,结果为:
<ol class="org-ol">
<li>4% 的概率没有肿瘤</li>
<li>11% 的概率为良性肿瘤</li>
<li>84% 的概率为恶性肿瘤</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org07f6afe" class="outline-2">
<h2 id="org07f6afe"><span class="section-number-2">9.</span> Chapter 09: Sampling And Distributions</h2>
<div class="outline-text-2" id="text-9">
</div>
<div id="outline-container-org7feb53e" class="outline-3">
<h3 id="org7feb53e"><span class="section-number-3">9.1.</span> Sampling variability and its annoyances</h3>
<div class="outline-text-3" id="text-9-1">
<ul class="org-ul">
<li>假设你是一名数据科学家,被一家健身公司聘请来研究男性和女性在一天中不同时间锻炼的可能性差异.
<ul class="org-ul">
<li>找到一名男性和一名女性,询问他们喜欢的锻炼时间,这是不是很简单?当然,这很简单&#x2013;但是这样的数据是
不能推广的.为什么呢?</li>
<li>你当然知道原因:每个人都有不同的偏好和时间安排;虽然整体上可能存在一些性别差异,但并不是每个男性
或每个女性都具有完全相同的偏好.</li>
</ul></li>
<li>好吧,你决定抽样1000名男性和1000名女性,总样本量为2000人.你计算了均值和标准差&#x2026;&#x2026;
<ul class="org-ul">
<li>但是你的狗把原始数据吃掉了 😅(你没备份原始数据可真该反省,不过这是另外一个话题!)所以你需要再重
新收集2000名不同的人的数据.</li>
</ul></li>
<li>那么,你是否期望新的均值和标准差与第一次的完全相同呢?
<ul class="org-ul">
<li>合理的预期是描述性统计结果会相似,但不会完全数值一致.</li>
</ul></li>
<li>这是因为抽样变异性(sampling variability).抽样变异性是指:
<ul class="org-ul">
<li>不同的样本&#x2013;即使来自同一个总体&#x2013;其描述性统计量(比如均值,标准差)会有不同的数值.换句话说,样本
之间是有差异的.</li>
</ul></li>
<li>这和样本内部的差异性不同(样本内部差异性是用标准差来衡量的).</li>
<li>样本内部 vs. 跨样本的差异性,对于参数估计,推断统计以及置信区间都有重要影响.</li>
<li>跨样本与样本内的变异性也是方差分析(ANOVA)的基础.</li>
<li>你将在之后的章节学习这些内容;现在我只是希望你能理解抽样变异性的概念:
<ul class="org-ul">
<li>不同的样本(每个样本都由多名个体组成)可能有不同的描述性统计量.</li>
</ul></li>
<li>抽样变异性(sampling variability)很让人头疼,因为它体现的是与之前讨论的不同层级的变异性.</li>
<li>回顾我们为什么要测量一个 <b>样本</b> 而不是单个个体:
<ul class="org-ul">
<li>因为单个个体可能并不能代表总体.
<ul class="org-ul">
<li>这就是我们为什么更信任样本的平均值,而不是单一数据点.</li>
</ul></li>
</ul></li>
<li>但是现在我要告诉你:</li>
<li>较大的样本 <b>也可能</b> 不能完全反映总体.</li>
<li>你的研究问题的答案可能会在不同的样本之间发生变化,而不仅仅是在同一个样本中的不同个体之间变化.</li>
<li>换句话说:
<ul class="org-ul">
<li>抽样变异性麻烦之处在于,尽管我们比单个数据点更信任一个样本,但我们仍然不能仅仅因为一个样本包含多
个数据点就完全信任它.</li>
</ul></li>
<li>我希望你有这样的直觉:
<ul class="org-ul">
<li><b>更大的样本</b> 受到抽样变异性的影响比小样本要小.</li>
<li>例如,样本量 \((N=10,000)\) 的描述性统计量更可能彼此相似,而样本量 \((N=10)\) 的统计量差异则更大.</li>
</ul></li>
<li>这种直觉被 <b>大数定律(Law of Large Numbers)</b> 形式化描述,你将在本章的后续内容中学习到它.</li>
</ul>
</div>
<div id="outline-container-org454b3b6" class="outline-4">
<h4 id="org454b3b6"><span class="section-number-4">9.1.1.</span> An example with random data</h4>
<div class="outline-text-4" id="text-9-1-1">
<ul class="org-ul">
<li>我创建了 50 个样本,每个样本包含 500 个随机值.
<ul class="org-ul">
<li><p>
如图9-1
</p>

<div id="org8d64068" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/9-11.png" alt="9-11.png" />
</p>
<p><span class="figure-number">Figure 92: </span>ms/9-11.png</p>
</div></li>
<li>尽管这些样本的总体特征相似,但它们的具体分布和均值却不同.</li>
<li>实际上,即使是"均值的均值"(即 50 个样本均值的平均值)也并不完全等于零&#x2013;而零才是总体期望值.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org2668231" class="outline-4">
<h4 id="org2668231"><span class="section-number-4">9.1.2.</span> Where does sampling variability come from ?</h4>
<div class="outline-text-4" id="text-9-1-2">
<ul class="org-ul">
<li>抽样变异性会对参数估计和统计推断的可靠性产生负面影响.</li>
<li>因此,理解抽样变异性的来源,并考虑是否能减少这种变异性,是非常有用的.</li>

<li><b>自然变异(Natural variation)</b></li>
<li>来自生物学的数据集(从细胞神经科学到心理学,再到医疗治疗和文化习俗)中,变异性很常见.</li>
<li>基因和环境因素的自然差异,会导致不同的人在相同情境中表现出不同反应.</li>
<li>这种变异性有两个含义:
<ul class="org-ul">
<li>不同的样本可能具有不同的特征;</li>
<li>来自不同年龄,性别,经济阶层或文化背景的样本更可能存在差异.</li>
<li>自然变异也存在于非生物系统中,例如:地震震级,每个星系中的恒星数量,城市中建筑物的高度等.</li>
</ul></li>
<li><b>测量噪声(Measurement noise)</b></li>
<li>测量传感器并不完美,会因为噪声而引入变异性.</li>
<li><b>动态变化(Dynamics and changes)</b></li>
<li>自然界是不断变化的.同一系统在不同时刻的测量结果可能不同,仅仅是因为系统发生了变化.
<ul class="org-ul">
<li>例如,人们对地缘政治事件的看法就是一个重复测量结果可能改变的例子.</li>
</ul></li>
<li><b>复杂系统(Complex systems)</b></li>
<li>大多数科学研究对象都是复杂的(简单系统不是早已被研究透彻,就是不值得研究).</li>
<li>而大多数实验的设计是为了简化,减少或忽略复杂性.
<ul class="org-ul">
<li>例如,如果重复对健身时间偏好进行抽样,而忽略一年中的季节变化,就会导致更高的抽样变异性.</li>
</ul></li>
<li><b>随机性(Stochasticity / randomness)</b></li>
<li>宇宙中存在我们无法测量或无法理解的随机性.</li>
<li>在微观尺度上,可以想到布朗运动;在宏观尺度上,可以想到天气变化影响我们的决策.</li>
<li>哪些因素会成为更大的变异来源?哪些是你可以控制的,或者至少可以测量的?</li>
<li>这些问题的答案取决于具体的研究项目,所以我无法给出统一的结论.</li>
<li>但在开始任何研究或统计工作之前,你都应该思考这些问题.
<ul class="org-ul">
<li>你应当努力减少抽样变异性,</li>
<li>但也要避免让样本过于同质化,以至于削弱结果的可推广性(generalizability).</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org5468a37" class="outline-3">
<h3 id="org5468a37"><span class="section-number-3">9.2.</span> Creating sample estimate distributions</h3>
<div class="outline-text-3" id="text-9-2">
<ul class="org-ul">
<li>假设我们从一个总体中抽取 <b>k</b> 个独立样本.每个样本称为 \((S)\),因此第 \((i)\) 个样本记作 \((S_i)\).</li>
<li>由于这些是独立样本,
<ul class="org-ul">
<li>每个样本可能有不同的样本量(sample size).</li>
<li>每个样本也有自己的平均值,我们记作 \((\overline{S_i})\).</li>
</ul></li>
<li>这意味着 <b>k</b> 个样本将产生 <b>k</b> 个平均值.
<ul class="org-ul">
<li><p>
这种情形在图 9.2A 中展示.
</p>

<div id="orgceb05d0" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/9-2.png" alt="9-2.png" />
</p>
<p><span class="figure-number">Figure 93: </span>ms/9-2.png</p>
</div></li>
</ul></li>
<li>每个样本 \((S_i)\) 拥有一个数据分布,可以用直方图表示(图 9.2B 中的细灰线).</li>
<li>因为这些样本来自同一个总体,我们期望重复抽样的分布相似,
<ul class="org-ul">
<li>但由于 <b>抽样变异性</b> (sampling variability),它们的直方图不会完全相同,尤其是样本量较小时.</li>
<li>因此,图 9.2B 展示了来自 <b>k</b> 个数据样本的 <b>k</b> 个直方图,星号表示每个样本的均值.</li>
</ul></li>
<li>另一方面,样本均值形成了一个包含 <b>k</b> 个值的集合,我们可以为这 <b>k</b> 个样本均值绘制直方图.
<ul class="org-ul">
<li><p>
这个直方图并不是直接基于原始数据计算,而是来自每个单独样本的描述性统计量(descriptive statistics).
</p>
<blockquote>
<p>
<b>重要提示</b>:样本均值的分布不同于样本直方图的平均值.
它们甚至可能有完全不同的形状.
更多示例请参阅"中心极限定理(Central Limit Theorem)"章节.
</p>
</blockquote></li>
</ul></li>
<li>样本估计分布的重要性在于:
<ul class="org-ul">
<li>它们能揭示总体中的变异性信息.</li>
<li>它们用于计算 *置信区间*(confidence intervals).</li>
</ul></li>
<li>本节的讨论和图示主要涉及均值,但你也可以针对其他描述性统计量计算样本估计分布,例如:
<ul class="org-ul">
<li>标准差(standard deviation)</li>
<li>中位数(median)</li>
<li>相关系数(correlation coefficient)等.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga6e48ae" class="outline-3">
<h3 id="orga6e48ae"><span class="section-number-3">9.3.</span> Standard error of the mean</h3>
<div class="outline-text-3" id="text-9-3">
<ul class="org-ul">
<li>这是一个难题:
<ul class="org-ul">
<li>样本均值是总体均值的一个估计值,但不同的样本会有不同的均值.</li>
<li>那么,这些估计值的精确度如何呢?</li>
<li>换句话说,样本均值作为总体均值的估计值,其不确定性有多大?</li>
</ul></li>
<li>这个问题就是 <b>均值标准误(Standard Error of the Mean, SEM)</b> 所要解决的.</li>
<li>SEM 是从单个样本计算出来的描述性统计量,用来估计该样本均值对总体均值的估计有多精确.</li>
<li>换句话说,SEM 表示在相同样本量条件下,从总体中随机抽取样本时,样本均值之间可预期的变异量.</li>
<li>它的定义是:
<ul class="org-ul">
<li>总体标准差除以样本量的平方根(即总体标准差按样本量进行缩放)</li>
</ul></li>
<li><p>
均值标准误(Standard Error of the Mean, SEM)公式:
</p>
\begin{equation}
\text{SEM} = \frac{\sigma}{\sqrt{N}} \tag{9.1}
\end{equation}</li>
<li>其中:
<ul class="org-ul">
<li>\((\sigma)\) 为总体标准差(true population standard deviation)</li>
<li>\((N)\) 为样本量(sample size)</li>
</ul></li>
<li>公式的直观含义
<ul class="org-ul">
<li>当样本量 \((N)\) 增加时,SEM 减小,表示样本均值对总体均值的估计更可靠.</li>
<li>当总体标准差 \((\sigma)\) 变小时,表示总体更加同质化(homogeneous),因此我们对总体均值的估计更准确.</li>
</ul></li>
<li><p>
在实际情况中,我们通常不知道总体标准差 \((\sigma)\),因此使用样本标准差 \((s)\) 来估计:
</p>
\begin{equation}
\text{SEM} \approx \frac{s}{\sqrt{N}} \tag{9.2}
\end{equation}</li>
<li>其中:
<ul class="org-ul">
<li>\((s)\) 是样本的标准差(sample standard deviation)</li>
<li>当样本是随机且具有代表性,并且样本量足够大时,我们认为 \((s)\) 是 \((\sigma)\) 的一个较好估计值.</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org6473f33" class="outline-4">
<h4 id="org6473f33"><span class="section-number-4">9.3.1.</span> Standard error of the mean vs. standard deviation</h4>
<div class="outline-text-4" id="text-9-3-1">
<ul class="org-ul">
<li>标准差(Standard Deviation, SD)与均值标准误(Standard Error of the Mean, SEM)在概念上是相似的,
<ul class="org-ul">
<li>因为它们都用于量化与样本数据集相关的变异性.</li>
<li>它们在数学上也相似,因为一个只是另一个的缩放版本.</li>
</ul></li>
<li>但是,它们反映的是不确定性的不同方面,并且具有不同的解释.这些区别将在下面进行说明.</li>
<li><b>概念意义(Conceptual meaning)</b>
<ul class="org-ul">
<li>标准差(SD):量化样本数据值围绕样本均值的分散程度.</li>
<li>SEM:量化样本均值作为总体均值估计值的精确度,即样本均值与未知总体均值的接近程度.</li>
</ul></li>
<li>另一种理解:
<ul class="org-ul">
<li>SD 反映 <b>样本内部</b> 的实际变异性.</li>
<li>SEM 反映 <b>多个样本均值之间</b> 的预期变异性.</li>
</ul></li>
<li><b>计算方法(Calculation)</b>
<ul class="org-ul">
<li>标准差(SD):基于每个数据点计算,为各数据点与样本均值的平方差的平均值再开平方.</li>
<li><p>
均值标准误(SEM):
</p>
\begin{equation}
  \text{SEM} = \frac{\text{标准差}}{\sqrt{N}}
\end{equation}</li>
<li>假设样本均值是总体中无数可能样本均值之一.</li>
</ul></li>
<li>为什么 SEM 除以 \((N)\) 而不是 \((N-1)\):
<ul class="org-ul">
<li>因为 SEM 不是描述单一样本的统计量,而是估计样本均值对应的 <b>大量样本均值的理论分布</b> 的不确定性.</li>
</ul></li>
<li><b>应用场景(Applications)</b></li>
<li>SD:
<ul class="org-ul">
<li>衡量样本数据内部变异性.</li>
<li>用于数据转换(如 z-score 标准化).</li>
</ul></li>
<li>SEM:
<ul class="org-ul">
<li>生成置信区间(confidence intervals).</li>
<li>评估 t 值,回归系数等统计显著性.</li>
</ul></li>
<li><b>样本量的影响Impact of sample siez</b></li>
<li>SD:
<ul class="org-ul">
<li>不会直接随样本量变化而改变.</li>
<li>大样本能提供更准确的总体 SD 估计,但公式中分母的 \((N-1)\) 抵消了分子上的累加,因此数值不会简化地变化.</li>
</ul></li>
<li>SEM:
<ul class="org-ul">
<li>随样本量 \((N)\) 增加而减小,即使 SD 不变(实际上总体标准差不会随样本量变化).</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org683fbdf" class="outline-3">
<h3 id="org683fbdf"><span class="section-number-3">9.4.</span> Random and representative sampling</h3>
<div class="outline-text-3" id="text-9-4">
<ul class="org-ul">
<li>推论统计(inferential statistics) 的整个目的在于评估某个样本的特征是否能推广到整个总体.</li>
<li>因此,确保数据集来自 <b>随机抽样且具有代表性</b> ,是实现有效推广的关键.</li>
<li><b>代表性样本(Representative samples)</b></li>
<li>示例 1(极端例子):
<ul class="org-ul">
<li>假设我们有一个数据集,记录美国家庭每年在垃圾食品上的花费.</li>
<li>能否用这个数据集去预测某个半径范围内黑洞的恒星质量?显然不行,因为该数据(美国家庭消费)并不代表
我们要研究的总体(恒星质量).</li>
</ul></li>
<li>示例 2(合理但不合适):
<ul class="org-ul">
<li>假设我们有一个数据集,记录大学生对"全民基本收入"(UBI)的看法.</li>
<li>能否用该数据集推断所有人的 UBI 观点?</li>
<li>不行,因为大学生与整体人口在年龄,教育程度,社会经济特征等方面不同.</li>
<li>但我们可以合理地预期,这个数据集的结果可以推广到其他大学生群体(包括其他大学或其他国家的大学生).</li>
</ul></li>
<li>示例 3(可能可推广):
<ul class="org-ul">
<li>这些大学生的数据来自一个视觉感知任务&#x2013;匹配两个三维旋转形状.</li>
<li>这样的数据是否可推广到所有人?这更有可能,因为低层次的视觉感知受教育程度的影响较小.</li>
</ul></li>
<li>样本结果能否推广到更大总体,取决于样本是否具有代表性.有时很容易判断样本的代表性,但有时需要仔细推
敲,甚至进一步研究.</li>

<li><b>随机抽样(Random samples)</b>
<ul class="org-ul">
<li>随机抽样是一种确保样本具有代表性的方法.</li>
<li>理想情况下,数据采集应当来自总体中**随机抽取**的个体.</li>
</ul></li>
<li>重新审视 UBI 示例:
<ul class="org-ul">
<li>如果所有受访学生都修读了一个名为"经济公平"的课程(文理学院),其观点可能与整体大学生群体差异很大.</li>
<li>如果受访学生修读的是某精英商学院的"税收与资本"课程,其观点同样不太可能代表整个大学生群体.</li>
</ul></li>
<li>数据集不具代表性,意味着结果无法推广到目标总体.</li>
<li>解决方案:确保样本随机从目标总体抽取.</li>
<li>招募研究参与者的途径常常会引入偏差:
<ul class="org-ul">
<li>提供金钱奖励 → 选择了需要钱且有时间的人.</li>
<li>提供课程学分 → 可能选择的是心理学专业学生.</li>
<li>在白天超市招募 → 选择了退休人员或没有工资工作的人.</li>
<li>随机拨打电话 → 选择了有固定电话且在家的人.</li>
<li>网络广告 → 选择了访问特定网站且有时间和兴趣点击的人.</li>
</ul></li>
<li>这些只是涉及人类参与者的例子,其他实证科学领域也面临同样的问题.</li>
<li>收集随机且具有代表性的样本是研究目标,但在现实中很难完全做到.</li>
<li>如何评估样本的随机性与代表性
<ul class="org-ul">
<li>目前没有公式或算法可以直接判断.</li>
<li>研究者需要对每个实验和样本进行**批判性和细致的评估**.</li>
<li>如果可能,将样本的描述性统计与已知总体特征进行比较:
<ol class="org-ol">
<li>年龄</li>
<li>性别比例</li>
<li>教育年限</li>
</ol></li>
<li>如果这些基本特征匹配,则研究结果的可推广性更有保障.</li>
</ul></li>
<li><b>积极的一面</b>
<ul class="org-ul">
<li>如果单个样本存在偏差,但不同样本的偏差方向不同,</li>
<li>那么样本均值的平均值在一定程度上可以补偿偏差.</li>
</ul></li>
<li>更多内容将在"大数定律"部分介绍.</li>
</ul>
</div>
<div id="outline-container-org90eb774" class="outline-4">
<h4 id="org90eb774"><span class="section-number-4">9.4.1.</span> Independent and identically distributed data</h4>
<div class="outline-text-4" id="text-9-4-1">
<ul class="org-ul">
<li>IID(也写作 iid 或 i.i.d.)是Independent and identically distributed data的缩写,是一个在统计学中涉
及大量数学讨论时使用的技术术语.</li>
<li>IID 对"随机且具有代表性数据样本"中的"随机"部分做了更深入的定义.根据 IID 这一概念,样本中的数据应满
足以下条件:
<ul class="org-ul">
<li>*独立(Independent)"独立"意味着每个数据点与其他数据点没有关联.
<ol class="org-ol">
<li>例如,根据社会安全号码(Social Security Number)的最后一位随机选择人,可以保证独立性;</li>
<li>而从同一个家庭中抽样会产生依赖性.</li>
</ol></li>
<li><b>同分布(Identically distributed)</b> 这意味着所有数据都来自同一个分布.
<ul class="org-ul">
<li>如果将来自不同分布(例如正态分布与幂律分布)的数据混合在一起,那么它们的描述性统计量也会混合,
从而使解释变得更加困难.</li>
<li>在实证数据中,这一假设很难完全确保或验证,但以相同的方式采集所有数据,有助于维持这一假设</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgf06ebbf" class="outline-3">
<h3 id="orgf06ebbf"><span class="section-number-3">9.5.</span> The Law of Large Numbers</h3>
<div class="outline-text-3" id="text-9-5">
<ul class="org-ul">
<li><b>抽样的必要性</b></li>
<li>抽样是出于实际的必要性,而不是因为它在统计上是最优的.
<ul class="org-ul">
<li>例如,如果你想知道企鹅的体重,理想情况下应当测量世界上每一只企鹅.</li>
<li>理论上这在可能性范围内,但这需要耗费大量时间和金钱,因此并不值得.</li>
<li>这就是我们进行抽样的原因.</li>
</ul></li>
<li>由于抽样变异性的存在,较大的样本能够更准确地反映总体特征.</li>
<li>这种直觉背后的数学原理被形式化为 <b><b>大数定律(LLN)</b></b>.
<ul class="org-ul">
<li><p>
大数定律可用以下数学公式表示:
</p>
\begin{equation}
\lim_{n \to \infty} P\left( \left| \overline{x_n} - \mu \right| > \epsilon \right) = 0 \tag{9.3}
\end{equation}</li>
<li>\(( \overline{x_n} )\) 为样本均值</li>
<li>\(( \mu )\) 为总体均值</li>
<li>\(( \epsilon )\) 为一个正数阈值</li>
<li>当样本量 \((n)\) 趋于无穷大时,样本均值与总体均值的差距超过 \((\epsilon)\) 的概率趋于零.</li>
<li><b>lim</b>:表示当样本量 \((n)\) 增加到无穷大时的极限.</li>
<li>绝对值符号:表示样本均值(\((\overline{x_n})\))与总体均值(\((\mu)\))之间差异的大小.</li>
<li>\((\epsilon)\)(希腊字母 epsilon):在微积分中常用于表示一个可以任意小的正数.</li>
</ul></li>
<li>公式(9.3)可以这样表述:
<ul class="org-ul">
<li>"当样本量无限增大时,样本均值显著不准确的概率趋近于零."</li>
</ul></li>
<li>公式含义
<ul class="org-ul">
<li>较大的样本可以更准确地估计总体均值.</li>
<li>当 \((n)\) 接近总体规模时,样本必然具有代表性.</li>
<li>对于较小的样本量,公式(9.3)依赖于假设:样本 \((x)\) 是总体的代表性样本.</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org04f1e9b" class="outline-4">
<h4 id="org04f1e9b"><span class="section-number-4">9.5.1.</span> LLN(Law of Large Number) and sample size (LLN demo #1)</h4>
<div class="outline-text-4" id="text-9-5-1">
<ul class="org-ul">
<li>我们使用 <b>模拟数据</b> 来做个实验
<ul class="org-ul">
<li>使用模拟数据,因为我们可以生成整个总体,并且知道真实的总体均值.</li>
<li>将序列 [1, 2, 3, 4] 重复多次,生成一个包含 4,194,304 个数据点的总体.</li>
<li>该总体的真实均值 \((\mu)\) 为 2.5.</li>
<li>从总体中随机抽取样本,样本量从 1 到 1500.</li>
<li><p>
计算每个样本的均值,并绘制样本均值随样本量变化的图像(图 9.3).
</p>

<div id="org62558f9" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/9-3.png" alt="9-3.png" />
</p>
<p><span class="figure-number">Figure 94: </span>ms/9-3.png</p>
</div></li>
<li>图中水平黑线表示真实总体均值 (\((\mu)\)).</li>
</ul></li>
<li>在阅读下文之前,建议你根据图 9.3 自行做出观察.
<ol class="org-ol">
<li>样本均值并不精确等于总体均值.
<ul class="org-ul">
<li>它们看起来"接近"真实均值,但这种判断受到 y 轴缩放的影响;如果放大 y 轴,差异会显得更明显.</li>
</ul></li>
<li>样本均值与总体均值的差异是 <b>无偏的</b> (unbiased).
<ul class="org-ul">
<li>有时样本均值小于总体均值,有时大于总体均值.</li>
</ul></li>
<li>随着样本量增加,样本均值的变异性逐渐减小.
<ul class="org-ul">
<li>可以观察到样本均值分布在 \((N)\) 增加时的"收缩"现象(pinching effect).</li>
</ul></li>
<li>即便当 \((N = 1500)\) 时,样本均值仍不完全等于总体均值.
<ul class="org-ul">
<li>这很惊人,因为总体中只有 <b>四个可能的取值(1, 2, 3, 4)</b>.</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgd7491f8" class="outline-4">
<h4 id="orgd7491f8"><span class="section-number-4">9.5.2.</span> LLN and repeated samples (LLN demo #2)</h4>
<div class="outline-text-4" id="text-9-5-2">
<ul class="org-ul">
<li>大数定律(LLN)通常用于抽样分布(sampling distributions)的情境中.其核心思想是:
<ul class="org-ul">
<li>任何一个样本(或一次实验)都可能受到抽样变异性,噪声以及其他非系统性变异的影响.</li>
<li>这意味着单个样本或一次实验不太可能给出真实总体均值的良好估计.</li>
</ul></li>
<li>因此,与其考虑单个样本的均值,我们可以考虑样本均值的平均值.</li>
<li>为了演示计算样本均值的平均值的力量,我对之前的演示进行了修改.
<ul class="org-ul">
<li>总体保持不变,但这次我抽取了 50 个样本,每个样本的大小为 (N=30).这个样本量比之前演示中的大多数样
本量要小得多.</li>
<li><p>
图 9.4A 显示了各个单独样本的均值.
</p>

<div id="orge55d52e" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/9-4.png" alt="9-4.png" />
</p>
<p><span class="figure-number">Figure 95: </span>ms/9-4.png</p>
</div></li>
<li>本次演示的关键创新之处在于:我计算了样本均值的累积平均值(见图 9.4B).</li>
<li>例如,在 x 轴位置 "s = 10" 的数据值,表示的是前 10 个样本均值的平均值.</li>
</ul></li>
<li>新实验的一些观察结果:
<ol class="org-ol">
<li>单个样本均值围绕真实均值分布(图 A),与之前的演示相同.</li>
<li>累积平均值在仅用了少量样本之后,就收敛到了真实总体均值.</li>
</ol></li>
<li>这非常引人注目.</li>
<li>为什么小样本量的样本均值的累积平均,会在短时间内收敛到总体均值呢?
<ul class="org-ul">
<li>关键的洞察在于:单个样本的变异性比样本均值的变异性更大.</li>
<li>由于 <b>均值标准误(SEM)</b> 小于标准差(SD),重复样本均值的平均值(图 B)相比单个样本的平均值(图 A)
具有更高的精确度.</li>
</ul></li>
<li>再看一个新例子
<ul class="org-ul">
<li><p>
图 9.5 展示了一个具有两个变量的总体数据.
</p>

<div id="org68d700c" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/9-5.png" alt="9-5.png" />
</p>
<p><span class="figure-number">Figure 96: </span>ms/9-5.png</p>
</div></li>
<li>蓝色圆圈和红色三角形代表随机抽取的样本,它们分布在整个数据空间中.</li>
<li>两个样本的平均值都接近数据空间的中心.</li>
</ul></li>
<li>这一现象说明:
<ul class="org-ul">
<li>单个样本的变异性大于样本均值的变异性.</li>
<li>原因与样本量增大时 <b>SEM</b> 会收缩而标准差不受样本量影响是相同的.</li>
</ul></li>

<li><b>平均多个样本的更大意义</b>
<ul class="org-ul">
<li>在现实世界中,收集巨大的样本量通常不可行.</li>
<li>更多情况下,研究的样本量受到资金或时间的限制.</li>
<li><b>解决方案</b>:将多个独立研究的数据结合起来,可以更准确地评估真实效应,这种方法称为 <b>元分析(Meta-analysis)</b>,
在医学研究中尤为重要.</li>
</ul></li>
<li>不同研究组收集的样本可能有偏差,原因包括:
<ul class="org-ul">
<li>非代表性样本</li>
<li>差的实验设计或设备</li>
<li>人为错误</li>
</ul></li>
<li>关键结论
<ul class="org-ul">
<li>如果偏差在不同样本中并不一致,*大数定律* 有助于确保样本均值可以被组合,从而得到更准确的总体均值估计.</li>
</ul></li>
<li>我们再来看一个例子
<ul class="org-ul">
<li><p>
如图9-6
</p>

<div id="org1dfb997" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/9-6.png" alt="9-6.png" />
</p>
<p><span class="figure-number">Figure 97: </span>ms/9-6.png</p>
</div></li>
<li>假设我们按照图 9.5 的数据收集 6 个样本,但每个样本都有偏差(图 9.6).</li>
<li>单个样本均值并不能准确逼近总体均值.</li>
<li>但是,由于这些偏差在不同样本中是随机的,多个样本均值的平均值能够准确估计总体均值.</li>
</ul></li>
<li>多个小规模研究的数据可以结合起来,从而更好地估计总体效应.</li>
<li>再次强调,元分析在医学与心理学研究中价值重大.</li>
<li>仔细观察图 9.3 中的样本均值:
<ul class="org-ul">
<li>它们分布在期望值的上下</li>
<li>远离 \((y = 2.5)\) 的均值较少,接近 \((y = 2.5)\) 的均值较多</li>
<li>我们甚至可以绘制这些样本均值的直方图,并思考其分布形状,从而引出下一节内容.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org7362fa6" class="outline-3">
<h3 id="org7362fa6"><span class="section-number-3">9.6.</span> The Central Limit Theorem</h3>
<div class="outline-text-3" id="text-9-6">
<ul class="org-ul">
<li>中心极限定理(CLT)指出:
<ul class="org-ul">
<li>样本均值的分布趋向于 <b>正态分布</b> ,即使原始变量的分布并不是正态分布.</li>
</ul></li>
<li>重要性
<ul class="org-ul">
<li>CLT 在推论统计中非常重要,因为它有助于确保各种统计假设的有效性.</li>
<li>接下来我会通过几个 CLT 的演示来解释其含义及影响.</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orga0a0fdb" class="outline-4">
<h4 id="orga0a0fdb"><span class="section-number-4">9.6.1.</span> CLT part 1: sampling distributions</h4>
<div class="outline-text-4" id="text-9-6-1">
<ul class="org-ul">
<li>数据生成
<ol class="org-ol">
<li>创建了一个总体数据集,包含整数 1-6.</li>
<li>使用第 8 章中的偏置骰子概率:
<ul class="org-ul">
<li>点数 "1" 和 "2" 的概率:\((p = \frac{1}{4})\)</li>
<li>其他点数的概率:\((p = \frac{1}{8})\)</li>
</ul></li>
</ol></li>
<li><p>
图 9.7A 显示了总体数据的直方图.
</p>

<div id="org0420666" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/9-7.png" alt="9-7.png" />
</p>
<p><span class="figure-number">Figure 98: </span>ms/9-7.png</p>
</div>
<ul class="org-ul">
<li>从总体数据中随机抽取 500 个样本.</li>
<li>每个样本的大小 \((N = 30)\).</li>
<li>计算并保存每个样本的均值.</li>
<li>图 9.7B 显示了这些样本均值的直方图.</li>
<li>总体数据直方图(图 9.7A)呈现明显的**非高斯形状**.</li>
<li>样本均值的直方图(图 9.7B)呈现**高斯形状**.</li>
</ul></li>
<li>结论:
<ul class="org-ul">
<li>样本均值的分布是高斯的,即使总体数据分布是非高斯的.</li>
</ul></li>

<li>示例 2:幂律分布数据
<ol class="org-ol">
<li>生成一个遵循幂律分布的数据集(图 9.8A)&#x2013;显然是 <b>非高斯分布</b></li>
<li>抽取 500 个样本,每个样本包含 30 个随机数据点.</li>
<li><p>
绘制这些样本均值的直方图(图 9.8B).
</p>

<div id="org8e354a6" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/9-8.png" alt="9-8.png" />
</p>
<p><span class="figure-number">Figure 99: </span>ms/9-8.png</p>
</div></li>
</ol></li>
<li>结果再次显示:
<ul class="org-ul">
<li><b>样本均值的分布是高斯的,即使总体数据分布不是高斯的</b>.</li>
</ul></li>
<li>大数定律(LLN)与中心极限定理(CLT)形成了一个强有力的组合:
<ul class="org-ul">
<li>随着样本量增加(或者在小样本量的情况下增加样本数量),总体均值的估计会更加准确.</li>
<li>这些样本估计值会围绕真实总体均值呈高斯分布.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb38d7c4" class="outline-4">
<h4 id="orgb38d7c4"><span class="section-number-4">9.6.2.</span> CLT part2: mixing variables</h4>
<div class="outline-text-4" id="text-9-6-2">
<ul class="org-ul">
<li>中心极限定理(CLT)的另一个推论是:
<ul class="org-ul">
<li>即使各个单独变量的分布是非高斯分布,它们的**随机混合**的分布也会趋近高斯分布.</li>
</ul></li>
<li>例子如下
<ul class="org-ul">
<li><p>
图 9.9
</p>

<div id="org634debc" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/9-9.png" alt="9-9.png" />
</p>
<p><span class="figure-number">Figure 100: </span>ms/9-9.png</p>
</div></li>
<li>创建了两个变量:
<ol class="org-ol">
<li><b>Data 1</b>:正弦波(sine wave)</li>
<li><b>Data 2</b>:均匀噪声(uniform noise)</li>
</ol></li>
</ul></li>
<li>图 9.9A 和图 9.9C 展示了各自的数据值:
<ul class="org-ul">
<li>Data 1 的分布是 <b>极其非高斯</b> 的,看起来几乎像一个倒置的高斯分布.</li>
<li>Data 2 的分布是均匀噪声所熟知的平坦形状.</li>
<li>将这两组数据进行逐点求和(面板 E).</li>
<li>结果:求和后的分布(面板 F)趋向于高斯分布.</li>
</ul></li>
<li>总结
<ul class="org-ul">
<li>CLT 不仅适用于样本均值的分布,</li>
<li>也解释了为何多个非高斯分布的变量混合后,会趋近于高斯分布.</li>
</ul></li>
<li>中心极限定理(CLT)的这一结果并不是理所当然的,它依赖于若干假设,其中包括变量需要处于相同的尺度(scale).</li>
<li><p>
为了演示这个尺度假设,在图 9.10 中,我使用了相同的代码,但将 Data 1 按比例因子 10 进行缩放(可以对
比两个图中 y 轴的尺度).
</p>

<div id="orgb43749f" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/9-10.png" alt="9-10.png" />
</p>
<p><span class="figure-number">Figure 101: </span>ms/9-10.png</p>
</div></li>
<li>在这种情况下,这两个变量求和后的直方图明显呈现非高斯分布.</li>
</ul>
</div>
</div>
<div id="outline-container-orgc4f25ab" class="outline-4">
<h4 id="orgc4f25ab"><span class="section-number-4">9.6.3.</span> The distribution of sample means</h4>
<div class="outline-text-4" id="text-9-6-3">
<ul class="org-ul">
<li>中心极限定理指出:
<ul class="org-ul">
<li>样本均值服从 <b>均值为 \((\mu)\),标准差为 SEM 的正态分布</b>.</li>
<li>换句话说,当你计算一个样本的均值时,这个均值本身是从"样本均值总体分布"中抽取的.</li>
<li><p>
数学表达
</p>
\begin{equation}
\overline{x} \sim \mathcal{N} \left( \mu, \frac{\sigma}{\sqrt{N}} \right) \tag{9.4}
\end{equation}</li>
<li>\((\overline{x})\):样本均值(sample mean)</li>
<li>\((\mu)\):总体均值(population average)</li>
<li>\((\sigma)\):总体标准差(population standard deviation)</li>
<li>\((N)\):样本量(sample size)</li>
</ul></li>
<li>注意:
<ul class="org-ul">
<li>这里的分散项 \((\frac{\sigma}{\sqrt{N}})\) 正是我们在之前章节中介绍的 <b>均值标准误(SEM)</b>.</li>
</ul></li>
<li>按样本量进行缩放意味着,样本量越大,样本均值的分布会越窄.
<ul class="org-ul">
<li>实际上,当 (N) 增加到无穷大时,SEM 会趋近于零,这意味着样本均值的分布将精确等于总体均值.</li>
</ul></li>
<li>另一方面,公式也表明,样本量不会影响分布的均值;样本均值分布的中心会始终位于总体均值上,而与分布的
散布程度无关.</li>
</ul>
</div>
</div>
<div id="outline-container-org93a361b" class="outline-4">
<h4 id="org93a361b"><span class="section-number-4">9.6.4.</span> Implications of the CLT</h4>
<div class="outline-text-4" id="text-9-6-4">
<ul class="org-ul">
<li>CLT 在统计学和信号处理领域中有许多重要作用,以下是几个关键原因:</li>

<li><b>从样本估计总体均值(Estimating population means from sapmples)</b>
<ul class="org-ul">
<li>CLT 与大数定律(LLN)结合表明:通过将多个样本的数据汇总,我们可以很好地估计总体均值.</li>
<li>应用示例:市场调查中,将来自不同商店或地理区域的小样本数据结合起来,用于整体市场分析.</li>
</ul></li>
<li><b>正态分布假设(Assumption of normal distribution)</b>
<ul class="org-ul">
<li>许多推论统计方法依赖于高斯分布(正态分布)的假设.</li>
<li>现实中,许多变量的分布并不是高斯分布.</li>
<li>解决方案:如果我们用**样本均值**作为统计推断的基础,而不是原始的单个数据值,CLT 保证正态性假设会成立.</li>
<li>相关应用:计算经验置信区间(详见第 13 章).</li>
</ul></li>
<li><b>多变量信号的解混(Demixing multivariate signals)</b>
<ul class="org-ul">
<li>在图像与信号处理领域,不同信号源可能会混合在一起(例如麦克风同时录下说话声,背景音乐,汽车噪音).</li>
<li>一种称为 <b>独立成分分析(Independent Components Analysis, ICA)</b> 的方法可以用来分离(解混)这些信
号源.</li>
<li>原理:假设各信号源是非高斯分布的,而它们的随机混合会趋向高斯分布(由 CLT 推导).</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org9874baa" class="outline-2">
<h2 id="org9874baa"><span class="section-number-2">10.</span> Chapter 10: Hypothesis Testing</h2>
<div class="outline-text-2" id="text-10">
</div>
<div id="outline-container-orgd6c1423" class="outline-3">
<h3 id="orgd6c1423"><span class="section-number-3">10.1.</span> Hypotheses</h3>
<div class="outline-text-3" id="text-10-1">
<ul class="org-ul">
<li>假设(hypothesis)是一种可被证伪的主张,这种主张需要验证,通常来源于实验或观测数据,并且能够对未来的观测做出预测.</li>
<li>让我们拆解一下这个定义.
<ul class="org-ul">
<li>在这个定义中,"可被证伪"或"可证伪性"可能是最重要的词语,也是科学进步的基础.</li>
<li>可证伪性意味着一个主张可以被推翻,这让科学家能够检验并改进他们对自然世界的想法.</li>
<li>虽然我们似乎更关注推翻观点而不是证明观点,但事实上,科学主张无法被"证明"到毫无疑问的程度;它们只
能被推翻或未能被推翻.这样做有哲学和统计学上的原因,我稍后会解释.</li>
</ul></li>
<li>为什么一个主张必须是可被证伪的很重要?
<ul class="org-ul">
<li>我可以声称"奇幻的小精灵在星期五很友好".</li>
<li>但是没办法验证这个说法,这意味着该主张不能用来帮助我们理解宇宙以及其中的一切.</li>
<li>需要说明的是,非可证伪的主张本身并没有错;它们可能很有趣,能激发想象力,增加生活的价值.</li>
<li>即使在科学领域中,非可证伪的主张有时也可以用来启动头脑风暴,提供解释,或者设计实验.</li>
<li>但当需要给出具体假设,并用统计方法去检验这些假设时&#x2013;只有可被证伪的主张才是相关的</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org09d0539" class="outline-4">
<h4 id="org09d0539"><span class="section-number-4">10.1.1.</span> How to specify a hypothesis</h4>
<div class="outline-text-4" id="text-10-1-1">
<ul class="org-ul">
<li>假设并不会凭空出现.</li>
<li>相反,假设是科学进步道路上的垫脚石.科学是一个永无止境的过程,通过观察数据或改进已有假设不断产生新
的假设.许多假设的起点是一个 研究问题,然后从这个问题推敲出具体的假设.</li>
<li>研究问题 是关于两个或多个变量之间关系的提问.
<ul class="org-ul">
<li>例如,我们可能会问:"咖啡能帮助大学生吗?"</li>
<li>这并不是一个假设,而是一个关于一个变量(咖啡)与另一个变量(关于学生的某些方面,但问题并没有具体
说明咖啡帮助的是什么)的关系的提问.</li>
</ul></li>
<li>研究问题可以通过建立一个直接连接变量的陈述转化为假设.
<ul class="org-ul">
<li>例如,我们可以假设:"每天饮用 1–2 杯含咖啡因的咖啡会提高一年级大学生的考试分数."</li>
<li>注意,这个假设是具体且明确的:它说明了咖啡的剂量(1–2 杯),相关人群(大学一年级学生),具体可测
量的结果(考试分数),以及影响的方向(分数提高).</li>
</ul></li>
<li>这只是一个假设;一个研究问题可以衍生出许多假设.
<ul class="org-ul">
<li>我们也可以假设咖啡改善学生的社交生活;</li>
<li>假设在下午 4 点之后饮用咖啡会降低睡眠质量;</li>
<li>假设咖啡会为在校园咖啡馆工作的学生创造就业机会,等等.</li>
</ul></li>
<li>回到本节开头的定义:
<ul class="org-ul">
<li>所有假设都必须是可验证的,</li>
<li>但并非所有假设都必须用实验或观测数据来验证.有些假设可以通过数学推理,模拟数据或逻辑推论来验证.但这
类假设与统计学无关.在统计学中&#x2013;尤其是在本书涉及的统计学范围内&#x2013;我们关注的是能够用数据来检验的假设.</li>
</ul></li>
<li>关于假设定义的最后一点是:
<ul class="org-ul">
<li>假设必须能够对未来做出预测.</li>
<li>这意味着假设可以推广到更广泛的人群,包括未来收集的数据.</li>
<li>例如,我可以假设"上周全球股市的波动与我的心情有关"&#x2013;而如果只有五个数据点,这个假设看起来也许是成
立的&#x2013;但这种假设对于预测未来股市毫无用处.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga9584f6" class="outline-4">
<h4 id="orga9584f6"><span class="section-number-4">10.1.2.</span> (Why) do we need hypotheses?</h4>
<div class="outline-text-4" id="text-10-1-2">
<ul class="org-ul">
<li>你并不总是需要假设才能进行科学研究或对世界做出发现.</li>
<li>事实上,许多科学领域都是从简单的观察开始的.生态学研究就是一个例子:
<ul class="org-ul">
<li>假设你前往一个偏远的岛屿,观察到有紫色翅膀的蝴蝶.这个观察本身就已经增进了我们对世界的理解,即使没有假设.</li>
</ul></li>
<li>随着科学知识的推进,基于观察的研究会逐渐发展成以假设为驱动的研究.在这个例子中,随后在该岛上的研究
可能会以这样的假设为驱动:
<ul class="org-ul">
<li>"紫色蝴蝶比非紫色蝴蝶能吸引更多配偶."</li>
</ul></li>
<li>以假设为驱动的研究有几个优势:
<ol class="org-ol">
<li>假设可以改善实验设计,批判性思维以及数据分析.</li>
<li>假设能将松散的想法转化为具体明确的主张.</li>
<li>假设有助于发展新的,更好的理论,并淘汰错误的理论.</li>
<li>假设使严格的定量评估成为可能.</li>
<li>假设能够帮助理解事物背后的机制.</li>
</ol></li>
<li>依我拙见,第5点最为重要.
<ul class="org-ul">
<li>通过观察去发现世界的状态是很棒的,但假设可以帮助我们理解为什么世界会是这样的,而这种理解反过来又能
帮助我们改变世界.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgf8fdb83" class="outline-4">
<h4 id="orgf8fdb83"><span class="section-number-4">10.1.3.</span> Strong and weak hypotheses</h4>
<div class="outline-text-4" id="text-10-1-3">
<ul class="org-ul">
<li>并不是所有假设都一样,我们可以区分强假设和弱假设.</li>
<li>可以想象,在可能的情况下,人们更倾向于使用强假设.</li>
<li>任何假设都应该具备以下特征:
<ul class="org-ul">
<li>清晰</li>
<li>具体</li>
<li>可证伪</li>
<li>基于已有数据或理论</li>
<li>能进行统计评估</li>
<li>是陈述而不是问题</li>
<li>对效应的方向有预测</li>
<li>与未观察到的数据或现象相关</li>
<li>并且与理解自然有关.</li>
</ul></li>
<li>而强假设在这些方面的表现比弱假设更好.</li>
<li>下面是一些例子:
<ul class="org-ul">
<li>医学研究对治疗疾病很重要. 这不是一个假设.这是一个很多人赞同的陈述,但它不是科学假设.</li>
<li>药物 X 会帮助病人吗? 这是一个研究问题,而不是假设.</li>
<li>该药物有作用. 这是一个弱假设.它符合假设的定义,但没有具体说明效应的性质,方向或持续时间.</li>
<li>药物 X 以剂量依赖的方式减少疾病 Y 的症状. 这个假设比上一个更强,因为它具体且明确.</li>
</ul></li>
<li>注意弱假设和强假设的区别:
<ul class="org-ul">
<li>弱假设预测某种效应,但没有提供效应的性质或方向的具体信息.</li>
</ul></li>
<li>为什么还要费心使用弱假设呢?我们是不是应该直接认为弱假设毫无意义,并禁止它们出现在科学讨论中?
<ul class="org-ul">
<li>在某些情况下,弱假设源于研究者的懒惰,这是应该避免的.</li>
<li>但我们不应该贬低所有弱假设:弱假设有时表明我们对某个主题的了解还太少,无法提出强假设.</li>
</ul></li>
<li>例如,想象某个政府推出了一项新的税收政策.独立研究者可能没有足够信息来对这项政策的影响提出强假设,
于是他们只能假设:"新政策会影响消费者的消费行为."</li>
<li>经过几年数据收集和对弱假设的检验,研究者就可能提出更强的假设.</li>
<li>新的科学领域通常会从观察发展到弱假设,再到强假设.
<ul class="org-ul">
<li>因此,尽量让你的假设尽可能强,同时也要承认,拥有一个弱假设并不一定意味着你本人或你的研究存在缺陷.</li>
</ul></li>
<li>小测试:判断下面每个陈述是弱假设,强假设,还是不是假设(我的答案在脚注中).
<ol class="org-ol">
<li>存在其他具有不同物理法则的宇宙.-&gt; 想法很有趣,但不是假设,因为我们无法对它进行证伪.</li>
<li>穿紫色内衣会改善心情. -&gt; 强假设.</li>
<li>植物在糖水中生长方式不同. -&gt; 弱假设.</li>
<li>Mike X Cohen 的书很棒. -&gt; 不是假设(这是一种无法被证明的观点,但我希望至少有几个人会赞同).</li>
<li>洗手 20 秒能减少传染病的传播. -&gt; 强假设.</li>
<li>一天一苹果,医生远离我. -&gt; 如果将 "让医生远离" 重新表述,例如改成 "导致去医院就诊次数减少",那
么它可以成为一个强假设.</li>
<li>人们在看单口喜剧后更有创意吗? -&gt; 这是一个研究问题,但可以将其改写成一个假设.</li>
</ol></li>
<li>我希望你已经开始对假设感到熟悉了.我也相信,你一定越来越好奇该如何用定量的方式来检验假设!</li>
<li>一些假设检验的方法适用于所有假设,而另一些方法则取决于假设的性质和数据的特征.</li>
<li>在本章节的剩余部分,你将学习假设检验的一些通用方面;至于假设检验的具体细节,则会在本书其它章节中进
行系统讲解.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgf526ca9" class="outline-3">
<h3 id="orgf526ca9"><span class="section-number-3">10.2.</span> IVS, DVs, models, and other stats lingo</h3>
<div class="outline-text-3" id="text-10-2">
<ul class="org-ul">
<li>你在本书中已经看到,统计学中有许多术语需要了解.更糟糕的是,一些术语在不同科学领域中可能令人困惑,
被过度使用,或者使用方式不一致.对此我能做的不多,除了尽力在本书中保持清晰与一致(虽然我确定偶尔也
会失败).</li>
<li>下面是一些与假设和假设检验相关的重要术语,你会在本书的其它部分持续看到这些术语.</li>
<li><b>因变量(Dependent variables,DVs)</b></li>
<li>因变量(也叫结果变量)是数据中你试图解释的特征.</li>
<li>因变量的例子包括:
<ul class="org-ul">
<li>医学研究中的疾病结局</li>
<li>健康研究中在某种饮食方案后减掉的体重</li>
<li>网络营销研究中的点击次数</li>
<li>经济学研究中的收入</li>
</ul></li>
<li>在研究中,通常只有一个因变量,但有些研究可能包含多个因变量.
<ul class="org-ul">
<li>例如,一项关于工作满意度的研究可能会有三个因变量:
<ol class="org-ol">
<li>自我报告的满意度</li>
<li>主管报告的绩效</li>
<li>员工贡献的收入</li>
</ol></li>
</ul></li>
<li><b>自变量(Independent variables,IVs)</b></li>
<li>自变量是用来解释因变量变化的变量.
<ul class="org-ul">
<li>一些自变量是在实验中被操控或控制的(例如,在医学研究中药物的剂量);</li>
<li>而另一些自变量是测量的但不被操控的(例如,自我报告的观看 YouTube 视频的时间).</li>
</ul></li>
<li>自变量有时也被称为:
<ul class="org-ul">
<li>解释变量(explanatory variables)</li>
<li>预测变量(predictor variables)</li>
<li>回归变量(regressors)</li>
</ul></li>
<li>多数研究中的自变量数量多于因变量.</li>
<li>区分自变量和因变量非常重要:统计学家使用自变量来解释因变量.
<ul class="org-ul">
<li><p>
图 10.1 展示了在不同类型研究中自变量与因变量的示例.
</p>

<div id="org7f9bd42" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/10-1.png" alt="10-1.png" />
</p>
<p><span class="figure-number">Figure 102: </span>ms/10-1.png</p>
</div></li>
</ul></li>
<li>自变量(IVs)和因变量(DVs)的角色并不是固定不变的.想象一个数据集,其中包含用于解释葡萄酒主观品质
的因素信息,例如:
<ul class="org-ul">
<li>糖含量</li>
<li>酒精含量</li>
<li>年份</li>
<li>产地</li>
<li>以及主观品质评分.</li>
</ul></li>
<li>我们可以用这个数据集来预测葡萄酒的品质评分,也可以用同样的数据来预测产地.</li>
<li>换句话说,
<ul class="org-ul">
<li>一个变量在某一次分析中可以是自变量,在另一分析中则可以是因变量.</li>
</ul></li>
<li>但在同一分析中,总是有一个作为预测目标的因变量,以及一个或多个用来进行预测的自变量.</li>
<li><b>残差(Residuals)</b></li>
<li>自变量(IVs)几乎从来不能完全解释因变量(DV).举个例子,
<ul class="org-ul">
<li>想象一个数据集包含 500 位成年人的体重,年龄,身高,以及每周中等强度运动的分钟数;</li>
<li>假设我们的目标是用这些变量(IVs)来解释体重(DV)的差异.</li>
<li>显然,仅靠这些因素,我们不可能预测 100% 的体重个体差异;在模型解释部分与实际观测体重之间的差距,
就是残差.</li>
</ul></li>
<li>换句话说,残差就是观测到的因变量的值与预测值之间的差.残差也常被称为
<ul class="org-ul">
<li>误差(errors),</li>
<li>偏差(deviations)</li>
<li>创新项(innovations).</li>
</ul></li>

<li><b>模型(Model)</b></li>
<li>在统计学中有很多不同类型的模型(这里不包括那些在杂志或抖音短视频中将自己的美貌与产品或服务联系起来
赚钱的时尚模特).</li>
<li>一般来说,模型是用来解释数据的一个框架.模型在深度,具体性,解释能力以及数学细节上都有所不同.</li>
<li>在本书中,当我使用"模型"这个词时,我指的是一种数学公式,它将自变量(IVs)与因变量(DV)连接起来,
<ul class="org-ul">
<li><p>
形式如下:
</p>
\begin{equation}
y = \beta_1 x_1  + \beta_2 x_2  + \beta_3 x_3  + \epsilon \notag
\end{equation}</li>
<li>其中,y 是我们试图用三个自变量 \(x_1,x_2,x\) 来解释的因变量.</li>
<li>β 值表示每个自变量对解释因变量的贡献大小,</li>
<li>ε 表示残差&#x2013;即自变量无法解释的因变量的变异部分.</li>
</ul></li>
<li>我假设成年人的身高取决于母亲的身高,父亲的身高,以及其童年时期的营养(用 0 到 10 的分值来量化).
<ul class="org-ul">
<li>这里,身高是因变量 y,</li>
<li>父母的身高和童年营养是自变量 \(x_1,x_2,x\)</li>
<li>我认为这些自变量是重要的,但我并不知道它们的具体重要程度;回归分析会计算这些缩放因子 β.</li>
<li>显而易见,一个人的身高由远不止这三个因素决定,因此这些未解释的高度差异就由 ε 来表示.</li>
</ul></li>
<li>在随后的章节中,你将学习如何把假设转化为模型,如何将模型拟合到数据中,以及如何确定模型的统计显著性.</li>
<li>目前,我希望你理解:
<ul class="org-ul">
<li>一个假设可以被翻译成一个数学方程,该方程将自变量与因变量连接起来,这个方程我们称之为"模型".</li>
</ul></li>
<li><b>检验统计量(Test Statistic)</b></li>
<li><p>
检验统计量是与某个假设相关的一种数据集描述性统计量.
</p>
<pre class="example" id="orgf0503fe">
A test statistic is a descriptive statistic of a dataset that is associated with a hypothesis
</pre></li>
<li>大多数推断统计都涉及确定观察到的检验统计量是否可能仅仅是偶然产生的(由于噪声或抽样变异性),而不是
反映了总体(population)中的真实效应.</li>
<li>检验统计量的类型很多,比如:
<ul class="org-ul">
<li>相关系数(correlation co-efficient),</li>
<li>t 值(t-value),</li>
<li>F 值(F-value),</li>
<li>卡方值(chi-square 值).</li>
</ul></li>
<li>当讨论适用于所有检验统计量的概念与方法时,我们通常使用通用术语 "检验统计量(test statistic)".</li>
<li><b>原假设与备择假设(Null and Alternative Hypotheses)</b></li>
<li>推断统计(Inferential statistics)涉及评估模型与数据的拟合程度.</li>
<li>在很多情况下,主要的统计结果(如 p 值)反映的是对两个模型的比较,而这两个模型分别来自两个假设,这两
个假设称为原假设(null hypothesis)和备择假设(alternative hypothesis).</li>
<li>原假设( \(H_0\) ):
<ul class="org-ul">
<li>原假设认为没有有趣的事情发生&#x2013;即因变量(DV)与自变量(IVs)之间没有关系.</li>
<li>在与  \(H_0\)  对应的数学方程中,会将部分或全部 β 缩放参数设为 0.</li>
<li>你可以将原假设理解为:变量之间不存在任何关系的假设.</li>
</ul></li>
<li>备择假设( \(H_a\) )
<ul class="org-ul">
<li>这是你在研究中提出的假设,是相对于原假设的另一个假设.(作者认为"效应假设"这个名字更好,因为它假
定存在效应,而不是单纯的一个"替代"假设&#x2026;&#x2026;但大家都使用 alternative 这个术语.)</li>
<li>在有多个备择假设的情况下,你可能会看到 H₁,H₂ 等符号.</li>
</ul></li>
<li>举例
<ul class="org-ul">
<li>\(H_a\) (备择假设):人们在看到广告 X 后会比看到广告 Y 后购买更多的小工具(widgets).</li>
<li>\(H_0\) (原假设):广告类型对小工具销售量没有影响.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org5e2acbc" class="outline-3">
<h3 id="org5e2acbc"><span class="section-number-3">10.3.</span> Can you prove a hypothesis?</h3>
<div class="outline-text-3" id="text-10-3">
<ul class="org-ul">
<li>或许你会觉得,提出一个假设的目的就是为了证明它,而统计的目的就是量化该假设是真的(被证明)还是假
的(被推翻).
<ul class="org-ul">
<li>不幸的是,事情并没有这么简单.</li>
</ul></li>
<li><b><b>推翻一个假设可能很简单</b></b>  要推翻一个假设,只需要一个与该假设不一致的观察即可.
<ul class="org-ul">
<li>例如,如果我有一个假设:"所有人类都有紫色皮肤",</li>
<li>那么只需要找到一个没有紫色皮肤的人类,就可以推翻这个假设.</li>
</ul></li>
<li><b><b>有些假设更难被推翻</b></b>  例如,"没有人类的身高可以超过 272 厘米"这个假设,
<ul class="org-ul">
<li>从理论上来说,只要观察到一个身高超过 272 厘米的人,就能被推翻.</li>
<li>但实际上,这样的人可能目前并不存活,或者从未被测量过.</li>
</ul></li>
<li><b><b>生物学相关领域的许多假设在实践中无法被推翻</b></b>  原因包括:
<ul class="org-ul">
<li>预测不足(不够具体或过于宽泛)</li>
<li>个体差异</li>
<li>抽样变异性</li>
<li>测量噪声</li>
<li>样本量有限</li>
</ul></li>
<li><b><b>一个例子</b></b>  我有一个假设:洗澡时穿袜子会使我每天写书的生产力平均提高 7 个单词.
<ul class="org-ul">
<li>这是一个完全有效(虽然有点奇怪)的科学假设,</li>
<li>但预测的效应量很小,且有太多其他因素会影响我的写书习惯,</li>
<li>因此要在实践中明确推翻这个假设几乎是不可能的.</li>
</ul></li>
<li>现实情况是,许多假设&#x2013;尤其是涉及复杂生物(如人类)的假设&#x2013;在实践中实际上很难被真正推翻.</li>
<li>这些假设可以被数据支持,或者与数据不一致,但这并不等同于被证明或推翻.</li>
<li><b><b>为什么还有无法明确推翻的假设?</b></b></li>
<li>也许你会觉得这种情况无法接受:如果假设不能被明确推翻,那提出它还有什么意义?
<ul class="org-ul">
<li>这种反对意见是有道理的&#x2013;可以认为,如果所有假设都能被无可辩驳地接受或推翻,科学进步会更快,更高效.</li>
<li>但现实是,我们对自然的理解极其有限,而我们渴望去理解它的愿望又如此强烈,</li>
<li>如果我们只限制自己提出那些能被明确推翻的假设,科学进步几乎会陷入停滞.</li>
<li>因此,我们接受并拥抱这种不完美,作为通向进步的一部分.</li>
</ul></li>
<li><b><b>保持假设的态度</b></b></li>
<li>即便数据与某个假设一致,我们也绝不会认为该假设是真的;
<ul class="org-ul">
<li>我们只是**暂时保留**该假设,直到我们提出一个更好的假设为止.</li>
<li>这或许可看作是一种语言上的细微差别,但我必须强调这一点的重要性&#x2013;
<ol class="org-ol">
<li>永远不要"相信"一个假设,也不要认定它是真的.  这样会阻碍科学进步,并有可能让你变成一个**顽固且
封闭的研究者**.</li>
<li>相反,你应该认为该假设**足够好,可以继续使用**,直到你或其他人提出一个**更好,更强,更准确**的
假设为止.</li>
</ol></li>
</ul></li>
<li><b>为什么不能接受假设为真?</b></li>
<li>记住,假设会被转换为数学方程.
<ul class="org-ul">
<li>当你收集数据并发现 \(H_a\) 模型比 \(H_0\) 模型更适合数据时,这并不意味着 \(H_a\) 已被证明,也不意味着
\(H_a\) 是可能存在的最佳模型.</li>
<li>仍有可能存在你尚未考虑的更好,更精确的模型.</li>
<li>唯一有效的结论是: <b>在某个概率范围内, \(H_a\)  比  \(H_0\)  更好</b>.</li>
</ul></li>
<li><b><b>重要的统计与哲学概念</b></b>  仅仅因为一个模型与某些数据契合,并不代表该模型是"正确"的.
<ul class="org-ul">
<li>假设有两个变量:
<ol class="org-ol">
<li>\(x_1 = 1\)</li>
<li>\(x_2 = 2\)</li>
<li>结果变量 \(y=3\)</li>
</ol></li>
<li>一个模型是: \(y = x_1 + x_2\) ,这看起来是个不错的模型.</li>
<li>但它是唯一的模型吗?当然不是!
<ol class="org-ol">
<li>它还可能是 \(y = -5 x_1 + 3 x_2\)</li>
<li>或者 \(y = x_1^3 + \sqrt{x_2^2}\)</li>
<li>甚至还有无限多其他可能的模型.</li>
</ol></li>
<li>不过,我们可以排除一些明显不一致的模型,比如 \(y=x_1 - x_2\),因为它与数据不符.</li>
</ul></li>
<li><b><b>如何在无限多可能模型中选择一个?</b></b>  在实践中,我们会基于一些假设对模型施加约束:
<ul class="org-ul">
<li>假设可能来自理论</li>
<li>假设可能来自直觉</li>
<li>假设可能来自追求简洁的便利性</li>
</ul></li>
<li><p>
例如,带平方根项的模型更复杂,我们通常会避免使用,除非复杂性是解释其他数据所必需的.
</p>
<pre class="example" id="org3493dbd">
奥卡姆剃刀原则假定简单优于复杂,这个例子说明了人类主观的审美感知如何指导我们对自然的探索.至于这
是否一种有效的方法,则(是一个开放的问题)
</pre></li>
<li><b><b>结论</b></b>
<ul class="org-ul">
<li>我们永远无法确定自己是否拥有"正确"或"最佳"模型.</li>
<li>我们能够知道的,是在所比较的模型中哪个更好&#x2013;通常是 \(H_a\) 与 \(H_0\) 的对比.</li>
</ul></li>
<li>这就是为什么假设不能被证明:
<ul class="org-ul">
<li>你只能推翻它</li>
<li>或者证明相比于 \(H_0\) 来说,数据与假设 \(H_a\) 更一致</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org1d39533" class="outline-3">
<h3 id="org1d39533"><span class="section-number-3">10.4.</span> Sample distributions under \(H_0\) and \(H_A\)</h3>
<div class="outline-text-3" id="text-10-4">
<ul class="org-ul">
<li>在上一章中,你已经知道,来自同一个总体的重复样本会产生不同的均值,  而这些样本均值的分布会趋近于高
斯(正态)形状,其离散程度由 <b>标准误差(SEM)</b> 决定. (SEM 本身等于样本标准差除以样本量的平方根.)</li>
<li>这种样本均值的变异性,是提供支持或反对某个假设的统计基础.</li>
<li><b><b>假设与虚拟实验</b></b>  我有一个假设:咖啡因能改善情绪(这是个合理的假设,因为咖啡因会激活多巴胺和腺苷
受体).
<ul class="org-ul">
<li><b>\(H_a\) (备择假设)</b> :一杯含咖啡因的咖啡会在饮用后一小时提升自我报告的情绪状态.</li>
<li><b>\(H_0\) (原假设)</b> :一杯含咖啡因的咖啡在饮用后一小时对自我报告的情绪状态没有影响.</li>
</ul></li>
<li><b><b>实验设计</b></b>
<ul class="org-ul">
<li>受试者人数:200</li>
<li>分组:含咖啡因组 100 人,脱咖啡因组 100 人</li>
<li>实验方法:双盲(受试者和研究者都不知道谁喝的是哪种咖啡)</li>
<li>测量时间点:饮咖啡前 与 饮咖啡后一小时</li>
<li>情绪测量方式:10 分制评分
<ol class="org-ol">
<li>1 分 = 情绪最差</li>
<li>10 分 = 情绪最好</li>
</ol></li>
<li>因变量(DV):饮用咖啡前后情绪变化的分值差</li>
</ul></li>
<li>将实验假设转化为数学模型
<ul class="org-ul">
<li><p>
数学形式:
</p>
\begin{equation}
\delta = \beta c + \epsilon \tag{10.1}
\end{equation}</li>
<li>δ:自我报告的情绪状态变化(饮用咖啡后分数减去饮用前分数)</li>
<li>c:咖啡条件(0 = 脱咖啡因,1 = 含咖啡因)</li>
<li>β:含咖啡因咖啡对情绪变化的影响值</li>
<li>ε:由其他非咖啡因因素引起的情绪变化</li>
</ul></li>
<li><b>将竞争假设转化为数学表达式</b>
<ul class="org-ul">
<li><p>
公式1
</p>
\begin{equation}
$H_A$ : \overline{\delta}_C > \overline{\delta}_D \notag
\end{equation}</li>
<li><p>
公式2
</p>
\begin{equation}
$H_0$ : \overline{\delta}_C = \overline{\delta}_D \notag
\end{equation}</li>
</ul></li>
<li>在上面面的公式中,
<ul class="org-ul">
<li>\(\overline{\delta}\) 表示样本平均的自我报告情绪变化值;</li>
<li>下标 C 表示含咖啡因组,下标 D 表示脱咖啡因组.</li>
<li>用通俗易懂的文字陈述假设很重要,但将其转化为数学形式可以提高精确性与简洁性.</li>
<li>在本实验中,我们需要一个对照组(脱咖啡因咖啡),因为可能的情况是:
<ol class="org-ol">
<li>不论饮用的热饮是否有精神活性成分,仅仅饮用一杯热饮,一小时后情绪都会提升.</li>
<li>如果没有这个对照组,那么原假设 \(\overline{\delta}_C = 0\)  就是一个"稻草人假设".</li>
</ol></li>
<li>该假设给出了效应的特定方向.
<ol class="org-ol">
<li>我原本可以写成"咖啡因改变情绪",并将其转化为 \(\overline{\delta}_C \neq \overline{\delta}_D\)</li>
<li>这体现了一尾假设(one-tailed)与双尾假设(two-tailed)以及相应的统计检验之间的区别,我会在本章后面
及后续章节中更详细地讨论.</li>
</ol></li>
<li><b>平均变化</b> 是指每组 100 名参与者的平均值.不需要每个参与者的情绪状态都增加,只要平均情绪有所提升,即
便部分人的情绪下降,假设依然成立.</li>
<li>有时出于概念或数学原因,可以将方程设为零.两个假设可以改写为:
<ol class="org-ol">
<li>\(\overline{\delta}_C - \overline{\delta}_D > 0\)</li>
<li>\(\overline{\delta}_C - \overline{\delta}_D = 0\)</li>
</ol></li>
<li>在某些分析中,例如回归分析,假设与模型的关联更直接.例如回到公式 10.1,我们可以写成:
<ul class="org-ul">
<li>\(H_0: \beta = 0\)</li>
<li>\(H_A: \beta > 0\)</li>
</ul></li>
</ul></li>
<li><b>简化讨论</b>,设置:
<ul class="org-ul">
<li>令 \(\Delta = \overline{\delta}_C - \overline{\delta}_D > 0\)</li>
<li>这意味着我们的假设是 \(\Delta > 0\),原假设是 \(\Delta = 0\)</li>
</ul></li>
<li><b>检验假设的思维实验</b>
<ul class="org-ul">
<li>假设世界的真实状态是  \(H_0\) ,也就是我们 100% 确信咖啡因对情绪状态没有任何影响.那么 \(\Delta\) 的期
望值是多少?</li>
<li>很多人会猜测 \(\Delta=0\)</li>
<li>那么在一个实证样本 N = 200 中会发生什么? 由于抽样变异,个体差异,以及各种影响情绪波动的因素,Δ 很可
能不会 <b>完全等于</b> 零.</li>
</ul></li>
<li>如果我们用新的 N = 200 样本重复实验,  再来一个样本,再来一个样  我们会得到一个 \(\Delta\) 值的分布.
<ul class="org-ul">
<li><b>大数定律(LLN)</b> 告诉我们,这些样本均值的平均值会收敛到零(because we assume that the true state of
the world is \(\Delta=0\) ).</li>
<li><b>中心极限定理(CLT)</b> 告诉我们,这些样本均值的分布会收敛到高斯(正态)分布.</li>
</ul></li>
<li><p>
Figure 10.2 展示了这个假设情况下的  \(H_0\)  分布.
</p>

<div id="org1e1ba56" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/10-2.png" alt="10-2.png" />
</p>
<p><span class="figure-number">Figure 103: </span>ms/10-2.png</p>
</div>
<ul class="org-ul">
<li><b>注意!</b> 该  \(H_0\)  下的 Δ 值分布来自一个 <b>假设的平行宇宙</b>,在那里我们 <b>绝对确定</b> 咖啡对情绪状态没
有任何影响.(在我们这个宇宙里面,咖啡对情绪肯定是有影响的)</li>
</ul></li>
<li>回到我们的现实世界&#x2013;我们并不知道真实效应&#x2013;我们有了实证 \(\delta\).  假设出现两种可能结果:
<ul class="org-ul">
<li>结果 A: \(\Delta=0.1\)</li>
<li>结果 B: \(\Delta=0.7\)</li>
</ul></li>
<li>从数值上看,这两个结果都大于零,因而在表面上与我们的假设一致.
<ul class="org-ul">
<li><p>
但当它们绘制到  \(H_0\)  分布(见图 10.3)上时,结果 B 显得更有说服力.
</p>

<div id="org553bf82" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/10-3.png" alt="10-3.png" />
</p>
<p><span class="figure-number">Figure 104: </span>ms/10-3.png</p>
</div></li>
</ul></li>
<li><b>为什么结果 A 说服力较弱?</b>
<ul class="org-ul">
<li>因为它正好落在  \(H_0\)  分布的高概率区域内.</li>
<li>事实上,在原假设为真时观察到 \(\Delta=0.1\) 的概率相当高,</li>
<li>因此,结果 A <b>不能提供足够证据</b> 来拒绝原假设,</li>
<li>我们会得出结论:在该实验中咖啡因并没有提升情绪状态.</li>
</ul></li>
<li><b>结果 B 的分析*</b>  \(\Delta=0.7\) 依然在  \(H_0\)  分布范围之内.
<ul class="org-ul">
<li>也就是说,即使原假设为真,我们仍可能观察到如此大的(甚至更大的)Δ.</li>
<li>但是,相比结果 A,这样的大值出现的概率很低.因此,我们可以说:如果原假设为真,结果 B 的出现概率如
此之低(虽非不可能),</li>
<li>以至于我们决定拒绝  \(H_0\) ,支持备择假设  \(H_a\) .</li>
</ul></li>
</ul>
<hr />
<ul class="org-ul">
<li><b>A Meta-comment</b> 你会认为我的  \(H_a\)  假设是弱还是强?</li>
<li>我们可以想象假设的渐进过程:
<ul class="org-ul">
<li>"咖啡因影响思维与情感"</li>
<li>"咖啡因改善情绪状态"</li>
<li>"咖啡因在 4 小时半衰期内提升积极情绪"</li>
<li>"咖啡因通过刺激伏隔核的 \(A_{2A}\) 受体引发正性情绪"</li>
<li>等等.</li>
</ul></li>
<li>这是科学如何通过在弱假设的基础上不断构建更强假设而进步的一个例子.</li>
</ul>
<hr />
<ul class="org-ul">
<li>希望这一节和之前的内容能让你直观理解 <b>大数定律(LLN)</b>, <b>中心极限定理(CLT)</b> 和 <b>原/备择假设</b> 如何结合
起来为拒绝原假设,支持备择假设提供证据.</li>
<li>但我要强调一点:
<ul class="org-ul">
<li>我对证据强度的解释是 <b>主观的</b>,仅仅基于对 \(H_0\) 分布的视觉观察.(也就是通过观察,看到了0.1落在了高概
率区域)</li>
<li>我们需要为这种主观评估赋予一个 <b>数值化</b> 的判断.</li>
</ul></li>
<li>换句话说,
<ul class="org-ul">
<li>我们需要得到一个 <b>精确的概率</b> ,表示在原假设为真时,我们在实证数据集中观察到这一描述性统计量 \(\Delta\)
的可能性.</li>
<li>这个概率值被称为 <b>p 值</b>,它是应用统计中最重要的单一数值.</li>
</ul></li>
<li>我相信你一定很期待学习 p 值,但在正式定义 p 值之前,  我还想先讨论一下关于  \(H_0\)  分布的最后一个话题,请继续阅读 :)</li>
</ul>
</div>
</div>
<div id="outline-container-org77a98e9" class="outline-3">
<h3 id="org77a98e9"><span class="section-number-3">10.5.</span> Where do H0 distribution come from?</h3>
<div class="outline-text-3" id="text-10-5">
<ul class="org-ul">
<li>你如何创建一个原假设( \(H_0\) )分布,来与观察到的结果进行比较呢?
<ul class="org-ul">
<li>你当然不能真的去创造一个平行宇宙,在那里你完全确定 \(H_0\) 为真,然后收集比全世界所有海滩上的沙粒还多
的数据点.</li>
<li>然而,我们仍然需要这个 \(H_0\) 分布,才能评估观察到的检验统计量是由随机偶然造成的概率.</li>
</ul></li>
<li>H₀ 分布来源主要有两种:</li>
<li><b>解析(Analytical)H₀ 分布</b></li>
<li>这是最常见的 H₀ 分布来源.</li>
<li>在解析方法中,H₀ 分布来自一个数学公式.具体的数学公式,以及你如何设置它的参数,取决于你所进行的检验
类型(例如 F 检验,t 检验,z 检验)以及实验的一些细节,比如样本量和条件数量.</li>
<li>你将在本书后面学到如何选择这些公式的细节.目前要记住的重要一点是,解析的 H₀ 分布来自数学公式,并且
是在没有任何数据的情况下获得的.</li>
<li><b>经验(Empirical)H₀ 分布</b></li>
<li>这是计算统计方法(computational  statistics,例如置换检验&#x2013;见第16章)所采用的方式.</li>
<li>创建经验 H₀ 分布的思路是:随机化你的数据,生成一个人工数据集(也称为打乱,置换或替代数据集),这种
数据集在零假设成立的情况下可能会出现.</li>
<li>本质上,这是一种生成与真实数据具有相同特征但你确定 H₀ 成立的假数据的方法.</li>
<li>经验 H₀ 分布的一个关键点是,它必须在已有数据的情况下才能创建;没有数据是无法生成的.</li>
<li>图 10.4 展示了这两种零假设检验统计量分布的示例.
<ul class="org-ul">
<li><p>
如图
</p>

<div id="org7b29baa" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/10-4.png" alt="10-4.png" />
</p>
<p><span class="figure-number">Figure 105: </span>ms/10-4.png</p>
</div></li>
<li>解析 H₀ 分布是一条平滑的曲线,因为它是由数学公式生成的,可以在任意所需的分辨率下计算;</li>
<li>经验 H₀ 分布则是一个直方图,因为它是由数据通过反复随机化生成的.</li>
</ul></li>
<li><p>
大多数统计方法 &#x2013; 也因此本书的大部分内容 &#x2013; 都会使用解析 H₀ 分布.
</p>
<pre class="example" id="org05f66bc">
Most statistics -- and therefore most of this book -- will deal with analytical H0 distributions
</pre></li>
<li>在第 16 章中,我会介绍经验 H₀ 分布,它的优点,局限性,以及在什么情况下应该使用它.</li>
</ul>
</div>
</div>
<div id="outline-container-orgcff832e" class="outline-3">
<h3 id="orgcff832e"><span class="section-number-3">10.6.</span> P-values: definition and misinterpretations</h3>
<div class="outline-text-3" id="text-10-6">
<ul class="org-ul">
<li>p 值(p-value)是指:
<ul class="org-ul">
<li><p>
在零假设(H₀)成立的情况下,获得当前观测到的检验统计量的概率.
</p>
<pre class="example" id="orgbca92da">
The p-value is the probability of obtaining the observed test statistic if the H0 were true.
</pre></li>
</ul></li>
<li>p 值越小,就说明零假设真实成立的可能性越低,而你的结果由偶然性,噪声,抽样变异,未解释的因素等造成
的可能性也越低.</li>
<li>就是这么简单:p 值是"在零假设成立的前提下,获得某个结果的概率".
<ul class="org-ul">
<li>更精确地说:p 值是在零假设成立的前提下,获得与观测到的检验统计量一样极端或更加极端的结果的概率.</li>
</ul></li>
<li><b>再次强调一个重要概念(Reiterating an important concept</b> :
<ul class="org-ul">
<li>小的 p 值并不能证明对立假设(Hₐ)是真的.</li>
<li>我们能做的只是计算:如果没有真实效应(即 H₀ 成立),与 Hₐ 相关联的检验统计量出现的概率.</li>
<li>一个较小的 p 值意味着数据与 Hₐ 所描述的模型更加一致,而与 H₀ 所描述的模型一致性较低,但这并不等于
Hₐ 就是真的.</li>
</ul></li>
<li>尽管从定义上看 p 值很简单,但它的含义十分微妙,也经常被误解.你对 p 值的理解会在本书接下来的内容中
不断加深.</li>
</ul>
</div>
<div id="outline-container-orgcb49e48" class="outline-4">
<h4 id="orgcb49e48"><span class="section-number-4">10.6.1.</span> P-values and statistical significance</h4>
<div class="outline-text-4" id="text-10-6-1">
<ul class="org-ul">
<li>术语"统计显著(statistical significance)"用于表示我们拒绝零假设(H₀)而支持备择假设(Hₐ).
<ul class="org-ul">
<li>所谓statistical significance是说我们的 \(H_a\) 里面的几个variable确实statistically significantly
related to teach other</li>
<li>p 值是使我们能够将某个结果标记为统计显著的关键量.</li>
</ul></li>
<li>p 值是连续的,范围从 0 到 1.</li>
<li>但在解释时,p 值常被二分为"统计显著"区间和"统计不显著"区间.
<ul class="org-ul">
<li>典型的显著性阈值是 p = 0.05,尽管有时也使用其他阈值,比如 p = 0.01 或 p = 0.001.</li>
<li>任何 p 值小于阈值的检验统计量都被视为统计显著.</li>
</ul></li>
<li>用于判断显著性的 p 值阈值用希腊字母 α 表示.
<ul class="org-ul">
<li>因此,显著性水平为 0.05 时,可以表示为 α = 0.05 或 α = 5%.</li>
</ul></li>
<li><b>将 H₀ 检验统计量分为显著与不显著区域</b>
<ul class="org-ul">
<li><p>
图 10.5 展示了将 H₀ 检验统计分布划分为"显著"和"不显著"区域的概念.
</p>

<div id="orgae0666e" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/10-5.png" alt="10-5.png" />
</p>
<p><span class="figure-number">Figure 106: </span>ms/10-5.png</p>
</div></li>
<li>面板 A:显示在 H₀ 成立时test statistic value(检验统计量)的分布.</li>
<li>面板 B:显示获得每个test statistic value(检验统计量)对应 p 值的概率.</li>
<li>任何关联的 p 值小于 0.05 的检验统计量都被认为是统计显著,图中以阴影区域表示.</li>
<li>该图为双尾检验示例,后续小节会更详细讨论.</li>
</ul></li>
<li><b>对 p 值函数的对数可视化</b>
<ul class="org-ul">
<li>面板 B 中 p 值函数的阴影区域非常小,因此我决定重新绘制该函数并采用对数缩放(面板 C).</li>
<li>面板 B 与面板 C 的区别表明:在显示函数以及渐近趋近于零的区域时,对数可视化通常很有用.</li>
</ul></li>
<li>图 10.5B 中 p 值函数的形状看起来可能有些奇怪,但它直接来源于正态分布的累积分布函数(cdf),并且在
z &gt; 0 的情况下被反转(这一概念在练习 8.10 中介绍过).</li>
<li>换句话说,p 值在 z &lt; 0 时对应的是 cdf,在 z &gt; 0 时对应的是 1 减去 cdf.这是因为在零假设(H₀)分布的
两侧,随着检验统计量值增大,极端值的概率会逐渐减小.</li>
<li>有时,人们会在 p 值介于 0.05 和 0.1 之间时增加一个第三类别,这类结果被称为"边缘显著"(marginally
significant).通常,这个术语在研究者希望结果达到显著性但 p 值又高于典型 0.05 阈值时使用.</li>
</ul>
</div>
</div>
<div id="outline-container-org709b82f" class="outline-4">
<h4 id="org709b82f"><span class="section-number-4">10.6.2.</span> P-values and distribution tails</h4>
<div class="outline-text-4" id="text-10-6-2">
<ul class="org-ul">
<li>你在第 3 章学到分布具有两个尾部.
<ul class="org-ul">
<li>分布尾部在假设检验中非常重要,因为假设并不总是只在一个方向上被设定.</li>
</ul></li>
<li>具体来说,根据你预测的是正效应,负效应,还是与零不同的任何效应,假设可分为三种情况:
<ul class="org-ul">
<li>单尾检验(One-tailed):
Δ &lt; 0
Δ &gt; 0</li>
<li>双尾检验(Two-tailed):
Δ ≠ 0</li>
</ul></li>
<li>双尾检验对应于一个弱假设(未预测效应的方向);</li>
<li>单尾检验对应于一个强假设(预测了效应的方向).</li>

<li><b>一尾与双尾的区别</b>
<ul class="org-ul">
<li><p>
图 10.6 展示了单尾检验与双尾检验的区别.
</p>

<div id="orga07509e" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/10-6.png" alt="10-6.png" />
</p>
<p><span class="figure-number">Figure 107: </span>ms/10-6.png</p>
</div></li>
<li>两种检验的 H₀ 分布是相同的,区别在于我们是否只考虑检验统计量的单侧,还是双侧.</li>
</ul></li>
<li>你可能会惊讶地发现,即使假设明确指定了效应方向,统计显著性通常还是基于双尾检验.</li>
<li>你或许会认为,对于预测效应方向的假设,我们应始终使用单尾检验.
<ul class="org-ul">
<li>尽管在某些情况下这样做是可以接受的,但统计学领域的文化几乎总是使用双尾检验,即使假设只涉及一个尾部.</li>
</ul></li>
<li><b>使用双尾检验的原因</b>
<ul class="org-ul">
<li>部分原因是为了允许出现意外结果(毕竟,如果我们总是知道结果,科学将会非常乏味);</li>
<li>另一部分原因是,这让假设更难被标记为显著.  事实上,如果你试图发表单尾检验的结果,你的读者可能会
怀疑你在进行所谓的 "p-hack" (操纵 p 值).</li>
</ul></li>
<li>结论是:即使你的假设只预测一个尾部,也应该使用双尾检验.</li>
<li>但单尾检验在某些情况下也会被使用.
<ul class="org-ul">
<li>例如,方差分析(ANOVA)和回归中的 F 检验就是单尾的,因为 H₀ 分布只取正值.</li>
<li>另一个例子是具有幂律分布(power-law distribution)的数据,这类分布只有一个尾部.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org126e9e8" class="outline-4">
<h4 id="org126e9e8"><span class="section-number-4">10.6.3.</span> Where do p-values come from?</h4>
<div class="outline-text-4" id="text-10-6-3">
<ul class="org-ul">
<li>由于 p 值被定义为在零假设(H₀)分布下某个特定值出现的概率,而 H₀ 分布的来源有两种(解析型或经验型),
因此 p 值的计算方法也有两种.</li>
<li><b>解析型 p 值 (Analytical p-values)</b></li>
<li>解析型 p 值是通过一个公式计算的,该公式将检验统计量转换为在理论 H₀ 分布中观察到至少与该值一样大的
统计量的概率.</li>
<li>举个例子,我们来关注 z 分数(z-score),看看如何把它映射为 p 值.
<ul class="org-ul">
<li>你知道 z 分数反映的是正态分布,其中心在 0,数值表示距离中心的标准差数值.</li>
<li>那么,z &gt; 1 的概率是多少?</li>
<li><p>
我们通过计算正态分布的概率密度函数(pdf)中 z = 1 右侧的面积来回答这个问题(见图 10.7).
</p>

<div id="org93a52ff" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/10-7.png" alt="10-7.png" />
</p>
<p><span class="figure-number">Figure 108: </span>ms/10-7.png</p>
</div></li>
<li>在这种情况下,z &gt; 1 的概率约为 p ≈ 0.158.</li>
</ul></li>
<li>关于实现的说明:
<ul class="org-ul">
<li>虽然图中展示的是 pdf,但在实际操作中,p 值是通过累积分布函数(cdf)计算的.</li>
<li>由于 cdf 表示的是某个指定值左侧的面积,因此右侧的面积就是 1 减去该 value 的 cdf 值.</li>
<li>你可以在生成图 10.7 的在线代码中看到这一实现方式,并且将在本章和下一章的练习中进一步熟悉这个过程.</li>
</ul></li>
<li><b>经验型 p 值(Empirical p-values)</b></li>
<li>如果你使用基于置换的统计方法来构建经验型 H₀ 分布,那么 p 值的计算方式是:
<ul class="org-ul">
<li>测试统计量到 H₀ 分布中心的归一化距离;</li>
<li>或H₀ 分布中比观测到的检验统计量更极端的值的数量.</li>
</ul></li>
<li>我会在第 16 章更详细地解释这种方法,在此之前我们只会使用解析型 p 值.</li>
<li>重要提示
<ul class="org-ul">
<li>不管 p 值是如何计算的,其 解释始终是一样的.</li>
</ul></li>
<li>p 值的精度
<ul class="org-ul">
<li>p 值可以被计算到任意精度,就像 √2 一样.</li>
<li>但在实际中,p 值通常会被截断或近似表示.</li>
<li><p>
例如,一个精确的 p 值 p = 0.020000000&#x2026; 在真实数据中几乎不可能出现;为了方便和简洁,人们会写成:
</p>
<pre class="example" id="orge904b3f">
p &lt; 0.05
p &lt; 0.02
p = 0.02
</pre></li>
</ul></li>
<li>当 p 值非常小的时候,会用科学计数法表示,例如: \(p < 10^{-7}\)</li>
</ul>
</div>
</div>
<div id="outline-container-orge378172" class="outline-4">
<h4 id="orge378172"><span class="section-number-4">10.6.4.</span> P-z combinations to memorize</h4>
<div class="outline-text-4" id="text-10-6-4">
<ul class="org-ul">
<li>数学老师总喜欢说:"不要死记硬背,要理解."</li>
<li>当你已经讲授同一份内容多年甚至几十年时,这话确实很好说.</li>
<li>但事实是,在应用数学中,很多概念和公式确实需要记住.</li>
<li>由于 z 值(即正态分布中的标准差单位)在统计学中极其重要,并且概率值可以直接映射到 z 值,
<ul class="org-ul">
<li><p>
我认为有一些 p−z 组合 是你应该牢记的(见图 10.8).
</p>

<div id="orgd98b420" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/10-8.png" alt="10-8.png" />
</p>
<p><span class="figure-number">Figure 109: </span>ms/10-8.png</p>
</div></li>
<li>这些组合可以帮助你在数据清理,分析和解释中建立直觉,并让你看起来像是一位令人印象深刻的资深统计学
专家.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org76abb8f" class="outline-4">
<h4 id="org76abb8f"><span class="section-number-4">10.6.5.</span> Misinterpretations</h4>
<div class="outline-text-4" id="text-10-6-5">
<ul class="org-ul">
<li>下面是关于 p 值 = 0.02 的五个错误陈述,它们反映了对 p 值的常见误解.</li>
<li>我希望你能解释这些陈述为什么是错误的,并重新表述成正确的说法.我的答案和讨论会在下文给出.
<ol class="org-ol">
<li>"我的 p 值是 0.02,所以该效应在 98% 的总体中存在."</li>
<li>"我的 p 值是 0.02,所以我的检验统计量与总体参数相等的概率是 98%."</li>
<li>"我的 p 值小于阈值,所以该效应是真实存在的."</li>
<li>"我的 p 值小于阈值,所以一个变量导致了另一个变量."</li>
<li>(假设 p = 0.08 对这个说法)"我的 p 值大于阈值,所以零假设成立."</li>
</ol></li>
<li>为了增加悬念并鼓励批判性思考,我会先写出修正后的陈述,然后在下面给出对每个误解的讨论.请注意,修正
上述陈述的方法有好几种.
<ol class="org-ol">
<li>"我的 p 值是 0.02,因此在没有效应的情况下,我的检验统计量是由于抽样变异,噪声,小样本量或系统偏
差而产生的概率是 2%."</li>
<li>"我的 p 值是 0.02,因此我的检验统计量是由于抽样变异或噪声而产生的概率仅为 2%.我可以结合置信区间
分析来确定观测到的样本均值与未知总体均值之间的关系."</li>
<li>"我的 p 值小于阈值,因此在假设零假设成立且样本具有代表性的情况下,在样本中观测到该效应的可能性很低."</li>
<li>"我的 p 值小于阈值,因此这些变量在统计学上具有显著的相关性."</li>
<li>"我的 p 值大于阈值,因此如果零假设成立,样本中的效应是有可能被观察到的.我所设定的备择假设(Hₐ)
不太可能是正确的,但可能存在比零假设更能解释数据的其他备择假设."</li>
</ol></li>
<li>我希望你已经对这些误解及其错误原因有了一些直观认识.下面是对每一点的简要讨论:
<ol class="org-ol">
<li>p 值是关于组内平均效应相对于变异性的衡量;它并不能说明效应是否在每个个体中都存在.虽然可以检测某
个效应在每个个体中是否显著,但这并不是 p 值所反映的内容.</li>
<li>p 值并不说明样本特征与总体特征之间的关系;它只是表示在零假设成立的情况下,观测到该检验统计量的概
率.你可以使用 置信区间(第 13 章)来量化样本特征与总体参数之间的关系.</li>
<li>p 值低于阈值并不能证明效应是真实存在的;它只能表明在零假设成立的情况下,观测到该效应的可能性很低.
(在日常口语中,当 p 值很小时,我们可能会说效应是"真实"的,但这并不是最严谨的统计学解释.)</li>
<li>p 值本身并不能建立因果关系.因果关系需要通过实验操控或特定类型的分析来判断.显著的 p 值可以表明
变量之间存在关系,但要确定因果性,还需要其他方法.</li>
<li>p 值高于阈值并不能证明零假设成立;它只是表示,在零假设成立的情况下,这种效应很可能被观察到.可能
还存在比零假设(H₀)或当前备择假设(Hₐ)更能解释数据的其他模型.</li>
</ol></li>
<li>我希望这些解释不会让你感到失望.
<ul class="org-ul">
<li>p 值只是统计学工具箱中的一个工具,它很重要,但并不能回答所有问题.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org163f348" class="outline-4">
<h4 id="org163f348"><span class="section-number-4">10.6.6.</span> Problems with p-values</h4>
<div class="outline-text-4" id="text-10-6-6">
<ul class="org-ul">
<li>我承认,这一节的标题有点像"标题党".事实上,p 值本身并没有问题.</li>
<li>不过,完全依赖 p 值来解释统计结果有一些重要的局限性.前面已经提到过其中几个:
<ul class="org-ul">
<li>p 值不能证明一个假设</li>
<li>p 值不能证明因果关系</li>
<li>p 值不能表示总体中具有该效应的比例</li>
</ul></li>
<li>随着你继续阅读本书,你会发现 p 值还有其他限制,例如:
<ul class="org-ul">
<li>即使效应非常微小,如果样本量足够大,也可能在统计上显著.</li>
</ul></li>
<li>最容易解释的 p 值是非常小(例如 &lt; 0.001)或相对较大(例如 &gt; 0.3)的情况,
<ul class="org-ul">
<li>因为极端的数值可以让我们非常有信心地判断效应确实存在或不存在,</li>
<li>并且多次重复实验很可能会得出相同的结论.</li>
</ul></li>
<li>p 值接近 0.05 时的解释会更困难,
<ul class="org-ul">
<li>因为如果在一个新样本中重复相同实验,甚至只是增加少量额外数据或改变数据清理流程,都有可能让 p 值跨
越显著性阈值,例如从 0.054 变成 0.046.</li>
</ul></li>
<li>另一方面,数据就是数据;
<ul class="org-ul">
<li>你只能从现有数据得到某个 p 值.</li>
<li>为了得到特定的 p 值而操纵数据或分析方法是违规且不道德的(第 18 章会对此作更多讨论).</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org7aae517" class="outline-3">
<h3 id="org7aae517"><span class="section-number-3">10.7.</span> P-values and significance categorization</h3>
<div class="outline-text-3" id="text-10-7">
<ul class="org-ul">
<li>理解宇宙的一大挑战在于,我们无法直接探究自然的运作机制.
<ul class="org-ul">
<li>相反,我们只能收集数据,并评估关于自然的某个假设是否与这些数据相符.</li>
<li>从抽象意义上说,我们的假设要么是真,要么是假;</li>
<li>然而,我们无法用有限且不完美的数据来完全证明它.因此,我们依赖 p 值 来对假设的有效性做出有根据的
判断.</li>
</ul></li>
<li>正如你已经知道的,我们会根据 p 值将检验统计量标记为 显著 或 不显著.这种分类产生了一个二维的决策空间,
<ul class="org-ul">
<li><p>
这在图 10.9 中有展示.
</p>

<div id="org8f8612a" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/10-9.png" alt="10-9.png" />
</p>
<p><span class="figure-number">Figure 110: </span>ms/10-9.png</p>
</div></li>
<li>图 10.9 显示了两种做出正确决定的方式:
<ol class="org-ol">
<li>当 H₀ 真的为假时拒绝它;</li>
<li>当 H₀ 真的为真时不拒绝它.</li>
</ol></li>
<li>同时也有两种可能的统计错误(我会在后面的小节再详细说明).</li>
<li>但这个方法的根本问题在于,我们并不知道世界的真实状态,即 H₀ 真的为真还是为假;我们手上只有经验得
来的检验统计量以及我们的假设.因此,我们所做的决定总是伴随着不确定性.</li>
</ul></li>
<li>顺便一提,注意该表中使用的决策措辞:"不拒绝 H₀" 与 "拒绝 H₀".你可能会觉得写成"接受 H₀"或者"接受 Hₐ"更好.
<ul class="org-ul">
<li>虽然在昏暗,烟雾缭绕的地下室里轻声说"接受假设"或许可以,</li>
<li>但在正式的统计语言中,最好保持精确&#x2013;统计决策只有两种:拒绝零假设,或者不拒绝它.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org90276e6" class="outline-3">
<h3 id="org90276e6"><span class="section-number-3">10.8.</span> Type-I and Type-II errors</h3>
<div class="outline-text-3" id="text-10-8">
<ul class="org-ul">
<li>设想以下两种情景:
<ul class="org-ul">
<li>医生对一位男性病人说:"你怀孕了."</li>
<li>医生对一位即将分娩的女性说:"你没怀孕."</li>
</ul></li>
<li>这些话在 1980 年代的深夜喜剧小品里可能会很有趣.</li>
<li>再想想这些情景:
<ul class="org-ul">
<li>法官对一个无辜的人说:"你有罪."</li>
<li>法官对一个杀人犯说:"你无罪."</li>
</ul></li>
<li>就没那么好笑了.但这两组陈述都形象地展示了在判断一个检验统计量是否显著时,可能出现的两种错误类型.</li>
<li><b>错误从哪里来</b></li>
<li>记住,我们无法 <b>证明</b> 备择假设 \(H_A\) 是真的,
<ul class="org-ul">
<li>而我们会在观察到的统计量像我们的实测统计量一样大的概率小于 5% 时,拒绝零假设 H_0.</li>
</ul></li>
<li>这意味着:
<ul class="org-ul">
<li>当 H_0 其实是真的时,我们也可能会错误地拒绝 H_0</li>
<li>当 H_A 其实是真的时,我们也可能会未能拒绝 H_0</li>
</ul></li>

<li>这两种错误分别被称为*第一类错误*和*第二类错误*
<ul class="org-ul">
<li><p>
可以加入之前的 2×2 决策表(见图 10.10).
</p>

<div id="org5c0f254" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/10-10.png" alt="10-10.png" />
</p>
<p><span class="figure-number">Figure 111: </span>ms/10-10.png</p>
</div></li>
</ul></li>
<li><b>第一类错误 (Type-I error)</b></li>
<li>发生在你拒绝了实际上为真的 H_0 的时候.
<ul class="org-ul">
<li>换句话说,你得出"存在显著效应"的结论,但实际上并没有.</li>
<li>犯第一类错误的概率就是显著性阈值(p 值阈值),因此第一类错误也叫 <b>α错误</b></li>
</ul></li>
<li><b>第二类错误 (Type-II error)</b></li>
<li>发生在你未能拒绝实际上是假的 H_0 的时候.
<ul class="org-ul">
<li>换句话说,你得出"没有显著效应"的结论,但实际上是有的.</li>
<li>犯第二类错误的概率与检验的 <b>统计功效</b> statistical power)相关,用希腊字母 β 表示.</li>
<li>统计功效定义为在 H_0 为假时能正确拒绝它的概率,即 (1 - β).</li>
<li>统计功效是我们希望尽量*提高*的,它会随着:
<ol class="org-ol">
<li>样本量增大而提高.</li>
<li>效应量变大而提高.</li>
<li>变异性减少而提高.</li>
</ol></li>
</ul></li>
</ul>
</div>
<div id="outline-container-org6fd6378" class="outline-4">
<h4 id="org6fd6378"><span class="section-number-4">10.8.1.</span> The balance of Type-I and Type-II errors</h4>
<div class="outline-text-4" id="text-10-8-1">
<ul class="org-ul">
<li>无论是在统计推断中还是在现实生活中,错误都应该被避免.那既然这样 <b>为什么不直接降低 α?</b>
<ul class="org-ul">
<li>比如,将显著性水平调到 p &lt; .0001,而不是传统的 p &lt; .05,这显然会比原来的阈值更能减少 α 错误(第一
类错误).</li>
</ul></li>
<li>问题在于,第一类错误(Type-I)和第二类错误(Type-II)之间存在"推拉关系":
<ul class="org-ul">
<li><p>
降低一种错误的概率,往往会提高另一种错误的概率.
</p>
<pre class="example" id="org31ab28d">
Decreasing the probability of one increases the probability of the other
</pre></li>
</ul></li>
<li>为了理解这一点,让我们进入一个假想世界:
<ul class="org-ul">
<li>我们确定 \(H_A\) 是真的</li>
<li>我们拥有无限多的资金和实验室资源,可以重复实验很多很多次</li>
</ul></li>
<li>这样,我们就不仅仅有一个检验统计量,而是拥有一个在 \(H_A\) 条件下成立的完整统计量分布.
<ul class="org-ul">
<li><p>
我们也可以想象在 \(H_0\) 条件下的分布,这两个分布可以绘制在图 10.11 中.
</p>

<div id="org4ff4793" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/10-11.png" alt="10-11.png" />
</p>
<p><span class="figure-number">Figure 112: </span>ms/10-11.png</p>
</div>
<ul class="org-ul">
<li>选择的 α 阈值决定了发生 <b>假阳性(false positives)</b> 或 <b>假阴性(false negatives)</b> 的概率</li>
<li>实际中,我们只有一个来自 \(H_A\) 的检验统计量</li>
<li>\(H_A\) 的分布是一个理论概念,它说明即使假设是真的,重复抽样得到的统计量值也会不同,其中一些可能低于
α 阈值(这会被错误地标记为"无显著性")</li>
<li><p>
如图10-12
</p>

<div id="org39be418" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/10-12.png" alt="10-12.png" />
</p>
<p><span class="figure-number">Figure 113: </span>ms/10-12.png</p>
</div></li>
<li>如果我们将拒绝 \(H_0\) 的 p 值阈值设得更严格(把图中的竖虚线往右移动)
<ol class="org-ol">
<li>第一类错误更少(好事)</li>
<li>第二类错误更多(坏事)</li>
<li>如图 10.12A:浅灰区域小(假阳性少),深灰区域大(假阴性多)</li>
<li>即使假设是真的,你也可能无法拒绝 \(H_0\)</li>
</ol></li>
<li>如果我们将临界值往左移(如图 10.12B)
<ol class="org-ol">
<li>第二类错误更少</li>
<li>第一类错误更多</li>
</ol></li>
</ul></li>
</ul></li>
<li>最优的情况是通过将两个分布尽量分开来减少任何统计错误(如图 10.12C)
<ul class="org-ul">
<li>可以通过增加效应量(effect size)或减少变异性(variability)来实现</li>
<li>这些会在下一章讨论 t 检验时进一步解释</li>
</ul></li>
<li>现实中,我们并不知道真实的世界状态,也就不知道 \(H_A\) 的分布
<ul class="org-ul">
<li>我们只有一个来自 \(H_A\) 的经验检验统计量(如图 10.12D)</li>
<li>问题在于,你不确定它是:
<ol class="org-ol">
<li>从 \(H_0\) 分布中取到的一个相对异常值</li>
<li>还是从 \(H_A\) 分布中取到的一个值</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgbb55af2" class="outline-3">
<h3 id="orgbb55af2"><span class="section-number-3">10.9.</span> Various interpretations of "significant"</h3>
<div class="outline-text-3" id="text-10-9">
<ul class="org-ul">
<li>当检验统计量的 p 值足够小,我们会说该效应在统计上是显著的.</li>
<li>但"显著"一词并不只有统计意义.研究结果可以在统计,理论,临床,社会或实践上具有显著性.</li>
<li>显著性的解读方式有很多.</li>
<li><b>统计显著性(Statistical significance)</b></li>
<li>这是统计学教材和研究报告中"显著性"最主要的用法:
<ul class="org-ul">
<li>统计显著性指在 <b>零假设成立</b> 的前提下,观察到某个经验检验统计量的概率.</li>
<li>换句话说,就是 p 值所表达的含义.</li>
</ul></li>
<li><b>理论显著性(Theoretical significance)</b></li>
<li>这里的"理论"是指科学理论,不是抽象概念或分析公式.</li>
<li>理论显著性意味着某个发现与某个理论相关,或者会引发新的实验.</li>
<li>这种显著性与统计显著性无关:一个结果在理论上可能高度相关,但在统计上不显著.</li>
<li><b>临床显著性</b></li>
<li>临床显著性意味着研究结果在诊断或治疗某种疾病方面具有相关性.</li>
<li>与理论显著性一样,这与统计显著性无关.
<ul class="org-ul">
<li>例如,发现疫苗与自闭症没有统计上显著的关系,虽然在统计上不显著,但在临床上却非常重要.</li>
</ul></li>
<li><b>实践,社会,教育等显著性(Practical, societal, educational, etc)</b></li>
<li>科研成果可能有多种现实世界的影响("显著性"),</li>
<li>这些影响可能会受到 p 值信息的辅助,但并不会因为 p 值小于 0.05 就自动在教育等领域显著.</li>
<li>这一点也表明:统计学处在硬科学(数学)与软科学(政策,决策)之间的交界处.</li>
<li>数学天才为我们带来了 p 值,计算机可以非常精确地计算它们.
<ul class="org-ul">
<li>但如何恰当地使用 p 值,解释数据的意义与限制,并利用这些数据推动知识进步和改善社会,是研究者的责任</li>
</ul></li>
<li>在统计讨论中,当你听到"显著性"一词时,通常是指统计显著性,即 p 值是否高于或低于 0.05.</li>
<li>其他类型的显著性一般会伴随限定词,如"临床显著性"或"实践显著性".</li>
<li>然而,"显著"与"非显著"会带来诸多含义.替代的分类方式是:
<ul class="org-ul">
<li>sub-threshold: p &lt; 0.05</li>
<li>supra-threshold:p &gt; 0.05</li>
</ul></li>
<li>这种分类在进行大量检验(比如多重比较校正)时特别有用.</li>
</ul>
</div>
</div>
<div id="outline-container-org3f5cfc3" class="outline-3">
<h3 id="org3f5cfc3"><span class="section-number-3">10.10.</span> Multiple comparisons</h3>
<div class="outline-text-3" id="text-10-10">
<ul class="org-ul">
<li>多重比较是指在同一数据集中检验多个假设的做法.</li>
<li>从数据集中尽可能多地提取信息是非常有益的,尤其是在实验代价高昂或耗时的情况下.</li>
<li>然而,在同一数据集上进行多次假设检验也会带来统计上的挑战.</li>

<li><b>主要问题</b> 随着每增加一次检验,出现假阳性(第一类错误,Type-I error)的风险都会增加.
<ul class="org-ul">
<li>这被称为 **多重比较问题**(Multiple Comparisons Problem).</li>
<li>幸运的是,统计学上有一些修正方法可以减少错误拒绝零假设(H₀)的可能性.</li>
</ul></li>

<li><b>家族(Family)概念</b> 在进行多重比较时,不要只考虑单独的一次检验;而是要考虑 <b>一组</b> 被检验的假设,这
被称为一个"家族"(Family).
<ul class="org-ul">
<li>家族的成员越多,至少出现一次被错误标记为"显著"的结果的可能性就越高.</li>
<li>换句话说,家族规模越大,第一类错误的风险就越高.</li>
</ul></li>
<li>回顾一下:p 值表示在零假设成立的前提下,获得某个大小的检验统计量的概率.
<ul class="org-ul">
<li>从零假设分布中抽样越多,就越有可能随机抽到分布尾部的样本.</li>
</ul></li>
<li>假设实验包含三个条件,我们想将每个条件与其他条件进行比较.
<ul class="org-ul">
<li><p>
在图 10.13 中有相应的可视化示例.
</p>

<div id="org6226223" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/10-13.png" alt="10-13.png" />
</p>
<p><span class="figure-number">Figure 114: </span>ms/10-13.png</p>
</div></li>
<li>假设我们在每次比较中使用阈值 p &lt; 0.05,那么三次检验的合并第一类错误概率就是 0.15,即 15%.</li>
<li>这意味着,当三个条件实际上并无差异时,我们有 15% 的概率会错误地认为至少一个比较结果是显著的.</li>
<li>每个单独检验的显著性水平是 0.05,但整个家族的总假阳性率是 0.15.</li>
<li>这个总错误率称为 <b>家族错误率(FWE)</b>.</li>
</ul></li>
<li><b>为什么是 0.15?</b> 你可能已经注意到,0.15 = 3 × 0.05,即 FWE ≈ nα,
<ul class="org-ul">
<li>其中 <b>n</b> 是检验的次数,*α* 是显著性阈值(通常为 0.05).</li>
</ul></li>
<li>不过,简单计算 FWE = nα 并不一定准确.
<ul class="org-ul">
<li>数据和检验之间的依赖性可能会降低 FWE 值.</li>
<li><p>
一个稍微不那么保守的估计公式是:
</p>
\begin{equation}
FWE = 1 − (1 − α)ⁿ.
\end{equation}</li>
</ul></li>
<li>依赖性往往难以测量或估计.
<ul class="org-ul">
<li>举例来说,如果比较条件 "A" vs "B" 和 "A" vs "C",</li>
<li>由于两次检验都涉及 "A",它们是相关的.</li>
<li>同样,"B" 和 "C" 的数据也可能存在关联.</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org73b5fd4" class="outline-4">
<h4 id="org73b5fd4"><span class="section-number-4">10.10.1.</span> Solutions to the multiple comparisons problem</h4>
<div class="outline-text-4" id="text-10-10-1">
<ul class="org-ul">
<li>解决多重比较问题的关键是 <b>调整 p 值显著性阈值</b>,使之匹配家族错误率(FWE),而不是单个检验的阈值.</li>
<li>根据数据性质和检验数量,可以使用不同的校正方法.以下列出几种常见方法(并不意味着未列出的方法无效或
不适用).在第 14 章的 ANOVA(方差分析)中,还会学习更多校正方法.</li>
<li><b>Bonferroni 校正</b>
<ul class="org-ul">
<li>原理:将 FWE 设置为所需阈值(如 0.05),单个检验的阈值设为 α ÷ n,其中 n 是检验次数.例:3 次检验
→ 每个检验阈值 p &lt; 0.05 / 3 = 0.01667.</li>
<li>优点:简单,常用,且适用于独立检验.</li>
<li>局限性:
<ol class="org-ol">
<li>太严格,因为它假设检验之间完全独立.</li>
<li>仅基于检验数量,不考虑数据特征,对于相关数据(如时间序列,图像)效果不佳.</li>
<li>会增加第二类错误(Type-II error)概率,从而可能漏掉真实结果.</li>
</ol></li>
</ul></li>
<li>适用情况:检验数量不多(通常是个位数)且测试相互独立.</li>
<li><b>FDR(错误发现率,False Discovery Rate)校正</b>
<ul class="org-ul">
<li>适用场景:多组 p 值且检验高度相关,如时间序列,图像,地理数据,基因组数据.</li>
<li>原理:FDR 算法会根据 p 值分布计算一个校正阈值 q. q 的定义:预期错误发现的数量 / 预期真实发现的数量.</li>
<li>局限性:</li>
<li>阈值依赖于所评估的 p 值集合,因此同一个结果在不同数据集下可能被判定为显著或不显著.</li>
</ul></li>
<li><b>聚类校正(Cluster Correction)</b>
<ul class="org-ul">
<li>适用场景:具有相关结构的数据,如时间序列或图像.</li>
<li>原理:只有位于连续显著结果组成的"簇"中,检验才被认为显著.</li>
<li>簇阈值的确定方式:
<ol class="org-ol">
<li>基于先验设定(例如簇必须包含至少 100 ms 的连续显著时间点,或至少 6 个空间连续显著像素).</li>
<li>基于置换检验(Permutation Test)来计算零假设下的经验簇大小.</li>
</ol></li>
</ul></li>
<li><b>是否总要进行多重比较校正?</b>
<ul class="org-ul">
<li>校正的目的:控制第一类错误(Type-I error).</li>
<li>代价:会增加第二类错误(Type-II error)的概率.</li>
<li>建议:在可能时使用校正方法,但要理解它会让一些真实效应不再显著.</li>
</ul></li>
<li><b>对多重比较校正结果的解释</b>
<ul class="org-ul">
<li>情况分类:
<ol class="org-ol">
<li>不显著(p &gt; 0.05)</li>
<li>显著(未进行多重比较校正)</li>
<li>显著(经过多重比较校正)</li>
</ol></li>
</ul></li>
<li>中间类别(显著但未通过校正):可能有理论意义,但应谨慎和定性解释.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org37504b4" class="outline-3">
<h3 id="org37504b4"><span class="section-number-3">10.11.</span> Degrees of freedom</h3>
<div class="outline-text-3" id="text-10-11">
<ul class="org-ul">
<li><b>自由度(Degrees of Freedom, df)直观示例</b>
<ul class="org-ul">
<li>假设一个由 3 个元素组成的数据集,其平均值为 5,且其中两个数字是 2 和 3,那么第三个数字是多少?</li>
<li>已知公式:5 = (2 + 3 + x) / 3</li>
<li>解得:x = 10</li>
</ul></li>
<li>说明:当已知 <b>N−1</b> 个数据值和一个样本统计量时,可以计算出第 N 个数据值.</li>
<li>这个系统只有 <b>两个自由度</b>,因为在已知平均值的约束下,只有两项可以自由变化,第三项固定由其他两项及平均值决定.</li>
<li><b>正式定义</b>
<ul class="org-ul">
<li>一个描述性统计量来源于数据,并可能收到约束条件的限制.</li>
<li>自由度是指在不破坏这些约束的情况下,可以自由变化的 <b>独立信息项数量</b>.</li>
<li>示例中,数据值 2 和 3 是两个独立的信息,而平均值是约束条件.</li>
</ul></li>
<li><b>自由度的作用</b>
<ul class="org-ul">
<li>自由度常作为零假设(H₀)分布的参数,用于推断总体特性.</li>
<li>在回归分析和方差分析(ANOVA)中,可用 df 检查错误和修正结果解释.</li>
</ul></li>
<li><b>一般计算思路</b> 没有单一公式适用于所有分析.通常可以简化为:
<ul class="org-ul">
<li>df = N − k</li>
<li>N = 观察值的数量(可能是研究中的个体数,或实验中的组/条件数)</li>
<li>k = 参数数量</li>
<li>说明:某些分析会对 df 进行修正,比如调整方差不等的情况.</li>
<li>要记住:"自由度 = 观察数 − 参数数" 是一个实用的简化公式,而不是普适的数学定律.</li>
</ul></li>
<li><b>一些具体示例</b>
<ul class="org-ul">
<li>t 检验(t-test):
<ol class="org-ol">
<li>单组:df = N − 1</li>
<li>双组:df = N − 2</li>
</ol></li>
</ul></li>
<li>相关分析(Correlation):
<ul class="org-ul">
<li>df = N − 2(两个变量)</li>
</ul></li>
<li>回归分析(Regression):
<ul class="org-ul">
<li>df = N − k(k 为自变量数量)</li>
</ul></li>
<li><b>自由度是否必须是整数?</b>
<ul class="org-ul">
<li>通常 df 是整数,但不一定必须是整数.</li>
<li>某些分析会对 df 施加校正,例如方差不等时的修正.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org53c7704" class="outline-2">
<h2 id="org53c7704"><span class="section-number-2">11.</span> Chapter 11: The T-Test Family</h2>
<div class="outline-text-2" id="text-11">
</div>
<div id="outline-container-org4e0a05b" class="outline-3">
<h3 id="org4e0a05b"><span class="section-number-3">11.1.</span> Purpose and interpretation of the t-test</h3>
<div class="outline-text-3" id="text-11-1">
<ul class="org-ul">
<li>t 检验是最重要,最常用的推论统计方法之一.</li>
<li>根据数据的性质(组数,样本量,方差等),t 检验有多种具体实现方式,但它们都共享一个共同的分析框架.</li>
<li><b>本节学习目标</b>
<ul class="org-ul">
<li>理解 t 检验的基本框架</li>
<li>学会解释 t 检验结果</li>
<li>掌握如何根据 t 值推导出 p 值</li>
<li>了解 t 检验的基本假设条件</li>
</ul></li>
<li>针对不同数据场景的 t 检验变体:
<ul class="org-ul">
<li>单样本 t 检验(One-sample t-test)</li>
<li>双样本 t 检验(Two-sample t-test)</li>
<li>方差相等 vs 方差不等的情形</li>
</ul></li>
<li>当数据不满足 t 检验假设时的非参数替代方法</li>
<li>t 检验的扩展家族成员:
<ul class="org-ul">
<li>例如用于检验相关系数显著性的 t 检验</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orgdbe4979" class="outline-4">
<h4 id="orgdbe4979"><span class="section-number-4">11.1.1.</span> The purpose of a t-test</h4>
<div class="outline-text-4" id="text-11-1-1">
<ul class="org-ul">
<li>t 检验的目的是判断样本均值是否显著不同于某个指定的零假设(H₀)值.</li>
<li><p>
常见的三种使用情境如下(见图 11.1),虽然表面上看是不同的情况,但在概念与数学上它们非常相似.
</p>

<div id="org0f35d14" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/ms/11-1.png" alt="11-1.png" />
</p>
<p><span class="figure-number">Figure 115: </span>ms/11-1.png</p>
</div></li>
<li><b>单样本 t 检验(One-sample t-test, 图 11.1A)</b></li>
<li>情景:
<ul class="org-ul">
<li>拥有一个数据样本,目标是判断样本均值是否显著偏离预设的 H₀ 值.</li>
<li>图示中每个圆表示一个数据点,水平虚线表示 H₀ 值.</li>
<li>例如:设虚线代表 IQ = 100,数据点是某班级儿童的 IQ 值.虽然不是所有数据都高于 H₀ 值,但平均值可能
显著高于 100.</li>
</ul></li>
<li><b>配对样本 t 检验(Paired-samples t-test, 图 11.1B)</b></li>
<li>情景:
<ul class="org-ul">
<li>只有一个群体,但对其进行了两次测量.</li>
<li>例如:一项研究在 30 家公司进行团队建设活动前(pre)和后(post)分别记录销售额.有些公司销售额增
加,有些减少,问题是:整个样本的平均销售额是否显著增加?</li>
</ul></li>
<li><b>独立样本 t 检验(Independent samples t-test, 图 11.1C)</b></li>
<li>情景:
<ul class="org-ul">
<li>有两个独立的群体,目标是判断两组均值是否不同.</li>
<li>两组样本大小和方差可能不同,但我们只关注均值差异.</li>
<li>例子:
<ol class="org-ol">
<li>一项研究对比参加学习辅导课的学生(组 1)和未参加的学生(组 2)的考试分数.</li>
<li>目标是判断参加学习辅导课是否对考试成绩有显著影响.</li>
<li>图示中两条线代表两组考试分数的直方图.</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org135f68b" class="outline-4">
<h4 id="org135f68b"><span class="section-number-4">11.1.2.</span> General t-test formula</h4>
<div class="outline-text-4" id="text-11-1-2">
<ul class="org-ul">
<li>方程 11.1 展示了 t 检验的一般公式.
<ul class="org-ul">
<li>本章后续内容会介绍针对前面提到的具体情景(单样本,配对样本,独立样本等)的公式改进版本.</li>
<li>不过,这个公式可以看作是 <b>t 检验家族的"通用模板"</b>,强烈建议你将它记忆下来.</li>
<li><p>
公式如下:
</p>
\begin{equation}
 t_{df} = \frac{\bar{x} - h_0}{s / \sqrt{n}} \tag{11.1}
\end{equation}</li>
<li>df :自由度(degrees of freedom)</li>
<li>\(( h_0 )\):零假设值(null hypothesis value)</li>
<li>\(( s )\):样本标准差(sample standard deviation</li>
<li>\(( n )\):样本量(sample size</li>
<li>分母:标准误(Standard Error of the Mean, SEM,参见第 9.3 节)</li>
</ul></li>
<li>关于 t 检验的一些说明
<ol class="org-ol">
<li>针对事先选定的值进行 t 检验的是**单样本 t 检验**.
<ul class="org-ul">
<li>一个典型的 H₀ 值是零,也就是说,单样本 t 检验常用于判断数据集的均值是否不同于零.在之前提到的
儿童 IQ 示例中,H₀ 值为 100.</li>
</ul></li>
<li>两个组之间的 t 检验称为**双样本 t 检验**.
<ul class="org-ul">
<li>双样本 t 检验可以是配对的或非配对的,标准差和样本量可以相同或不同.</li>
<li>这些情况会对 t 检验公式进行相应的修改,我将在本章后面讨论.</li>
</ul></li>
<li>t 检验基于均值和标准差.
<ul class="org-ul">
<li>即使某些数据点的效果方向与总体均值相反,t 检验也可能在统计上显著.</li>
<li>例如,比较男性与女性的身高的显著 t 检验:平均来看,成年男性比成年女性高,但并不是每个男性都比
每个女性高.</li>
</ul></li>
<li>一种理解 t 检验公式的方式是:它表示**标准化的均值差异**,
<ul class="org-ul">
<li>即将平均效应除以数据的变异度.这种理解会将 t 检验与检验统计量的一般概念&#x2013;信噪比&#x2013;联系起来.</li>
<li>另一种理解 t 值的方法:*分子是均值效应,分母是 SEM(均值标准误)*</li>
<li>SEM 反映了我们估计总体均值的精确程度.</li>
<li>SEM 越小,说明我们可以更准确地估计总体均值,从而更容易区分样本均值与给定的 H₀ 值.</li>
<li>分子与分母的单位相同,因此 t 值是无单位的,它表示样本均值与 H₀ 值的距离.</li>
</ul></li>
<li>t 值受样本量影响.
<ul class="org-ul">
<li>特别是,当样本量增加时,即使均值和标准差不变,t 值也会增大.</li>
<li>这意味着 H₀ 下 t 值的分布部分取决于样本量(可以想象在分子中加入 √n 因子:即使均值和标准差保持
不变,样本量增加也会使 t 值变大).</li>
<li>因此,我们需要知道**自由度**来将某个 t 值与一个 p 值对应起来.</li>
</ul></li>
<li>t 检验的符号在一定程度上是任意的.
<ul class="org-ul">
<li>你可以写成 (x − H₀) 或 (H₀ − x),</li>
<li>t 值的大小及其对应的双尾 p 值都不会受到影响.</li>
<li>你可以选择更方便解释的符号:如果测试读书后考试成绩的增加,那么正的 t 值更直观;如果测试新外科
手术技术减少术后疼痛,那么负的 t 值更好解释.</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgd1506fb" class="outline-4">
<h4 id="orgd1506fb"><span class="section-number-4">11.1.3.</span> Degrees of freeedom of t-tests</h4>
<div class="outline-text-4" id="text-11-1-3">
<ul class="org-ul">
<li>请记住,**df(自由度)**表示样本数据集中可以*独立变化*的数据值的最大数量.</li>
<li>因为 **t 检验**是在检验样本均值,所以与某个 t 值相关的自由度,就是在已知样本均值的情况下,可以自由变化的数据个数.</li>
<li><b>单样本 t 检验</b>
<ul class="org-ul">
<li>只有一个样本且只有一个均值</li>
<li>自由度公式:df = N - 1</li>
<li>其中 N 为样本量</li>
</ul></li>

<li><b>配对样本 t 检验</b>
<ul class="org-ul">
<li>实际上是单样本 t 检验的变形</li>
<li>方法:将两次测量的结果相减,得到每个个体一个数据值</li>
<li>自由度公式:df = N - 1</li>
</ul></li>
<li><b>独立样本 t 检验</b>
<ul class="org-ul">
<li>两个独立样本</li>
<li>自由度公式:df = N₁ + N₂ - 2</li>
<li>推导:(N₁ - 1) + (N₂ - 1) → N₁ + N₂ - 2</li>
<li>在某些情况下,自由度计算需要考虑样本方差的不同(后续章节会讨论)</li>
</ul></li>
<li><b>t 值与自由度的排版</b>
<ul class="org-ul">
<li>下标格式:t₁₃ = 2.56</li>
<li>括号格式:t(13) = 2.56</li>
<li>使用哪种格式取决于个人喜好或出版商规定</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: harrifeng@outlook.com</p>
<p class="date">Created: 2025-10-22 Wed 20:17</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
