<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-02-23 Thu 22:13 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>tic</title>
<meta name="author" content="harrifeng@outlook.com" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">tic</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgf4d70d4">1. Introduction to this book</a>
<ul>
<li><a href="#org784a56a">1.1. What is linear algebra and why learn it?</a></li>
<li><a href="#org23d4d18">1.2. About this book</a></li>
<li><a href="#org66661f0">1.3. Prerequisites</a></li>
<li><a href="#orgd1cbaa0">1.4. Practice, exercises and code challenges</a></li>
<li><a href="#orgcc5ff57">1.5. Online and other resources</a></li>
</ul>
</li>
<li><a href="#org4d29a9d">2. Vectors</a>
<ul>
<li><a href="#org3ff6a52">2.1. Scalars</a></li>
<li><a href="#org99e66f2">2.2. Vectors: geometry and algebra</a></li>
<li><a href="#orga793a6b">2.3. Transpose operation</a></li>
<li><a href="#org1bedc45">2.4. Vector addition and subtraction</a></li>
<li><a href="#org06be245">2.5. Vector-scalar multiplication</a></li>
</ul>
</li>
<li><a href="#org0c5be5c">3. Vector multiplications</a>
<ul>
<li><a href="#orgc1f3702">3.1. Vector dot product: Algebra</a></li>
<li><a href="#orgab46260">3.2. Dot product properties</a>
<ul>
<li><a href="#org6edc1fb">3.2.1. vector product和scalar的结合律</a></li>
<li><a href="#org8e1e45e">3.2.2. vector product和vector的结合律</a></li>
<li><a href="#org52cc786">3.2.3. commutative property</a></li>
<li><a href="#org306d748">3.2.4. Distributive property</a></li>
</ul>
</li>
<li><a href="#org2d87343">3.3. Vector dot product: Geometry</a></li>
<li><a href="#orgde9a35f">3.4. Algebraic and geometric equivalence</a></li>
<li><a href="#org1ea753a">3.5. Linear weighted combination</a></li>
<li><a href="#orgb0add86">3.6. The outer product</a></li>
<li><a href="#org1dcc7c0">3.7. Element-wise (Hadamard) vector product</a></li>
<li><a href="#org253680a">3.8. Cross product</a></li>
<li><a href="#org3ba77a4">3.9. Unit vectors</a></li>
</ul>
</li>
<li><a href="#orgb55fc5c">4. Vector Spaces</a>
<ul>
<li><a href="#orgd7ae83f">4.1. Dimensions and fields in linear algebra</a></li>
<li><a href="#orgc53bce4">4.2. Vector spaces</a></li>
<li><a href="#org7ac703f">4.3. Subspaces and ambient spaces</a></li>
<li><a href="#org1cb53c9">4.4. Subsets</a></li>
<li><a href="#org12879fc">4.5. Span</a></li>
<li><a href="#orgdde7fc2">4.6. Linear independence</a></li>
<li><a href="#org748bff0">4.7. Basis</a></li>
</ul>
</li>
<li><a href="#orgce822ba">5. Matrices</a>
<ul>
<li><a href="#org82e99cf">5.1. Interpretations and uses of matrices</a></li>
<li><a href="#org59481d5">5.2. Matrix terminology and notation</a></li>
<li><a href="#org7217f7e">5.3. Matrix dimensionalities</a></li>
<li><a href="#org3557ed4">5.4. The transpose opertion</a></li>
<li><a href="#org0df7ed1">5.5. Matrix zoology</a></li>
<li><a href="#org1a40683">5.6. Matrix addition and subtraction</a></li>
<li><a href="#org17aa249">5.7. Scalar-matrix multiplication</a></li>
<li><a href="#org6e15453">5.8. "Shifting" a matrix</a></li>
<li><a href="#orgb2a22c2">5.9. Diagonal and trace</a></li>
</ul>
</li>
<li><a href="#orge4407b1">6. Matrix multiplication</a>
<ul>
<li><a href="#org24cbec4">6.1. "Standard" matrix multiplication</a>
<ul>
<li><a href="#orgd32c0c2">6.1.1. The "element perspective"</a></li>
<li><a href="#org82d827e">6.1.2. The "layer perspective"</a></li>
<li><a href="#org67c6173">6.1.3. The "column perspective"</a></li>
<li><a href="#org6ff267d">6.1.4. The "row perspective"</a></li>
<li><a href="#orge417cb6">6.1.5. Visually summarizes the different perspective</a></li>
</ul>
</li>
<li><a href="#org27e3e5e">6.2. Multiplication and equations</a></li>
<li><a href="#org028012b">6.3. Matrix multiplication with a diagonal matrix</a></li>
<li><a href="#orge1ce0c5">6.4. LIVE EVIL !(a.k.a order of operations)</a></li>
<li><a href="#org5996f7d">6.5. Matrix-vector multiplication</a></li>
<li><a href="#org26dd487">6.6. Creating symmetric matrices</a></li>
<li><a href="#org47c6228">6.7. Multiplication of two symmetric matrices</a></li>
<li><a href="#org4f52c05">6.8. Element-wise(Hadamard) multiplication</a></li>
<li><a href="#orgf20128d">6.9. Frobenius dot product</a></li>
<li><a href="#org8f52af7">6.10. Matrix norms</a></li>
<li><a href="#org72e37e0">6.11. What about matrix division?</a></li>
</ul>
</li>
<li><a href="#org711736b">7. Rank</a>
<ul>
<li><a href="#org7d641b1">7.1. Six things about matrix rank</a></li>
<li><a href="#orgb9bfc46">7.2. Interpretations of matrix rank</a></li>
<li><a href="#orgee7da04">7.3. Computing matrix rank</a></li>
<li><a href="#org33a65be">7.4. Rank and scalar multiplication</a></li>
<li><a href="#orge7cca5e">7.5. Rank of added metrics</a></li>
<li><a href="#orgc00c3be">7.6. Rank of multiplied matrices</a></li>
<li><a href="#org1332d98">7.7. Rank of A, AT, ATA, and AAT</a></li>
<li><a href="#org01fd343">7.8. Rank of random matrices</a></li>
<li><a href="#org42c5035">7.9. Full-rank by "shifting"</a></li>
<li><a href="#org2c3ab1f">7.10. Difficulties in computing rank in practice</a></li>
<li><a href="#org7ac0d32">7.11. Rank and span</a></li>
</ul>
</li>
<li><a href="#org5677fe7">8. Matrix spaces</a>
<ul>
<li><a href="#org667bb4f">8.1. Column space of a matrix</a></li>
<li><a href="#org5220fa4">8.2. Column space: A and AAT</a></li>
<li><a href="#org91ea50a">8.3. Determining whether \(v \in C(A)\)</a></li>
<li><a href="#org1d65c13">8.4. Row space of a matrix</a></li>
<li><a href="#org0b4db5b">8.5. Row spaces of A and ATA</a></li>
<li><a href="#org3b8338c">8.6. Null space of a matrix</a></li>
<li><a href="#org55794e8">8.7. Geometry of the null space</a></li>
<li><a href="#orgcf300c2">8.8. Orthogonal subspaces</a></li>
<li><a href="#orga9e8bdb">8.9. Matrix space orthogonalities</a></li>
<li><a href="#org27a1a22">8.10. Dimensionalitis of matrix spaces</a></li>
<li><a href="#org764bdc5">8.11. More on Ax = b and Ay = 0</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgf4d70d4" class="outline-2">
<h2 id="orgf4d70d4"><span class="section-number-2">1.</span> Introduction to this book</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org784a56a" class="outline-3">
<h3 id="org784a56a"><span class="section-number-3">1.1.</span> What is linear algebra and why learn it?</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>线性代数是数学中关于vector和matrix的分支</li>
<li>在现代,线性代数的重要性得到加强,因为很多数据都是以matrix的形势存储的,比如:
<ul class="org-ul">
<li>统计学</li>
<li>机器学习</li>
<li>计算机图形学</li>
<li>压缩算法</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org23d4d18" class="outline-3">
<h3 id="org23d4d18"><span class="section-number-3">1.2.</span> About this book</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>本书对机器学习爱好者很有益处</li>
<li>本书仅仅需要高中数学知识</li>
<li>对于希望了解了线性代数之后进行深度学习,统计学的人来说,太过于抽象的线性代数学习比较浪费时间</li>
<li>本书注重实践,而不是理论</li>
<li>本书是一本数学书,所以请不要奇怪书中有公式.但是数学不仅仅是公式:
<ul class="org-ul">
<li>在我看来,数学的目的是理解概念</li>
<li>公式是展示概念的一种方式</li>
<li>但是文章,图片,甚至是代码都非常重要</li>
</ul></li>
<li>公式和其他表现形式有个微妙的平衡:
<ul class="org-ul">
<li>公式提供了正规而严格的表现形式,但是无法提供直觉力</li>
<li>其他表达形式(文章,类比,图表,代码)提供了直觉力,但是不够严格和正规</li>
</ul></li>
<li>本书的公式按照重要性分为三个等级:
<ol class="org-ol">
<li>简单的,或者是为了回忆之前讨论过的公式.那么就是优先度最低的公式,他们会和文本在一块,比如 \(x(yz) = (xy)z\)</li>
<li><p>
更加重要的公式,会有自己单独的行
</p>
\begin{equation}
\sigma = x(yz) = (xy)z\tag{1.1}
\end{equation}</li>
<li>最最重要的公式会有自己的区域来说明
<ul class="org-ul">
<li><p>
公式1.2
</p>
\begin{equation}
\sigma = x(yz) = (xy)z\tag{1.2}
\end{equation}</li>
<li>这个公式的要点1</li>
<li>这个公式的要点2</li>
</ul></li>
</ol></li>
<li>线性代数的很多概念可以使用如下两种数学分支的公式来表示:
<ul class="org-ul">
<li>Geometric: 几何方法,优点是提供图形化的直观展示,缺点是人类只能理解2D和3D的图像</li>
<li>Algebraic: 代数方法,优点是严谨的证明和计算机的介入,可以非常容易的扩展到N维</li>
</ul></li>
<li>注意,并不是所有的线性代数概念都可以使用几何和代数法来展示</li>
</ul>
</div>
</div>
<div id="outline-container-org66661f0" class="outline-3">
<h3 id="org66661f0"><span class="section-number-3">1.3.</span> Prerequisites</h3>
<div class="outline-text-3" id="text-1-3">
<ul class="org-ul">
<li>需要有学习线性代数的主动性</li>
<li>需要有高中数学基础</li>
<li>不需要有微积分知识</li>
<li>不需要任何线性代数知识,知道矩阵的计算肯定有好处</li>
<li>在计算机发明以前,数学里面的高阶概念,通常都是天才们依靠自己"能够把公式想象成图像"的能力来理解的,
现在有了计算机,我们可以享受到天才们的超能力了</li>
<li>本书使用Matlab(Octave)和Python来解决问题,其中Matlab更为容易实现线性代数</li>
</ul>
</div>
</div>
<div id="outline-container-orgd1cbaa0" class="outline-3">
<h3 id="orgd1cbaa0"><span class="section-number-3">1.4.</span> Practice, exercises and code challenges</h3>
<div class="outline-text-3" id="text-1-4">
<ul class="org-ul">
<li>为了真正理解线性代数,必须做题</li>
<li>本书习题不多,目的是希望你全部都做完</li>
<li>本书习题分为三类:
<ul class="org-ul">
<li>Practice problem: 在subsection之后的,easy级别,答案就在后面,如果做不出来,
那么不需要继续向前读</li>
<li>Exercise: 在chapter之后的,中等难度,答案就在后面,需要手算,而不是用计算机算</li>
<li>Codechallenges: 需要使用计算机编程来实现的,比较难,也有答案</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgcc5ff57" class="outline-3">
<h3 id="orgcc5ff57"><span class="section-number-3">1.5.</span> Online and other resources</h3>
<div class="outline-text-3" id="text-1-5">
<ul class="org-ul">
<li>本书中的解释如果你理解不了,可以从网络上搜索从其他角度的解释来让你明白</li>
<li>本书有配套网络课程,喜欢网络课程学习方法的可以关注</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org4d29a9d" class="outline-2">
<h2 id="org4d29a9d"><span class="section-number-2">2.</span> Vectors</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org3ff6a52" class="outline-3">
<h3 id="org3ff6a52"><span class="section-number-3">2.1.</span> Scalars</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>我们不是从向量(vector)开始,而是从标量(scalar)开始</li>
<li>所谓标量(scalar)就是一个单独的数字,比如4或者-17.3等等</li>
<li>在数学的其他领域,标量有时候会被称之为常量(constant)</li>
<li>标量虽然简单,但是在线性代数里面却扮演者很多重要的角色:
<ul class="org-ul">
<li>subspaces</li>
<li>linear combination</li>
<li>eigendecomposition</li>
</ul></li>
<li>标量的名字(scalar)是scale的名词形式:
<ul class="org-ul">
<li>scale就有伸展,拉长的意思</li>
<li>scalar就有伸展拉长vector和matrix,并且不改变他们的方向(direction)</li>
</ul></li>
<li>标量在图上线上就是线上的一个空心的point,比如下图中的scalar就是一个1.5
<ul class="org-ul">
<li><p>
图2-1
</p>

<div id="orgfb382e2" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/2-1.png" alt="2-1.png" />
</p>
<p><span class="figure-number">Figure 1: </span>tic/2-1.png</p>
</div></li>
</ul></li>
<li>注意:本书中标量都使用希腊小写字母( \(\lambda, \alpha, \gamma\) ),以便和vector和matrix区分</li>
<li><p>
使用python来表示标量,就是一个变量
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">aScalar</span> = 5
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org99e66f2" class="outline-3">
<h3 id="org99e66f2"><span class="section-number-3">2.2.</span> Vectors: geometry and algebra</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li><b>Geometry</b> vector是一个line,由两个属性决定:
<ul class="org-ul">
<li>magnitude(长度)</li>
<li>direction(方向)</li>
</ul></li>
<li>line可以在任意维度存在(1维,2维,3维,&#x2026;N维)</li>
<li>如图
<ul class="org-ul">
<li><p>
图2-2
</p>

<div id="org2581e0a" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/2-2.png" alt="2-2.png" />
</p>
<p><span class="figure-number">Figure 2: </span>tic/2-2.png</p>
</div></li>
<li>上图左边是在2维空间的vector[2,3]</li>
<li>上图右边是在3维空间的vector[2,3,5]</li>
</ul></li>
<li>需要注意的是,vector的定义不包含它的起止位置(position)的,这是和坐标系不同的地方</li>
<li>在坐标系里面,每个坐标都是在空间中唯一的</li>
<li>从另外一个角度上讲,如果假设vector是从[0,0]开的话,那么vector和coordinate就是同一回事了.</li>
<li><p>
所以,起点(英文叫tail,注意是尾巴的意思,英文认为终点的是箭头,起点是尾巴)为[0,0]的vector被叫做在他
的standard position
</p>
<pre class="example" id="orgc350832">
A vector with its tail at the origin is said to be in its standard position
</pre></li>
<li>如图
<ul class="org-ul">
<li><p>
图2-3
</p>

<div id="org90e486b" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/2-3.png" alt="2-3.png" />
</p>
<p><span class="figure-number">Figure 3: </span>tic/2-3.png</p>
</div></li>
<li>上图中三个vector(line)都是相同的,因为他们的长度和方向都一样</li>
<li>上图中的三个坐标(圆圈)都是不相同的,因为坐标本来就全局唯一,没有两个一样的坐标</li>
<li>比较黑的line就是vector in its standard position. 这种情况下的vector[1,-2]的head和坐标[1,-2]相重叠</li>
</ul></li>
<li><p>
使用如下代码画vector
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np
<span style="color: #859900; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #859900; font-weight: bold;">as</span> plt

<span style="color: #268bd2;">v</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>2, -1<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
plt.plot<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>0, v<span style="color: #859900;">[</span>0<span style="color: #859900;">]</span><span style="color: #d33682;">]</span>, <span style="color: #d33682;">[</span>0, v<span style="color: #859900;">[</span>1<span style="color: #859900;">]</span><span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
plt.axis<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>-3, 3, -3, 3<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
plt.show<span style="color: #268bd2;">()</span>
</pre>
</div></li>
<li><b>Algebra</b> 从代数的角度上说,vector就是一个ordered list(成员是number)</li>
<li>一个vector内部number的数量就叫做vector的dimensionality,比如:
<ul class="org-ul">
<li><p>
2D的vector例子
</p>
<pre class="example" id="org1c56813">
[1 -2], [4 1], [10000 0]
</pre></li>
<li><p>
3D的vector例子
</p>
<pre class="example" id="org33edbe1">
[3.14 e 0], [3 1 4], [2 -7 8]
</pre></li>
</ul></li>
<li>vector内部number的顺序是非常重要的,不同的顺序代表不同的vector,比如下面两个vector就不同,虽然他们
的dimensionality一样,数据也一样:
<ul class="org-ul">
<li>[3 1]</li>
<li>[1 3]</li>
</ul></li>
<li><b>Brackets</b> vector可以使用square bracket(方括号)或者是parentheses(园括号)</li>
<li>我个人认为方括号更加优雅,也不容易混淆,所以一直用方括号</li>
<li>但是有些情况下,你可能会遇到使用圆括号来代替方括号,比如下面两者在这种情况下是等价的:
<ul class="org-ul">
<li>[2 5 5]</li>
<li>(2 5 5)</li>
</ul></li>
<li>vector的几何表示,在2D表达中非常有用,在3D表达中也马马虎虎,但是更多维度就不行了</li>
<li><p>
vector的代数表示,却可以让我们在任何维度上,扩展vector,比如下面的公式就非常清晰的解释了什么是6D vector
</p>
<pre class="example" id="org408a42f">
[3 4 6 1 -4 5]
</pre></li>
<li><p>
vector成员也不仅限于number,其成员还可以是function,比如下面的例子
</p>
\begin{equation}
\mathbf{v} = [\cos(t)\; \sin(t)\; t]
\end{equation}</li>
<li>本书不讨论上面的情况,本书中vector的所有成员都是普通number</li>
<li><b>Vector orientation</b> vector可以"站着",也可以"躺着":
<ul class="org-ul">
<li><p>
站着的vector被叫做column vector,如下
</p>
\begin{equation}
\left[ {\begin{array}{cccc}
7 \\
3 \\
5 \\
0 \\
\end{array} } \right]
\end{equation}</li>
<li><p>
躺着的vector被叫做row vector,如下
</p>
\begin{equation}
[0 \;1 \;3]
\end{equation}</li>
</ul></li>
<li><b>IMPORTANT</b> 默认情况下,vector是column orientation的,原因可能是在和matrix进行相乘的时候,vector在
matrix右边(作为被乘matrix,一个某个方向上只有一维的matrix)才有意义, 一般matrix都是MxN的大小,那么
在matrix右边,必须是Nx1,而不能是1xN的形状</li>
<li><p>
在matrix中,使用空格分离是row vector, 使用`;`分离,是column vector
</p>
<pre class="example" id="org7c9c949">
v1 = [2 5 4 7] % row vector
v2 = [2; 5; 4; 7] % column vector
</pre></li>
<li><p>
在python中, list(以及numpy array)没有默认的orientation,所以在某些情况下一定要指定orientation的
时候,numpy要用比较麻烦的方式实现
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">v1</span> = <span style="color: #268bd2;">[</span>2, 5, 4, 7<span style="color: #268bd2;">]</span>               <span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">list</span>
<span style="color: #268bd2;">v2</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>2, 5, 4, 7<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>     <span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">array, no orientation</span>
<span style="color: #268bd2;">v3</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span><span style="color: #859900;">[</span>2<span style="color: #859900;">]</span>, <span style="color: #859900;">[</span>5<span style="color: #859900;">]</span>, <span style="color: #859900;">[</span>4<span style="color: #859900;">]</span>, <span style="color: #859900;">[</span>7<span style="color: #859900;">]</span><span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span> <span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">column vector</span>
<span style="color: #268bd2;">v4</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span><span style="color: #859900;">[</span>2, 5, 4, 7<span style="color: #859900;">]</span><span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>       <span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">row vector</span>
</pre>
</div></li>
<li><b>Notation</b> 在书面书写中,我们只需要boldface字母就可以表示vector了,比如 \(\mathbf{v}\), 但是如果是论
文中,我们一定需要在vector上面加上剪头,比如 \(\vec{\mathbf{v}}\)</li>
<li>为了表达vector里面的一个特定成员,我们会使用下标,比如 \(\mathbf{v} = [4\;0\;2]\), 的第二个成员表示
为 \(v_2 = 0\), 第ith个表示为 \(v_i\) ,注意这里的小写字母没有加粗</li>
<li><p>
如果小写字母加粗的下划线加i,也就是 \(\mathbf{v_i}\) 那么表示相关的如下vectors
</p>
\begin{equation}
(\mathbf{v_1},\mathbf{v_2},...,\mathbf{v_i})
\end{equation}</li>
<li><b>Zeros vector</b>, 所有成员都是0的vector叫做zeros vector,注意是所有成员,缺一个都不叫zeros vector</li>
<li>zeros vector有一些特殊的地方,比如:
<ul class="org-ul">
<li>zeros 没有direction, 我的意思不是说它的direction为0,我是说它的direction未知(undefined),因为
zeros vector的magnitude为0,讨论一个magnitude为0的vector的direction是没有意义的</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga793a6b" class="outline-3">
<h3 id="orga793a6b"><span class="section-number-3">2.3.</span> Transpose operation</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>把vector在column vector和row vector之间相互转换的操作,叫做transpose</li>
<li>transpose只更改orientation,其他的element内容和排序都不变</li>
<li>我们使用一个上标T来代表这个操作,那么就有如下三个例子:
<ul class="org-ul">
<li><p>
row vector转换成 column vector
</p>
\begin{equation}
 [7\;3\;5]^T  = \left[ {\begin{array}{cccc}
7 \\
3 \\
5 \\
\end{array} } \right]
\end{equation}</li>
<li><p>
column vector转换成 row vector
</p>
\begin{equation}
\left[ {\begin{array}{cccc}
7 \\
3 \\
5 \\
\end{array} } \right]^T = [7\;3\;5]
\end{equation}</li>
<li><p>
两次TT操作,可以抵消
</p>
\begin{equation}
[7\;3\;5]^{TT}=[7\;3\;5]
\end{equation}</li>
</ul></li>
<li>我们之前说过,我们assume, vector是column vector,所以:
<ul class="org-ul">
<li>\(\mathbf{v}\) 就是column vector</li>
<li>\(\mathbf{v}^T\) 就是row vector</li>
</ul></li>
<li>在印刷书籍中,在文字间写column vector非常不方便,所以文字书籍中往往是把column vector写成row vector
的转置形式,比如 \(\mathbf{w} = [1\;2\;3]^T\)</li>
<li><p>
在代码中转置很方便
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">v1</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span><span style="color: #859900;">[</span>2, 5, 4, 7<span style="color: #859900;">]</span><span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>  <span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">row vector</span>
<span style="color: #268bd2;">v2</span> = v1.T  <span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">column vector</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org1bedc45" class="outline-3">
<h3 id="org1bedc45"><span class="section-number-3">2.4.</span> Vector addition and subtraction</h3>
<div class="outline-text-3" id="text-2-4">
<ul class="org-ul">
<li><b>Geometry</b> 我们主要通过下面的四个图来理解vector的加和减
<ul class="org-ul">
<li><p>
图2-5
</p>

<div id="orgeae0207" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/2-5.png" alt="2-5.png" />
</p>
<p><span class="figure-number">Figure 4: </span>tic/2-5.png</p>
</div></li>
<li>第一幅图是介绍我们这次加减法的两个成员:
<ol class="org-ol">
<li>v1: [0 2]</li>
<li>v2: [1 1]</li>
</ol></li>
<li>第二幅图是介绍如何计算加法: 把v2的起点从standard position移动到v1的终点,那么从v1的起点到v2的终
点,就是新的vector</li>
<li>第三幅图是介绍减法的第一种做法,就把v2乘以-1,变成[-1,-1], 那么v1-v2就成了v1 + (-1 * v2),加法计算
方法和图二一致</li>
<li>第四幅图是介绍减法的第二种做法,就是从被减数的终点(作为起点)引出一条vector,终点是减数的终点,其
实就是v2 - v1 = v3 转换成v2 = v1 + v3</li>
</ul></li>
<li><p>
vector的加法满足交换律,也就是说
</p>
\begin{equation}
\mathbf{a}  + \mathbf{b} = \mathbf{b} + \mathbf{a}
\end{equation}</li>
<li><b>Algebra</b> 加法和减法的代数解释那就简单了,就是相对应的element进行加或者减:
<ul class="org-ul">
<li><p>
比如
</p>
\begin{equation}
[1\;2] + [3\;4] = [4\;6]
\end{equation}</li>
<li><p>
用公式来解释就是
</p>
\begin{equation}
\mathbf{c} = \mathbf{a} + \mathbf{b} = [a_1 + b_1 \; a_2 + b_2 \;...\; a_n + b_n]^T
\end{equation}</li>
</ul></li>
<li><b>Important</b> 加法和减法有意义的前提是参与运算的两个vector有同样的维度</li>
</ul>
</div>
</div>
<div id="outline-container-org06be245" class="outline-3">
<h3 id="org06be245"><span class="section-number-3">2.5.</span> Vector-scalar multiplication</h3>
<div class="outline-text-3" id="text-2-5">
<ul class="org-ul">
<li><b>Geometry</b> Scaling一个vector,就是:
<ul class="org-ul">
<li>增加或者减少这个vector的长度</li>
<li>并且不改变这个vector的angle</li>
</ul></li>
<li>scalar multiplication也不会改变原始的orientation</li>
<li>当然,如果scalar为0的话,最后的结果全部变成0,但是这种情况下,vector转换成了一个point,我们不能说point
有任何的的angle</li>
<li><b>Algebra</b> Scalar-vector的乘法,就是把vector的每个成员都乘以scalar</li>
<li><p>
对于scalar \(\lambda\) 和 vector \(\mathbf{v}\) , 我们有如下的公式
</p>
\begin{equation}
\lambda \mathbf{v} = [\lambda \mathbf{v}_1 \; \lambda \mathbf{v}_2 \; ... \; \lambda \mathbf{v}_n]^T \tag{2.3}
\end{equation}</li>
<li><p>
一个简单的例子如下
</p>
<pre class="example" id="orgf596b02">
3 [-1 3 0 2] = [-3 9 0 6]
</pre></li>
<li><p>
scalar-vector multipleication满足交换律,也就是说
</p>
\begin{equation}
\lambda \mathbf{v} =  \mathbf{v} \lambda
\end{equation}</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org0c5be5c" class="outline-2">
<h2 id="org0c5be5c"><span class="section-number-2">3.</span> Vector multiplications</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>有四种方法来对两个vector进行乘法:
<ul class="org-ul">
<li>dot product</li>
<li>outer product</li>
<li>element-wise multiplication</li>
<li>cross product</li>
</ul></li>
<li>其中最重要的也是我们讲的最多的,就是dot product</li>
</ul>
</div>
<div id="outline-container-orgc1f3702" class="outline-3">
<h3 id="orgc1f3702"><span class="section-number-3">3.1.</span> Vector dot product: Algebra</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>dot product也叫做inner product, 是线性代数里面最重要的操作</li>
<li>dot product是如下高级操作的基础:
<ul class="org-ul">
<li>convolution(卷积)</li>
<li>correlation</li>
<li>Fourier transform</li>
<li>matrix multiplication</li>
<li>signal filtering</li>
</ul></li>
<li>dot product是使用一个number来提供两个vector之间relationship的方法</li>
<li><p>
由于两个vector的dot product结果是一个scalar, 所以dot product又称之为scalar product
</p>
<pre class="example" id="org3c80501">
注意,是scalar product,而不是scalar-vector product
</pre></li>
<li>至于inner product,这是在"非欧几里得空间"里面对dot product的命名,在欧几里何空间,我们可以认为inner
product和dot product等价.</li>
<li>inner product和dot product(scalar product)的实际关系如下</li>
<li>本书只使用dot product这一个称呼</li>
<li>从几何角度上来说,计算dot product,只需要如下两步:
<ul class="org-ul">
<li>把两个vector对应的N个element相乘,得到N个数字</li>
<li>把这N个数字相加</li>
</ul></li>
<li><p>
dot product的过程可以使用如下公式表达,注意,公式中中间三个是对dot product的三种表达方式(我们经常
使用的是 \(\mathbf{a}^T\mathbf{b}\),因为这个体现了矩阵乘法的原理)
</p>
\begin{equation}
\alpha = \mathbf{a} \cdot \mathbf{b} = \left \langle \mathbf{a}, \mathbf{b} \right \rangle =\mathbf{a}^T \mathbf{b} = \sum_{i=1}^n a_i b_i \tag{3.1}
\end{equation}</li>
<li><p>
我们举个例子来计算一下
</p>
<pre class="example" id="org10ea550">
[1 2 3 4] * [5 6 7 8] = 1*5 + 2*6 + 3*7 + 4*8
                      = 5 + 12 + 21 + 32
                      = 70
</pre></li>
<li>由于dot product计算过程的特性,那么我们需要dot product参与的两个vector都是相同的dimensionality</li>
<li>vector和它自己的dimensionality肯定是相同的,所以,我们可以计算vector和它自己的dot product
<ul class="org-ul">
<li><p>
这个操作可以在公式3.2中显示
</p>
\begin{equation}
\mathbf{a}^T\mathbf{a} = \left \| \mathbf{a} \right \|^2 = \sum_{i=1}^n a_i a_i = \sum_{i=1}^n a_i^2 \tag{3.2}
\end{equation}</li>
<li>\(\left \| \mathbf{a} \right \|\) 叫做 vector \(\mathbf{a}\) 的length, magnitude或者是norm</li>
<li>vector自己和自己dot product的结果是\(\left \| \mathbf{a}^2 \right \|\) , 其实就是 vector的length-squared,
magnitude-squared或者是sauared-norm</li>
</ul></li>
<li><p>
使用如下代码计算dot product
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">v1</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>2, 5, 4, 7<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">v2</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>4, 1, 0, 2<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">dp</span> = np.dot<span style="color: #268bd2;">(</span>v1, v2<span style="color: #268bd2;">)</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-orgab46260" class="outline-3">
<h3 id="orgab46260"><span class="section-number-3">3.2.</span> Dot product properties</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li><b>Associative property</b> 我们想看看结合律是否在dot production上面适用,需要从两个角度看,必须两个角
度都满足才能说dot production 满足结合律</li>
</ul>
</div>
<div id="outline-container-org6edc1fb" class="outline-4">
<h4 id="org6edc1fb"><span class="section-number-4">3.2.1.</span> vector product和scalar的结合律</h4>
<div class="outline-text-4" id="text-3-2-1">
<ul class="org-ul">
<li>这种情况其实就是scalar-vector multiplication嵌套在dot product里面</li>
<li>这种情况下显然满足结合律,因为scalar和每个vector的结果都是"维度不变(长度变化)的新vector"</li>
<li><p>
用公式表达,就是公式3.3是成立的.
</p>
\begin{equation}
\gamma(\mathbf{u}^T \mathbf{v}) = (\gamma \mathbf{u}^T) \mathbf{v} = \mathbf{u}^T (\gamma \mathbf{v}) = (\mathbf{u}^T \mathbf{v})\gamma \tag{3.3}
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-org8e1e45e" class="outline-4">
<h4 id="org8e1e45e"><span class="section-number-4">3.2.2.</span> vector product和vector的结合律</h4>
<div class="outline-text-4" id="text-3-2-2">
<ul class="org-ul">
<li>先说结论,这种情况下的结合律是不满足的</li>
<li><p>
用公式表达,就是公式3.4是不成立的!
</p>
\begin{equation}
\mathbf{u}^T(\mathbf{v}^T \mathbf{w}) = (\mathbf{u}^T\mathbf{v})^T \mathbf{w}
\end{equation}</li>
<li>要想理解这个不可能,我们可以从很多方向来理解:
<ul class="org-ul">
<li>我们首先假设三个vector的维度相同,那么我们会发现,上面公式的左右两边甚至都不是dot product,因为
<ol class="org-ol">
<li>左边是row vector \(\mathbf{u}^T\) 和一个scalar(两个vector相乘得到的)相乘, 所以左边是一个row vector</li>
<li>右边是一个scalr(两个vector相乘得到的)和一个column vector相乘,所以右边是一个 column vector(注
意对于scalar来说 \(4^T = 4\)</li>
<li><p>
其实不仅仅是row vector和column vector的不一样,他们的成员其实也有可能是不一样的,比如下面的例子
</p>
\begin{equation}
\mathbf{u} = \begin{bmatrix}
             1 \\
             2 \\
             \end{bmatrix} ,
\mathbf{v} = \begin{bmatrix}
             1 \\
             3 \\
             \end{bmatrix} ,
\mathbf{w} = \begin{bmatrix}
             2 \\
             3 \\
             \end{bmatrix}
\end{equation}</li>
<li><p>
左边的结果为
</p>
\begin{equation}
\mathbf{u}^T(\mathbf{v}^T \mathbf{w}) =
             \begin{bmatrix}
             1 \; 2 \\
             \end{bmatrix}
             \left(
             \begin{bmatrix}
             1 \; 3 \\
             \end{bmatrix}
             \begin{bmatrix}
             2 \\
             3 \\
             \end{bmatrix}
             \right)
             =
             \begin{bmatrix}
             11 \; 22 \\
             \end{bmatrix} \tag{3.5}
\end{equation}</li>
<li><p>
右边的结果为,可见,显然和左边的不一样,不仅orientation不一样,element维度也不一样
</p>
 \begin{equation}
( \mathbf{u}^T\mathbf{v})^T \mathbf{w} =
              \left(
              \begin{bmatrix}
              1 \; 2 \\
              \end{bmatrix}
              \begin{bmatrix}
              1 \\ 3 \\
              \end{bmatrix}
              \right)^T
              \begin{bmatrix}
              2 \\
              3 \\
              \end{bmatrix}
              =
              \begin{bmatrix}
              14 \\ 21 \\
              \end{bmatrix} \tag{3.6}
 \end{equation}</li>
</ol></li>
<li>如果这三个vector的维度不同,那么甚至有一边的计算都是invalid的,都不用考虑是否相等了</li>
</ul></li>
<li>综上所述,我们可以得到结论,就是vector dot product不遵守结合律.(但是,matrix的乘法遵守结合律,所以
后面不要和这里混淆)</li>
</ul>
</div>
</div>
<div id="outline-container-org52cc786" class="outline-4">
<h4 id="org52cc786"><span class="section-number-4">3.2.3.</span> commutative property</h4>
<div class="outline-text-4" id="text-3-2-3">
<ul class="org-ul">
<li><p>
dot product 满足交换律,用公式表达如下
</p>
\begin{equation}
\mathbf{a}^T \mathbf{b} = \mathbf{b} \mathbf{a}^T \tag{3.7}
\end{equation}</li>
<li><p>
dot product 满足交换律是很显然的事情,因为dot production是在element维度完成的,两element的相乘,
其实就是两个scalar的乘积,而scalar乘法是符合交换律的(如公式3.8),那么我们也可以说dot product也是
符合交换律的
</p>
\begin{equation}
\sum_{i=1}^na_ib_i = \sum_{i=1}^nb_ia_i \tag{3.8}
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-org306d748" class="outline-4">
<h4 id="org306d748"><span class="section-number-4">3.2.4.</span> Distributive property</h4>
<div class="outline-text-4" id="text-3-2-4">
<ul class="org-ul">
<li>首先抛出结论: dot product是符合分配率的,符合分配率这件事情能让"代数表达"和"几何表达"联系起来</li>
<li><p>
分配率可以用如下公式解释:(当然了,这里的vector必须维度相同)
</p>
\begin{equation}
\mathbf{w}^T (\mathbf{u} + \mathbf{v}) = \mathbf{w}^T \mathbf{u} + \mathbf{w}^T \mathbf{v} \tag{3.9}
\end{equation}</li>
<li>分配率说的是这么一个事儿:我们可以把一个dot product分成两个dot product的和,只需要把那个vector拆
成两个就好了</li>
<li>当然了也可以反过来用,假设两个vector都和同一个vector相乘,而这两vector的维度一样,那么就可以先把
这两个vector加起来</li>
<li>我们可以用一个例子来加深我们的理解:
<ul class="org-ul">
<li><p>
假设三个vector如下
</p>
\begin{equation}
\mathbf{u} = \begin{bmatrix}
             1 \\
             2 \\
             \end{bmatrix} ,
\mathbf{v} = \begin{bmatrix}
             1 \\
             3 \\
             \end{bmatrix} ,
\mathbf{w} = \begin{bmatrix}
             2 \\
             3 \\
             \end{bmatrix}
\end{equation}</li>
<li><p>
公式左边的计算结果是19
</p>
\begin{equation}
\mathbf{w}^T(\mathbf{u} + \mathbf{v}) =
             \begin{bmatrix}
             2 \; 3 \\
             \end{bmatrix}
             \left(
             \begin{bmatrix}
             1 \\ 2 \\
             \end{bmatrix}
+
              \begin{bmatrix}
              1 \\
              3 \\
              \end{bmatrix}
              \right)
              =
              \begin{bmatrix}
              2 \; 3 \\
              \end{bmatrix}
              \times
              \begin{bmatrix}
              2 \\
              5 \\
              \end{bmatrix}
              = 19
              \tag{3.11}
\end{equation}</li>
<li><p>
公式右边的计算结果也是19
</p>
\begin{equation}
\mathbf{w}^T \mathbf{u} + \mathbf{w}^T \mathbf{v} =
             \begin{bmatrix}
             2 \; 3 \\
             \end{bmatrix}
             \begin{bmatrix}
             1 \\ 2 \\
             \end{bmatrix}
+
              \begin{bmatrix}
              2 \; 3 \\
              \end{bmatrix}
              \begin{bmatrix}
              1 \\
              3 \\
              \end{bmatrix}
              =
              8 + 11
              = 19
              \tag{3.12}
\end{equation}</li>
</ul></li>
<li>下面我们把结合律应用到一种特殊的情况,那就是vector自己分成两个sub_vector, 然后这两个sub_vector再
乘以自己.</li>
<li><p>
由于分配率的存在,我们可以得到如下的等式
</p>
\begin{align}
(\mathbf{u} + \mathbf{v})^T(\mathbf{u} + \mathbf{v}) &= \| \mathbf{u} + \mathbf{v} \|^2 \\
                                                     &= \mathbf{u}^T\mathbf{u} + 2\mathbf{u}^T\mathbf{v} + \mathbf{v}^T\mathbf{v} \\
                                                     &= \| \mathbf{u} \|^2 + \| \mathbf{v} \|^2 + 2\mathbf{u}^T\mathbf{v}
\end{align}</li>
<li>上述公式是连接"代数解释"和"几何解释"之间的桥梁</li>
<li><p>
<b>Cauchy-Schwarz inqauality</b> 柯西-斯瓦茨不等式提供了两个vector进行dot product的上限,,不等式如下
</p>
\begin{equation}
| \mathbf{v}^T \mathbf{w} | \leq \| v \| \| w \| \tag{3.14}
\end{equation}</li>
<li><p>
用英语来说,上面的不等式就是说
</p>
<pre class="example" id="org38e04d5">
两个vector的dot product的magnitude不会比这两个vector magitude的product值大
</pre></li>
<li>这个不等式的等于会在下面的情况下得到满足的时候,出现:
<ul class="org-ul">
<li>一个vector是另外一个vector的scaled version,也就是说 \(\mathbf{v} = \lambda \mathbf{w}\)</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org2d87343" class="outline-3">
<h3 id="org2d87343"><span class="section-number-3">3.3.</span> Vector dot product: Geometry</h3>
<div class="outline-text-3" id="text-3-3">
<ul class="org-ul">
<li><p>
从几何上来说
</p>
<pre class="example" id="org3445ba2">
dot product就是两个vector之间的cosinec乘以这两个vector
</pre></li>
<li>其实,从几何和代数的方向上看这个公式,其实是使用不同的方式来表达同样的concept</li>
<li><p>
dot product 的几何定义
</p>
\begin{equation}
\mathbf{a}^T \mathbf{b} = \| \mathbf{a} \| \| b \| \cos(\theta_{ab}) \tag{3.15}
\end{equation}</li>
<li>如果上面的vector长度为1(也就是 \(\| \mathbf{a} \| = \| b \| = 1\) ),那么dot product的结果也就变成
了vector的cosine值</li>
<li>公式3.15可以转为如下两种写法:
<ul class="org-ul">
<li><p>
求cosine值
</p>
\begin{equation}
\cos(\theta_{ab}) = \cfrac{\mathbf{a}^T \mathbf{b}}{\| \mathbf{a} \| \| b \|}\tag{3.16}
\end{equation}</li>
<li><p>
求vector之间的角度
</p>
\begin{equation}
\theta_{ab} = \cos^{-1}\left(\cfrac{\mathbf{a}^T \mathbf{b}}{\| \mathbf{a} \| \| b \|}\right)\tag{3.17}
\end{equation}</li>
</ul></li>
<li>公式3.17意义非凡,在一个2D空间求两个vector之间夹角的方法,放到3维,4维,甚至更高维度都是同样成立的</li>
<li><p>
如果只考虑dot product的正负,那么我们可以得到一个结论:
</p>
<pre class="example" id="orgeda6695">
dot product的正负只由两个vector之间的夹角来决定
</pre></li>
<li>原因很简单 dot product是cosine乘以两个vector的长度,长度都是正数,那么它的乘积也是正数,在判断结果
正负的时候,就可以不考虑他们</li>
<li>根据vector之间的角度,我们可以把dot product分成五种,我们以 \(\theta\) 代表vector之间的角度, \(\alpha\)
代表dot product的结果:
<ol class="org-ol">
<li>\(\theta < 90^{\circ} \rightarrow \alpha > 0\): 锐角的cosine总是正数,所以dot product也是正数</li>
<li>\(\theta > 90^{\circ} \rightarrow \alpha < 0\): 钝角的cosine总是负数,所以dot product也是负数</li>
<li>\(\theta = 90^{\circ} \rightarrow \alpha = 0\): 直角的cosine是0,所以dot product也是0.这种情况下
非常重要,所以有一个自己的名字: <b>orthogonal</b> (正交), 而且正交还有一个特殊的符号,如果两个vector
是正交的,那么,我们可以使用如下的公式表示 \(\mathbf{w} \bot \mathbf{v}\)</li>
<li>\(\theta = 0^{\circ} \rightarrow \alpha = \| \mathbf{a} \| \| \mathbf{b} \|\): 0度角的cosine是1
所以,dot product就是两个vector长度的乘积,这种情况也有一个单独的名字: <b>collinear</b> (共线)</li>
<li>\(\theta = 180^{\circ} \rightarrow \alpha = -\| \mathbf{a} \| \| \mathbf{b} \|\):180度角的cosine
是-1,所以dot product是两个vector长度的乘积再乘以-1,这种情况也叫共线</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgde9a35f" class="outline-3">
<h3 id="orgde9a35f"><span class="section-number-3">3.4.</span> Algebraic and geometric equivalence</h3>
<div class="outline-text-3" id="text-3-4">
<ul class="org-ul">
<li>对于dot product的解释,几何解释和代数解释非常的不同,但是实质上是一样的.我们本节就是
来讨论这个问题的</li>
<li><p>
下面公式的3.18,中间是代数解释,右边是几何解释
</p>
\begin{equation}
\mathbf{a}^T \mathbf{b} = \sum_{i=1}^n \mathbf{a}_i \mathbf{b}_i = \| \mathbf{a} \| \| \mathbf{b} \| \cos(\theta_{ab}) \tag{3.18}
\end{equation}</li>
<li>论证代数和几何表达的内涵统一性:
<ul class="org-ul">
<li>首先要了解dot product是满足交换律和分配率的</li>
<li>其次要了解Law of Cosine</li>
</ul></li>
<li>勾股定理(Pythagorean theorem)是大家非常熟悉的 \(\mathbf{a}^2 + \mathbf{b}^2 = \mathbf{c}^2\)</li>
<li><p>
勾股定理其实是Law of Cosine的一个特例
</p>
\begin{equation}
\mathbf{c}^2 = \mathbf{a}^2 + \mathbf{b}^2 - 2\mathbf{a}\mathbf{b} \cos \theta_{ \mathbf{a} \mathbf{b}}
\end{equation}</li>
<li>下面是证明dot product代数和结合表示内涵相同的过程:
<ul class="org-ul">
<li><p>
从代数方向上看,由余弦定理计算 \(\mathbf{c}^2\) 得到:
</p>
\begin{equation}
\mathbf{c}^2 = \mathbf{a}^2 + \mathbf{b}^2 - 2\mathbf{a}\mathbf{b} \cos (\theta_{ \mathbf{a} \mathbf{b}})
\end{equation}</li>
<li>从几何方向上看, 我们可以把vector \(\mathbf{c}\) 看成是 vector \(\mathbf{a}\) 和 vector \(\mathbf{b}\)
之间的差值:
<ol class="org-ol">
<li>原始方程 \(\| c \| = \| a - b \|\)</li>
<li><p>
我们也去求 \(\mathbf{c}\) 的平方
</p>
\begin{align}
\| \mathbf{a} - \mathbf{b} \| &= ( \mathbf{a} - \mathbf{b} )^T ( \mathbf{a} - \mathbf{b} )   \\
            &= \mathbf{a} ^T \mathbf{a} - 2 \mathbf{a} ^T \mathbf{b} + \mathbf{b} ^T \mathbf{b} \\
            &= \| \mathbf{a} \|^2 + \| \mathbf{b} \|^2 - 2 \mathbf{a} ^T \mathbf{b} \\
\end{align}</li>
<li><p>
综合前面代数方向得到的 \(\mathbf{c}^2\),就得到
</p>
\begin{equation}
\| \mathbf{a} \|^2 + \| \mathbf{b} \|^2 - 2 \mathbf{a} ^T \mathbf{b} = \| \mathbf{a} \|^2 + \| \mathbf{b} \|^2  - 2 \| \mathbf{a} \| \| \mathbf{b} \| \cos \theta
\end{equation}</li>
<li><p>
两边消掉共同项,就得到
</p>
\begin{equation}
\mathbf{a} ^T \mathbf{b} = \| \mathbf{a} \| \| \mathbf{b} \| \cos \theta \tag{3.15}
\end{equation}</li>
</ol></li>
</ul></li>
<li><b>Proof of Cauchy-Schwarz inequality</b> 一旦得到了公式3-15这个结论,那么柯西不等式的证明就很简单了:
<ul class="org-ul">
<li><p>
由于如下等式成立
</p>
\begin{equation}
\mathbf{a} ^T \mathbf{b} = \| \mathbf{a} \| \| \mathbf{b} \| \cos \theta \tag{3.33}
\end{equation}</li>
<li>又由于cosine区间是[0,1]</li>
<li><p>
所以得以证明柯西不等式
</p>
\begin{equation}
\mathbf{a} ^T \mathbf{b} \le \| \mathbf{a} \| \| \mathbf{b} \| \tag{3.34}
\end{equation}</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org1ea753a" class="outline-3">
<h3 id="org1ea753a"><span class="section-number-3">3.5.</span> Linear weighted combination</h3>
<div class="outline-text-3" id="text-3-5">
<ul class="org-ul">
<li>线性加权组合由于非常常见,所以我们也单独给了他一个章节</li>
<li>线性加权组合还有其他常见的名字,比如:
<ul class="org-ul">
<li>linear mixture</li>
<li>weighted combination</li>
<li>linear coefficient combination</li>
</ul></li>
<li>线性加权组合其实就是:
<ul class="org-ul">
<li>scalar和vector相乘, 得到新的vector</li>
<li>新得到的N个vector再相加</li>
<li>这里就要assume所有的vector的维度相同</li>
<li><p>
公式如下
</p>
\begin{equation}
\mathbf{w} = \lambda_1 \mathbf{v}_1 + \lambda_2 \mathbf{v}_2 + \cdot\cdot\cdot + \lambda_n \mathbf{v}_n \tag{3.35}
\end{equation}</li>
</ul></li>
<li><p>
使用代码来实现linear weighted combination非常容易
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">l1</span> = 1
<span style="color: #268bd2;">l2</span> = 2
<span style="color: #268bd2;">l3</span> = -3
<span style="color: #268bd2;">v1</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>4, 5, 1<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">v2</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>-4, 0, -4<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">v3</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>1, 3, 2<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">ret</span> = l1 * v1 + l2 * v2 + l3 * v3
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[ret] ==&gt;"""</span>, ret<span style="color: #268bd2;">)</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[ret] ==&gt; [ -7  -4 -13]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-orgb0add86" class="outline-3">
<h3 id="orgb0add86"><span class="section-number-3">3.6.</span> The outer product</h3>
<div class="outline-text-3" id="text-3-6">
<ul class="org-ul">
<li>所谓outer product,就是一种把两个vector合并,产生一个matrix的操作</li>
<li>outer product 最容易让人难以理解的地方,就是表达符号,实在是和dot product太像了(注意这里 \(\mathbf{v}\)
是M-element column vector, \(\mathbf{m}\) 是N-element column vector ):
<ul class="org-ul">
<li><p>
我们首先看看dot product(注意,在dot product里, V和M必须相等)
</p>
\begin{equation}
\mathbf{v}^T \mathbf{w} = 1 \times 1
\end{equation}</li>
<li><p>
再来看看out product,只不过是带T的放到了后面
</p>
\begin{equation}
\mathbf{v} \mathbf{w}^T = M \times N
\end{equation}</li>
<li><p>
从上帝视角,我们其实学过矩阵乘法了,从矩阵乘法的角度其实可以理解最后为什么结果一个是scalar,一个
是matrix,因为
</p>
<pre class="example" id="org2ce4e8b">
Dot product and outer product are special cases of matrix multiplication.
</pre></li>
</ul></li>
<li>我们后面会从三个角度来理解outer product:
<ul class="org-ul">
<li>element perspective</li>
<li>column perspective</li>
<li>row perspective</li>
</ul></li>
<li><b>Element perspetive</b>, 从element的角度考虑,每个对于outer product矩阵里面的成员 element_ij, 其是
如下两个element的scalar multilication值:
<ul class="org-ul">
<li>第一个vector的ith element</li>
<li>第二个vector的jth element</li>
</ul></li>
<li><p>
由此,我们可以总结出element perspective的公式:
</p>
\begin{equation}
(\mathbf{v} \mathbf{w}^T)_{i,j} = v_i w_j\tag{3.36}
\end{equation}</li>
<li><p>
下面是使用字母替代数字来显示outer product的过程
</p>
\begin{equation}
\begin{bmatrix}
a \\
b \\
c \\
\end{bmatrix}
\begin{bmatrix}
d e f  \\
\end{bmatrix}
=
\begin{bmatrix}
ad \;ae \;af \;\\
bd \;be \;bf \;\\
cd \;ce \;cf \;\\
\end{bmatrix}
\end{equation}</li>
<li><b>Column perspective</b> 我们再来从column 维度来看看每个outer product是怎么构成的,从column纬度看outer
product, outer product里面的每一列,都可以看做是一个scalar-vector multiplication:
<ul class="org-ul">
<li>其中,vector是左边的column vector(每次重复使用)</li>
<li>另外,scalar是右边的row vector里面每次取一个</li>
<li>所以,outer product的column的数目和右侧row vector的个数相同</li>
<li><p>
同时,outer product的每个column都是left column vector 的scaled version
</p>
<pre class="example" id="orga6732cf">
Each column of the outer product matrix is a scaled version of the left column vector
</pre></li>
</ul></li>
<li><b>Row perspective</b>, 其实类比column perspective就可以得出结论了,outer product里面的每一行,都可以
看做是一个scalar-vector multiplication:
<ul class="org-ul">
<li>其中,vector是右边的row vector(每次重复使用)</li>
<li>另外,scalar是左边的column vector里面每次取一个</li>
<li>所以,outer product的row数目和左侧的left vector的个数相同</li>
<li>同时,outer product的每个row都是right row vector的scaled version</li>
</ul></li>
<li>如果swap 两个vector的order(注意swap里之后,在左边的还是要以column vector的形式,右边的还是要以row
vector的形式),那么我们就会发现新的两个matrix非常像,只不过row和column也给swap了</li>
<li><p>
<b>Code</b> 使用如下代码来完成out product的计算,顺便看一下上一条说的matrix的swap现象
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">v1</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>2, 5, 4, 7<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">v2</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>4, 1, 0, 2<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">op</span> = np.outer<span style="color: #268bd2;">(</span>v1, v2<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span>op<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">op</span> = np.outer<span style="color: #268bd2;">(</span>v2, v1<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span>op<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[[ 8  2  0  4]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[20  5  0 10]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[16  4  0  8]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[28  7  0 14]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[[ 8 20 16 28]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 2  5  4  7]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0  0  0  0]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 4 10  8 14]]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org1dcc7c0" class="outline-3">
<h3 id="org1dcc7c0"><span class="section-number-3">3.7.</span> Element-wise (Hadamard) vector product</h3>
<div class="outline-text-3" id="text-3-7">
<ul class="org-ul">
<li><p>
所谓Element-wise vector product,就是两个一样的vector的每个位置上的scalar相乘,得到一个和前两者一
样的新vector,公式如下
</p>
\begin{equation}
\mathbf{c} = \mathbf{a} \odot \mathbf{b} = [a_1 b_1 \; a_2 b_2 \; \cdot\cdot\cdot a_n b_n] \tag{3.37}
\end{equation}</li>
<li>这个操作其实不算是线性代数的操作,更像是一组有序的scalar multiplication</li>
<li><p>
<b>Code</b> 计算element-wise multiplication的代码如下
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">v1</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>2, 5, 4, 7<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">v2</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>4, 1, 0, 2<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">v3</span> = v1 * v2
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[v3] ==&gt;"""</span>, v3<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[v3] ==&gt; [ 8  5  0 14]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org253680a" class="outline-3">
<h3 id="org253680a"><span class="section-number-3">3.8.</span> Cross product</h3>
<div class="outline-text-3" id="text-3-8">
<ul class="org-ul">
<li>cross product的限定条件是:
<ul class="org-ul">
<li>参与者必须是3维vector</li>
<li>结果自然也是3维vector</li>
</ul></li>
<li><p>
计算公式如下
</p>
\begin{equation}
\mathbf{a} \times \mathbf{b} = \begin{bmatrix}
                        a_2 b_3  - a_3 b_2\\
                        a_3 b_1  - a_1 b_3\\
                        a_1 b_2  - a_2 b_1\\
                        \end{bmatrix} \tag{3.38}
\end{equation}</li>

<li><p>
cross product的magnitude等于参与乘法的两个vector的magnitude再乘以两者的sin
</p>
\begin{equation}
\| \mathbf{a} \times \mathbf{b} \| = \| \mathbf{a} \| \| \mathbf{b} \| \sin(\theta_{ab}) \tag{3.39}
\end{equation}</li>
<li>cross product主要应用在几何领域(而不是线性代数领域),主要作用是创建一个vector \(\mathbf{c}\) , 使得
能够正交于 vector \(\mathbf{a}\)  和 vector \(\mathbf{b}\) 所定义的平面</li>
<li>cross product主要用于多变量微积分,而对于如下行业却完全不适用,所以我们后面不会再提及这个概念,这
里写出来是为了知识的完整性:
<ul class="org-ul">
<li>data analysis</li>
<li>statistics</li>
<li>machine-learning</li>
<li>signal-processing</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org3ba77a4" class="outline-3">
<h3 id="org3ba77a4"><span class="section-number-3">3.9.</span> Unit vectors</h3>
<div class="outline-text-3" id="text-3-9">
<ul class="org-ul">
<li>拥有长度为1的vector( \(\| \mathbf{v} \| = 1\)  ), 有时候是非常重要的</li>
<li>长度为1的vector就叫做unit vector</li>
<li>unit vector还可以用来创建特殊的matrix: orthogonal matrix</li>
<li>本节主要是要我们设计公式来求一个普通vector的unit vector:
<ul class="org-ul">
<li>这个普通vector并不一定是unit vector</li>
<li>求出啦的unit vector和这个普通vector的方向是一样的,只不过长度为1</li>
</ul></li>
<li><p>
求unit vector其实就是求一个scalar \(\mu\) ,使得其满足如下公式
</p>
\begin{equation}
\mu \mathbf{v} \; s.t. \; \| \mu \mathbf{v} \| = 1 \tag{3.40}
\end{equation}</li>
<li>我们求得 \(\mu\) 的值之后,就可以计算unit vector啦, \(\mu\) 的值其实就是:1除以普通vector的magnitude</li>
<li><p>
公式如下,其中unit vector使用一个hat标识 \(\hat{\mathbf{v}}\)
</p>
\begin{equation}
\hat{\mathbf{v}} = \cfrac{1}{\| \mathbf{v} \|} \mathbf{v} = \cfrac{1}{\sqrt{\sum_{i=1}^n v_i^2}} \mathbf{v} \tag{3.41}
\end{equation}</li>
<li><p>
一个例子如下
</p>
\begin{equation}
\mathbf{v}  = \begin{bmatrix}
          0 \\
          2 \\
         \end{bmatrix},
\hat{\mathbf{v}} = \cfrac{1}{\sqrt{0^2 + 2^2}}
\begin{bmatrix}
0 \\
2 \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
1 \\
\end{bmatrix}
\end{equation}</li>
<li>注意,求某个普通vector的unit vector的时候, 普通vector的长度必须不是0</li>
<li><p>
<b>Code</b> 代码如下
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">v</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>2, 5, 4, 7<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">vMag</span> = np.linalg.norm<span style="color: #268bd2;">(</span>v<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[vMag] ==&gt;"""</span>, vMag<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">v_unit</span> = v / vMag
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[v_unit] ==&gt;"""</span>, v_unit<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[vMag] ==&gt; 9.695359714832659</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[v_unit] ==&gt; [0.20628425 0.51571062 0.4125685  0.72199487]</span>
</pre>
</div></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb55fc5c" class="outline-2">
<h2 id="orgb55fc5c"><span class="section-number-2">4.</span> Vector Spaces</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-orgd7ae83f" class="outline-3">
<h3 id="orgd7ae83f"><span class="section-number-3">4.1.</span> Dimensions and fields in linear algebra</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li><b>Dimension</b> 前面我们说过,dimension的定义很简单,就是vector里面有几个数字,就是几个dimension</li>
<li>但是由于这个概念太重要,值的我们从更详细的维度进行定义:
<ul class="org-ul">
<li><b>Algebraically</b>:
<ol class="org-ol">
<li>从代数的角度,dimension就是vector里面元素的个数</li>
<li>而且顺序很重要,比如vector [3, 1, 5]里面的,在第二个dimension的element是1</li>
<li>同时,不同dimension上的数字之间是平等的,没有哪个dimension比另外的dimension重要</li>
</ol></li>
<li><b>Geometrically</b>:
<ol class="org-ol">
<li>从几何角度上讲,纬度就是vectro所在的坐标轴系里面的坐标数目</li>
<li>在2D space里面, 2D vector是一个line</li>
<li>在3D space里面, 3D vector也是一个line</li>
<li>2D vectro, 3D vectro,甚至ND vector都是line,只不过dimension不同</li>
</ol></li>
</ul></li>
<li>如图
<ul class="org-ul">
<li><p>
图4-3
</p>

<div id="org6b5f013" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/4-3.png" alt="4-3.png" />
</p>
<p><span class="figure-number">Figure 5: </span>tic/4-3.png</p>
</div></li>
<li>我们往往会从正交的笛卡尔平面(正交坐标系)来理解vector,但是其实坐标系也有非平面的,如上图</li>
</ul></li>
<li><b>Field</b>, 在数学中,field是指一系列的number, 在这些number上面定义了最基本的算数操作(加减乘除等)</li>
<li>常见的field有:
<ul class="org-ul">
<li>\(\mathbb{R}\) 代表实数</li>
<li>\(\mathbb{C}\) 代表复数</li>
<li>\(\mathbb{Q}\) 代表有理数</li>
</ul></li>
<li>本书主要使用field来表示dimension,比如:
<ul class="org-ul">
<li>\(\mathbb{R}^4\) 代表一个四维vector</li>
<li>如果这个四维vector的样子如[a b c d], 那么,a,b,c,d这四个数字都是实数</li>
</ul></li>
<li>有些时候,具体dimension是多少不重要,但是需要和其他dimension进行比较,这个时候可以用大写字母来代替
dimension,比如,如果说如下两个vector能够计算dot product的唯一可能是 \(M = N\) :
<ul class="org-ul">
<li>\(\mathbf{w}\) in \(\mathbb{R}^M\)</li>
<li>\(\mathbf{v}\) in \(\mathbb{R}^N\)</li>
</ul></li>
<li>再计算机上,如果没有latex,我们的field也可以写成如下两种形式:
<ul class="org-ul">
<li>R2</li>
<li>R^6</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgc53bce4" class="outline-3">
<h3 id="orgc53bce4"><span class="section-number-3">4.2.</span> Vector spaces</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>所谓的vector space,是指一系列对象,这些对象上面定义了addition 和 scalar multiplication</li>
<li>addition和scalar multiplication遵守如下的公理:
<ul class="org-ul">
<li>Additive inverse: \(\mathbf{v} + ( - \mathbf{v}) = 0\)</li>
<li>Associativity(结合律): \((\mathbf{v} + \mathbf{w}) + \mathbf{u} = \mathbf{v} + (\mathbf{w} + \mathbf{u})\)</li>
<li>Commutativity(交换律): \(\mathbf{v} + \mathbf{w} = \mathbf{w} + \mathbf{v}\)</li>
<li>Additive identity: \(\mathbf{v} + \mathbf{0} = \mathbf{v}\)</li>
<li>Multiplicative identity: \(\mathbf{v1} = \mathbf{v}\)</li>
<li>Distributivity(分配率): \((\alpha + \beta)( \mathbf{v} + \mathbf{w}) = \alpha \mathbf{v} + \alpha \mathbf{w} + \beta \mathbf{v} + \beta \mathbf{w}\)</li>
</ul></li>
<li>vector space其实在本书几乎不会再次被提及,之所以在这里介绍vector space,是为了不让大家把他和另外
一个更加重要的概念混淆,那就是下面要讲的vector subspace</li>
</ul>
</div>
</div>
<div id="outline-container-org7ac703f" class="outline-3">
<h3 id="org7ac703f"><span class="section-number-3">4.3.</span> Subspaces and ambient spaces</h3>
<div class="outline-text-3" id="text-4-3">
<ul class="org-ul">
<li>vector subspace是所有线性代数高级概念的核心,其同样具有代数和几何两种方向上的理解</li>
<li><p>
<b>Geometry</b> 从几何上来说,subspace是一系列point的集合,这些point可以通过延长(scalar multiplication)
并且组合(addition)一系列的vector来达到
</p>
<pre class="example" id="org53db0e8">
A subspace is the set of all points that you can reach by stretching and combining
a collection of vector(that is, addition and scalar multiplication)
</pre></li>
<li>我们下面从一个简单的例子来解释下vector subspace的概念:
<ul class="org-ul">
<li><p>
图4-4
</p>

<div id="org5a60253" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/4-4.png" alt="4-4.png" />
</p>
<p><span class="figure-number">Figure 6: </span>tic/4-4.png</p>
</div></li>
<li>我们图中黑色的线是从原点到一个vector \(\mathbf{v}\) = [-1 2]的线</li>
<li>vector \(\mathbf{v}\) = [-1 2] 自己不是一个subspace,但是通过乘以一个区间是[-inf, inf]的 \(\lambda\),
可以触及到这个黑色线定义的"线路"上,无数的不同vector,也就是这个gray dashed line</li>
<li>我们把gray dashed line定义成一个1D subspace</li>
<li>这个1D subspace,之所以叫做1D subspace,因为它生活在一维世界里面,注意不要和vector的维度相混淆,
vector是2D vector,但是这个vector和一个scalar配合,产生了一个1D subspace</li>
<li>我们最后来契合一下subspace的定义: 显然我们这里的线上有无数的point(vector),他们都可以通过scalar
multiplication操作来生成,生成后把这些point收集起来就形成了这个1D subspace</li>
</ul></li>
<li>现在我们知道了,由一个vector单打独斗的结果是一个1D subspace(infinitely long line)</li>
<li>那么很显然,如果有两个vector(不能是共线的)的情况下:
<ul class="org-ul">
<li>至少能得到两条infinitely long line</li>
<li>然后再结合scaling以及adding,那么我们会得到非常多的新的point</li>
<li>比如下图中的圆点,就是我们使用如下scaling和adding获得的 \(v_1 + .5v_2\)</li>
<li><p>
图4-5
</p>

<div id="org24eef08" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/4-5.png" alt="4-5.png" />
</p>
<p><span class="figure-number">Figure 7: </span>tic/4-5.png</p>
</div></li>
</ul></li>
<li>实际上,对两个vector实行了scaling和adding之后,得到的points其实组成了一个新的2D subspace</li>
<li>之前的1D subspace是一条长度无限线, 那么这里的2D subspace就是一个没有边界的平面(plane)了</li>
<li>在2D上面(图4-5)没有边界的平面就是整个2D坐标空间,没有办法形象的展示"两个vector组成一个plane"这件
事,我们使用下图来展示在3D空间中两个3维vector(注意vector可以在任意空间)组成的一个2D subspace(plane)</li>
<li><p>
图4-6
</p>

<div id="org11e0fd3" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/4-6.png" alt="4-6.png" />
</p>
<p><span class="figure-number">Figure 8: </span>tic/4-6.png</p>
</div></li>
<li><b>Ambient dimensionality</b> 图4-6同时也展示了如下两个概念的区别:
<ul class="org-ul">
<li>subspace</li>
<li>ambient space (subspace嵌入在ambient space里面)</li>
</ul></li>
<li>在图4-6这个例子中, subspace是2维的,而它所嵌入的ambient space则是3维的</li>
<li>ambient space的维度N则是vector的维度,也就是 \(\mathbb{R}^N\) 里面的N</li>
<li>所以在一个ambient space里面有多少个subspace?
<ul class="org-ul">
<li>答案很显然,对于维度超过1的ambient space来说,其拥有的subspace数目是无限多的</li>
</ul></li>
<li>问一个不太显然的问题:对于一个N维的ambient space来说,其所拥有的subspace分属于几个维度?
<ul class="org-ul">
<li>答案是N+1</li>
<li>我们以 \(\mathbb{R}^3\) 为例来数一下:
<ol class="org-ol">
<li>最小的subspace是一个0维的point,也就是[0 0 0] (注意所有subspace必须通过 vector \(\mathbf{0}\),
对三维vector来说就是[0 0 0])</li>
<li>接下来是1维的line,在三维空间里面,所有通过原点的line,都是一个subspace.在三维空间有无数的一
维subspace</li>
<li>再来是2维的plane,在三维空间里面,所有通过原点的plane,都是一个subspace.在三维空间有无数的二
维subspace</li>
<li>最后是3维的subspace, ambient space自己就是自己的3维subspace.在三维空间,只有一个3维subspace,
就是ambient space自己</li>
</ol></li>
</ul></li>
<li>最后一个问题是:所有的两个vector都能组成一个plane么?
<ul class="org-ul">
<li>答案是否定的</li>
<li>如果两个vector在同一个line上面,那么他们显然无法组成一个新的plane.这是我们通过直觉得到的判断</li>
<li>在后面,我们会学到,如果两个vector能够组成一个新的plane,那么我们说这两个vector是linear independence的</li>
</ul></li>
<li><b>Algebra</b> 除了额外的公式以外, 代数定义和几何定义都是一样的,只不过代数定义因为有公式,所以显得更
加正式一点</li>
<li>我们通常使用斜体的大写斜体字母来代替subspace,比如 \(V\)</li>
<li>需要注意的是,大写斜体字母还会用来指代a set of vectors, 所以遇到大写斜体字母的时候,你需要结合上
下文来判断其具体意思</li>
<li>在使用公式表述subspace之前,我们先用语言来定义一下subspace: 一个subspace是一系列point是的集合,而
且这些集合满足如下条件:
<ol class="org-ol">
<li>Closed under addition and scalar multiplication</li>
<li>Contains the zeros vector \(\mathbf{0}\)</li>
</ol></li>
<li>第二条很容易理解,就是所有的subspace,无论几维,都必须穿过原点</li>
<li>第一条里面的closed有点难以理解,其实意思就是:集合里的point经过特定的操作(也就是addition和scalar
multiplication)之后,还在集合里面</li>
<li>举个例子,假设有 \(\mathbf{v} \in V\):
<ul class="org-ul">
<li>如果 \(\mathbf{v}\) 乘以一个scalar \(\lambda\), 得到的结果还是在subspace \(V\)</li>
<li>如果 \(\mathbf{v}\) 乘以一个scalar \(\lambda\),再加上另外一个在subspace \(V\) 里面的另外一个vector
\(\alpha \mathbf{w}\), 得到的结果还是在subspace \(V\)</li>
</ul></li>
<li>经过前面的铺垫,我们终于可以使用公式来表达subspace的定义了:
<ul class="org-ul">
<li><p>
公式如下:
</p>
\begin{equation}
\forall \mathbf{v}, \mathbf{w} \in V, \forall \lambda, \alpha \in \mathbb{R}; \lambda \mathbf{v} + \alpha \mathbf{w} \in V, \tag{4.1}
\end{equation}</li>
<li>其中的 \(\forall\), 代表对所有</li>
<li>那么上面的例子翻译出来就是:对任意在subspace里面的vector \(\mathbf{v\;w}\),以及任意的实数 scalar
\(\lambda \; \alpha\), 所有对 \(\mathbf{v\;w}\) 的linearly weighted combination依然存在于subspace \(V\) 里面</li>
</ul></li>
<li>我们下面来看两个例子来加深对定义的理解:</li>
<li>第一个例子是我们知道有一个1D subspace \(V\),定义在3D ambient space \(\mathbb{R}^3\), 我们来判断某些
vector是否属于这个subspace:
<ul class="org-ul">
<li><p>
\(V\) 的定义如下
</p>
\begin{equation}
V = {\lambda [1\;3\;4], \lambda \in \mathbb{R} },\tag{4.2}
\end{equation}</li>
<li>第一个vector是[3 9 12], 它是属于 subspace \(V\) 的,因为我们可以取 \(\lambda\) 为3</li>
<li>第一个vector是[-2 -6 -8], 它是属于 subspace \(V\) 的,因为我们可以取 \(\lambda\) 为-2</li>
<li>但是对于第三个vector [1 3 5],我们就找不到一个 \(\lambda\) 来让他属于 \(V\) 啦,所以[1 3 5]不属于
subspace \(V\)</li>
</ul></li>
<li>第二个例子是一直有一些points,我们来判断这些points能否组成一个新的subspace,我们要看的这个例子描
述如下: 所有 \(\mathbb{R}^2\) 中y轴为正的points</li>
<li>我们来逐个分析看看这些points是否组成了一个新的subspace:
<ul class="org-ul">
<li>第二个条件比较容易判断,这鞋point包含原点[0 0]</li>
<li>再来看第一个条件,我们最终发现它是不满足条件的,因为我们可以举出一个反例:
<ol class="org-ol">
<li>\(\mathbf{v} = [2,3]\) 显然是在这一堆point里面的</li>
<li>我们找到一个实数 \(\lambda = -1\), 从而让 \(\lambda \mathbf{v}\) 不再这一堆point里面了</li>
</ol></li>
<li>综上,这一堆piont没有在scalar multiplication运算下闭合,所以这一堆point不是subspace</li>
<li>当然了我们也没白忙活,因为这一堆point其实是一个subset的例子,也就是我们下一节要介绍的概念</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org1cb53c9" class="outline-3">
<h3 id="org1cb53c9"><span class="section-number-3">4.4.</span> Subsets</h3>
<div class="outline-text-3" id="text-4-4">
<ul class="org-ul">
<li>在线性代数里面,subset不是一个重要的概念,之所以拿出篇幅来介绍,是因为不想让如下两个概念相互混淆:
<ul class="org-ul">
<li>subset</li>
<li>subspace</li>
</ul></li>
<li>和subspace不同的两点是:
<ol class="org-ol">
<li>subset是一个(可以有,也可以没有)边界的region,而不是无限长的</li>
<li>subset不需要通过原点</li>
</ol></li>
<li>常见的subspace的例子有:
<ol class="org-ol">
<li>在XY平面里面所有满足x&gt;0并且y&gt;0的point</li>
<li>所有满足如下公式的点 \(4 > x > 2, y > x^2\)</li>
<li>满足如下公式的点 \(y = 4x\), x的取值范围是[-inf, inf]</li>
</ol></li>
<li>上面的第三个不仅仅是一个subset,而且还是一个1D subspace,满足1D subspace的定义: 无限长的线,并且通
过原点</li>
</ul>
</div>
</div>
<div id="outline-container-org12879fc" class="outline-3">
<h3 id="org12879fc"><span class="section-number-3">4.5.</span> Span</h3>
<div class="outline-text-3" id="text-4-5">
<ul class="org-ul">
<li><b>Geometry</b> span和subspace其实是非常相近的概念,我们可以使用语法的方式来区别两者:
<ul class="org-ul">
<li>subspace是一个名词: A subspace is the region of ambient space that can be reached by any
linear combination</li>
<li>span是一个动词: Thoose vectors span that subspace</li>
</ul></li>
<li><p>
总结起来可以说:
</p>
<pre class="example" id="org989ce51">
A set of vectors spancs, and the result of their spanning is a subspace
</pre></li>
<li>举个例子, 所有的 \(\mathbb{R}^2\) 上的点定义的subspace,可以通过如下两个vector的span得到:
<ul class="org-ul">
<li>[0 1]</li>
<li>[1 0]</li>
</ul></li>
<li>另外两个例子:
<ul class="org-ul">
<li>vector [0 1] span了一个1D subspace</li>
<li>vector [1 2] 也span了一个1D subspace, 但是是和 vector [0 1] span出来的不是同一个subspace</li>
</ul></li>
<li><p>
<b>Algebra</b> 一系列vector的span的结果,是一个set, 包含所有的vector进行计算(linear weighted combination)
得到的点,公式如下
</p>
\begin{equation}
span(\{\mathbf{v_1},\cdot\cdot\cdot,\mathbf{v_n}\}) = \{\alpha_1 \mathbf{v_1} + \cdot\cdot\cdot + \alpha_n\mathbf{v_n}, \alpha \in \mathbb{R} \},\tag{4.3}
\end{equation}</li>
<li>上面公式的右边就是linear weighted combination,从公式3.35引入</li>
<li>span在matrix里面会用到很多,比如在matrix里面有个概念叫column space,其实就是matrix的column进行span
得到的subspace</li>
<li><p>
<b>In the span?</b> 在线性代数里面最常见的问题就是是否一个vector 是被其他多个vector span来的?
</p>
<pre class="example" id="orgb441a85">
Whether one vector is "in the span" of another vector or set of vectors
</pre></li>
<li><p>
上面的数学说法,用大白话就是
</p>
<pre class="example" id="org1693fab">
对于特定的vector w,你是否可以通过scalar-multiplying and adding 其他vector(从set S)来得到
</pre></li>
<li>举个例子:
<ul class="org-ul">
<li><p>
有Set s
</p>
\begin{equation}
S= \Biggl\{
   \begin{bmatrix}
   1 \\
   1 \\
   0 \\
   \end{bmatrix},
   \begin{bmatrix}
   1 \\
   7 \\
   0 \\
   \end{bmatrix}
   \Biggl\}
\end{equation}</li>
<li><p>
有vector \(\mathbf{v}\)
</p>
\begin{equation}
\mathbf{v} = \begin{bmatrix}
1 \\
2 \\
0 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
有vector \(\mathbf{w}\)
</p>
\begin{equation}
\mathbf{w} = \begin{bmatrix}
3 \\
2 \\
1 \\
\end{bmatrix}
\end{equation}</li>
</ul></li>
<li><p>
很显然, v在S的span中,因为可以得到
</p>
\begin{equation}
v \in span(S) \; because \; \begin{bmatrix}
                            1 \\
                            2 \\
                            0 \\
                            \end{bmatrix}
                           =
                           \cfrac{5}{6}
                           \begin{bmatrix}
                           1 \\
                           1 \\
                           0 \\
                           \end{bmatrix}
+
                             \cfrac{1}{6}
                             \begin{bmatrix}
                             1 \\
                             7 \\
                             0 \\
                             \end{bmatrix}

\end{equation}</li>
<li>所以 \(\mathbf{v}\) 其实就是 \(S\) 中的vector进行weighted combination的结果,其中的weighting数据为
5/6, 1/6</li>
<li>想要得到这些weights其实不是一件容易的事情,我们有算法来计算得到,但是现在还不能告诉你这个算法,因
为这个算法涉及的如下知识,我们还没有学到:
<ul class="org-ul">
<li>determinant</li>
<li>Gaussian elimination</li>
</ul></li>
<li>找到 \(\mathbf{w}\) 所对应的weights非常困难,但是我们却一眼可以看出 \(\mathbf{w}\) 不在S的span里面:
<ul class="org-ul">
<li>因为, \(S\) 的第三个成员都是0,而 \(\mathbf{w}\) 的第三个参数是非0, 两个0无论用什么weight也构造不
出非0值</li>
<li><p>
图4-8
</p>

<div id="orgc8c415a" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/4-8.png" alt="4-8.png" />
</p>
<p><span class="figure-number">Figure 9: </span>tic/4-8.png</p>
</div></li>
<li>从上图可以看出, span of S其实是一个3D ambient space里面的2D plane</li>
<li>而 \(\mathbf{v}\) 则是这个plane里面的一个vector</li>
<li>虽然 \(\mathbf{w}\) 也是一个vector,但是它飞出了这个平面</li>
</ul></li>
<li><p>
还有一点要牢记在心,那就是set里面的某个vector也可以是set里面其他vector linear combination得来的
</p>
<pre class="example" id="org99fe75d">
It doesn't matter if the vectors in the set are linear combinations of other vectors
in that same set.
</pre></li>
<li>比如:
<ul class="org-ul">
<li><p>
下面是一个valid set
</p>
\begin{equation}
\left \{
\begin{bmatrix}
1 \\
2 \\
0 \\
1 \\
\end{bmatrix},
\begin{bmatrix}
-3 \\
-6 \\
0 \\
3 \\
\end{bmatrix},
\begin{bmatrix}
10 \\
20 \\
0 \\
10 \\
\end{bmatrix},
\begin{bmatrix}
0 \\
4 \\
1 \\
0 \\
\end{bmatrix},
\begin{bmatrix}
0 \\
2 \\
.5 \\
0 \\
\end{bmatrix}
\right \}
\end{equation}</li>
<li>这个set定义在ambient \(\mathbb{R}^4\)</li>
<li>但是这个set的前三个共线,后两个共线,所以它其实只组成了一个2D plane(通过两个独立的vector)</li>
</ul></li>
<li>我们可以把span理解成机器人以什么样的灵活度来挥舞激光:
<ul class="org-ul">
<li>只有一个vector的情况下, 机器人只能让激光照亮一个方向</li>
<li>有两个vector的情况下,机器人可以挥舞激光,从而形成一个平面</li>
<li>有三个vector的情况下,机器人可以在屋里随意的挥舞,如果有蚊子,那么最终都能打死那个蚊子(如果蚊子
不动的情况下)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgdde7fc2" class="outline-3">
<h3 id="orgdde7fc2"><span class="section-number-3">4.6.</span> Linear independence</h3>
<div class="outline-text-3" id="text-4-6">
<ul class="org-ul">
<li>Linear independence(简称为indepence)对理解线性代数中的如下概念非常重要:
<ul class="org-ul">
<li>matrix rank</li>
<li>statistics</li>
<li>singular value decomposition</li>
</ul></li>
<li><p>
对于理解linear independence来说,最重要的是要理解到一个事实:线性无关是针对一组vector来说的
</p>
<pre class="example" id="orgdd38183">
Linear independence is a property of a set of vectors.
</pre></li>
<li>对于一个vector来说,linear independence与否无法成立</li>
<li><b>Geometry</b> 如果如下两个数值相同,那么参与的一系列vector就是independent的:
<ul class="org-ul">
<li>这一系列的vector span出来的subspace的dimensionality</li>
<li>这一系列的vector的数量</li>
</ul></li>
<li>换句话说:
<ul class="org-ul">
<li>含有两个vector的independent set中的所有vector能够span出来一个plane</li>
<li>含有三个vector的independent set中的所有vector能够span出来一个3D space</li>
</ul></li>
<li>但是反过来说,并不是说一个set of vectors里面有三个成员就一定能span一个3D space,所以,仅仅有三个成
员并不一定保证independent,我们用三个例子可以更清楚的理解这件事情:
<ul class="org-ul">
<li><p>
图4-9
</p>

<div id="org4bca0c4" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/4-9.png" alt="4-9.png" />
</p>
<p><span class="figure-number">Figure 10: </span>tic/4-9.png</p>
</div></li>
<li>图A包含两个共线的vector,这个set of vector就是linearly dependent,因为你可以把一个vector
看做是另外的vector的scaled version</li>
<li>图B也包含两个vector,但是他们指向了不同方向,那么就不可能使用其中一个vector加个scalar就形成另外
一个</li>
<li>图C中包含了三个vector(都在 \(\mathbb{R}^2\) ),但是他们是linearly dependent的,因为任意一个vector
都能通过其他两个vector linear combination之后得到</li>
</ul></li>
<li>从4-9中的三个例子我们其实可以得到一个定理:
<ul class="org-ul">
<li>如果一个set中vector的数目M大于当前vector所在的维度N (\(\mathbb{R}^N\)), 那么这些vector必然linearly
dependent</li>
<li>如果一个set中vector的数目M大于d等于当前vector所在的维度N (\(\mathbb{R}^N\)), 那么这些vector有可
能linearly independent</li>
</ul></li>
<li>截止到本章学习的内容,不足以让我们完成对这个定理的证明,但是我们可以从直觉出发去理解这个问题:
<ul class="org-ul">
<li>比如定理第一部分,我们在4-9中的图C就可以证明这件事情,我们在2D subspace(N=)里面有三个vector(M=3),他
们无论如何无法创建出一个立方体(3D subspace),所以他们必然不可能做到linear independent</li>
<li><p>
当M&lt;=N的时候,就具备了linear indepedent的可能性,但是具体是否independent要看vectors自己的属性,
举个极端的例子
</p>
<pre class="example" id="org47acc73">
在 ambient $\mathbb{R}^25$ subspace里面,有一个set of 20个vector,但是这20个vector都是
在同一个line里面,那么他们也就能组成一个1D subspace
</pre></li>
</ul></li>
<li>最后,我们可以给一个比较书面化的定义: 一系列vector是independent的,那么必须满足如下两个数值相等:
<ul class="org-ul">
<li>这系列里面vector的数目(number of vectors in the set)</li>
<li>这个set span出来的subspace的维度(dimensionality of the subspace spanned by that set)</li>
</ul></li>
<li><p>
<b>Algebra</b> 从代数的角度上讲, 一系列的vector如果dependent的条件是:
</p>
<pre class="example" id="org5cf7377">
至少一个vector (set中的)可以表示成其他vector的linear weighted combination形式
</pre></li>
<li>下面是两个dependent set的例子,每个vector里面都找到了能用其他vector combine(linear combination)成
自己的方法:
<ul class="org-ul">
<li><p>
\(\mathbf{w}_2\) 可以被 \(\mathbf{w}_1\) 表示
</p>
\begin{equation}
  \{\mathbf{w_1}, \mathbf{w_2} \} = \left \{
      \begin{bmatrix}
        1 \\
        2 \\
        3 \\
      \end{bmatrix},
      \begin{bmatrix}
        2 \\
        4 \\
        6 \\
      \end{bmatrix}
    \right \}
  . \mathbf{w_2} = 2\mathbf{w_1} \tag{4.4}
\end{equation}</li>

<li><p>
\(\mathbf{v}_2\) 可以被 \(\mathbf{v}_1\) \(\mathbf{v}_2\) 表示
</p>
\begin{equation}
 \{\mathbf{v_1}, \mathbf{v_2}, \mathbf{v_3} \} = \left \{
     \begin{bmatrix}
       0 \\
       2 \\
       5 \\
     \end{bmatrix},
     \begin{bmatrix}
       -27 \\
       5 \\
       -37 \\
     \end{bmatrix},
     \begin{bmatrix}
       3 \\
       1 \\
       8 \\
     \end{bmatrix}
   \right \}
 . \mathbf{v_2} = 7\mathbf{v_1} - 9\mathbf{v_3} \tag{4.5}
\end{equation}</li>
</ul></li>
<li>下面两个例子是linear independent set的例子:
<ul class="org-ul">
<li><p>
\(\mathbf{w}_2\) 可以被 \(\mathbf{w}_1\) 表示
</p>
\begin{equation}
  \{\mathbf{w_1}, \mathbf{w_2} \} = \left \{
      \begin{bmatrix}
        1 \\
        2 \\
        3 \\
      \end{bmatrix},
      \begin{bmatrix}
        2 \\
        4 \\
        7 \\
      \end{bmatrix}
    \right \}
\end{equation}</li>

<li><p>
\(\mathbf{v}_2\) 可以被 \(\mathbf{v}_1\) \(\mathbf{v}_2\) 表示
</p>
\begin{equation}
 \{\mathbf{v_1}, \mathbf{v_2}, \mathbf{v_3} \} = \left \{
     \begin{bmatrix}
       0 \\
       2 \\
       5 \\
     \end{bmatrix},
     \begin{bmatrix}
       -27 \\
       0 \\
       -37 \\
     \end{bmatrix},
     \begin{bmatrix}
       3 \\
       1 \\
       9 \\
     \end{bmatrix}
   \right \}
\end{equation}</li>
</ul></li>
<li>你会发现无论你怎样努力都找不到一个vector使用linear combination被其他vector所替代的例子</li>
<li>其实判断一个set of vector是不是linear independent在线性代数领域非常非常的重要,我们后面将学习如
何判断(叫做augment-rank),大致的思路如下:
<ul class="org-ul">
<li>把所有的vector放进一个matrix</li>
<li>计算matrix的rank(秩)</li>
<li>如果矩阵的秩和vector的数目一致,那么这个set的vectors是independent的,否则则是dependent的</li>
</ul></li>
<li>下面我们来用公式来定义一下linear dependence:
<ul class="org-ul">
<li><p>
公式如下
</p>
\begin{equation}
\mathbf{0} = \lambda_1 \mathbf{v_1} + \lambda_2 \mathbf{v_2} + \cdot\cdot\cdot + \lambda_n\mathbf{v_n}, \lambda \in \mathbb{R}, \tag{4.6}
\end{equation}</li>
<li>书面解释如下: 如果所有的vector能使用linear combination(参数不能都为0)得到zero vector,那么这些
vector是dependent的</li>
<li>如果 \(\lambda\) 都为0,那么显然所有的vector set都是zero vector,所以我们要排除这种情况</li>
<li>如果公式4.6不成立(至少一个 \(\lambda\) 为非0), 那么这个vector set就是linear independent的</li>
</ul></li>
<li>这个公式的定义看上去非常的拗口,但是其实你如果使用下面的例子就非常容易理解这个至少一非0 \(\lambda\)
是怎么来的:
<ul class="org-ul">
<li>假设 \(\lambda_1\) 就是那个非0的参数,后面会看到这个数会作为分母,所以必然不能为0</li>
<li><p>
我们从公式4-6左右都减去 \(\lambda_1 \mathbf{v_1}\) 得到
</p>
\begin{equation}
-\lambda_1 \mathbf{v_1} = \lambda_2 \mathbf{v_2} + \cdot\cdot\cdot + \lambda_n \mathbf{v_n}
\end{equation}</li>
<li><p>
我们左右都除以 \(-\lambda_1\) (使用 \(\lambda_{n1}\) 代替) 得到
</p>
\begin{equation}
  \mathbf{v_1} = \cfrac{\lambda_2}{\lambda_{n1}} \mathbf{v_2} + \cdot\cdot\cdot
+ \cfrac{\lambda_n}{\lambda_{n1}} \mathbf{v_n}, \lambda \in \mathbb{R}, \lambda_{n1} \neq 0, \tag{4.7}
\end{equation}</li>
</ul></li>
<li>公式4-6还揭示了一个有趣的属性,那就是: 一旦在一系列vector里面包含了zero vector,那么这系列的vector
必然是linear dependent的!</li>
<li>原因很简单,我们可以把zero vector的scalar设置为非0, 其他vector的scalar全部设置为了0,那么所有的linear
combination的结果必然为0.</li>
<li><b>Determining Whether a set is linearly dependent or indepent</b> 在我们学习matrix-based的算法来判断
一个系列的vector是否linear independent之前,我们先来看看一个比较笨四步判断法来判断一个系列的vector
是否是linear independent的.注意,这个方法只是为了加深理解概念,实践中不会使用:
<ol class="org-ol">
<li>第一步就是判断vector的数目M是否小于等于维度N(也就是vector的成员个数),只有M小于等于N才有可能independent</li>
<li>第二步检查所有的vector成员,有一个是zero vector,那么肯定是dependent</li>
<li>第三步就是检查某些vector中的0值,这些会对第四步起作用</li>
<li>第四步就是"一行行"的开始实验了,比如下面的这个例子:
<ul class="org-ul">
<li><p>
例子如下
</p>
\begin{equation}
\left \{
\begin{bmatrix}
1 \\
2 \\
3 \\
\end{bmatrix},
\begin{bmatrix}
2 \\
1 \\
3 \\
\end{bmatrix},
\begin{bmatrix}
4 \\
5 \\
8 \\
\end{bmatrix}
\right \}
\end{equation}</li>
<li>我们看第一行(1, 2, 4) 很容易想到 2*1+1*2 = 4,</li>
<li>然后第二行也符合这个scalar组合(2, 1), 2*2 +1*1 = 5</li>
<li>但是第三行就不符合了这个scalar组合(2,1)了, 2*3 + 1*3 = 9, 而不是8</li>
<li>所以这个vector的组合不是independent的</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org748bff0" class="outline-3">
<h3 id="org748bff0"><span class="section-number-3">4.7.</span> Basis</h3>
<div class="outline-text-3" id="text-4-7">
<ul class="org-ul">
<li>basis的定义是span和independence两个概念的组合:</li>
<li>一系列的在 \(\mathbb{R}^N\) 的vector如果满足下面的条件,就说他们组成了basis:
<ul class="org-ul">
<li>vector span了这个subsapce</li>
<li>并且这些个vector是independent的</li>
</ul></li>
<li>basis这个名字就能看出来,这个概念是用来描述一个space的基础和规则的,有了basis,你就能知道测量你的
space使用的基础单位(length, direction)了</li>
<li><p>
最常见的basis set是笛卡尔轴(也叫直角坐标系),只包含0和1
</p>
\begin{equation}
\mathbb{R}^2 : \left\{
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix},
\begin{bmatrix}
0 \\
1 \\
\end{bmatrix}
\right\},
\mathbb{R}^3 : \left\{
\begin{bmatrix}
1 \\
0 \\
0 \\
\end{bmatrix},
\begin{bmatrix}
0 \\
1 \\
0 \\
\end{bmatrix},
\begin{bmatrix}
0 \\
0 \\
1 \\
\end{bmatrix}
\right\}
\end{equation}</li>
<li>这类basis set非常常用因为他有如下优点:
<ul class="org-ul">
<li>每个vector都是unit length</li>
<li>所有的vector都是正交的(也就是说任意两个vector的dot product都是0)</li>
</ul></li>
<li>basis的定义很容易让大家误以为必须要span整个 \(\mathbb{R}^N\) 才能做basis,其实不是的,因为basis的定
义除了indepdent以外,主要关注的就是要vector span一个subspace,span subspace只需要通过原点就可以,
并不一定要充满整个 \(\mathbb{R}^N\)</li>
<li>所以下面两个也是basis sets:
<ul class="org-ul">
<li><p>
Set S1是一个independent set但是没有cover整个 \(\mathbb{R}^2\), 它只span了 \(\mathbb{R}^2\) 一个1D
subspace
</p>
\begin{equation}
S_1 = \left\{
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}
\right\}
\end{equation}</li>
<li>Set S2是一个XY轴的plane(2D)的basis</li>
</ul></li>
<li>下面再来讨论一下,为什么linearly independent非常重要.这是因为如果set成员vector不是independent的,
那么可能有好几种方法来组合成一个vector:
<ul class="org-ul">
<li><p>
假设我们的set如下,注意这不是一个basis for \(\mathbf{R}^2\)
</p>
\begin{equation}
\left\{
\begin{bmatrix}
1 \\
1 \\
\end{bmatrix},
\begin{bmatrix}
1 \\
3 \\
\end{bmatrix},
\begin{bmatrix}
0 \\
2 \\
\end{bmatrix}
\right\}
\end{equation}</li>
<li>如果我们想要得到vector[-2 6],那么可以有如下三种scalar组合:
<ol class="org-ol">
<li>(-6,4,0)</li>
<li>(0,2,6)</li>
<li>(-2,0,4)</li>
</ol></li>
<li>有多种可能性对于数学家来说是非常令人费解的,所以数学家决定basis里面的成员要想组成一个vector,必
须只能有一种组合,那么就只能要求set必须linearly independent了</li>
</ul></li>
<li><b>Infinite base</b> 虽然某个vector在特定的basis里面有unique的解,但是却存在无数的这种basis能够提供解,
实际上,任意linearly independent的2D vectors(穿过原点),都是 \(\mathbb{R}^2\) 的basis</li>
<li>那么为什么数学家又不烦了呢,为啥不规定只有一部分vector set来做basis呢?比如笛卡尔轴(直角坐标系).原因是因为:
<ul class="org-ul">
<li><p>
某些问题使用特定的basis更简单
</p>
<pre class="example" id="org6ad8fb3">
Some problems are easier to solve in certain bases and harder to solve in other bases.
</pre></li>
<li>在multivariate data science, data compression等问题中,寻找最佳basis set是最重要的问题</li>
</ul></li>
<li><p>
除了线性代数,其他的科学里面也有basis的概念,其实basis就是一系列最小的能够表达其他事物的metrics
</p>
<pre class="example" id="orgbb48d55">
A basis is the set of the minimum number of metrics needed to describe something.
</pre></li>
<li>比如在傅里叶转换中,正弦波就是basis function,因为所有的信号都能被解析为不同的正弦波(频率,周期,振
幅不同的正弦波)的组合</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgce822ba" class="outline-2">
<h2 id="orgce822ba"><span class="section-number-2">5.</span> Matrices</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org82e99cf" class="outline-3">
<h3 id="org82e99cf"><span class="section-number-3">5.1.</span> Interpretations and uses of matrices</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>本章主要是用来为矩阵入门,了解:
<ul class="org-ul">
<li>矩阵长什么样子</li>
<li>如何代表矩阵</li>
<li>一些重要的,且特殊的矩阵</li>
<li>基本的矩阵运算</li>
</ul></li>
<li>本书我们讨论的矩阵是 rows 乘以 column类型的,换句话说,就是你可以把他们打印在table里面的矩阵(比如
3x2, 7x8)</li>
<li>注意,这种'rows乘以column'类型的矩阵其实不代表这个矩阵是2D矩阵,因为他们可以被解释成存在于更高纬度</li>
<li>矩阵也可以不是在纸上的table就可以画出来的,而是需要3D打印机的那种.那种叫做tensor,我们本书不讨论tensor</li>
<li>下面就是两个矩阵的例子:</li>
<li>我们可以把矩阵理解为:
<ul class="org-ul">
<li>一系列column vector挨个站着</li>
<li>或者是一系列的row vector一个叠一个</li>
</ul></li>
<li>矩阵的应用非常广泛,下面是常见的矩阵应用:
<ul class="org-ul">
<li>表达线性转换或者线性映射</li>
<li>存储多变量系统的偏导数</li>
<li>代表system of equation</li>
<li>存储数据</li>
<li>代表统计模型里面的回归量(regressor)</li>
<li>为图形学存储几何转换</li>
<li>在卷积中存储kernel</li>
<li>存储金融信息</li>
<li>存储模型的参数(parameter for a model)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org59481d5" class="outline-3">
<h3 id="org59481d5"><span class="section-number-3">5.2.</span> Matrix terminology and notation</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>我们首先来熟悉一下矩阵的术语(terminology):
<ul class="org-ul">
<li>当我们指代一个矩阵的时候,我们会使用粗体的大写字母,比如(matrix \(\mathbf{A}\) )</li>
<li>当我们指代矩阵当中的一个成员的时候,使用小写 \(a_{ij}\)</li>
<li><p>
下图让我们了解矩阵当中的如下几个属于: column, row, element, diagonal
</p>

<div id="org5afae07" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/5-1.png" alt="5-1.png" />
</p>
<p><span class="figure-number">Figure 11: </span>tic/5-1.png</p>
</div></li>
</ul></li>
<li>矩阵可以包含小的矩阵,这就能够得到非常有用的block-matrix notation.</li>
<li>注意,block matrices不是本书要了解的特性,但是我们必须要了解他们的符号</li>
<li>如图:
<ul class="org-ul">
<li><p>
图5-2
</p>

<div id="orgbd220ec" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/5-2.png" alt="5-2.png" />
</p>
<p><span class="figure-number">Figure 12: </span>tic/5-2.png</p>
</div></li>
<li>bloack matrix是能够包含较小矩阵的矩阵,这能够提高计算和检查效率</li>
<li>上图中的矩阵 \(\mathbf{A}\) 就包含了四个小矩阵</li>
</ul></li>
<li>当我们描述一个matrix大小的时候,我们要按照如下顺序进行描述(默认的convention):
<ol class="org-ol">
<li>首先是row的数目, 通常以M来指代</li>
<li>其次是column的数目, 通常以N来指代</li>
</ol></li>
<li><p>
我们可以使用一个词组来记住顺序:
</p>
<pre class="example" id="orgea5d7bb">
MR NiCe =&gt; M Row, N Column
</pre></li>
</ul>
</div>
</div>
<div id="outline-container-org7217f7e" class="outline-3">
<h3 id="org7217f7e"><span class="section-number-3">5.3.</span> Matrix dimensionalities</h3>
<div class="outline-text-3" id="text-5-3">
<ul class="org-ul">
<li>相比vector的dimension的简洁(有几个element就有几个dimension), matrix的dimension就非常的灵活,从而也让人难以理解</li>
<li>matrix的灵活性表现在,它的dimension从不同的角度会有不同的解释,比如对于一个 \(M \times N\) 的matrix来说:
<ul class="org-ul">
<li>如果每个element都有自己的dimenson,那么整个matrix的dimension就是 \(\mathbb{R}^MN\)</li>
<li>如果matrix被理解为一系列的column vector,那么整个matrix的dimension就是M</li>
<li>如果matrix被理解为一系列的row vector,那么整个matrix的dimension就是N</li>
</ul></li>
<li>实践当中,一个matrix的dimension会case-by-case的分析会通过上下文(或者明确写出来)得到需要如何解释一个matrix</li>
</ul>
</div>
</div>
<div id="outline-container-org3557ed4" class="outline-3">
<h3 id="org3557ed4"><span class="section-number-3">5.4.</span> The transpose opertion</h3>
<div class="outline-text-3" id="text-5-4">
<ul class="org-ul">
<li><p>
本章会学习matrix 的transpose(转置)操作,其概念是和vector的transpose是一样的,都是颠倒rows和columns
</p>
<pre class="example" id="org27f2619">
Swap rows for columns and vice-versa
</pre></li>
<li>matrix transpose的notation也和vector transpose的一样, transpose of A写作 \(\mathbf{A}^T\)</li>
<li><p>
假设B是A的转置( \(\mathbf{B} = \mathbf{A}^T\),那么我们可以得到如下的公式
</p>
\begin{equation}
\mathbf{B}_{i,j} = \mathbf{A}_{j,i}  \tag{5.1}
\end{equation}</li>
<li><p>
matrix转置的转置还是自己,这是一个重要特性
</p>
\begin{equation}
\mathbf{A}^{TT} = \mathbf{A} \tag{5.2}
\end{equation}</li>
<li><p>
下面是一个matrix转置的例子
</p>
\begin{equation}
\begin{bmatrix}
1\;2\;3 \\
4\;5\;6 \\
\end{bmatrix}^T
=
\begin{bmatrix}
1\;4 \\
2\;5 \\
3\;6 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
一个非常容易犯的错误,就是不小心把column(或者row)的顺序给颠倒了,比如下面这个错误的转置,错就错在
虽然转置了,但是转置前的 \(E_{1,1}\) 还是转置后的 \(E_{1,1}\)
</p>
\begin{equation}
\begin{bmatrix}
1\;2\;3 \\
4\;5\;6 \\
\end{bmatrix}^T
\neq
\begin{bmatrix}
4\;1 \\
5\;2 \\
6\;3 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
<b>Code</b> 使用python进行转置的代码如下
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">A</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span><span style="color: #859900;">(</span>1, 2, 3<span style="color: #859900;">)</span>, <span style="color: #859900;">(</span>4, 5, 6<span style="color: #859900;">)</span><span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">'''[A] ==&gt;\n'''</span>, A<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">At1</span> = A.T
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">'''[At1] ==&gt;\n'''</span>, At1<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">At2</span> = np.transpose<span style="color: #268bd2;">(</span>A<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">'''[At2] ==&gt;\n'''</span>, At2<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[A] ==&gt;</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[[1 2 3]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[4 5 6]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[At1] ==&gt;</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[[1 4]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[2 5]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[3 6]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[At2] ==&gt;</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[[1 4]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[2 5]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[3 6]]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org0df7ed1" class="outline-3">
<h3 id="org0df7ed1"><span class="section-number-3">5.5.</span> Matrix zoology</h3>
<div class="outline-text-3" id="text-5-5">
<ul class="org-ul">
<li>有很多特殊特点的matrix会以如下方式命名:
<ul class="org-ul">
<li>发现这些特点的人的名字</li>
<li>根据这些特点来命名</li>
</ul></li>
<li>下面我们会列出并且解释这些特殊的矩阵,熟悉他们会对以后的工作非常重要</li>
<li><p>
<b>Square or rectangular</b> 方阵(square matrix)是说,这个矩阵的row和column的数目相同,例子如下
</p>
\begin{equation}
\begin{bmatrix}
  0 & 1 \\
  -1 & 0 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
非方阵(non-square matrix)是叫做矩形矩阵,例子如下
</p>
\begin{equation}
\begin{bmatrix}
  0 & 1 & 2 & 3 & -5 \\
  -1 & 0 & 3 & 23 & 0 \\
  -2 & 3 & 0 & 0 & 0 \\
\end{bmatrix}
\end{equation}</li>
<li>在线性代数领域,矩形矩阵的row和column不相同,也就是说:
<ul class="org-ul">
<li>\(M \neq N\)</li>
<li>方阵不能算是矩形矩阵</li>
</ul></li>
<li>对于一个矩形矩阵来说:
<ul class="org-ul">
<li>如果column大于row,那么它会被称之为"fat matrix"或者是"wide matrix"</li>
<li>如果column小于row,那么它会被称之为"skinny matrix"或者是"tall matrix"</li>
</ul></li>
<li><b>Symmetric</b>, 如果一个矩阵相对于对角线(diagonal)对称,那么我们就说它是对称矩阵(symmetric matrix)</li>
<li><p>
很显然,对称矩阵妈祖如下公式
</p>
\begin{equation}
\mathbf{A} = \mathbf{A}^T \tag{5.3}
\end{equation}</li>
<li><p>
另外很显然的是,只有方阵才有可能是对阵矩阵
</p>
<pre class="example" id="org74b9fcf">
Only square matrices can be symmetric
</pre></li>
<li>也正是这个原因,vector不可能具有symmetric的性质</li>
<li><p>
对阵矩阵的例子如下
</p>
\begin{equation}
\begin{bmatrix}
  1 & 4 & \pi \\
  4 & 7 & 2 \\
  \pi & 2 & 0 \\
\end{bmatrix},
\begin{bmatrix}
  a & b  \\
  b & c  \\
\end{bmatrix},
\begin{bmatrix}
  a & e & f & g \\
  e & b & h & i \\
  f & h & c & j \\
  g & i & j & d \\
\end{bmatrix}
\end{equation}</li>
<li>所有其他矩阵都叫做非对称矩阵,所有矩形矩阵都是非对称矩阵</li>
<li>对称矩阵拥有非常多的特殊属性,后面会有大量篇幅介绍</li>
<li><p>
我们可以使用很多方法从非对称(甚至是矩形)矩阵来创建对称矩阵,而且这是统计学和机器学习的核心,后面
的章节就会介绍
</p>
<pre class="example" id="org4128534">
Indeed, creating symmetric from non-symmetric matrices is central to many statistics
and machine learning applications
</pre></li>
<li><b>Skew-symmetric</b> 反对称矩阵是说一个矩阵:
<ul class="org-ul">
<li>以对角线为边界,上半部分每个成员是下半部分对应成员的值乘以-1</li>
<li>对角线上必须都是0</li>
</ul></li>
<li><p>
下面是一个斜对称矩阵的例子
</p>
\begin{equation}
\begin{bmatrix}
  0 & 1 & 2 \\
  -1 & 0 & 3 \\
  -2 & -3 & 0 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
斜对称矩阵有如下的特性:
</p>
\begin{equation}
\mathbf{A} = -\mathbf{A}^T,\tag{5.5}
\end{equation}</li>
<li><p>
<b>Identity</b> 单位矩阵(identity matrix)是非常重要的概念,所谓单位矩阵是说某个矩阵乘以单位矩阵还是自
己,用公式表示如下
</p>
\begin{equation}
\mathbf{AI} = \mathbf{A}
\end{equation}</li>
<li>一个常见的错误观念会以为单位矩阵是整个矩阵每个成员都是1的矩阵,其实恰恰不是.单位矩阵是这么一种矩阵:
<ul class="org-ul">
<li>对角线全部为1</li>
<li>其他位置全部为0</li>
</ul></li>
<li><p>
单位矩阵的例子如下
</p>
\begin{equation}
\mathbf{I} = \begin{bmatrix}
               1 & 0 & \cdot\cdot\cdot & 0 \\
               0 & 1 & \cdot\cdot\cdot & 0 \\
               \cdot & \cdot &\cdot  & \cdot \\
               0 & 0 & \cdot\cdot\cdot & 1 \\
             \end{bmatrix}
\end{equation}</li>
<li><p>
我们还经常使用角标来提升用户单位矩阵的size,如下
</p>
\begin{equation}
\mathbf{I_2} = \begin{bmatrix}
                 1 & 0 \\
                 0 & 1 \\
               \end{bmatrix},
\mathbf{I_3} = \begin{bmatrix}
                 1 & 0 & 0 \\
                 0 & 1 & 0 \\
                 0 & 0 & 1 \\
               \end{bmatrix},
\end{equation}</li>
<li>单位矩阵的全称应该是"乘法单位矩阵",也就是一个矩阵和它相乘还是自己</li>
<li>相应的其实也有"加法单位矩阵",也就是一个矩阵和它相加还是自己</li>
<li><b>Zeros</b> 零矩阵,就是这么一种"加法单位矩阵",由于所有成员都是0, 所以任何矩阵和它相加都是自己</li>
<li>我们使用 \(\mathbf{0}\) 来代表零矩阵,对应于单位矩阵的 \(\mathbf{I}\)</li>
<li>除了编程(用来初始化矩阵)等特殊情况,绝大部分情况下零矩阵都是方阵(square matrix)</li>
<li><p>
<b>Code</b>, 使用如下代码来初始化单位矩阵,零矩阵,和vector
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np
<span style="color: #268bd2;">I</span> = np.eye<span style="color: #268bd2;">(</span>4<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">'''[I] ==&gt;'''</span>, I<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">O</span> = np.ones<span style="color: #268bd2;">(</span>4<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">'''[O] ==&gt;'''</span>, O<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">Z</span> = np.zeros<span style="color: #268bd2;">(</span><span style="color: #d33682;">(</span>4, 4<span style="color: #d33682;">)</span><span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">'''[Z] ==&gt;'''</span>, Z<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[I] ==&gt; [[1. 0. 0. 0.]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[0. 1. 0. 0.]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[0. 0. 1. 0.]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[0. 0. 0. 1.]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[O] ==&gt; [1. 1. 1. 1.]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[Z] ==&gt; [[0. 0. 0. 0.]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[0. 0. 0. 0.]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[0. 0. 0. 0.]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[0. 0. 0. 0.]]</span>

</pre>
</div></li>
<li>\(\mathbf{A^TA}\) 叫做"A transpose A",有时候写作"AtA",是线性代数里面最重要的一个矩阵公式之一</li>
<li>AtA具有很多特性,值得再这里列出来,如下 TODO</li>
<li><b>Diagonal</b> diagonal matrix(对角矩阵)的特性是:
<ul class="org-ul">
<li>只有对角线(从左上到右下)的位置上的数字可能为非0</li>
<li>其他位置上的数字都是0</li>
</ul></li>
<li>单位矩阵 \(\mathbf{I}\) 就是一种对角矩阵</li>
<li><p>
对角矩阵的例子如下
</p>
\begin{equation}
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 2 & 0 \\
  0 & 0 & 3 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
如果对角矩阵的成员都是一样的,那么一个矩阵可以写成常量乘以单位矩阵(identity matrix)的形式
</p>
\begin{equation}
\begin{bmatrix}
  7 & 0 \\
  0 & 7 \\
\end{bmatrix}
= 7
\begin{bmatrix}
  1 & 0 \\
  0 & 1 \\
\end{bmatrix}
=
7 \mathbf{I}_2
\end{equation}</li>
<li><p>
对角矩阵也可以是非方阵,只要保证下标i,j相等的位置不为0即可(这条非常反直觉)
</p>
\begin{equation}
\begin{bmatrix}
  2 & 0 & 0 & 0 \\
  0 & 6 & 0 & 0 \\
\end{bmatrix},
\begin{bmatrix}
  7 & 0 \\
  0 & 1 \\
  0 & 0 \\
  0 & 0 \\
\end{bmatrix}
\end{equation}</li>
<li>对角矩阵非常有用,因为他可以简化如下两种操作:
<ul class="org-ul">
<li>matrix multiplication</li>
<li>matrix powers ( \(\mathbf{A}^n\) )</li>
</ul></li>
<li>而把一个普通矩阵转换成对角矩阵的操作叫做对角线化(diagonalization),这个操作可以通过如下两种技术得到:
<ul class="org-ul">
<li>特征分解(eigendecomposition)</li>
<li>奇异值分解(singular value decomposition)</li>
</ul></li>
<li>大写字母 \(\mathbf{D}\) 常常用来表示对角矩阵,某些情况下 \(\mathbf{\Lambda}, \mathbf{\Sigma}\) 也会用来表示对角矩阵</li>
<li>和对角矩阵相对(opposite)的是空心矩阵(hollow matrix),它的特点是:
<ul class="org-ul">
<li>对角线(从左上到右下)的位置上的数字全部为0</li>
<li>只有其他位置上的数字可能为非0</li>
</ul></li>
<li>反对称矩阵 skew-symmetric也是空心矩阵</li>
<li>空心矩阵常用来做distance matrix(因为每个node和自己的距离是0), 本书中不会再讨论空心矩阵</li>
<li><p>
<b>Code</b> 使用如下代码生成对角矩阵,或者获取对角矩阵的数据
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np


<span style="color: #268bd2;">D</span> = np.diag<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span>1, 2, 3, 5<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>       <span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&#21019;&#24314;&#19968;&#20010;&#23545;&#35282;&#30697;&#38453;</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">'''[D] ==&gt;'''</span>, D<span style="color: #268bd2;">)</span>

<span style="color: #268bd2;">R</span> = np.random.randn<span style="color: #268bd2;">(</span>3, 4<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">'''[R] ==&gt;'''</span>, R<span style="color: #268bd2;">)</span>         <span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&#38543;&#26426;&#29983;&#25104;&#19968;&#20010;3*4&#30697;&#38453;</span>

<span style="color: #268bd2;">d</span> = np.diag<span style="color: #268bd2;">(</span>R<span style="color: #268bd2;">)</span>                  <span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&#33719;&#21462;&#38543;&#26426;&#30697;&#38453;&#30340;&#23545;&#35282;&#32447;&#25968;&#25454;</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">'''[d] ==&gt;'''</span>, d<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[D] ==&gt; [[1 0 0 0]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[0 2 0 0]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[0 0 3 0]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[0 0 0 5]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[R] ==&gt; [[-2.1938143  -0.85677168 -0.27007231 -1.22768217]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.83367635  1.44634774  0.45579445  0.29923832]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 2.26729013 -0.65103393 -0.52931206 -0.39227102]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[d] ==&gt; [-2.1938143   1.44634774 -0.52931206]</span>
</pre>
</div></li>
<li><p>
<b>Augmented</b> 增广矩阵(augmented matrix)就是把两个矩阵在水平维度上面"加起来",如下
</p>
\begin{equation}
\begin{bmatrix}
  1 & 4 & 2 \\
  3 & 1 & 9 \\
  4 & 2 & 0 \\
\end{bmatrix}
\sqcup
\begin{bmatrix}
  7 & 2 \\
  7 & 2 \\
  7 & 1 \\
\end{bmatrix}
=
\begin{bmatrix}
  1 & 4 & 2 & 7 & 2 \\
  3 & 1 & 9 & 7 & 2 \\
  4 & 2 & 0 & 7 & 1 \\
\end{bmatrix}
\end{equation}</li>
<li>两个matrix能够"增广"的前提,是两个matrix拥有相同的row</li>
<li><p>
<b>Code</b> 在python中,使用concatenate来代表augment这个操作
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">A</span> = np.random.randn<span style="color: #268bd2;">(</span>3, 5<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[A] ==&gt;"""</span>, A<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">B</span> = np.random.randn<span style="color: #268bd2;">(</span>3, 2<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[B] ==&gt;"""</span>, B<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">AB</span> = np.concatenate<span style="color: #268bd2;">(</span><span style="color: #d33682;">(</span>A, B<span style="color: #d33682;">)</span>, axis=1<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[AB] ==&gt;"""</span>, AB<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[A] ==&gt; [[-0.14425489 -1.22366268 -0.27740708  1.07126456  0.70006477]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-1.28607063 -1.29396936  0.34630739  1.46735437  1.28085203]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.61064391 -0.35512087 -0.83978548 -1.67325096 -0.03125103]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[B] ==&gt; [[ 2.56938822  0.08952084]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.02576563  0.17668451]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 1.51715998 -1.43679035]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[AB] ==&gt; [[-0.14425489 -1.22366268 -0.27740708  1.07126456  0.70006477  2.56938822</span>
<span style="color: #96A7A9; font-style: italic;">#    </span><span style="color: #96A7A9; font-style: italic;">0.08952084]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-1.28607063 -1.29396936  0.34630739  1.46735437  1.28085203 -0.02576563</span>
<span style="color: #96A7A9; font-style: italic;">#    </span><span style="color: #96A7A9; font-style: italic;">0.17668451]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.61064391 -0.35512087 -0.83978548 -1.67325096 -0.03125103  1.51715998</span>
<span style="color: #96A7A9; font-style: italic;">#   </span><span style="color: #96A7A9; font-style: italic;">-1.43679035]]</span>
</pre>
</div></li>
<li><b>Triangular</b> 三角矩阵是对角矩阵和full matrix的一个中间状态</li>
<li>它的有两种表现形式:
<ol class="org-ol">
<li>upper triangular
<ul class="org-ul">
<li>对角线(从左上到右下)的位置上方,包括对角线可能是非0</li>
<li>其他位置上的数字全部为0</li>
<li><p>
例子如下
</p>
\begin{equation}
\begin{bmatrix}
  1 & 8 & 4 \\
  0 & 2 & 1 \\
  0 & 0 & 9 \\
\end{bmatrix}
\end{equation}</li>
</ul></li>
<li>lower triangular
<ul class="org-ul">
<li>对角线(从左上到右下)的位置下方,包括对角线可能是非0</li>
<li>其他位置上的数字全部为0</li>
<li><p>
例子如下
</p>
\begin{equation}
\begin{bmatrix}
  1 & 0 & 0 & 0 \\
  3 & 2 & 0 & 0 \\
  6 & 5 & 9 & 0 \\
  2 & 0 & 4 & 1 \\
\end{bmatrix}
\end{equation}</li>
</ul></li>
</ol></li>
<li><p>
上面的例子中三角矩阵都是方阵,但其实三角矩阵也可以是非方阵(rectangular),比如下面的例子
</p>
\begin{equation}
\begin{bmatrix}
  1 & 0 & 0 & 0 & 0 \\
  3 & 2 & 0 & 0 & 0 \\
  6 & 0 & 9 & 0 & 0 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
很多时候,由于三角矩阵里面有一半是0,我们可以空着不写那为0的一般(另外一半可能为0的必须写出来,0不能省略)
</p>
\begin{equation}
\begin{bmatrix}
  1 & 3 & 2 & 1 \\
    & 2 & 7 & 9 \\
    &   & 0 & 5 \\
    &   &   & 2 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
<b>Code</b> 使用如下代码来把一个matrix变成upper triangle和lower triangle
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">A</span> = np.random.randn<span style="color: #268bd2;">(</span>5, 5<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[A] ==&gt;"""</span>, A<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">L</span> = np.tril<span style="color: #268bd2;">(</span>A<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[L] ==&gt;"""</span>, L<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">U</span> = np.triu<span style="color: #268bd2;">(</span>A<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[U] ==&gt;"""</span>, U<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[A] ==&gt; [[ 0.39866979 -1.22350387 -0.33729558 -1.0857667  -1.30051086]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.8020152  -0.35123035 -1.34653201  1.48667849 -0.34774193]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.01468234  1.83371032  0.27659977 -0.97204034  0.51079151]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 1.01177767  0.35195454 -0.89955996  0.66733842 -0.46424685]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.86648848  0.43285399 -1.91123781  0.07286382  0.18129945]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[L] ==&gt; [[ 0.39866979  0.          0.          0.          0.        ]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.8020152  -0.35123035  0.          0.          0.        ]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.01468234  1.83371032  0.27659977  0.          0.        ]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 1.01177767  0.35195454 -0.89955996  0.66733842  0.        ]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.86648848  0.43285399 -1.91123781  0.07286382  0.18129945]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[U] ==&gt; [[ 0.39866979 -1.22350387 -0.33729558 -1.0857667  -1.30051086]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.         -0.35123035 -1.34653201  1.48667849 -0.34774193]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.          0.          0.27659977 -0.97204034  0.51079151]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.          0.          0.          0.66733842 -0.46424685]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.          0.          0.          0.          0.18129945]]</span>
</pre>
</div></li>
<li><b>Dense and sparse</b> 如果一个matrix的大部分或者全部element是非0的,那么我们给这个matrix起了如下两个名字:
<ul class="org-ul">
<li>dense matrix(稠密矩阵)</li>
<li>full matrix</li>
</ul></li>
<li>dense matrix这个概念是要有上下文的,比如比较如下两个概念的时候:
<ul class="org-ul">
<li>对角矩阵</li>
<li>稠密矩阵(dense matrix)</li>
</ul></li>
<li>稀疏矩阵(sparse matrix)是指一个矩阵里面大部分员为0,只有很少一部分成员不为0的矩阵</li>
<li>稀疏矩阵计算效率超高,所以现代算法里面把稀疏矩阵作为重点来介绍</li>
<li><p>
下面10*10的稀疏矩阵,可以使用简单的列举非0成员而表示
</p>
\begin{equation}
  \begin{bmatrix}
    0 & 0 & 4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 8 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
  \end{bmatrix}
  \Rightarrow
  (1, 3)4,(4, 6)8
\end{equation}</li>
<li><b>Orthogonal</b> 正交矩阵是满足如下两个条件的矩阵:
<ul class="org-ul">
<li>我们把一个矩阵的每一列(column)看成是一个vector,那么正交矩阵的任意两列的vector的dot product都是0</li>
<li>我们把一个矩阵的每一列(column)看成是一个vector,某个vector和自己的dot product 都是1</li>
</ul></li>
<li><p>
一般来说,我们使用 \(\mathbf{Q}\) 来代表正交矩阵,正交矩阵的正式定义如下
</p>
\begin{equation}
< \mathbf{Q_i}, \mathbf{Q_j} > =
  \begin{cases}
  1, if \; i == j \\
  0, if \; j \ne j \\
  \end{cases},\tag{5.8}
\end{equation}</li>
<li>注意,这里的&lt;&gt;是dot product的符号,我们这里每个成员都是一矩阵的一列</li>
<li><p>
公式5-8有一个更加专业的,只用矩阵乘法(后面会学到)的公式,如下
</p>
\begin{equation}
\mathbf{Q}^T \mathbf{Q} = \mathbf{I},\tag{5.9}
\end{equation}</li>
<li><b>Toeplitz</b> 特布里茨矩阵(也叫常对角矩阵),是指每条对角线上的成员数字都一样的矩阵</li>
<li><p>
我们可以使用一个vector来生成常对角矩阵,如下
</p>
\begin{equation}
\begin{bmatrix}
a & b & c & d \\
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
  a & b & c & d \\
  b & a & b & c \\
  c & b & a & b \\
  d & c & b & a \\
\end{bmatrix}
\end{equation}</li>
<li>其实上图很好理解,把初始的vector作为常对角矩阵的第一行(以及竖着的第一列),后面按照规则(每个对角线
必须一致),继续添加就可以了.总体看起来就是:
<ul class="org-ul">
<li>vector的第一个成员是主对角线(main diagonal)上的数字</li>
<li>vector的第N个成员是第N个对角线上的数字</li>
</ul></li>
<li>需要注意的是上面的例子是一个方阵,而且是对称矩阵(symmetric),但是其实常对角矩阵的定义只是要求对角
线数据一致就可以,常对角矩阵既可以是非方阵,也当然可以是非对称的</li>
<li><b>Hankel</b> 汉克矩阵,是指每条副对角线上的成员数字都一样的方阵:
<ul class="org-ul">
<li>汉克矩阵由于每个副对角线都一样,所以相对于对角线对称</li>
</ul></li>
<li>下面两个都是汉克矩阵:
<ul class="org-ul">
<li><p>
使用一个vector生成的汉克矩阵,先固定好第一行第一列,后面自动生成.因为是副对角线
</p>
\begin{equation}
\begin{bmatrix}
a & b & c & d \\
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
  a & b & c & d \\
  b & c & d & 0 \\
  c & d & 0 & 0 \\
  d & 0 & 0 & 0 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
甚至可以wrap abcd(每次wrap一个位置),最终生成的full matrix也是汉克矩阵
</p>
\begin{equation}
\begin{bmatrix}
  a & b & c & d \\
  b & c & d & a \\
  c & d & a & b \\
  d & a & b & c \\
\end{bmatrix}
\end{equation}</li>
</ul></li>
<li><p>
汉克矩阵的生成公式如下:
</p>
\begin{equation}
  \mathbf{Y}_{i,j} = \mathbf{x}_{i+j-1}, \tag{5.10}
\end{equation}</li>
<li>其实上面两种汉克矩阵,从公式看,都可以看成是从vector生成的矩阵,只不过:
<ul class="org-ul">
<li>第一个例子abcd后面都是0,也就是说,我们的vector延长后就是[a b c d 0 0 0]</li>
<li>第二个例子abcd后面就是不停的wrap,我们的vector延长后就是[a b c d a b c]</li>
</ul></li>
<li><p>
<b>Code</b> 使用如下python代码来生成常对角矩阵和汉克矩阵
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">from</span> scipy.linalg <span style="color: #859900; font-weight: bold;">import</span> hankel, toeplitz

<span style="color: #268bd2;">t</span> = <span style="color: #268bd2;">[</span>1, 2, 3, 4<span style="color: #268bd2;">]</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[t] ==&gt;"""</span>, t<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">T</span> = toeplitz<span style="color: #268bd2;">(</span>t<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[T] ==&gt;"""</span>, T<span style="color: #268bd2;">)</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&#31532;&#19968;&#20010;&#21442;&#25968;&#26159;&#31532;&#19968;&#34892;, r&#36825;&#20010;&#21442;&#25968;&#26159;&#26368;&#21518;&#19968;&#34892;,&#22914;&#26524;&#19981;&#21152;r,&#37027;&#20040;&#19979;&#21322;&#37096;&#20998;&#37117;&#26159;0</span>
<span style="color: #268bd2;">H</span> = hankel<span style="color: #268bd2;">(</span>t, r=<span style="color: #d33682;">[</span>2, 3, 4, 1<span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[H] ==&gt;"""</span>, H<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[t] ==&gt; [1, 2, 3, 4]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[T] ==&gt; [[1 2 3 4]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[2 1 2 3]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[3 2 1 2]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[4 3 2 1]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[H] ==&gt; [[1 2 3 4]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[2 3 4 3]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[3 4 3 4]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[4 3 4 1]]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org1a40683" class="outline-3">
<h3 id="org1a40683"><span class="section-number-3">5.6.</span> Matrix addition and subtraction</h3>
<div class="outline-text-3" id="text-5-6">
<ul class="org-ul">
<li>matrix的加法是非常简单的,两个matrix相加,就是两个matrix对应位置的成员相加,所以matrix加法要求:
<ul class="org-ul">
<li>相加的两个matrix必须是一样的size,比如 \(M \times N\)</li>
<li>相加的结果,也是同一个size,也就是\(M \times N\)</li>
</ul></li>
<li><p>
下面是一个加法的例子
</p>
\begin{equation}
\begin{bmatrix}
  1 & 2 & 5 \\
  9 & 8 & 7 \\
\end{bmatrix}
+
\begin{bmatrix}
  -1 & 0 & -5 \\
  3  & a & \pi \\
\end{bmatrix}
=
\begin{bmatrix}
  0  &   2   & 0 \\
  12 & 8 + a & 7 + \pi \\
\end{bmatrix}
\end{equation}</li>
<li>matrix减法和加法一样,只不过"+"换成"-",我们就不再赘述</li>
<li><p>
就像vector addition一样, matrix addition也满足交换律(commutative),也就是说,如下
</p>
\begin{equation}
\mathbf{C} = \mathbf{A} + \mathbf{B} = \mathbf{B} + \mathbf{A}
\end{equation}</li>
<li>看起来非常显然的知识我们还是列出来了,是因为有些显然是成立的,有些不成立,比如matrix的乘法就不满足
交换律</li>
<li>matrix满足交换律对于生成symmetric matrix非常重要</li>
</ul>
</div>
</div>
<div id="outline-container-org17aa249" class="outline-3">
<h3 id="org17aa249"><span class="section-number-3">5.7.</span> Scalar-matrix multiplication</h3>
<div class="outline-text-3" id="text-5-7">
<ul class="org-ul">
<li>matrix multiplication非常重要,且内容繁多,我们将使用一整章来介绍,这里我们介绍相对简单的"乘法":
scalar-matrix multiplication</li>
<li><p>
scalar-matrix multiplication简单来说,就是给matrix的每个成员乘以一个系数,和scalar-vector multiplication
的原理是一样的,例子如下
</p>
\begin{equation}
\delta
\begin{bmatrix}
  a & b \\
  c & d \\
\end{bmatrix}
=
\begin{bmatrix}
  \delta a & \delta b \\
  \delta c & \delta d \\
\end{bmatrix}
=
\begin{bmatrix}
  a \delta & b \delta \\
  c \delta & d \delta \\
\end{bmatrix}
=
\begin{bmatrix}
  a & b \\
  c & d \\
\end{bmatrix}
\delta
\tag{5.12}
\end{equation}</li>
<li><p>
由于scalar-matrix multiplication都是在element维度上面进行的,所以它也是满足交换律(commutative)的,
所以scalar可以到处移动,这对于后面的很多证明都是非常重要的.scalar到处移动可以用公式表达为
</p>
\begin{equation}
\lambda \mathbf{AB} = \mathbf{A} \lambda \mathbf{B} = \mathbf{AB} \lambda
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-org6e15453" class="outline-3">
<h3 id="org6e15453"><span class="section-number-3">5.8.</span> "Shifting" a matrix</h3>
<div class="outline-text-3" id="text-5-8">
<ul class="org-ul">
<li>我们可以把如下两个操作结合成shift操作,应用于一个matrix上:
<ul class="org-ul">
<li>matrix addition</li>
<li>scalar-matrix multiplication</li>
</ul></li>
<li>shift操作是把一个identity matrix(单位矩阵)进行scalar之后,加到原来的matrix上面,所以原来的matrix也
必须是方阵,因为identity matrix(单位矩阵)是方阵</li>
<li><p>
新生成的矩阵一般在字母上面加一个波浪线,比如 \(\tilde{\mathbf{A}}\). 下面就是shitfing a matrix操作的公式表达
</p>
\begin{equation}
\tilde{\mathbf{A}} = \mathbf{A} + \lambda \mathbf{I}, \mathbf{A} \in \mathbb{R}^{M \times M}, \lambda \in \mathbb{R}, \tag{5.13}
\end{equation}</li>
<li><p>
我们使用一个真实的例子来展现下shift操作
</p>
\begin{equation}
\begin{bmatrix}
  1 & 3 & 0 \\
  1 & 3 & 0 \\
  2 & 2 & 7 \\
\end{bmatrix}
+ \; .1
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 \\
\end{bmatrix}
=
\begin{bmatrix}
  1.1 & 3 & 0 \\
  1   & 3.1 & 0 \\
  2   & 2 & 7.1 \\
\end{bmatrix}
\end{equation}</li>
<li>从上面的例子,我们可以看到shifting操作的如下特性:
<ul class="org-ul">
<li>只有对角线上的成员受到影响,非对角线上的成员不受影响,因为单位矩阵的非对角线上都是0</li>
<li>在shift之前矩阵的第一行和第二行是一样的,但是shift之后两者不一样了,所以shift matrix可以用来让
矩阵中相同的行变的不太一样</li>
<li>如果 \(\lambda\) 越接近0, 那么 $~{\mathbf{A}} 就和 \(\mathbf{A}\) 非常的接近, 极端情况下
\(\lambda\) 为0的情况下, \(\tilde{\mathbf{A}} = \mathbf{A}\). 在实践当中,选择一个尽可能满足条件的
足够小的 \(\lambda\) 是工作当中的重要工作</li>
</ul></li>
<li>shifting matrix有着广泛的应用,比如:
<ul class="org-ul">
<li>在统计学中为了让模型适应low-rank数据,通常要进行正则化(Regularization), 正则化的过程当中就设计shifting matrix</li>
<li>另外shifting matrix可以把一个rank-deficient matrix转换成一个full-rank matrix</li>
</ul></li>
<li><p>
<b>Code</b> shifting的代码包含scalar multiplication和addition,如下
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">l</span> = 0.01
<span style="color: #268bd2;">I</span> = np.eye<span style="color: #268bd2;">(</span>4<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">A</span> = np.random.randn<span style="color: #268bd2;">(</span>4, 4<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[A] ==&gt;"""</span>, A<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">As</span> = A + l * I
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[As] ==&gt;"""</span>, As<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[A] ==&gt; [[-1.57995308 -0.26785428 -0.2266074   1.11107041]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.06172473  0.10330103 -0.4369862   1.35241956]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-1.86982424  1.14082253  0.87677916  0.81019919]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.10328674  0.47264134  0.02081591  2.13364769]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[As] ==&gt; [[-1.56995308 -0.26785428 -0.2266074   1.11107041]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.06172473  0.11330103 -0.4369862   1.35241956]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-1.86982424  1.14082253  0.88677916  0.81019919]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.10328674  0.47264134  0.02081591  2.14364769]]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-orgb2a22c2" class="outline-3">
<h3 id="orgb2a22c2"><span class="section-number-3">5.9.</span> Diagonal and trace</h3>
<div class="outline-text-3" id="text-5-9">
<ul class="org-ul">
<li>matrix对角线上的数据可以取出来,并且放到一个vector里面</li>
<li>上面的操作在某些特定场景下有用,比如:
<ul class="org-ul">
<li>在统计学领域,covariance matrix(协方差矩阵)的对角线成员包含每个variable的variance(方差)</li>
</ul></li>
<li>注意,提取对角线数据的操作不要求操作对象是方阵</li>
<li>如图:
<ul class="org-ul">
<li><p>
图5-6
</p>

<div id="org939fe96" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/5-6.png" alt="5-6.png" />
</p>
<p><span class="figure-number">Figure 13: </span>tic/5-6.png</p>
</div></li>
</ul></li>
<li><b>Trace</b> trace是把一个方阵(注意只能是方阵)的对角线数据加起来求和</li>
<li><p>
我们通常使用 \(tr(\mathbf{A})\) 来表示trace操作,公式如下
</p>
\begin{equation}
tr(\mathbf{A}) = \sum_{i=1}^M a_{i,i},\tag{5.15}
\end{equation}</li>
<li>由于非对角线数据对于trace操作没有贡献,所以两个差距非常大的方阵可能trace相同</li>
<li>trace操作对于机器学习非常重要:
<ul class="org-ul">
<li>它用来计算两个matrix的Frobenius norm(范数)</li>
<li>范数用来度量两个matrix之间的距离</li>
</ul></li>
<li>trace操作只能用于方阵是由于"历史原因":
<ul class="org-ul">
<li>matrix的trace等于matrix 特征(eigenvalue)的综合</li>
<li>特征分解只作用于方阵,所以从古时候开始,古代数学家就规定了只有方阵才能进行trace操作</li>
</ul></li>
<li><p>
<b>Code</b> 使用如下代码进行trace操作
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">A</span> = np.random.randn<span style="color: #268bd2;">(</span>4, 4<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[A] ==&gt;"""</span>, A<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">tr</span> = np.trace<span style="color: #268bd2;">(</span>A<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[tr] ==&gt;"""</span>, tr<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[A] ==&gt; [[ 0.59060411  1.20311925 -1.02348206 -0.46843217]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.21939289 -2.09379843 -1.20578246 -0.26266209]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-1.47517321 -0.55440747  0.54005967 -1.51054155]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.51872691  0.98704069  2.48983979  1.31954482]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[tr] ==&gt; 0.35641017102727246</span>
</pre>
</div></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orge4407b1" class="outline-2">
<h2 id="orge4407b1"><span class="section-number-2">6.</span> Matrix multiplication</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-org24cbec4" class="outline-3">
<h3 id="org24cbec4"><span class="section-number-3">6.1.</span> "Standard" matrix multiplication</h3>
<div class="outline-text-3" id="text-6-1">
<ul class="org-ul">
<li>两个matrix相乘的时候,如果没有明确的说明,那么我们就默认两个matrix进行的乘法(比如 \(\mathbf{AB}\) )
是标准乘法(standard matrix multiplication)</li>
<li><b>Terminology</b> 最先需要知道的是,matrix multiplication不满足交换律(not commutative),所以 \(\mathbf{AB} \neq \mathbf{BA}\)</li>
<li>由于不满足交换律,所以矩阵乘法的定义非常麻烦,以 \(\mathbf{AB}\) 为例,其有如下五种英语表达方式:
<ul class="org-ul">
<li>A times B</li>
<li>A left-multiplies B</li>
<li>A pre-multiplies B</li>
<li>B right-multiplies A</li>
<li>B post-multiplies A</li>
</ul></li>
<li><b>Validity</b> 在了解如何进行乘法之前,我们先要了解什么样的两个矩阵可以进行相乘</li>
<li>matrix mulitiplication只有在如下情况下valid: 两个matrix的inner dimension相同</li>
<li>同时,新得到的matrix的size是由两个matrix的outer dimension决定的</li>
<li>如图
<ul class="org-ul">
<li><p>
图6-1
</p>

<div id="orgd72a8eb" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/6-1.png" alt="6-1.png" />
</p>
<p><span class="figure-number">Figure 14: </span>tic/6-1.png</p>
</div></li>
<li>N就是两个矩阵的inner dimension,他们必须相同</li>
<li>M和K是两个矩阵的outer dimension,他们不需要相同,但是矩阵乘法得到的新矩阵,其size由这两个dimension决定</li>
</ul></li>
<li>一旦理解了什么样的乘法是valid的,以及resulting matrix的size如何而来,我们就可以理解dot product和
outer product的区别了
<ul class="org-ul">
<li><p>
图6-2
</p>

<div id="org63aa67d" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/6-2.png" alt="6-2.png" />
</p>
<p><span class="figure-number">Figure 15: </span>tic/6-2.png</p>
</div></li>
<li>我们从这里就可以看到:
<ol class="org-ol">
<li>dot product的结果是一个数字,那么必然要用 \(\mathbf{V}^T \mathbf{W}\)</li>
<li>out product的结果是一个矩阵,那么必然要用 \(\mathbf{V} \mathbf{W}^T\)</li>
</ol></li>
</ul></li>
<li><p>
<b>Code</b> 使用如下python代码来代表矩阵乘法
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np
<span style="color: #268bd2;">M1</span> = np.random.randn<span style="color: #268bd2;">(</span>4, 3<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">M2</span> = np.random.randn<span style="color: #268bd2;">(</span>3, 5<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">C</span> = M1 @ M2
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">'''[C] ==&gt;'''</span>, C<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[C] ==&gt; [[-0.23848307 -1.54327638 -0.58576378  1.10745737  1.73813371]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.12136803  0.06643498 -1.03968181  1.2177658  -0.68164259]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.10025755  1.3981402  -0.9935196  -0.1241486  -1.14903948]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.73051258 -1.08758839  0.74398132  1.40448759 -0.89103988]]</span>
</pre>
</div></li>
<li>下面我们终于开始学习如何进行乘法了.有四种方法来思考如何实施矩阵乘法,四种计算方法得到的结果一样,
但是提供了不同的视角来理解乘法</li>
</ul>
</div>
<div id="outline-container-orgd32c0c2" class="outline-4">
<h4 id="orgd32c0c2"><span class="section-number-4">6.1.1.</span> The "element perspective"</h4>
<div class="outline-text-4" id="text-6-1-1">
<ul class="org-ul">
<li>公式 \(\mathbf{AB=C}\)</li>
<li>其中每个成员 \(c_{ij}\) 都是如下两个矩阵的dot product
<ul class="org-ul">
<li>\(\mathbf{A}\) 的ith row</li>
<li>\(\mathbf{B}\) 的jth column</li>
</ul></li>
<li><p>
下面的例子解释了这个过程: top-left的成员如何获得
</p>
\begin{equation}
  \begin{bmatrix}
    1 & 2 \\
    3 & 4 \\
  \end{bmatrix}
  \begin{bmatrix}
    a & b \\
    c & d \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    1a+2c &  \\
     &  \\
  \end{bmatrix} \tag{6.1}
\end{equation}</li>
<li><p>
下面是一个完整的例子
</p>
\begin{equation}
\begin{bmatrix}
  3  & 4 \\
  -1 & 2 \\
  0  & 4 \\
\end{bmatrix}
\begin{bmatrix}
  5 & 1 \\
  3 & 1 \\
\end{bmatrix}
=
\begin{bmatrix}
  3*5 + 4*3  & 3*1+4*1 \\
  -1*5 + 2*3 & -1*1+2*1 \\
  0*5 + 4*3  & 0*1+4*1 \\
\end{bmatrix}
=
\begin{bmatrix}
  27 & 7 \\
  1  & 1 \\
  12 & 4 \\
\end{bmatrix}
\end{equation}</li>
<li>如图
<ul class="org-ul">
<li><p>
图6-3-1
</p>

<div id="org74e57a2" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/6-3-1.png" alt="6-3-1.png" />
</p>
<p><span class="figure-number">Figure 16: </span>tic/6-3-1.png</p>
</div></li>
<li>我们把matrix的一行(或者一列)抽象成一个向量,从这方式我们可以看到matrix multiplication的三个特性:
<ol class="org-ol">
<li>matrix \(\mathbf{C}\) 的对角线上,包含了这样一些vector的dot product, 这些vector的下标( \(\mathbf{A}\)
里面的i, \(\mathbf{B}\) 里面的j)相同</li>
<li>matrix \(\mathbf{C}\) 的对角线下半部分,包含了这样一些vector的dot product, 这些vector的下标中,( \(\mathbf{A}\)
里面的i 大于 \(\mathbf{B}\) 里面的j)</li>
<li>matrix \(\mathbf{C}\) 的对角线上半部分,包含了这样一些vector的dot product, 这些vector的下标中,( \(\mathbf{A}\)
里面的i 小于 \(\mathbf{B}\) 里面的j)</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org82d827e" class="outline-4">
<h4 id="org82d827e"><span class="section-number-4">6.1.2.</span> The "layer perspective"</h4>
<div class="outline-text-4" id="text-6-1-2">
<ul class="org-ul">
<li>上面的"element perspective"是每个element计算自己的,不同element中间是互相不干扰的</li>
<li>layer perspective则完全不同, 它是把matrix multiplication的计算分成了多个层(而不是m*n个element),
然后把这些个层叠加起来</li>
<li>如图
<ul class="org-ul">
<li><p>
图6-4
</p>

<div id="org9e8356c" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/6-4.png" alt="6-4.png" />
</p>
<p><span class="figure-number">Figure 17: </span>tic/6-4.png</p>
</div></li>
<li>我们知道out product的结果都是一样的,所以把这些结果加起来就是matrix multiplication的product,
就好比上图中多个透明纸上的图案叠加起来</li>
</ul></li>
<li><p>
下面是一个使用outer product来计算matrix multiplication的过程
</p>
\begin{equation}
\begin{bmatrix}
  3  & 4 \\
  -1 & 2 \\
  0  & 4 \\
\end{bmatrix}
\begin{bmatrix}
  5 & 1 \\
  3 & 1 \\
\end{bmatrix}
=
\begin{bmatrix}
  15 & 3 \\
  -5 & -1 \\
  0  & 0 \\
\end{bmatrix}
+
\begin{bmatrix}
  12 & 4 \\
  6  & 2 \\
  12 & 4 \\
\end{bmatrix}
=
\begin{bmatrix}
  27 & 7 \\
  1  & 1 \\
  12 & 4 \\
\end{bmatrix}
\end{equation}</li>
<li>outer product有个特殊的点,我们可以通过下面的例子看到:
<ul class="org-ul">
<li><p>
例子
</p>
\begin{equation}
\begin{bmatrix}
a \\
b \\
c \\
\end{bmatrix}
\begin{bmatrix}
d e f  \\
\end{bmatrix}
=
\begin{bmatrix}
ad & ae & af & \\
bd & be & bf & \\
cd & ce & cf & \\
\end{bmatrix}
\end{equation}</li>
<li>特点: 由于都是使用同样的前缀,那么很显然每个outer product得到的matrix里面的column vector都是
dependent set</li>
</ul></li>
<li>那么很显然以layer perspective来看:
<ul class="org-ul">
<li>每个layer matrix的column都是dependent set</li>
<li>但是,这些layer matrix的和(也就是matrix multiplication的结果缺是independent set)</li>
</ul></li>
<li>layer perspective的解释对于矩阵的spectral theorem非常有用</li>
<li><p>
所谓的谱定理(spectral theorem)是说: 任意的matrix都可以看成是一系列"秩为1矩阵"的和
</p>
<pre class="example" id="orgcb3742d">
Any matrix can be represented as a sum of rank-1 matrices
</pre></li>
<li>我们还没学到秩,但是我们可以简单的理解为所谓秩为1,是指只有一个column有意义,其他column都是这个有
意义column的scaled version</li>
<li>谱定理(spectral theorem)是奇异值分解(singular value decomposition)的基础,这个我们后面会介绍到</li>
</ul>
</div>
</div>
<div id="outline-container-org67c6173" class="outline-4">
<h4 id="org67c6173"><span class="section-number-4">6.1.3.</span> The "column perspective"</h4>
<div class="outline-text-4" id="text-6-1-3">
<ul class="org-ul">
<li>从column perspective来看,所有的matrix(无论是multiplying matrix还是product matrix),搜索可以看做
是一系列column vector的set</li>
<li>我们这里单看product matrix,它可以看做每次创建一个column:
<ul class="org-ul">
<li>product matrix里面的第一个column是一个linear weighted combination的值,这个linear weighted combination:
<ol class="org-ol">
<li>weight是right matrix的第一个column</li>
<li>对应的vector是left matrix的vector set(每一个column是一个vector)</li>
</ol></li>
<li>product matrix里面的第二个column是一个linear weighted combination的值,这个linear weighted combination:
<ol class="org-ol">
<li>weight是right matrix的第二个column</li>
<li>对应的vector是left matrix的vector set(每一个column是一个vector)</li>
</ol></li>
</ul></li>
<li><p>
上面的情况,我们使用例子更能深刻理解,如下
</p>
\begin{equation}
\begin{bmatrix}
1 & 2 \\
3 & 4 \\
\end{bmatrix}
\begin{bmatrix}
a & b \\
c & d \\
\end{bmatrix}
=
\left[
a
\begin{bmatrix}
1 \\

3 \\
\end{bmatrix}
+
c
\begin{bmatrix}
2 \\
4 \\
\end{bmatrix}
\;b
\begin{bmatrix}
1 \\
3 \\
\end{bmatrix}
+
d
\begin{bmatrix}
2 \\
4 \\
\end{bmatrix}
\right],\tag{6.2}
\end{equation}</li>
<li>column perspective的解释在统计学中非常有用:
<ul class="org-ul">
<li>左侧matrix的column包含了一系列的regressor(data的简单模型)</li>
<li>右侧matrix包含coefficient, coefficient包含了对于每个regressor的重要性评估</li>
<li>统计学中的model-fitting其实就寻找最佳的coefficient,能够使得weighted combination of regressor
能够更好的match data</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org6ff267d" class="outline-4">
<h4 id="org6ff267d"><span class="section-number-4">6.1.4.</span> The "row perspective"</h4>
<div class="outline-text-4" id="text-6-1-4">
<ul class="org-ul">
<li>和column perspective的概念类似,但是现在创建product是每次创建一行(而不是每次创建一列):
<ul class="org-ul">
<li>在product matrix里面,每一行都是weighted sum of all rows(right matrix),而weight的来源则是左边的row</li>
<li><p>
下面就是整个过程的例子
</p>
\begin{equation}
\begin{bmatrix}
1 & 2 \\
3 & 4 \\
\end{bmatrix}
\begin{bmatrix}
a & b \\
c & d \\
\end{bmatrix}
=
\begin{bmatrix}
  1 [a \; b] + 2 [c \; d] \\
  3 [a \; b] + 4 [c \; d] \\
\end{bmatrix},\tag{6.3}
\end{equation}</li>
<li>以第1行为例,第一行是一个1*2的矩阵,来源于两个1*2矩阵的和,这两个1*2的矩阵是以如下方式组成的:
<ol class="org-ol">
<li>vector来自右边矩阵的行,分别是[a b]和[c d]</li>
<li>weight来自左边矩阵的第2行,分别是1, 2</li>
</ol></li>
<li>以第2行为例,第一行是一个1*2的矩阵,来源于两个1*2矩阵的和,这两个1*2的矩阵是以如下方式组成的:
<ol class="org-ol">
<li>vector来自右边矩阵的行,分别是[a b]和[c d]</li>
<li>weight来自左边矩阵的第2行,分别是3, 4</li>
</ol></li>
</ul></li>
<li>row perspective在principal components analysis里面非常有用</li>
</ul>
</div>
</div>
<div id="outline-container-orge417cb6" class="outline-4">
<h4 id="orge417cb6"><span class="section-number-4">6.1.5.</span> Visually summarizes the different perspective</h4>
<div class="outline-text-4" id="text-6-1-5">
<ul class="org-ul">
<li><p>
如图
</p>

<div id="org95b95b0" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/6-5.png" alt="6-5.png" />
</p>
<p><span class="figure-number">Figure 18: </span>tic/6-5.png</p>
</div></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org27e3e5e" class="outline-3">
<h3 id="org27e3e5e"><span class="section-number-3">6.2.</span> Multiplication and equations</h3>
<div class="outline-text-3" id="text-6-2">
<ul class="org-ul">
<li>在高中数学中,我们学到,可以给一个等式的左右都乘以一个term,比如:
<ul class="org-ul">
<li><p>
如下不等式成立的话
</p>
\begin{equation}
4 + x  =  5(y + 3)
\end{equation}</li>
<li><p>
两边都乘以7,还是成立
</p>
\begin{equation}
7(4+x) = 7(5(y+3))
\end{equation}</li>
<li><p>
又由于普通乘法是遵守交换律的,所以等式右边的7其实也可以放到最后,如下
</p>
\begin{equation}
7(4+x) = (5(y+3))7
\end{equation}</li>
</ul></li>
<li>但是,matrix multiplication是不遵守交换律的,所以如果朝矩阵等式左右同时乘以一个矩阵的话,必须要保证
新乘以的矩阵要么都在左边,要么都在右边,也就是说
<ul class="org-ul">
<li><p>
假设如下等式成立
</p>
\begin{equation}
\mathbf{B} = \lambda(\mathbf{C} + \mathbf{D})
\end{equation}</li>
<li><p>
那么如下两个等式都成立,注意A一定要紧贴对应的矩阵, \(\lambda\) 由于是一个标量,满足交换律,可以到
处移动,又由于为了美观,他必须在最左或者最右,不能混在两个矩阵中间
</p>
\begin{gather*}
\mathbf{AB} = \lambda \mathbf{A}(\mathbf{C} + \mathbf{D}) \\
\mathbf{AB} = \mathbf{A}(\mathbf{C} + \mathbf{D}) \lambda
\end{gather*}</li>
</ul></li>
<li><p>
注意!下面是错误的示范,矩阵乘法在等式两边添加矩阵的时候,必须全部在左边或者全部在右边,不能一个左边一个右边!
</p>
\begin{gather*}
  \mathbf{B} = \lambda(\mathbf{C} + \mathbf{D}) \\
  \mathbf{AB} = \lambda(\mathbf{C} + \mathbf{D}) \mathbf{A}
\end{gather*}</li>
<li><p>
<b>Code</b> matrix multiplication不符合交换律可以很容易使用代码表示
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np
<span style="color: #268bd2;">A</span> = np.random.randn<span style="color: #268bd2;">(</span>2, 2<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">B</span> = np.random.randn<span style="color: #268bd2;">(</span>2, 2<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">'''[A] ==&gt;\n'''</span>, A<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">'''[B] ==&gt;\n'''</span>, B<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">C1</span> = A@B
<span style="color: #268bd2;">C2</span> = B@A
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">'''[C1] ==&gt;\n'''</span>, C1<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">'''[C2] ==&gt;\n'''</span>, C2<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[A] ==&gt;</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[[-0.88387037 -0.58445233]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 1.2873149   0.15789067]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[B] ==&gt;</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[[ 0.77248555 -0.46482874]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.90373689  0.8331224 ]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[C1] ==&gt;</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[[-1.21096822 -0.07607197]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 1.13712378 -0.46683871]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[C2] ==&gt;</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[[-1.28115805 -0.5248731 ]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.27370461 -0.39664887]]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org028012b" class="outline-3">
<h3 id="org028012b"><span class="section-number-3">6.3.</span> Matrix multiplication with a diagonal matrix</h3>
<div class="outline-text-3" id="text-6-3">
<ul class="org-ul">
<li>当一个matrix是对角矩阵(siagonal matrix),另外一个矩阵是常规矩阵的时候,会有如下效果:
<ul class="org-ul">
<li><p>
Pre-multiplication一个对角矩阵,会scale右边矩阵的row,如下
</p>
\begin{equation}
\begin{bmatrix}
  a & 0 & 0  \\
  0 & b & 0  \\
  0 & 0 & c \\
\end{bmatrix}
\begin{bmatrix}
  1 & 2 & 3 \\
  4 & 5 & 6 \\
  7 & 8 & 9 \\
\end{bmatrix}
=
\begin{bmatrix}
  1a & 2a & 3a \\
  4b & 5b & 6b \\
  7c & 8c & 9c \\
\end{bmatrix} \tag{6.4}
\end{equation}</li>
<li><p>
Post-multiplication一个对角矩阵,会scale右边矩阵的column,如下
</p>
\begin{equation}
\begin{bmatrix}
  1 & 2 & 3 \\
  4 & 5 & 6 \\
  7 & 8 & 9 \\
\end{bmatrix}
\begin{bmatrix}
  a & 0 & 0  \\
  0 & b & 0  \\
  0 & 0 & c \\
\end{bmatrix}
=
\begin{bmatrix}
  1a & 2b & 3c \\
  4a & 5b & 6c \\
  7a & 8b & 9c \\
\end{bmatrix} \tag{6.5}
\end{equation}</li>
</ul></li>
<li>我们可以使用如下的方法来辅助我们记住上面的规律:
<ul class="org-ul">
<li>PRe-multiply to affect Rows -&gt; <b>R</b></li>
<li>POst-multiply to affect cOlumns -&gt; <b>O</b></li>
</ul></li>
<li><p>
<b>Multiplying two diagonal matrices</b> 两个对角矩阵的结果还是对角矩阵,并且每个位置上的值都是相乘的两个矩阵对应位置值的乘积
</p>
\begin{equation}
\begin{bmatrix}
  a & 0 & 0 \\
  0 & b & 0 \\
  0 & 0 & c \\
\end{bmatrix}
\begin{bmatrix}
  d & 0 & 0 \\
  0 & e & 0 \\
  0 & 0 & f \\
\end{bmatrix}
\begin{bmatrix}
  ad & 0 & 0 \\
  0 & be & 0 \\
  0 & 0 & cf \\
\end{bmatrix} \tag{6.6}
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-orge1ce0c5" class="outline-3">
<h3 id="orge1ce0c5"><span class="section-number-3">6.4.</span> LIVE EVIL !(a.k.a order of operations)</h3>
<div class="outline-text-3" id="text-6-4">
<ul class="org-ul">
<li><p>
LIVE EVIL 是一个palindrome(也就是说LIVE倒过来就是EVIL), 这个成语是用来比喻对于matrix multiplication
的操作等于分别对每个matrix操作,但是以相反的order:
</p>
<pre class="example" id="orgae8469e">
An operation applied to multiplied matrices gets applied to each matrix
individually but in reverse order
</pre></li>
<li>我们举个例子:
<ol class="org-ol">
<li><p>
两个矩阵乘法的结果求转置,结果如下
</p>
\begin{equation}
  \bigg (
  \begin{bmatrix}
    2 & 4 \\
    1 & 2 \\
  \end{bmatrix}
  \begin{bmatrix}
    0 & 1 \\
    1 & 1 \\
  \end{bmatrix}
  \bigg )^T
  =
  \begin{bmatrix}
    4 & 2 \\
    6 & 3 \\
  \end{bmatrix}
\end{equation}</li>
<li><p>
两个矩阵分别求转置,然后再相乘,结果和第一条并不一样
</p>
\begin{equation}
\begin{bmatrix}
  2 & 4 \\
  1 & 2 \\
\end{bmatrix}^T
\begin{bmatrix}
  0 & 1 \\
  1 & 1 \\
\end{bmatrix}^T
=
\begin{bmatrix}
  1 & 3 \\
  2 & 6 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
两个矩阵分别求转置,然后倒过来相乘,结果和第一条一样
</p>
\begin{equation}
\begin{bmatrix}
  0 & 1 \\
  1 & 1 \\
\end{bmatrix}^T
\begin{bmatrix}
  2 & 4 \\
  1 & 2 \\
\end{bmatrix}^T
=
\begin{bmatrix}
  4 & 2 \\
  6 & 3 \\
\end{bmatrix}
\end{equation}</li>
</ol></li>
<li><p>
公式如下
</p>
\begin{equation}
(\mathbf{A} \cdot\cdot\cdot \mathbf{B})^T = \mathbf{B}^T \cdot\cdot\cdot \mathbf{A}^T \tag{6.7}
\end{equation}</li>
<li><p>
我们看一下超过两个的矩阵是如何应用这个定理的
</p>
\begin{equation}
(\mathbf{ABCD})^T=\mathbf{D}^T \mathbf{C}^T \mathbf{B}^T \mathbf{A}^T
\end{equation}</li>
<li><p>
LIVE EVIL不仅仅能对转置这个操作起作用,还能对其他操作起作用,比如matrix inverse
</p>
\begin{equation}
(\mathbf{ABC})^{-1} = \mathbf{C}^{-1} \mathbf{B}^{-1} \mathbf{A}^{-1}
\end{equation}</li>
<li>对于方阵来说,忽略LIVE EVIL仍然能够得到一个结果(虽然不正确,上面的例子已经列出了),对于普通矩阵来
说,如果忽略LIVE EVIL可能根本无法进行乘法操作:
<ul class="org-ul">
<li><p>
图6-6
</p>

<div id="org9669afb" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/6-6.png" alt="6-6.png" />
</p>
<p><span class="figure-number">Figure 19: </span>tic/6-6.png</p>
</div></li>
<li>我们可以看到,如果不进行交换,上面的乘法无法进行(NxM法和KxN进行乘法)</li>
<li>交换后就可以进行乘法了, KxN和NxM就可以进行乘法了</li>
</ul></li>
<li>需要注意的是,虽然transpose是对所有矩阵都能进行操作的,但是有些其他的operation就不一定了,比如matrix
inverse就不是对所有的矩阵有意义,比如:
<ul class="org-ul">
<li>\((\mathbf{X}^T \mathbf{X})^{-1}\) 是valid的</li>
<li>\(\mathbf{X}^{-1} \mathbf{X}^T\) 就是undefined的了</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org5996f7d" class="outline-3">
<h3 id="org5996f7d"><span class="section-number-3">6.5.</span> Matrix-vector multiplication</h3>
<div class="outline-text-3" id="text-6-5">
<ul class="org-ul">
<li>matric-vector multiplication和普通的matrix multiplication一样,只不过把vector看成:
<ul class="org-ul">
<li>或者M x 1 矩阵 (乘法的时候,在普通矩阵右边)</li>
<li>或者1 x N 矩阵 (乘法的时候,在普通矩阵左边)</li>
</ul></li>
<li>matrix-vector multiplication的重要特性是,其结果是一个vector.这是一个重要的特征,因为它提供了如下
两个概念的connection:
<ul class="org-ul">
<li>linear transformation</li>
<li>matrix</li>
</ul></li>
<li>假设你希望对vector提供一个改变,你们你需要把这个改变翻译成matrix,然后你把matrix乘以vector,就等于
把改动赋予了vector(当然,为了让vector保持其特性,matrix要么在乘法左边,要么在乘法右边):
<ul class="org-ul">
<li><p>
对于M x 1的矩阵,transform matrix要放到左边,比如:
</p>
\begin{equation}
  \mathbf{Ab}
  =
  \begin{bmatrix}
    4 & 2 \\
    1 & 3 \\
  \end{bmatrix}
  \begin{bmatrix}
    5 \\
    2 \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    4 \times 5 + 2 \times 2 \\
    1 \times 5 + 3 \times 2 \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    24 \\
    11 \\
  \end{bmatrix}
\end{equation}</li>
<li><p>
对于1 x N的矩阵,transform matrix要放到右边,比如:
</p>
\begin{equation}
  \mathbf{b}^T \mathbf{A}
  =
  \begin{bmatrix}
  5 & 2 \\
  \end{bmatrix}
  \begin{bmatrix}
    4 & 2 \\
    1 & 3 \\
  \end{bmatrix}
  =
  \begin{bmatrix}
  4 \times 5 + 1 \times 2 && 5 \times 2 + 2 \times 3 \\
  \end{bmatrix}
  =
  \begin{bmatrix}
  24 & 16 \\
  \end{bmatrix}
\end{equation}</li>
<li>对于上面两个公式,我们有如下结论:
<ol class="org-ol">
<li>假设 \(\mathbf{B}\) 是column vector(默认情况下,就是columen vector) \(\mathbf{bA}\) 是not defined的</li>
<li>上面成立的前提是 \(\mathbf{A}\) 是一个方阵,如果 \(\mathbf{A}\) 换成是普通矩阵的话,那么 \(\mathbf{b}^T \mathbf{A}\)
和 \(\mathbf{Ab}\) 都是undefined的</li>
<li>\(\mathbf{Ab} \neq \mathbf{b}^T \mathbf{A}\)</li>
</ol></li>
</ul></li>
<li>上面的第三个结论有个特例,那就是:如果transform matrix是symmetric的,那无论matrix是在左边还是在右边,
得到的结果是`一样`的,当然这个`一样`加引号是因为数据一样,但是方向不一样:
<ul class="org-ul">
<li><p>
一个是column vector,比如
</p>
\begin{equation}
  \mathbf{Ab}
  =
  \begin{bmatrix}
    a & b \\
    b & c \\
  \end{bmatrix}
  \begin{bmatrix}
    d \\
    e \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    ad + be \\
    bd + ce \\
  \end{bmatrix}
\end{equation}</li>
<li><p>
一个是row vector,比如
</p>
\begin{equation}
  \mathbf{b}^T \mathbf{A}
  =
  \begin{bmatrix}
  d & e \\
  \end{bmatrix}
  \begin{bmatrix}
    a & b \\
    b & c \\
  \end{bmatrix}
  =
  \begin{bmatrix}
  ad + be & bd + ce \\
  \end{bmatrix}
\end{equation}</li>
</ul></li>
<li><p>
我们可以使用如下的公式来概括我们刚才说的
</p>
\begin{equation}
if \; \mathbf{A} = \mathbf{A}^T \; then \; \mathbf{Ab} = (\mathbf{b}^T \mathbf{A})^T \tag{6.8}
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-org26dd487" class="outline-3">
<h3 id="org26dd487"><span class="section-number-3">6.6.</span> Creating symmetric matrices</h3>
<div class="outline-text-3" id="text-6-6">
<ul class="org-ul">
<li>由于对称矩阵非常的有作用,那么问题来了,如何让非对称矩阵转换为对称矩阵?</li>
<li>总的来说,有两种方法将非对称矩阵转换为对称矩阵:
<ul class="org-ul">
<li>additive</li>
<li>multiplicative</li>
</ul></li>
<li><p>
additive的方法不是很常用,因为这个方法只对方阵起作用,公式如下
</p>
\begin{equation}
\mathbf{C} = \cfrac{1}{2} (\mathbf{A}^T + \mathbf{A}) \tag{6.9}
\end{equation}</li>
<li><p>
一个例子如下:
</p>
\begin{equation}
\begin{bmatrix}
  a & b & c \\
  d & e & f \\
  h & i & j \\
\end{bmatrix}
+
\begin{bmatrix}
  a & d & h \\
  b & e & i \\
  c & f & j \\
\end{bmatrix}
=
\begin{bmatrix}
  a + a & b + d & c + h \\
  b + d & e + e & f + i \\
  c + h & f + i & j + j \\
\end{bmatrix}
\tag{6.10}
\end{equation}</li>
<li>上面的例子只能作为一个示例,而不能作为证明,为什么呢?因为在矩阵这种类型的数据里面,2*2和3*3都有很大
不同.不能以一个例子来总结全部情况,所以我们要使用不包含矩阵size的公式证明:
<ol class="org-ol">
<li><p>
假设 \(\mathbf{C}\) 是一个方阵(我们最后要证明它是对称的)可以写成一个普通方阵和他的转置的和的形式
</p>
\begin{equation}
\mathbf{C} = \mathbf{A}^T + \mathbf{A} \tag{6.11}
\end{equation}</li>
<li><p>
上述公式两边都做转置处理,得到
</p>
\begin{equation}
\mathbf{C} = (\mathbf{A}^T + \mathbf{A})^T \tag{6.12}
\end{equation}</li>
<li><p>
把转置对右半部分进行代入,得到
</p>
\begin{equation}
\mathbf{C}^T = \mathbf{A}^{TT} + \mathbf{A}^T \tag{6.13}
\end{equation}</li>
<li><p>
于是得到
</p>
\begin{equation}
\mathbf{C}^T = \mathbf{A} + \mathbf{A}^T \tag{6.14}
\end{equation}</li>
<li>由于 矩阵加法支持交换律,所以 \(\mathbf{A} + \mathbf{A}^T = \mathbf{A}^T + \mathbf{A}\), 所以得
到 \(\mathbf{C} = \mathbf{C}^T\), 从而证明 \(\mathbf{C}\) 就是一个对称矩阵.</li>
</ol></li>
<li><b>The multiplicative method</b>, 使用乘法来生成对称矩阵使用范围就广多了(不再需要是方阵).所谓乘法方法
其实就是使用矩阵的转置乘以本矩阵: \(\mathbf{A}^T \mathbf{A}\)</li>
<li>我们前面声明过 \(\mathbf{A}^T \mathbf{A}\) 肯定是对称矩阵,我们现在证明一下:
<ol class="org-ol">
<li>首先证明 \(\mathbf{A}^T \mathbf{A}\) 是一个方阵,这个很简单: 假设 \(\mathbf{A}\) 大小是N*M,那么
\(\mathbf{A}^T \mathbf{A}\) 就是(M*N)*(N*M), 最终大小就是M*M</li>
<li><p>
再证明 \(\mathbf{A}^T \mathbf{A}\) 是对称的,思路如下
</p>
\begin{equation}
(\mathbf{A}^T \mathbf{A})^T = \mathbf{A}^T \mathbf{A}^{TT} = \mathbf{A}^T \mathbf{A} \tag{6.15}
\end{equation}</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org47c6228" class="outline-3">
<h3 id="org47c6228"><span class="section-number-3">6.7.</span> Multiplication of two symmetric matrices</h3>
<div class="outline-text-3" id="text-6-7">
<ul class="org-ul">
<li>如果两个对称矩阵相乘,结果也是对称矩阵么?</li>
<li><p>
答案是否定的,我们随便举一个两个2*2矩阵相乘的例子就是反例
</p>
\begin{equation}
\begin{bmatrix}
  a & b \\
  b & c \\
\end{bmatrix}
\begin{bmatrix}
  d & e \\
  e & f \\
\end{bmatrix}
=
\begin{bmatrix}
  ad + be & ae + bf \\
  bd + ce & be + cf \\
\end{bmatrix}
\tag{6.16}
\end{equation}</li>
<li><p>
所以我们可以总结一句就是两个对称矩阵的乘积,不是对称矩阵
</p>
<pre class="example" id="org937ec8b">
The product of two symmetric matrics is not a symmetric matrix
</pre></li>
<li><p>
严谨起见,我们使用公式(所以就不涉及矩阵的size)来证明一下,如下
</p>
\begin{equation}
(\mathbf{AB})^T = \mathbf{B}^T \mathbf{A}^T = \mathbf{BA} \neq \mathbf{AB} \tag{6.18}
\end{equation}</li>
<li>最后一个不等式成立的原因是由于matrix multiplication是不支持交换律的</li>
<li>两个对称矩阵乘积不对称, 这个结论看起来好像没什么用,其实不然:
<ul class="org-ul">
<li>它导致了principal component analysis(主成分分析)的最大缺陷</li>
<li>它也带来了generalized eigendecomposition(概括特征分解,机器学习中的重要部分)的最大优势</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org4f52c05" class="outline-3">
<h3 id="org4f52c05"><span class="section-number-3">6.8.</span> Element-wise(Hadamard) multiplication</h3>
<div class="outline-text-3" id="text-6-8">
<ul class="org-ul">
<li><p>
Hadamard multiplication是普通人对于矩阵相乘最朴素的理解,所谓hadamard multiplication,就是把两个
矩阵的对应元素一一相乘. 我们使用 \(\odot\) 来代表hadamard multiplication
</p>
\begin{equation}
\mathbf{C} = \mathbf{A} \odot \mathbf{B} \tag{6.19}
\end{equation}</li>
<li><p>
更容易为人所理解的公式如下
</p>
\begin{equation}
c_{i,j} = a_{i,j} \times b_{i,j} \tag{6.20}
\end{equation}</li>
<li><p>
<b>Code</b> 在python里面A*B就是Hadamard multiplication(matrix multiplicaiton使用的是A@B)
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np
<span style="color: #268bd2;">M1</span> = np.random.randn<span style="color: #268bd2;">(</span>4, 3<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">M2</span> = np.random.randn<span style="color: #268bd2;">(</span>4, 3<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[M1] ==&gt;"""</span>, M1<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[M2] ==&gt;"""</span>, M2<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">C</span> = M1 * M2
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[C] ==&gt;"""</span>, C<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;====================OUTPUT====================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[M1] ==&gt; [[ 0.55679318 -0.72163745  0.06036395]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-1.344214   -0.87930469 -0.77704764]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.39921718 -0.40399721  1.06999532]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 2.87783853  1.16609078  0.37574351]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[M2] ==&gt; [[ 1.22878486 -1.08744391 -0.79295114]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.19754341 -2.54697231 -0.67714881]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.67836237 -0.39632046 -1.09006107]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 1.05393543  0.18432773 -0.12275139]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[C] ==&gt; [[ 0.68417903  0.78474025 -0.04786566]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.26554062  2.2395647   0.52617688]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.27081391  0.16011236 -1.16636023]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 3.03305599  0.21494286 -0.04612304]]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-orgf20128d" class="outline-3">
<h3 id="orgf20128d"><span class="section-number-3">6.9.</span> Frobenius dot product</h3>
<div class="outline-text-3" id="text-6-9">
<ul class="org-ul">
<li>Frobenius dot product 也叫Frobenius inner product是一种给定两个相同size(M*N)的矩阵,返回一个标量
(scalar)的运算</li>
<li>为了得到Frobenius dot product,你首先要向量化(vectorize)两个矩阵,然后这两个矩阵向量化之后得到的
两个vector的dot product,就是Frobenius dot product</li>
<li><p>
所以重点就是向量化,所谓向量化,就是把一个矩阵的单独一列列的垒起来,最后得到一个column vector,一个
例子如下
</p>
\begin{equation}
  vec (
  \begin{bmatrix}
    a & c & e \\
    b & d & f \\
  \end{bmatrix}
  )
  =
  \begin{bmatrix}
    a \\
    b \\
    c \\
    d \\
    e \\
    f \\
  \end{bmatrix}
\end{equation}</li>
<li><p>
<b>Code</b> 默认情况下Python使用的是row-based的向量化,也可以通过传递参数,来使用Fortran convention
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">A</span> = np.array<span style="color: #268bd2;">(</span><span style="color: #d33682;">[</span><span style="color: #859900;">[</span>1, 2, 3<span style="color: #859900;">]</span>, <span style="color: #859900;">[</span>4, 5, 6<span style="color: #859900;">]</span><span style="color: #d33682;">]</span><span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[A] ==&gt;"""</span>, A<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">B</span> = A.flatten<span style="color: #268bd2;">(</span>order=<span style="color: #2aa198;">"F"</span><span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[B] ==&gt;"""</span>, B<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;====================OUTPUT====================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[A] ==&gt; [[1 2 3]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[4 5 6]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[B] ==&gt; [1 4 2 5 3 6]</span>
</pre>
</div></li>
<li>我们也给Frobenius dot product了一个特殊的符号(角括号加F)来标记 \(\left\langle\mathbf{A,B}\right\rangle_F\)</li>
<li><p>
下面是一个计算Frobenius dot product的例子:
</p>
\begin{equation}
  \left\langle \mathbf{A,B} \right\rangle_F
  =
  \Bigg<
    \begin{bmatrix}
      1 & 5 & 0 \\
      -4 & 0 & 2 \\
    \end{bmatrix}
    ,
    \begin{bmatrix}
      4 & -1 & 3 \\
      2 & 6 & 7 \\
    \end{bmatrix}
  \Bigg> _F
  = 5
\end{equation}</li>
<li><p>
一个不寻常,但是有用的计算两个matrix \(\mathbf{A}\) 和 \(\mathbf{B}\) Frobenius dot product(参与计算
Frobenius dot product的两个矩阵size一致)的方法,是计算 $\mathbf{A}^T \mathbf{B}$得到一个方阵,计算
这个方阵的trace就可以得到Frobenius值
</p>
<pre class="example" id="orgc7628f1">
trace是把一个方阵(注意只能是方阵)的对角线数据加起来求和
</pre></li>
<li><p>
整个过程可以用如下公式表达
</p>
\begin{equation}
\left \langle \mathbf{A,B} \right \rangle _F = tr(\mathbf{A^TB}) \tag{6.23}
\end{equation}</li>
<li>Frobenius dot product在信号处理和机器学习领域有很多用处,比如可以用来粗略的计算两个矩阵之间的"distance"</li>
<li>如果参与Frobenius dot product的两个矩阵都是自己,那么其Frobenius dot product值是所有squared element
的和,也有另外两个名字:
<ul class="org-ul">
<li>squared Frobenius norm</li>
<li>squared Euclidean norm</li>
</ul></li>
<li><p>
<b>Code</b> 使用trace的方法求Frobenius dot product的代码如下
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">A</span> = np.random.randn<span style="color: #268bd2;">(</span>4, 3<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">B</span> = np.random.randn<span style="color: #268bd2;">(</span>4, 3<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">f</span> = np.trace<span style="color: #268bd2;">(</span>A.T @ B<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[f] ==&gt;"""</span>, f<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;====================OUTPUT====================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[f] ==&gt; -1.7945940308866906</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org8f52af7" class="outline-3">
<h3 id="org8f52af7"><span class="section-number-3">6.10.</span> Matrix norms</h3>
<div class="outline-text-3" id="text-6-10">
<ul class="org-ul">
<li>在3.9我们了解到,一个vector自己和自己的dot product,开方之后就是vector的magnitude
或者说是vector的长度,也叫做vector的norm</li>
<li>对于matrix来说,要求其norm,就非常的复杂了,复杂的原因,在于对于normal的定义就有很多种</li>
<li>即便是有很多中,但是其核心的概念是一样的,所有的norm定义:
<ul class="org-ul">
<li>都是一个数字</li>
<li>这个数字用来从各个维度来解释matrix的magnitude</li>
</ul></li>
<li>本章会介绍部分matrix norm,剩下的会在16章继续讨论(引入了单值特征分解之后)</li>
<li>上面讲过,如果matrix自己和自己进行Frobenius dot product运算,那么得到的是squared Frobenius norm,
squared Frobenius norm开根号之后,就是Frobenius norm.</li>
<li>之前介绍过如何计算Frobenius dot product:
<ol class="org-ol">
<li>vectorizing之后计算dot product,然后开根号</li>
<li>计算 \(tr(\mathbf{A}^T\mathbf{A})\), 然后开根号</li>
<li><p>
现在还有第三种计算方法,当然这里计算是从定义角度出发的,公式如下
</p>
\begin{equation}
\| A \|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n(a_{ij})^2} \tag{6.24}
\end{equation}</li>
</ol></li>
<li>如果把matrix space看成是欧几里何空间,那么两个matrix的Frobenius norm的差,其实是两个matrix的欧几
里何距离:
<ul class="org-ul">
<li><p>
公式如下
</p>
\begin{equation}
\| \mathbf{A} - \mathbf{B} \|_F = \sqrt{\sum_{i,j}(a_{i,j} - b_{i,j})^2} \tag{6.25}
\end{equation}</li>
<li>注意,上面的两个矩阵必须是相同的size,否则没有意义.</li>
</ul></li>
<li><p>
Frobenius norm 叫做 \(\ell2\) norm, \(\ell1\) norm的定义则是:每一列都绝对值相加,值最大的一列就是
\(\ell1\) norm
</p>
<pre class="example" id="org253563d">
Dum the absolute values of all individual elements in each column,
then take the largest maximum column sum.
</pre></li>
<li><b>Cauchy-Schwarz inquality</b> 我们在第三章学了Cauchy-Schwarz不等式,说的是:
<ul class="org-ul">
<li><p>
中文
</p>
<pre class="example" id="orgd550028">
两个vector的dot product的magnitude不会比这两个vector magitude的product值大
</pre></li>
<li><p>
英文-这里的norm其实就是magnitude
</p>
<pre class="example" id="orgcf51d67">
The magnitude of the dot produt between two vectors is no larger than the product
of the norms of the two vectors
</pre></li>
</ul></li>
<li>本章我们有一个类似的Cauchy-Schwarz inequality,这里参与相乘的是一个matrix和一个vector,也就是
matrix-vector multiplication:
<ul class="org-ul">
<li><p>
公式如下
</p>
\begin{equation}
\| \mathbf{Av} \| \leq \| \mathbf{A} \| _F  \| \mathbf{v} \| \tag{6.27}
\end{equation}</li>
</ul></li>
<li><p>
<b>Code</b>, 下面是获取fHfrobenius norm的代码
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">A</span> = np.random.randn<span style="color: #268bd2;">(</span>4, 5<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[A] ==&gt;"""</span>, A<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">ret</span> = np.linalg.norm<span style="color: #268bd2;">(</span>A, <span style="color: #2aa198;">"fro"</span><span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[ret] ==&gt;"""</span>, ret<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;====================OUTPUT====================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[A] ==&gt; [[ 0.93099611 -0.29883209 -0.77704968 -1.40360709 -1.26983558]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.01110609 -0.12571539  0.54530583 -0.41002687  0.18841694]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.63899639  0.07784653  0.80574053  1.68523204 -0.41361239]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[-0.63634513  0.13304891 -0.42435354  0.89428213 -0.17268711]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[ret] ==&gt; 3.341603795882524</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org72e37e0" class="outline-3">
<h3 id="org72e37e0"><span class="section-number-3">6.11.</span> What about matrix division?</h3>
<div class="outline-text-3" id="text-6-11">
<ul class="org-ul">
<li>前面都在讨论乘法,那么matrix的除法是什么样的呢?</li>
<li><p>
当你听到矩阵除法,第一个想到的公式可能是如下:
</p>
\begin{equation}
\cfrac{ \mathbf{A} }{ \mathbf{B} }
\end{equation}</li>
<li>注意这种公式是不存在的: 不能用一个matrix去除以另外的matrix</li>
<li>在矩阵里面,除法可以使用如下公式表示:
<ul class="org-ul">
<li><p>
公式
</p>
\begin{equation}
\mathbf{A} \mathbf{B}^{-1}
\end{equation}</li>
<li>其中的 \(\mathbf{B}^{-1}\) 叫做matrix inverse, 我们会在第12章使用一整章来讲解,现在我们只需要了
解关于matrix inverse的如下要点:
<ol class="org-ol">
<li>所谓matrix inverse其实是满足如下公式的矩阵: \(\mathbf{A} \mathbf{A}^{-1} = \mathbf{I}\)</li>
<li>不是所有的矩阵都有inverse, 一个矩阵只有是方阵且full-rank的情况下,才能拥有full inverse</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org711736b" class="outline-2">
<h2 id="org711736b"><span class="section-number-2">7.</span> Rank</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-org7d641b1" class="outline-3">
<h3 id="org7d641b1"><span class="section-number-3">7.1.</span> Six things about matrix rank</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>matrix的秩(rank)是一个和matrix相关的,单独的数字.这个数字非常关键,因为几乎所有的线性代数的应用都
和秩相关</li>
<li>在了解如何计算rank之前,甚至在正式定义rank之前,我们在这里展示rank的最重要的六个fact:
<ol class="org-ol">
<li>matrix 的rank使用小写字母 \(r\) 或在 \(rank(\mathbf{A})\) 来表示. 并且rank是一个非负整数(其实只
有zero matrix的rank是零,其他matrix的rank都大于0)</li>
<li><p>
对于一个M*N的matrix来说,其rank的最大值,就是M或N中的较小值,用公式表示如下
</p>
\begin{equation}
r \in \mathbb{N}, s.t. \; 0 \leq r \leq min\{M,N\} \tag{7.1}
\end{equation}</li>
<li>rank是整个matrix的属性,所以讨论matrix的column的rank是没有意义的(同样,讨论matrix的row的rank也
是没有意义的),后面引入matrix space之后, null space of matrix的秩,也是没有意义的</li>
<li>下图为我们展示了三种full-rank: full-rank, full-column-rank, full-row-rank:
<ul class="org-ul">
<li><p>
如图
</p>

<div id="org101cc48" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/7-1.png" alt="7-1.png" />
</p>
<p><span class="figure-number">Figure 20: </span>tic/7-1.png</p>
</div></li>
<li>如果matrix rank没有达到min(M,N),那么我们可以给这个rank取很多名字(这些名字很重要):
<ul class="org-ul">
<li>reduced rank</li>
<li>rank-deficient</li>
<li>degenerate</li>
<li>low-rank</li>
<li>singular</li>
<li>dummy rank (开玩笑的,但很形象)</li>
<li>loser rank (开玩笑的,但很形象)</li>
</ul></li>
</ul></li>
<li><p>
rank揭示了matrix的dimension of information的数目. 这个概念很抽象,比如下面的matrix有两行三列
但是其rank值为1(因为其column是linearly dependent的)
</p>
\begin{equation}
\begin{bmatrix}
  1 & 3 & 4 \\
  3 & 9 & 12 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
本书会了解很多rank的定义,并且介绍很多计算rank的方法.但是重点是要记住rank的核心本质: rank是一
个matrix的"能够组成linearly independent set"的column(或者row)的数目
</p>
<pre class="example" id="orgab9690c">
Rank of a matrix is the largest number of columns that can form a linearly independent set.
This is exactly the same as the largest number of rows that can form a linearly independent set.
</pre></li>
</ol></li>
<li>为什么我们这么关注rank? 为什么full-rank matrix如此重要?
<ul class="org-ul">
<li>因为线性代数里面有一些操作(比如matrix inverse)只对full-rank matrix有效</li>
<li>另外一些操作(比如,eigendecomposition,特征分解)可以在reduced-rank matrix上进行,但是在full-rank
matrix上面运行会有额外的特性</li>
<li>除此以外,很多计算机算法在full-rank matrix上面,会给出更加reliable的结果</li>
</ul></li>
<li>在统计学和机器学习领域,正则化(regularization)的最主要目标,就是提高numerical stability,提高的途径
就是确保data matrix是full rank的</li>
</ul>
</div>
</div>
<div id="outline-container-orgb9bfc46" class="outline-3">
<h3 id="orgb9bfc46"><span class="section-number-3">7.2.</span> Interpretations of matrix rank</h3>
<div class="outline-text-3" id="text-7-2">
<ul class="org-ul">
<li><b>Algebraic interpretatin</b> 就像前面一节说的,如果你把matrix理解成一系列的vector,那么matrix的rank
就是"vector可以组成的多个linearly independent set"中,容量最大的set中vector的数目</li>
<li><b>Geometric interpretation</b>, rank是matrix里面的column(row)描绘出来的subspace的维度(注意是
dimensionality of subspace,而不是ambient dimensionality of space containing the matrix)</li>
<li>我们以下面的例子来解释一下:
<ul class="org-ul">
<li><p>
3*1的matrix
</p>
\begin{equation}
\mathbf{v} = \begin{bmatrix}
             4 & 0 & 1 \\
             \end{bmatrix}^T
\end{equation}</li>
<li>这个object生活在 \(\mathbb{R}^3\), 但是它只span了1D subspace(一条直线),所以这个matrix的rank就是1</li>
<li>实际上,所有vector的rank都是1,除了zero vector,它的rank是0</li>
</ul></li>
<li>我们把上面的3*1横过来,变成一个1*3 matrix
<ul class="org-ul">
<li><p>
1*3的matrix
</p>
\begin{equation}
\mathbf{v} = \begin{bmatrix}
             4 & 0 & 1 \\
             \end{bmatrix}
\end{equation}</li>
<li>那么这个matrix就生活在 \(\mathbb{R}^1\) 上,可以看做是real number line上的三个点.这种情况下(三个非
零点)每个点都可以通过乘以scalar来得到另外一个点,所以rank还是1</li>
</ul></li>
<li>上面的matrix其实是vector,我们再来看看真的matrix的例子:一个2*3的
<ul class="org-ul">
<li><p>
2*3的matrix
</p>
\begin{equation}
\begin{bmatrix}
  1 &  1 & -4 \\
  2 & -1 &  2 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
图7-2
</p>

<div id="org8cd1061" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/7-2.png" alt="7-2.png" />
</p>
<p><span class="figure-number">Figure 21: </span>tic/7-2.png</p>
</div></li>
<li>我们可以把这个matrix理解为生活在 \(\mathbb{R}^2\) 上的三个column vector, 这三个vector不共线,会
span一个2D plane,所以rank就是2.如上图左边</li>
<li>我们再把他解释成在 \(\mathbb{R}^3\) 上的两个vector(如上图),他们俩也就能span一个2D plane(虽然是
embed在ambient 3D space)里面,所以rank还是2</li>
</ul></li>
<li>结论就是,一个matrix无论是看成column vector还是row vector的组合,其最终结果(算出来的rank),都是一样的.</li>
</ul>
</div>
</div>
<div id="outline-container-orgee7da04" class="outline-3">
<h3 id="orgee7da04"><span class="section-number-3">7.3.</span> Computing matrix rank</h3>
<div class="outline-text-3" id="text-7-3">
<ul class="org-ul">
<li>在现在这个阶段,计算matrix的rank不是一件容易的事情</li>
<li>真实的情况是,对于计算机来说,矩阵一旦大到一定程度,计算机就不能compute出矩阵的rank了,它开始estimate矩阵的rank</li>
<li>总结起来有三种方法来计算矩阵的rank,这些方法的依赖相同的matrix feature,只是在不同场景下各自有自己的优势:
<ol class="org-ol">
<li>计算最大的column(或者row) set(这个set能够组成最大的linearly independent set), 第四章介绍了这方面内容</li>
<li>计算number of pivot in the echelon 或者 row-reduced echelon form of the matrix, 第十章介绍了这方面内容</li>
<li>计算number of nonzero singular value from a singular value decomposition of the matrix</li>
</ol></li>
<li><p>
<b>Code</b> 除了在线性代数的考试当中,不会有其他场景会手动计算matrix rank,使用python计算rank代码如下
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">A</span> = np.random.randn<span style="color: #268bd2;">(</span>3, 6<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[A] ==&gt;"""</span>, A<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">r</span> = np.linalg.matrix_rank<span style="color: #268bd2;">(</span>A<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[r] ==&gt;"""</span>, r<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;====================OUTPUT====================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[A] ==&gt; [[-2.51317436 -1.30181078  2.00031178  1.85125585 -0.4851883   0.13357728]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 0.16903736 -0.99143404 -0.03890734  0.37457375 -0.75098614 -0.27840034]</span>
<span style="color: #96A7A9; font-style: italic;">#  </span><span style="color: #96A7A9; font-style: italic;">[ 1.08877788  1.11233659 -0.39927652  0.91024227 -0.82840601  0.92600432]]</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[r] ==&gt; 3</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org33a65be" class="outline-3">
<h3 id="org33a65be"><span class="section-number-3">7.4.</span> Rank and scalar multiplication</h3>
<div class="outline-text-3" id="text-7-4">
<ul class="org-ul">
<li><p>
我言简意赅的总结一下: Scalar multiplication不会对matrix的rank值造成任何影响
</p>
<pre class="example" id="org9527236">
Scalar multiplication has no effect on the rank of a matrix
</pre></li>
<li>原因很简单,因为scalar只是拉伸了information,它不会去做如下操作,也不会产生新的information:
<ul class="org-ul">
<li>transform</li>
<li>rotate</li>
<li>mix</li>
<li>unmix</li>
<li>change</li>
<li>combine</li>
</ul></li>
<li><b>Algebraically</b>, scaling 不会对linear independence造成任何硬性,因为所有的vector都乘以了同样的
scalar(当然,0是个例外)</li>
<li><p>
我们使用如下来表达scalar不会对rank的计算产生影响(0除外)
</p>
\begin{equation}
rank(\alpha\mathbf{A}) = rank(\mathbf{A}), \alpha \ne 0 \tag{7.2}
\end{equation}</li>
<li><p>
<b>Code</b>, 我们使用如下例子来证明,scaling不会影响rank的值
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #859900; font-weight: bold;">import</span> numpy <span style="color: #859900; font-weight: bold;">as</span> np

<span style="color: #268bd2;">s</span> = np.random.randn<span style="color: #268bd2;">()</span>
<span style="color: #268bd2;">M</span> = np.random.randn<span style="color: #268bd2;">(</span>3, 5<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">r1</span> = np.linalg.matrix_rank<span style="color: #268bd2;">(</span>M<span style="color: #268bd2;">)</span>
<span style="color: #268bd2;">r2</span> = np.linalg.matrix_rank<span style="color: #268bd2;">(</span>s * M<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[r1] ==&gt;"""</span>, r1<span style="color: #268bd2;">)</span>
<span style="color: #d33682; font-style: italic;">print</span><span style="color: #268bd2;">(</span><span style="color: #2aa198;">"""[r2] ==&gt;"""</span>, r2<span style="color: #268bd2;">)</span>

<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">&lt;====================OUTPUT====================&gt;</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[r1] ==&gt; 3</span>
<span style="color: #96A7A9; font-style: italic;"># </span><span style="color: #96A7A9; font-style: italic;">[r2] ==&gt; 3</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-orge7cca5e" class="outline-3">
<h3 id="orge7cca5e"><span class="section-number-3">7.5.</span> Rank of added metrics</h3>
<div class="outline-text-3" id="text-7-5">
<ul class="org-ul">
<li>如果你知道两个matrix A和B的rank,那么你能否知道matrix A+B的rank呢?
<ul class="org-ul">
<li>答案是否定的</li>
</ul></li>
<li><p>
但是,知道两个matrix(A和B)单独的rank,确实可以知道matrix(A+B) randk的上限,公式如下
</p>
\begin{equation}
rank(\mathbf{A} + \mathbf{B}) \leq rank( \mathbf{A}) + rank( \mathbf{B}) \tag{7.3}
\end{equation}</li>
<li><b>Multiple constraints</b>, matrix有很多的限制,所以各种运算放在matrix上面也不可能破解这些限制:
<ul class="org-ul">
<li>比如, "Thing 2"告诉我们, matrix rank不可能高于min(M, N)</li>
<li>那么两个3*4的matrix,每个都是rank 3,那么加起来也不可能得到rank6, 因为两个3*4的matrix加起来还是
3*4的matrix,新matrix的rank不可能超过3</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgc00c3be" class="outline-3">
<h3 id="orgc00c3be"><span class="section-number-3">7.6.</span> Rank of multiplied matrices</h3>
<div class="outline-text-3" id="text-7-6">
<ul class="org-ul">
<li><p>
和matrix加法一样,知道两个matrix \(\mathbf{A}\) 和 \mathbf{B} 的rank,无法得知 matrix( \(\mathbf{C} = \mathbf{A} \mathbf{B}\))
的rank,但是可以知道新matrix rank的上限
</p>
\begin{equation}
rank(\mathbf{AB}) \leq min \{rank(\mathbf{A}), rank(\mathbf{B})\} \tag{7.8}
\end{equation}</li>
<li>我们来理解一下这个公式:
<ul class="org-ul">
<li>假设有如下公式成立: \(\mathbf{C} = \mathbf{AB}\)</li>
<li>column space的概念是指(具体下一章会讲):一个matrix的column所span出来的subspace</li>
<li><p>
对于 \(\mathbf{C}\) 的jth column来说,其都是通过如下公式得到的
</p>
\begin{equation}
\mathbf{A} \mathbf{b}_j = \mathbf{c}_j \tag{7.13}
\end{equation}</li>
<li>\(\mathbf{C}\) 的每一列都是由一个linear combination得来的,这个linear combination是由 \(\mathbf{A}\)
的所有column 乘以对应的weight(由 $\mathbf{B}$来提供)</li>
<li>换句话说,$\mathbf{C}$的每个column都在由 \(\mathbf{A}\) span出来的subspace里面</li>
<li>又由于rank的定义式dimensionality of the column space of a matrix, 所以dimensionality of \(\mathbf{C}\)
不可能大于 \(\mathbf{A}\)</li>
<li>同样可以做出判断: dimensionality of \(\mathbf{C}\) 不可能大于 \(\mathbf{B}\)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org1332d98" class="outline-3">
<h3 id="org1332d98"><span class="section-number-3">7.7.</span> Rank of A, AT, ATA, and AAT</h3>
<div class="outline-text-3" id="text-7-7">
<ul class="org-ul">
<li>先来一个提纲挈领的总结,如下几种矩阵的形式,所拥有的rank值一样:
<ul class="org-ul">
<li>\(\mathbf{A}\)</li>
<li>\(\mathbf{A}^T\)</li>
<li>\(\mathbf{A}^T\mathbf{A}\)</li>
<li>\(\mathbf{A}\mathbf{A}^T\)</li>
</ul></li>
<li>\(\mathbf{A}\) 和 $\mathbf{A}^T$拥有一样的rank比较好理解: rank是matric的属性,且不区分column,row,
所以转置一个矩阵不会影响rank值</li>
<li>TODO</li>
</ul>
</div>
</div>
<div id="outline-container-org01fd343" class="outline-3">
<h3 id="org01fd343"><span class="section-number-3">7.8.</span> Rank of random matrices</h3>
<div class="outline-text-3" id="text-7-8">
<ul class="org-ul">
<li>所谓random matrix, 是指matrix的element是随机得来的,随机的distribution可以是多种,比如:
<ul class="org-ul">
<li>normal(高斯)</li>
<li>uniform</li>
<li>Poisson</li>
</ul></li>
<li>random matrix有很多有趣的特性,而且很多定理都是基于random matrix的</li>
<li>random matrix最出名的一个特性是: 它们大多数都是full rank的.原因也很简单,因为你使用随机数生成的
column不大可能是linear dependency的</li>
<li>当然了,这个特性是说绝大部分情况下是full rank的,小部分情况是什么呢?
<ul class="org-ul">
<li>小部分情况就是随机的数字是固定的range选出来的</li>
<li><p>
比如,如果限定了数字从{1,2}中选的话,即便生成如下的matrix,它的rank还是只是2
</p>
\begin{equation}
\begin{bmatrix}
  0 & 0 & 0 & 0 \\
  0 & 0 & 0 & 0 \\
  1 & 0 & 1 & 1 \\
  1 & 1 & 0 & 0 \\
\end{bmatrix}
\end{equation}</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org42c5035" class="outline-3">
<h3 id="org42c5035"><span class="section-number-3">7.9.</span> Full-rank by "shifting"</h3>
<div class="outline-text-3" id="text-7-9">
<ul class="org-ul">
<li>full-rank的方阵是非常好的合作伙伴,但是实践当中绝大部分矩阵都是rank-deficient的</li>
<li>一个常见的做法,是把rank-deficient的矩转换成full-rank matrix, 通过shifting(5.8节学习过):
<ul class="org-ul">
<li><p>
所谓shifting, 是指add一个identity matrix的倍数,从而增加一点quantity到matrix里面,但是只动对角
线,对其他数据不动
</p>
\begin{equation}
\mathbf{A} + \lambda \mathbf{I} = \tilde{\mathbf{A}}
\end{equation}</li>
<li>注意这里的 \(\lambda\) 要尽可能的小,否则会增加太多的information到matrix里面:
<ol class="org-ol">
<li><p>
一个极端反例如下,如果我们的 \(\lambda\) 为1的话,那么虽然获得了rank为3的新matrix,但是相比zero
matrix,实在是引入了太多的information
</p>
\begin{equation}
\begin{bmatrix}
  0 & 0 & 0 \\
  0 & 0 & 0 \\
  0 & 0 & 0 \\
\end{bmatrix}
+
  \lambda
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 \\
\end{bmatrix}
  =
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 \\
\end{bmatrix} \tag{7.24}
\end{equation}</li>
<li><p>
一个正向的例子如下,我们的 \(\lambda\) 为0.01,即实现了转化成full-rank matrix,又只添加了非常小的information
</p>
\begin{equation}
\begin{bmatrix}
   1 & 3 & -19 \\
   5 & -7 & 59 \\
   -5 & 2 & -24 \\
 \end{bmatrix}
+
  .01
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 \\
\end{bmatrix}
  =
\begin{bmatrix}
  1.01 & 3 & -19 \\
  5 & -6.99 & 59 \\
  -5 & 2 & -23.99 \\
\end{bmatrix} \tag{7.25}
\end{equation}</li>
</ol></li>
</ul></li>
<li>最后我们再举一个非zero矩阵的反例:
<ul class="org-ul">
<li><p>
\(\lambda\) 为1000
</p>
\begin{equation}
\begin{bmatrix}
   1 & 3 & -19 \\
   5 & -7 & 59 \\
   -5 & 2 & -24 \\
 \end{bmatrix}
+
10^3
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 \\
\end{bmatrix}
  =
\begin{bmatrix}
  101 & 3 & -19 \\
  5 & 993 & 59 \\
  -5 & 2 & 976 \\
\end{bmatrix} \tag{7.26}
\end{equation}</li>
<li>这个例子能够说明一个问题,关于matrix "shifting"的:
<ol class="org-ol">
<li>当 \(\lambda\) 趋近于0的时候, \(\tilde{\mathbf{A}}\) 趋近于 \(\mathbf{A}\)</li>
<li>当 \(\lambda\) 不断增大,增大到比 \(\mathbf{A}\) 大很多的时候, \(\tilde{\mathbf{A}}\) 就趋近于 \(\lambda \mathbf{I}\)</li>
</ol></li>
</ul></li>
<li>在统计学和机器学习领域, "shifting"也叫做:
<ul class="org-ul">
<li>正则化(regularization)</li>
<li>矩阵平滑(matrix smoothing)</li>
</ul></li>
<li>shifting也是如下工程的重要步骤:
<ul class="org-ul">
<li>principal components analysis</li>
<li>generalized eigendecomposition</li>
</ul></li>
<li>当前data science行业的一大重要工作,就是给出一个好的 \(\lambda\) 值</li>
</ul>
</div>
</div>
<div id="outline-container-org2c3ab1f" class="outline-3">
<h3 id="org2c3ab1f"><span class="section-number-3">7.10.</span> Difficulties in computing rank in practice</h3>
<div class="outline-text-3" id="text-7-10">
<ul class="org-ul">
<li>本节主要介绍为什么计算matrix的rank很难</li>
<li>首先一点,就是计算机很难去判断那些特别小的数字,比如比$10<sup>-15</sup>$还小的数字,计算机就认为是0了</li>
<li><b>Geometry</b> 我们举一个例子来说明下在几何情况下,rank计算为什么难?
<ul class="org-ul">
<li>假设你有一个3*3的矩阵,数据是从卫星上收集来的</li>
<li>又假设你知道最终的结果,三个vector其实是在一个2D plane上面的,所以,其实这个matrix的rank是2</li>
<li><p>
如图
</p>

<div id="orgf2190fb" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/7-3.png" alt="7-3.png" />
</p>
<p><span class="figure-number">Figure 22: </span>tic/7-3.png</p>
</div></li>
<li>由于卫星的传感器没有那么完美,由于数据的传输会corrupt某些数据,比如原来是3,现在变成了2.999,那么
在计算机看来,肯定这三vector不是完美的在这个2D plane啦,有几个点就不在这个平面上了,那么rank-2就
不被计算机承认了.计算机认为rank是3</li>
<li>所以我们使用者其实希望rank-estimating-algorithm能偶忽略一些noise(根据matrix里面已有的数据)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org7ac0d32" class="outline-3">
<h3 id="org7ac0d32"><span class="section-number-3">7.11.</span> Rank and span</h3>
<div class="outline-text-3" id="text-7-11">
<ul class="org-ul">
<li><p>
在第四章,我们学到了,在线性代数里面,一个重要的问题是: 一个vector是否在另外一堆vector所span出来的
空间里面
</p>
<pre class="example" id="orge6999af">
An important question in linear algebra is whether a vector is in the span
of another set of vectors.
</pre></li>
<li>我们下面来介绍一下augment-rank algorithm,来表述下vector和 matrix rank的关系:
<ol class="org-ol">
<li>从set S里面取得vectors,然后组成 matrix \(\mathbf{S}\)</li>
<li>计算 \(\mathbf{S}\) 的rank值,记为 \(r_1\)</li>
<li>把 \(\mathbf{v}\) 添加到 \(\mathbf{S}\) 里面组成新的矩阵 \(\mathbf{S_v}\)</li>
<li>计算新矩阵 \(\mathbf{S_v}\) 的rank,记为 \(r_2\)</li>
<li>如果 \(r_2 > r_1\), 那么 \(\mathbf{v}\) 不在 S的span里面
如果 \(r_2 = r_1\), 那么 \(\mathbf{v}\) 就在 S的span里面
如果 \(r_2 < r_1\), 那么,你的计算出了问题</li>
</ol></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org5677fe7" class="outline-2">
<h2 id="org5677fe7"><span class="section-number-2">8.</span> Matrix spaces</h2>
<div class="outline-text-2" id="text-8">
<ul class="org-ul">
<li>matrix space 和vector space一样的概念,但是其属性范围比a set of vector要广,它还包括matrix的很多其
他属性</li>
<li>本章会联系如下的概念:
<ul class="org-ul">
<li>vector subspace</li>
<li>basis</li>
<li>linear independence</li>
<li>span</li>
</ul></li>
<li>本章可以总结为试图解释如下两个matrix-vector multiplication问题:
<ul class="org-ul">
<li><p>
是否存在一个matrix A (解释为 weighted combination of columns), 能够和一个vector x相乘得到vector b
</p>
\begin{equation}
\mathbf{Ax} = \mathbf{b} ? \tag{8.1}
\end{equation}</li>
<li><p>
是否存在一个matrix A (解释为 weighted combination of columns), 能够和一个vector y相乘得到vector b
</p>
\begin{equation}
\mathbf{Ay} = \mathbf{0} ? \tag{8.2}
\end{equation}</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org667bb4f" class="outline-3">
<h3 id="org667bb4f"><span class="section-number-3">8.1.</span> Column space of a matrix</h3>
</div>
<div id="outline-container-org5220fa4" class="outline-3">
<h3 id="org5220fa4"><span class="section-number-3">8.2.</span> Column space: A and AAT</h3>
</div>
<div id="outline-container-org91ea50a" class="outline-3">
<h3 id="org91ea50a"><span class="section-number-3">8.3.</span> Determining whether \(v \in C(A)\)</h3>
</div>
<div id="outline-container-org1d65c13" class="outline-3">
<h3 id="org1d65c13"><span class="section-number-3">8.4.</span> Row space of a matrix</h3>
</div>
<div id="outline-container-org0b4db5b" class="outline-3">
<h3 id="org0b4db5b"><span class="section-number-3">8.5.</span> Row spaces of A and ATA</h3>
</div>
<div id="outline-container-org3b8338c" class="outline-3">
<h3 id="org3b8338c"><span class="section-number-3">8.6.</span> Null space of a matrix</h3>
</div>
<div id="outline-container-org55794e8" class="outline-3">
<h3 id="org55794e8"><span class="section-number-3">8.7.</span> Geometry of the null space</h3>
</div>
<div id="outline-container-orgcf300c2" class="outline-3">
<h3 id="orgcf300c2"><span class="section-number-3">8.8.</span> Orthogonal subspaces</h3>
</div>
<div id="outline-container-orga9e8bdb" class="outline-3">
<h3 id="orga9e8bdb"><span class="section-number-3">8.9.</span> Matrix space orthogonalities</h3>
</div>
<div id="outline-container-org27a1a22" class="outline-3">
<h3 id="org27a1a22"><span class="section-number-3">8.10.</span> Dimensionalitis of matrix spaces</h3>
</div>
<div id="outline-container-org764bdc5" class="outline-3">
<h3 id="org764bdc5"><span class="section-number-3">8.11.</span> More on Ax = b and Ay = 0</h3>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: harrifeng@outlook.com</p>
<p class="date">Created: 2023-02-23 Thu 22:13</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
