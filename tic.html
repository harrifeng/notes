<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-07-21 Thu 19:23 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>tic</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="harrifeng@outlook.com" />
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">tic</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org9a44eef">1. Introduction to this book</a>
<ul>
<li><a href="#orga42bd05">1.1. What is linear algebra and why learn it?</a></li>
<li><a href="#org78f96fa">1.2. About this book</a></li>
<li><a href="#org6ba00af">1.3. Prerequisites</a></li>
<li><a href="#org78dbc20">1.4. Practice, exercises and code challenges</a></li>
<li><a href="#orgda82ab5">1.5. Online and other resources</a></li>
</ul>
</li>
<li><a href="#org9908a3c">2. Vectors</a>
<ul>
<li><a href="#org65d05f1">2.1. Scalars</a></li>
<li><a href="#org3eeb666">2.2. Vectors: geometry and algebra</a></li>
<li><a href="#orgad801bf">2.3. Transpose operation</a></li>
<li><a href="#orgd09585a">2.4. Vector addition and subtraction</a></li>
<li><a href="#org7a63bbc">2.5. Vector-scalar multiplication</a></li>
</ul>
</li>
<li><a href="#orgf7a7570">3. Vector multiplications</a>
<ul>
<li><a href="#org0f7d73e">3.1. Vector dot product: Algebra</a></li>
<li><a href="#org45d00b8">3.2. Dot product properties</a>
<ul>
<li><a href="#org91313b3">3.2.1. vector product和scalar的结合律</a></li>
<li><a href="#orgd39a2cb">3.2.2. vector product和vector的结合律</a></li>
<li><a href="#org021be4c">3.2.3. commutative property</a></li>
<li><a href="#org38fa763">3.2.4. Distributive property</a></li>
</ul>
</li>
<li><a href="#org362900b">3.3. Vector dot product: Geometry</a></li>
<li><a href="#org0022b1a">3.4. Algebraic and geometric equivalence</a></li>
<li><a href="#orgb8a5c63">3.5. Linear weighted combination</a></li>
<li><a href="#orgef64cf6">3.6. The outer product</a></li>
<li><a href="#orga32c8c2">3.7. Element-wise (Hadamard) vector product</a></li>
<li><a href="#orgd1f5f02">3.8. Cross product</a></li>
<li><a href="#org662be6e">3.9. Unit vectors</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org9a44eef" class="outline-2">
<h2 id="org9a44eef"><span class="section-number-2">1</span> Introduction to this book</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-orga42bd05" class="outline-3">
<h3 id="orga42bd05"><span class="section-number-3">1.1</span> What is linear algebra and why learn it?</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>线性代数是数学中关于vector和matrix的分支</li>
<li>在现代,线性代数的重要性得到加强,因为很多数据都是以matrix的形势存储的,比如:
<ul class="org-ul">
<li>统计学</li>
<li>机器学习</li>
<li>计算机图形学</li>
<li>压缩算法</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org78f96fa" class="outline-3">
<h3 id="org78f96fa"><span class="section-number-3">1.2</span> About this book</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>本书对机器学习爱好者很有益处</li>
<li>本书仅仅需要高中数学知识</li>
<li>对于希望了解了线性代数之后进行深度学习,统计学的人来说,太过于抽象的线性代数学习比较浪费时间</li>
<li>本书注重实践,而不是理论</li>
<li>本书是一本数学书,所以请不要奇怪书中有公式.但是数学不仅仅是公式:
<ul class="org-ul">
<li>在我看来,数学的目的是理解概念</li>
<li>公式是展示概念的一种方式</li>
<li>但是文章,图片,甚至是代码都非常重要</li>
</ul></li>
<li>公式和其他表现形式有个微妙的平衡:
<ul class="org-ul">
<li>公式提供了正规而严格的表现形式,但是无法提供直觉力</li>
<li>其他表达形式(文章,类比,图表,代码)提供了直觉力,但是不够严格和正规</li>
</ul></li>
<li>本书的公式按照重要性分为三个等级:
<ol class="org-ol">
<li>简单的,或者是为了回忆之前讨论过的公式.那么就是优先度最低的公式,他们会和文本在一块,比如 \(x(yz) = (xy)z\)</li>
<li><p>
更加重要的公式,会有自己单独的行
</p>
\begin{equation}
\sigma = x(yz) = (xy)z\tag{1.1}
\end{equation}</li>
<li>最最重要的公式会有自己的区域来说明
<ul class="org-ul">
<li><p>
公式1.2
</p>
\begin{equation}
\sigma = x(yz) = (xy)z\tag{1.2}
\end{equation}</li>
<li>这个公式的要点1</li>
<li>这个公式的要点2</li>
</ul></li>
</ol></li>
<li>线性代数的很多概念可以使用如下两种数学分支的公式来表示:
<ul class="org-ul">
<li>Geometric: 几何方法,优点是提供图形化的直观展示,缺点是人类只能理解2D和3D的图像</li>
<li>Algebraic: 代数方法,优点是严谨的证明和计算机的介入,可以非常容易的扩展到N维</li>
</ul></li>
<li>注意,并不是所有的线性代数概念都可以使用几何和代数法来展示</li>
</ul>
</div>
</div>
<div id="outline-container-org6ba00af" class="outline-3">
<h3 id="org6ba00af"><span class="section-number-3">1.3</span> Prerequisites</h3>
<div class="outline-text-3" id="text-1-3">
<ul class="org-ul">
<li>需要有学习线性代数的主动性</li>
<li>需要有高中数学基础</li>
<li>不需要有微积分知识</li>
<li>不需要任何线性代数知识,知道矩阵的计算肯定有好处</li>
<li>在计算机发明以前,数学里面的高阶概念,通常都是天才们依靠自己"能够把公式想象成图像"的能力来理解的,
现在有了计算机,我们可以享受到天才们的超能力了</li>
<li>本书使用Matlab(Octave)和Python来解决问题,其中Matlab更为容易实现线性代数</li>
</ul>
</div>
</div>
<div id="outline-container-org78dbc20" class="outline-3">
<h3 id="org78dbc20"><span class="section-number-3">1.4</span> Practice, exercises and code challenges</h3>
<div class="outline-text-3" id="text-1-4">
<ul class="org-ul">
<li>为了真正理解线性代数,必须做题</li>
<li>本书习题不多,目的是希望你全部都做完</li>
<li>本书习题分为三类:
<ul class="org-ul">
<li>Practice problem: 在subsection之后的,easy级别,答案就在后面,如果做不出来,
那么不需要继续向前读</li>
<li>Exercise: 在chapter之后的,中等难度,答案就在后面,需要手算,而不是用计算机算</li>
<li>Codechallenges: 需要使用计算机编程来实现的,比较难,也有答案</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgda82ab5" class="outline-3">
<h3 id="orgda82ab5"><span class="section-number-3">1.5</span> Online and other resources</h3>
<div class="outline-text-3" id="text-1-5">
<ul class="org-ul">
<li>本书中的解释如果你理解不了,可以从网络上搜索从其他角度的解释来让你明白</li>
<li>本书有配套网络课程,喜欢网络课程学习方法的可以关注</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org9908a3c" class="outline-2">
<h2 id="org9908a3c"><span class="section-number-2">2</span> Vectors</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org65d05f1" class="outline-3">
<h3 id="org65d05f1"><span class="section-number-3">2.1</span> Scalars</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>我们不是从向量(vector)开始,而是从标量(scalar)开始</li>
<li>所谓标量(scalar)就是一个单独的数字,比如4或者-17.3等等</li>
<li>在数学的其他领域,标量有时候会被称之为常量(constant)</li>
<li>标量虽然简单,但是在线性代数里面却扮演者很多重要的角色:
<ul class="org-ul">
<li>subspaces</li>
<li>linear combination</li>
<li>eigendecomposition</li>
</ul></li>
<li>标量的名字(scalar)是scale的名词形式:
<ul class="org-ul">
<li>scale就有伸展,拉长的意思</li>
<li>scalar就有伸展拉长vector和metrix,并且不改变他们的方向(direction)</li>
</ul></li>
<li>标量在图上线上就是线上的一个空心的point,比如下图中的scalar就是一个1.5</li>
<li>注意:本书中标量都使用希腊小写字母( \(\lambda, \alpha, \gamma\) ),以便和vector和matrix区分</li>
<li><p>
使用python来表示标量,就是一个变量
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FFB8D1;">aScalar</span> = 5
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org3eeb666" class="outline-3">
<h3 id="org3eeb666"><span class="section-number-3">2.2</span> Vectors: geometry and algebra</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li><b>Geometry</b> vector是一个line,由两个属性决定:
<ul class="org-ul">
<li>magnitude(长度)</li>
<li>direction(方向)</li>
</ul></li>
<li>line可以在任意维度存在(1维,2维,3维,&#x2026;N维)</li>
<li>如图
<ul class="org-ul">
<li><p>
图2-2
</p>

<div id="orgc4f160c" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/2-2.png" alt="2-2.png" />
</p>
<p><span class="figure-number">Figure 1: </span>tic/2-2.png</p>
</div></li>
<li>上图左边是在2维空间的vector[2,3]</li>
<li>上图右边是在3维空间的vector[2,3,5]</li>
</ul></li>
<li>需要注意的是,vector的定义不包含它的起止位置(position)的,这是和坐标系不同的地方</li>
<li>在坐标系里面,每个坐标都是在空间中唯一的</li>
<li>从另外一个角度上讲,如果假设vector是从[0,0]开的话,那么vector和coordinate就是同一回事了.</li>
<li><p>
所以,起点(英文叫tail,注意是尾巴的意思,英文认为终点的是箭头,起点是尾巴)为[0,0]的vector被叫做在他
的standard position
</p>
<pre class="example" id="orgeaf31bf">
A vector with its tail at the origin is said to be in its standard position
</pre></li>
<li>如图
<ul class="org-ul">
<li><p>
图2-3
</p>

<div id="orgc7039f9" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/2-3.png" alt="2-3.png" />
</p>
<p><span class="figure-number">Figure 2: </span>tic/2-3.png</p>
</div></li>
<li>上图中三个vector(line)都是相同的,因为他们的长度和方向都一样</li>
<li>上图中的三个坐标(圆圈)都是不相同的,因为坐标本来就全局唯一,没有两个一样的坐标</li>
<li>比较黑的line就是vector in its standard position. 这种情况下的vector[1,-2]的head和坐标[1,-2]相重叠</li>
</ul></li>
<li><p>
使用如下代码画vector
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #96CBFE; font-style: italic;">import</span> numpy <span style="color: #96CBFE; font-style: italic;">as</span> np
<span style="color: #96CBFE; font-style: italic;">import</span> matplotlib.pyplot <span style="color: #96CBFE; font-style: italic;">as</span> plt

<span style="color: #FFB8D1;">v</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span>2, -1<span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>
plt.plot<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span>0, v<span style="color: #C2FFDF;">[</span>0<span style="color: #C2FFDF;">]</span><span style="color: #FFB8D1;">]</span>, <span style="color: #FFB8D1;">[</span>0, v<span style="color: #C2FFDF;">[</span>1<span style="color: #C2FFDF;">]</span><span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>
plt.axis<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span>-3, 3, -3, 3<span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>
plt.show<span style="color: #55b3cc;">()</span>
</pre>
</div></li>
<li><b>Algebra</b> 从代数的角度上说,vector就是一个ordered list(成员是number)</li>
<li>一个vector内部number的数量就叫做vector的dimensionality,比如:
<ul class="org-ul">
<li><p>
2D的vector例子
</p>
<pre class="example" id="orgb0244b5">
[1 -2], [4 1], [10000 0]
</pre></li>
<li><p>
3D的vector例子
</p>
<pre class="example" id="org895bc94">
[3.14 e 0], [3 1 4], [2 -7 8]
</pre></li>
</ul></li>
<li>vector内部number的顺序是非常重要的,不同的顺序代表不同的vector,比如下面两个vector就不同,虽然他们
的dimensionality一样,数据也一样:
<ul class="org-ul">
<li>[3 1]</li>
<li>[1 3]</li>
</ul></li>
<li><b>Brackets</b> vector可以使用square bracket(方括号)或者是parentheses(园括号)</li>
<li>我个人认为方括号更加优雅,也不容易混淆,所以一直用方括号</li>
<li>但是有些情况下,你可能会遇到使用圆括号来代替方括号,比如下面两者在这种情况下是等价的:
<ul class="org-ul">
<li>[2 5 5]</li>
<li>(2 5 5)</li>
</ul></li>
<li>vector的几何表示,在2D表达中非常有用,在3D表达中也马马虎虎,但是更多维度就不行了</li>
<li><p>
vector的代数表示,却可以让我们在任何维度上,扩展vector,比如下面的公式就非常清晰的解释了什么是6D vector
</p>
<pre class="example" id="org835c1e8">
[3 4 6 1 -4 5]
</pre></li>
<li><p>
vector成员也不仅限于number,其成员还可以是function,比如下面的例子
</p>
\begin{equation}
\mathbf{v} = [\cos(t)\; \sin(t)\; t]
\end{equation}</li>
<li>本书不讨论上面的情况,本书中vector的所有成员都是普通number</li>
<li><b>Vector orientation</b> vector可以"站着",也可以"躺着":
<ul class="org-ul">
<li><p>
站着的vector被叫做column vector,如下
</p>
\begin{equation}
\left[ {\begin{array}{cccc}
7 \\
3 \\
5 \\
0 \\
\end{array} } \right]
\end{equation}</li>
<li><p>
躺着的vector被叫做row vector,如下
</p>
\begin{equation}
[0 \;1 \;3]
\end{equation}</li>
</ul></li>
<li><b>IMPORTANT</b> 默认情况下,vector是column orientation的,原因可能是在和matrix进行相乘的时候,vector在
matrix右边(作为被乘matrix,一个某个方向上只有一维的matrix)才有意义, 一般matrix都是MxN的大小,那么
在matrix右边,必须是Nx1,而不能是1xN的形状</li>
<li><p>
在matrix中,使用空格分离是row vector, 使用`;`分离,是column vector
</p>
<pre class="example" id="org4cbb3d2">
v1 = [2 5 4 7] % row vector
v2 = [2; 5; 4; 7] % column vector
</pre></li>
<li><p>
在python中, list(以及numpy array)没有默认的orientation,所以在某些情况下一定要指定orientation的
时候,numpy要用比较麻烦的方式实现
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FFB8D1;">v1</span> = <span style="color: #55b3cc;">[</span>2, 5, 4, 7<span style="color: #55b3cc;">]</span>               <span style="color: #E6C000;"># </span><span style="color: #E6C000;">list</span>
<span style="color: #FFB8D1;">v2</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span>2, 5, 4, 7<span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>     <span style="color: #E6C000;"># </span><span style="color: #E6C000;">array, no orientation</span>
<span style="color: #FFB8D1;">v3</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span><span style="color: #C2FFDF;">[</span>2<span style="color: #C2FFDF;">]</span>, <span style="color: #C2FFDF;">[</span>5<span style="color: #C2FFDF;">]</span>, <span style="color: #C2FFDF;">[</span>4<span style="color: #C2FFDF;">]</span>, <span style="color: #C2FFDF;">[</span>7<span style="color: #C2FFDF;">]</span><span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span> <span style="color: #E6C000;"># </span><span style="color: #E6C000;">column vector</span>
<span style="color: #FFB8D1;">v4</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span><span style="color: #C2FFDF;">[</span>2, 5, 4, 7<span style="color: #C2FFDF;">]</span><span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>       <span style="color: #E6C000;"># </span><span style="color: #E6C000;">row vector</span>
</pre>
</div></li>
<li><b>Notation</b> 在书面书写中,我们只需要boldface字母就可以表示vector了,比如 \(\mathbf{v}\), 但是如果是论
文中,我们一定需要在vector上面加上剪头,比如 \(\vec{\mathbf{v}}\)</li>
<li>为了表达vector里面的一个特定成员,我们会使用下标,比如 \(\mathbf{v} = [4\;0\;2]\), 的第二个成员表示
为 \(v_2 = 0\), 第ith个表示为 \(v_i\) ,注意这里的小写字母没有加粗</li>
<li><p>
如果小写字母加粗的下划线加i,也就是 \(\mathbf{v_i}\) 那么表示相关的如下vectors
</p>
\begin{equation}
(\mathbf{v_1},\mathbf{v_2},...,\mathbf{v_i})
\end{equation}</li>
<li><b>Zeros vector</b>, 所有成员都是0的vector叫做zeros vector,注意是所有成员,缺一个都不叫zeros vector</li>
<li>zeros vector有一些特殊的地方,比如:
<ul class="org-ul">
<li>zeros 没有direction, 我的意思不是说它的direction为0,我是说它的direction未知(undefined),因为
zeros vector的magnitude为0,讨论一个magnitude为0的vector的direction是没有意义的</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgad801bf" class="outline-3">
<h3 id="orgad801bf"><span class="section-number-3">2.3</span> Transpose operation</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>把vector在column vector和row vector之间相互转换的操作,叫做transpose</li>
<li>transpose只更改orientation,其他的element内容和排序都不变</li>
<li>我们使用一个上标T来代表这个操作,那么就有如下三个例子:
<ul class="org-ul">
<li><p>
row vector转换成 column vector
</p>
\begin{equation}
 [7\;3\;5]^T  = \left[ {\begin{array}{cccc}
7 \\
3 \\
5 \\
\end{array} } \right]
\end{equation}</li>
<li><p>
column vector转换成 row vector
</p>
\begin{equation}
\left[ {\begin{array}{cccc}
7 \\
3 \\
5 \\
\end{array} } \right]^T = [7\;3\;5]
\end{equation}</li>
<li><p>
两次TT操作,可以抵消
</p>
\begin{equation}
[7\;3\;5]^{TT}=[7\;3\;5]
\end{equation}</li>
</ul></li>
<li>我们之前说过,我们assume, vector是column vector,所以:
<ul class="org-ul">
<li>\(\mathbf{v}\) 就是column vector</li>
<li>\(\mathbf{v}^T\) 就是row vector</li>
</ul></li>
<li>在印刷书籍中,在文字间写column vector非常不方便,所以文字书籍中往往是把column vector写成row vector
的转置形式,比如 \(\mathbf{w} = [1\;2\;3]^T\)</li>
<li><p>
在代码中转置很方便
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #96CBFE; font-style: italic;">import</span> numpy <span style="color: #96CBFE; font-style: italic;">as</span> np

<span style="color: #FFB8D1;">v1</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span><span style="color: #C2FFDF;">[</span>2, 5, 4, 7<span style="color: #C2FFDF;">]</span><span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>  <span style="color: #E6C000;"># </span><span style="color: #E6C000;">row vector</span>
<span style="color: #FFB8D1;">v2</span> = v1.T  <span style="color: #E6C000;"># </span><span style="color: #E6C000;">column vector</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-orgd09585a" class="outline-3">
<h3 id="orgd09585a"><span class="section-number-3">2.4</span> Vector addition and subtraction</h3>
<div class="outline-text-3" id="text-2-4">
<ul class="org-ul">
<li>*Geometry*我们主要通过下面的四个图来理解vector的加和减
<ul class="org-ul">
<li><p>
图2-5
</p>

<div id="org8f97bab" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/2-5.png" alt="2-5.png" />
</p>
<p><span class="figure-number">Figure 3: </span>tic/2-5.png</p>
</div></li>
<li>第一幅图是介绍我们这次加减法的两个成员:
<ol class="org-ol">
<li>v1: [0 2]</li>
<li>v2: [1 1]</li>
</ol></li>
<li>第二幅图是介绍如何计算加法: 把v2的起点从standard position移动到v1的终点,那么从v1的起点到v2的终
点,就是新的vector</li>
<li>第三幅图是介绍减法的第一种做法,就把v2乘以-1,变成[-1,-1], 那么v1-v2就成了v1 + (-1 * v2),加法计算
方法和图二一致</li>
<li>第四幅图是介绍减法的第二种做法,就是从被减数的终点(作为起点)引出一条vector,终点是减数的终点,其
实就是v2 - v1 = v3 转换成v2 = v1 + v3</li>
</ul></li>
<li><p>
vector的加法满足交换律,也就是说
</p>
\begin{equation}
\mathbf{a}  + \mathbf{b} = \mathbf{b} + \mathbf{a}
\end{equation}</li>
<li><b>Algebra</b> 加法和减法的代数解释那就简单了,就是相对应的element进行加或者减:
<ul class="org-ul">
<li><p>
比如
</p>
\begin{equation}
[1\;2] + [3\;4] = [4\;6]
\end{equation}</li>
<li><p>
用公式来解释就是
</p>
\begin{equation}
\mathbf{c} = \mathbf{a} + \mathbf{b} = [a_1 + b_1 \; a_2 + b_2 \;...\; a_n + b_n]^T
\end{equation}</li>
</ul></li>
<li><b>Important</b> 加法和减法有意义的前提是参与运算的两个vector有同样的维度</li>
</ul>
</div>
</div>
<div id="outline-container-org7a63bbc" class="outline-3">
<h3 id="org7a63bbc"><span class="section-number-3">2.5</span> Vector-scalar multiplication</h3>
<div class="outline-text-3" id="text-2-5">
<ul class="org-ul">
<li><b>Geometry</b> Scaling一个vector,就是:
<ul class="org-ul">
<li>增加或者减少这个vector的长度</li>
<li>并且不改变这个vector的angle</li>
</ul></li>
<li>scalar multiplication也不会改变原始的orientation</li>
<li>当然,如果scalar为0的话,最后的结果全部变成0,但是这种情况下,vector转换成了一个point,我们不能说point
有任何的的angle</li>
<li><b>Algebra</b> Scalar-vector的乘法,就是把vector的每个成员都乘以scalar</li>
<li><p>
对于scalar \(\lambda\) 和 vector \(\mathbf{v}\) , 我们有如下的公式
</p>
\begin{equation}
\lambda \mathbf{v} = [\lambda \mathbf{v}_1 \; \lambda \mathbf{v}_2 \; ... \; \lambda \mathbf{v}_n]^T \tag{2.3}
\end{equation}</li>
<li><p>
一个简单的例子如下
</p>
<pre class="example" id="org5cdee13">
3 [-1 3 0 2] = [-3 9 0 6]
</pre></li>
<li><p>
scalar-vector multipleication满足交换律,也就是说
</p>
\begin{equation}
\lambda \mathbf{v} =  \mathbf{v} \lambda
\end{equation}</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgf7a7570" class="outline-2">
<h2 id="orgf7a7570"><span class="section-number-2">3</span> Vector multiplications</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>有四种方法来对两个vector进行乘法:
<ul class="org-ul">
<li>dot product</li>
<li>outer product</li>
<li>element-wise multiplication</li>
<li>cross product</li>
</ul></li>
<li>其中最重要的也是我们讲的最多的,就是dot product</li>
</ul>
</div>
<div id="outline-container-org0f7d73e" class="outline-3">
<h3 id="org0f7d73e"><span class="section-number-3">3.1</span> Vector dot product: Algebra</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>dot product也叫做inner product, 是线性代数里面最重要的操作</li>
<li>dot product是如下高级操作的基础:
<ul class="org-ul">
<li>convolution(卷积)</li>
<li>correlation</li>
<li>Fourier transform</li>
<li>matrix multiplication</li>
<li>signal filtering</li>
</ul></li>
<li>dot product是使用一个number来提供两个vector之间relationship的方法</li>
<li><p>
由于两个vector的dot product结果是一个scalar, 所以dot product又称之为scalar product
</p>
<pre class="example" id="org9719f8d">
注意,是scalar product,而不是scalar-vector product
</pre></li>
<li>至于inner product,这是在"非欧几里得空间"里面对dot product的命名,在欧几里何空间,我们可以认为inner
product和dot product等价.</li>
<li>inner product和dot product(scalar product)的实际关系如下</li>
<li>本书只使用dot product这一个称呼</li>
<li>从几何角度上来说,计算dot product,只需要如下两步:
<ul class="org-ul">
<li>把两个vector对应的N个element相乘,得到N个数字</li>
<li>把这N个数字相加</li>
</ul></li>
<li><p>
dot product的过程可以使用如下公式表达,注意,公式中中间三个是对dot product的三种表达方式(我们经常
使用的是 \(\mathbf{a}^T\mathbf{b}\),因为这个体现了矩阵乘法的原理)
</p>
\begin{equation}
\alpha = \mathbf{a} \cdot \mathbf{b} = \left \langle \mathbf{a}, \mathbf{b} \right \rangle =\mathbf{a}^T \mathbf{b} = \sum_{i=1}^n a_i b_i \tag{3.1}
\end{equation}</li>
<li><p>
我们举个例子来计算一下
</p>
<pre class="example" id="orgb03dc21">
[1 2 3 4] * [5 6 7 8] = 1*5 + 2*6 + 3*7 + 4*8
                      = 5 + 12 + 21 + 32
                      = 70
</pre></li>
<li>由于dot product计算过程的特性,那么我们需要dot product参与的两个vector都是相同的dimensionality</li>
<li>vector和它自己的dimensionality肯定是相同的,所以,我们可以计算vector和它自己的dot product
<ul class="org-ul">
<li><p>
这个操作可以在公式3.2中显示
</p>
\begin{equation}
\mathbf{a}^T\mathbf{a} = \left \| \mathbf{a} \right \|^2 = \sum_{i=1}^n a_i a_i = \sum_{i=1}^n a_i^2 \tag{3.2}
\end{equation}</li>
<li>\(\left \| \mathbf{a} \right \|\) 叫做 vector \(\mathbf{a}\) 的length, magnitude或者是norm</li>
<li>vector自己和自己dot product的结果是\(\left \| \mathbf{a}^2 \right \|\) , 其实就是 vector的length-squared,
magnitude-squared或者是sauared-norm</li>
</ul></li>
<li><p>
使用如下代码计算dot product
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FFB8D1;">v1</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span>2, 5, 4, 7<span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>
<span style="color: #FFB8D1;">v2</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span>4, 1, 0, 2<span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>
<span style="color: #FFB8D1;">dp</span> = np.dot<span style="color: #55b3cc;">(</span>v1, v2<span style="color: #55b3cc;">)</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org45d00b8" class="outline-3">
<h3 id="org45d00b8"><span class="section-number-3">3.2</span> Dot product properties</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li><b>Associative property</b> 我们想看看结合律是否在dot production上面适用,需要从两个角度看,必须两个角
度都满足才能说dot production 满足结合律</li>
</ul>
</div>
<div id="outline-container-org91313b3" class="outline-4">
<h4 id="org91313b3"><span class="section-number-4">3.2.1</span> vector product和scalar的结合律</h4>
<div class="outline-text-4" id="text-3-2-1">
<ul class="org-ul">
<li>这种情况其实就是scalar-vector multiplication嵌套在dot product里面</li>
<li>这种情况下显然满足结合律,因为scalar和每个vector的结果都是"维度不变(长度变化)的新vector"</li>
<li><p>
用公式表达,就是公式3.3是成立的.
</p>
\begin{equation}
\gamma(\mathbf{u}^T \mathbf{v}) = (\gamma \mathbf{u}^T) \mathbf{v} = \mathbf{u}^T (\gamma \mathbf{v}) = (\mathbf{u}^T \mathbf{v})\gamma \tag{3.3}
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-orgd39a2cb" class="outline-4">
<h4 id="orgd39a2cb"><span class="section-number-4">3.2.2</span> vector product和vector的结合律</h4>
<div class="outline-text-4" id="text-3-2-2">
<ul class="org-ul">
<li>先说结论,这种情况下的结合律是不满足的</li>
<li><p>
用公式表达,就是公式3.4是不成立的!
</p>
\begin{equation}
\mathbf{u}^T(\mathbf{v}^T \mathbf{w}) = (\mathbf{u}^T\mathbf{v})^T \mathbf{w}
\end{equation}</li>
<li>要想理解这个不可能,我们可以从很多方向来理解:
<ul class="org-ul">
<li>我们首先假设三个vector的维度相同,那么我们会发现,上面公式的左右两边甚至都不是dot product,因为
<ol class="org-ol">
<li>左边是row vector \(\mathbf{u}^T\) 和一个scalar(两个vector相乘得到的)相乘, 所以左边是一个row vector</li>
<li>右边是一个scalr(两个vector相乘得到的)和一个column vector相乘,所以右边是一个 column vector(注
意对于scalar来说 \(4^T = 4\)</li>
<li><p>
其实不仅仅是row vector和column vector的不一样,他们的成员其实也有可能是不一样的,比如下面的例子
</p>
\begin{equation}
\mathbf{u} = \begin{bmatrix}
             1 \\
             2 \\
             \end{bmatrix} ,
\mathbf{v} = \begin{bmatrix}
             1 \\
             3 \\
             \end{bmatrix} ,
\mathbf{w} = \begin{bmatrix}
             2 \\
             3 \\
             \end{bmatrix}
\end{equation}</li>
<li><p>
左边的结果为
</p>
\begin{equation}
\mathbf{u}^T(\mathbf{v}^T \mathbf{w}) =
             \begin{bmatrix}
             1 \; 2 \\
             \end{bmatrix}
             \left(
             \begin{bmatrix}
             1 \; 3 \\
             \end{bmatrix}
             \begin{bmatrix}
             2 \\
             3 \\
             \end{bmatrix}
             \right)
             =
             \begin{bmatrix}
             11 \; 22 \\
             \end{bmatrix} \tag{3.5}
\end{equation}</li>
<li><p>
右边的结果为,可见,显然和左边的不一样,不仅orientation不一样,element维度也不一样
</p>
 \begin{equation}
( \mathbf{u}^T\mathbf{v})^T \mathbf{w} =
              \left(
              \begin{bmatrix}
              1 \; 2 \\
              \end{bmatrix}
              \begin{bmatrix}
              1 \\ 3 \\
              \end{bmatrix}
              \right)^T
              \begin{bmatrix}
              2 \\
              3 \\
              \end{bmatrix}
              =
              \begin{bmatrix}
              14 \\ 21 \\
              \end{bmatrix} \tag{3.6}
 \end{equation}</li>
</ol></li>
<li>如果这三个vector的维度不同,那么甚至有一边的计算都是invalid的,都不用考虑是否相等了</li>
</ul></li>
<li>综上所述,我们可以得到结论,就是vector dot product不遵守结合律.(但是,matrix的乘法遵守结合律,所以
后面不要和这里混淆)</li>
</ul>
</div>
</div>
<div id="outline-container-org021be4c" class="outline-4">
<h4 id="org021be4c"><span class="section-number-4">3.2.3</span> commutative property</h4>
<div class="outline-text-4" id="text-3-2-3">
<ul class="org-ul">
<li><p>
dot product 满足交换律,用公式表达如下
</p>
\begin{equation}
\mathbf{a}^T \mathbf{b} = \mathbf{b} \mathbf{a}^T \tag{3.7}
\end{equation}</li>
<li><p>
dot product 满足交换律是很显然的事情,因为dot production是在element维度完成的,两element的相乘,
其实就是两个scalar的乘积,而scalar乘法是符合交换律的(如公式3.8),那么我们也可以说dot product也是
符合交换律的
</p>
\begin{equation}
\sum_{i=1}^na_ib_i = \sum_{i=1}^nb_ia_i \tag{3.8}
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-org38fa763" class="outline-4">
<h4 id="org38fa763"><span class="section-number-4">3.2.4</span> Distributive property</h4>
<div class="outline-text-4" id="text-3-2-4">
<ul class="org-ul">
<li>首先抛出结论: dot product是符合分配率的,符合分配率这件事情能让"代数表达"和"几何表达"联系起来</li>
<li><p>
分配率可以用如下公式解释:(当然了,这里的vector必须维度相同)
</p>
\begin{equation}
\mathbf{w}^T (\mathbf{u} + \mathbf{v}) = \mathbf{w}^T \mathbf{u} + \mathbf{w}^T \mathbf{v} \tag{3.9}
\end{equation}</li>
<li>分配率说的是这么一个事儿:我们可以把一个dot product分成两个dot product的和,只需要把那个vector拆
成两个就好了</li>
<li>当然了也可以反过来用,假设两个vector都和同一个vector相乘,而这两vector的维度一样,那么就可以先把
这两个vector加起来</li>
<li>我们可以用一个例子来加深我们的理解:
<ul class="org-ul">
<li><p>
假设三个vector如下
</p>
\begin{equation}
\mathbf{u} = \begin{bmatrix}
             1 \\
             2 \\
             \end{bmatrix} ,
\mathbf{v} = \begin{bmatrix}
             1 \\
             3 \\
             \end{bmatrix} ,
\mathbf{w} = \begin{bmatrix}
             2 \\
             3 \\
             \end{bmatrix}
\end{equation}</li>
<li><p>
公式左边的计算结果是19
</p>
\begin{equation}
\mathbf{w}^T(\mathbf{u} + \mathbf{v}) =
             \begin{bmatrix}
             2 \; 3 \\
             \end{bmatrix}
             \left(
             \begin{bmatrix}
             1 \\ 2 \\
             \end{bmatrix}
+
              \begin{bmatrix}
              1 \\
              3 \\
              \end{bmatrix}
              \right)
              =
              \begin{bmatrix}
              2 \; 3 \\
              \end{bmatrix}
              \times
              \begin{bmatrix}
              2 \\
              5 \\
              \end{bmatrix}
              = 19
              \tag{3.11}
\end{equation}</li>
<li><p>
公式右边的计算结果也是19
</p>
\begin{equation}
\mathbf{w}^T \mathbf{u} + \mathbf{w}^T \mathbf{v} =
             \begin{bmatrix}
             2 \; 3 \\
             \end{bmatrix}
             \begin{bmatrix}
             1 \\ 2 \\
             \end{bmatrix}
+
              \begin{bmatrix}
              2 \; 3 \\
              \end{bmatrix}
              \begin{bmatrix}
              1 \\
              3 \\
              \end{bmatrix}
              =
              8 + 11
              = 19
              \tag{3.12}
\end{equation}</li>
</ul></li>
<li>下面我们把结合律应用到一种特殊的情况,那就是vector自己分成两个sub_vector, 然后这两个sub_vector再
乘以自己.</li>
<li><p>
由于分配率的存在,我们可以得到如下的等式
</p>
\begin{align}
(\mathbf{u} + \mathbf{v})^T(\mathbf{u} + \mathbf{v}) &= \| \mathbf{u} + \mathbf{v} \|^2 \\
                                                     &= \mathbf{u}^T\mathbf{u} + 2\mathbf{u}^T\mathbf{v} + \mathbf{v}^T\mathbf{v} \\
                                                     &= \| \mathbf{u} \|^2 + \| \mathbf{v} \|^2 + 2\mathbf{u}^T\mathbf{v}
\end{align}</li>
<li>上述公式是连接"代数解释"和"几何解释"之间的桥梁</li>
<li><p>
<b>Cauchy-Schwarz inqauality</b> 柯西-斯瓦茨不等式提供了两个vector进行dot product的上限,,不等式如下
</p>
\begin{equation}
| \mathbf{v}^T \mathbf{w} | \leq \| v \| \| w \| \tag{3.14}
\end{equation}</li>
<li><p>
用英语来说,上面的不等式就是说
</p>
<pre class="example" id="orgcaa0dee">
两个vector的dot product的magnitude不会比这两个vector magitude的product值大
</pre></li>
<li>这个不等式的等于会在下面的情况下得到满足的时候,出现:
<ul class="org-ul">
<li>一个vector是另外一个vector的scaled version,也就是说 \(\mathbf{v} = \lambda \mathbf{w}\)</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org362900b" class="outline-3">
<h3 id="org362900b"><span class="section-number-3">3.3</span> Vector dot product: Geometry</h3>
<div class="outline-text-3" id="text-3-3">
<ul class="org-ul">
<li><p>
从几何上来说
</p>
<pre class="example" id="orge3ad9b2">
dot product就是两个vector之间的cosinec乘以这两个vector
</pre></li>
<li>其实,从几何和代数的方向上看这个公式,其实是使用不同的方式来表达同样的concept</li>
<li><p>
dot product 的几何定义
</p>
\begin{equation}
\mathbf{a}^T \mathbf{b} = \| \mathbf{a} \| \| b \| \cos(\theta_{ab}) \tag{3.15}
\end{equation}</li>
<li>如果上面的vector长度为1(也就是 \(\| \mathbf{a} \| = \| b \| = 1\) ),那么dot product的结果也就变成
了vector的cosine值</li>
<li>公式3.15可以转为如下两种写法:
<ul class="org-ul">
<li><p>
求cosine值
</p>
\begin{equation}
\cos(\theta_{ab}) = \cfrac{\mathbf{a}^T \mathbf{b}}{\| \mathbf{a} \| \| b \|}\tag{3.16}
\end{equation}</li>
<li><p>
求vector之间的角度
</p>
\begin{equation}
\theta_{ab} = \cos^{-1}\left(\cfrac{\mathbf{a}^T \mathbf{b}}{\| \mathbf{a} \| \| b \|}\right)\tag{3.17}
\end{equation}</li>
</ul></li>
<li>公式3.17意义非凡,在一个2D空间求两个vector之间夹角的方法,放到3维,4维,甚至更高维度都是同样成立的</li>
<li><p>
如果只考虑dot product的正负,那么我们可以得到一个结论:
</p>
<pre class="example" id="orge1f5875">
dot product的正负只由两个vector之间的夹角来决定
</pre></li>
<li>原因很简单 dot product是cosine乘以两个vector的长度,长度都是正数,那么它的乘积也是正数,在判断结果
正负的时候,就可以不考虑他们</li>
<li>根据vector之间的角度,我们可以把dot product分成五种,我们以 \(\theta\) 代表vector之间的角度, \(\alpha\)
代表dot product的结果:
<ol class="org-ol">
<li>\(\theta < 90^{\circ} \rightarrow \alpha > 0\): 锐角的cosine总是正数,所以dot product也是正数</li>
<li>\(\theta > 90^{\circ} \rightarrow \alpha < 0\): 钝角的cosine总是负数,所以dot product也是负数</li>
<li>\(\theta = 90^{\circ} \rightarrow \alpha = 0\): 直角的cosine是0,所以dot product也是0.这种情况下
非常重要,所以有一个自己的名字: <b>orthogonal</b> (正交), 而且正交还有一个特殊的符号,如果两个vector
是正交的,那么,我们可以使用如下的公式表示 \(\mathbf{w} \bot \mathbf{v}\)</li>
<li>\(\theta = 0^{\circ} \rightarrow \alpha = \| \mathbf{a} \| \| \mathbf{b} \|\): 0度角的cosine是1
所以,dot product就是两个vector长度的乘积,这种情况也有一个单独的名字: <b>collinear</b> (共线)</li>
<li>\(\theta = 180^{\circ} \rightarrow \alpha = -\| \mathbf{a} \| \| \mathbf{b} \|\):180度角的cosine
是-1,所以dot product是两个vector长度的乘积再乘以-1,这种情况也叫共线</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org0022b1a" class="outline-3">
<h3 id="org0022b1a"><span class="section-number-3">3.4</span> Algebraic and geometric equivalence</h3>
<div class="outline-text-3" id="text-3-4">
<ul class="org-ul">
<li>对于dot product的解释,几何解释和代数解释非常的不同,但是实质上是一样的.我们本节就是
来讨论这个问题的</li>
<li><p>
下面公式的3.18,中间是代数解释,右边是几何解释
</p>
\begin{equation}
\mathbf{a}^T \mathbf{b} = \sum_{i=1}^n \mathbf{a}_i \mathbf{b}_i = \| \mathbf{a} \| \| \mathbf{b} \| \cos(\theta_{ab}) \tag{3.18}
\end{equation}</li>
<li>论证代数和几何表达的内涵统一性:
<ul class="org-ul">
<li>首先要了解dot product是满足交换律和分配率的</li>
<li>其次要了解Law of Cosine</li>
</ul></li>
<li>勾股定理(Pythagorean theorem)是大家非常熟悉的 \(\mathbf{a}^2 + \mathbf{b}^2 = \mathbf{c}^2\)</li>
<li><p>
勾股定理其实是Law of Cosine的一个特例
</p>
\begin{equation}
\mathbf{c}^2 = \mathbf{a}^2 + \mathbf{b}^2 - 2\mathbf{a}\mathbf{b} \cos \theta_{ \mathbf{a} \mathbf{b}}
\end{equation}</li>
<li>下面是证明dot product代数和结合表示内涵相同的过程:
<ul class="org-ul">
<li><p>
从代数方向上看,由余弦定理计算 \(\mathbf{c}^2\) 得到:
</p>
\begin{equation}
\mathbf{c}^2 = \mathbf{a}^2 + \mathbf{b}^2 - 2\mathbf{a}\mathbf{b} \cos (\theta_{ \mathbf{a} \mathbf{b}})
\end{equation}</li>
<li>从几何方向上看, 我们可以把vector \(\mathbf{c}\) 看成是 vector \(\mathbf{a}\) 和 vector \(\mathbf{b}\)
之间的差值:
<ol class="org-ol">
<li>原始方程 $ \| c \| = \| a - b \|$</li>
<li><p>
我们也去求 \(\mathbf{c}\) 的平方
</p>
\begin{align}
\| \mathbf{a} - \mathbf{b} \| &= ( \mathbf{a} - \mathbf{b} )^T ( \mathbf{a} - \mathbf{b} )   \\
            &= \mathbf{a} ^T \mathbf{a} - 2 \mathbf{a} ^T \mathbf{b} + \mathbf{b} ^T \mathbf{b} \\
            &= \| \mathbf{a} \|^2 + \| \mathbf{b} \|^2 - 2 \mathbf{a} ^T \mathbf{b} \\
\end{align}</li>
<li><p>
综合前面代数方向得到的 \(\mathbf{c}^2\),就得到
</p>
\begin{equation}
\| \mathbf{a} \|^2 + \| \mathbf{b} \|^2 - 2 \mathbf{a} ^T \mathbf{b} = \| \mathbf{a} \|^2 + \| \mathbf{b} \|^2  - 2 \| \mathbf{a} \| \| \mathbf{b} \| \cos \theta
\end{equation}</li>
<li><p>
两边消掉共同项,就得到
</p>
\begin{equation}
\mathbf{a} ^T \mathbf{b} = \| \mathbf{a} \| \| \mathbf{b} \| \cos \theta \tag{3.15}
\end{equation}</li>
</ol></li>
</ul></li>
<li><b>Proof of Cauchy-Schwarz inequality</b> 一旦得到了公式3-15这个结论,那么柯西不等式的证明就很简单了:
<ul class="org-ul">
<li><p>
由于如下等式成立
</p>
\begin{equation}
\mathbf{a} ^T \mathbf{b} = \| \mathbf{a} \| \| \mathbf{b} \| \cos \theta \tag{3.33}
\end{equation}</li>
<li>又由于cosine区间是[0,1]</li>
<li><p>
所以得以证明柯西不等式
</p>
\begin{equation}
\mathbf{a} ^T \mathbf{b} \le \| \mathbf{a} \| \| \mathbf{b} \| \tag{3.34}
\end{equation}</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb8a5c63" class="outline-3">
<h3 id="orgb8a5c63"><span class="section-number-3">3.5</span> Linear weighted combination</h3>
<div class="outline-text-3" id="text-3-5">
<ul class="org-ul">
<li>线性加权组合由于非常常见,所以我们也单独给了他一个章节</li>
<li>线性加权组合还有其他常见的名字,比如:
<ul class="org-ul">
<li>linear mixture</li>
<li>weighted combination</li>
<li>linear coefficient combination</li>
</ul></li>
<li>线性加权组合其实就是:
<ul class="org-ul">
<li>scalar和vector相乘, 得到新的vector</li>
<li>新得到的N个vector再相加</li>
<li>这里就要assume所有的vector的维度相同</li>
<li><p>
公式如下
</p>
\begin{equation}
\mathbf{w} = \lambda_1 \mathbf{v}_1 + \lambda_2 \mathbf{v}_2 + \cdot\cdot\cdot + \lambda_n \mathbf{v}_n \tag{3.35}
\end{equation}</li>
</ul></li>
<li><p>
使用代码来实现linear weighted combination非常容易
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #96CBFE; font-style: italic;">import</span> numpy <span style="color: #96CBFE; font-style: italic;">as</span> np

<span style="color: #FFB8D1;">l1</span> = 1
<span style="color: #FFB8D1;">l2</span> = 2
<span style="color: #FFB8D1;">l3</span> = -3
<span style="color: #FFB8D1;">v1</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span>4, 5, 1<span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>
<span style="color: #FFB8D1;">v2</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span>-4, 0, -4<span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>
<span style="color: #FFB8D1;">v3</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span>1, 3, 2<span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>
<span style="color: #FFB8D1;">ret</span> = l1 * v1 + l2 * v2 + l3 * v3
<span style="color: #96CBFE; font-style: italic;">print</span><span style="color: #55b3cc;">(</span><span style="color: #FFEA00;">"""[ret] ==&gt;"""</span>, ret<span style="color: #55b3cc;">)</span>
<span style="color: #E6C000;"># </span><span style="color: #E6C000;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #E6C000;"># </span><span style="color: #E6C000;">[ret] ==&gt; [ -7  -4 -13]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-orgef64cf6" class="outline-3">
<h3 id="orgef64cf6"><span class="section-number-3">3.6</span> The outer product</h3>
<div class="outline-text-3" id="text-3-6">
<ul class="org-ul">
<li>所谓outer product,就是一种把两个vector合并,产生一个matrix的操作</li>
<li>outer product 最容易让人难以理解的地方,就是表达符号,实在是和dot product太像了(注意这里 \(\mathbf{v}\)
是M-element column vector, \(\mathbf{m}\) 是N-element column vector ):
<ul class="org-ul">
<li><p>
我们首先看看dot product(注意,在dot product里, V和M必须相等)
</p>
\begin{equation}
\mathbf{v}^T \mathbf{w} = 1 \times 1
\end{equation}</li>
<li><p>
再来看看out product,只不过是带T的放到了后面
</p>
\begin{equation}
\mathbf{v} \mathbf{w}^T = M \times N
\end{equation}</li>
<li><p>
从上帝视角,我们其实学过矩阵乘法了,从矩阵乘法的角度其实可以理解最后为什么结果一个是scalar,一个
是matrix,因为
</p>
<pre class="example" id="org0944ec2">
Dot product and outer product are special cases of matrix multiplication.
</pre></li>
</ul></li>
<li>我们后面会从三个角度来理解outer product:
<ul class="org-ul">
<li>element perspective</li>
<li>column perspective</li>
<li>row perspective</li>
</ul></li>
<li><b>Element perspetive</b>, 从element的角度考虑,每个对于outer product矩阵里面的成员 element_ij, 其是
如下两个element的scalar multilication值:
<ul class="org-ul">
<li>第一个vector的ith element</li>
<li>第二个vector的jth element</li>
</ul></li>
<li><p>
由此,我们可以总结出element perspective的公式:
</p>
\begin{equation}
(\mathbf{v} \mathbf{w}^T)_{i,j} = v_i w_j\tag{3.36}
\end{equation}</li>
<li><p>
下面是使用字母替代数字来显示outer product的过程
</p>
\begin{equation}
\begin{bmatrix}
a \\
b \\
c \\
\end{bmatrix}
\begin{bmatrix}
d e f  \\
\end{bmatrix}
=
\begin{bmatrix}
ad \;ae \;af \;\\
bd \;be \;bf \;\\
cd \;ce \;cf \;\\
\end{bmatrix}
\end{equation}</li>
<li><b>Column perspective</b> 我们再来从column 维度来看看每个outer product是怎么构成的,从column纬度看outer
product, outer product里面的每一列,都可以看做是一个scalar-vector multiplication:
<ul class="org-ul">
<li>其中,vector是左边的column vector(每次重复使用)</li>
<li>另外,scalar是右边的row vector里面每次取一个</li>
<li>所以,outer product的column的数目和右侧row vector的个数相同</li>
<li><p>
同时,outer product的每个column都是left column vector 的scaled version
</p>
<pre class="example" id="org5a6c2d6">
Each column of the outer product matrix is a scaled version of the left column vector
</pre></li>
</ul></li>
<li><b>Row perspective</b>, 其实类比column perspective就可以得出结论了,outer product里面的每一行,都可以
看做是一个scalar-vector multiplication:
<ul class="org-ul">
<li>其中,vector是右边的row vector(每次重复使用)</li>
<li>另外,scalar是左边的column vector里面每次取一个</li>
<li>所以,outer product的row数目和左侧的left vector的个数相同</li>
<li>同时,outer product的每个row都是right row vector的scaled version</li>
</ul></li>
<li>如果swap 两个vector的order(注意swap里之后,在左边的还是要以column vector的形式,右边的还是要以row
vector的形式),那么我们就会发现新的两个matrix非常像,只不过row和column也给swap了</li>
<li><p>
<b>Code</b> 使用如下代码来完成out product的计算,顺便看一下上一条说的matrix的swap现象
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #96CBFE; font-style: italic;">import</span> numpy <span style="color: #96CBFE; font-style: italic;">as</span> np

<span style="color: #FFB8D1;">v1</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span>2, 5, 4, 7<span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>
<span style="color: #FFB8D1;">v2</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span>4, 1, 0, 2<span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>
<span style="color: #FFB8D1;">op</span> = np.outer<span style="color: #55b3cc;">(</span>v1, v2<span style="color: #55b3cc;">)</span>
<span style="color: #96CBFE; font-style: italic;">print</span><span style="color: #55b3cc;">(</span>op<span style="color: #55b3cc;">)</span>
<span style="color: #FFB8D1;">op</span> = np.outer<span style="color: #55b3cc;">(</span>v2, v1<span style="color: #55b3cc;">)</span>
<span style="color: #96CBFE; font-style: italic;">print</span><span style="color: #55b3cc;">(</span>op<span style="color: #55b3cc;">)</span>

<span style="color: #E6C000;"># </span><span style="color: #E6C000;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #E6C000;"># </span><span style="color: #E6C000;">[[ 8  2  0  4]</span>
<span style="color: #E6C000;">#  </span><span style="color: #E6C000;">[20  5  0 10]</span>
<span style="color: #E6C000;">#  </span><span style="color: #E6C000;">[16  4  0  8]</span>
<span style="color: #E6C000;">#  </span><span style="color: #E6C000;">[28  7  0 14]]</span>
<span style="color: #E6C000;"># </span><span style="color: #E6C000;">[[ 8 20 16 28]</span>
<span style="color: #E6C000;">#  </span><span style="color: #E6C000;">[ 2  5  4  7]</span>
<span style="color: #E6C000;">#  </span><span style="color: #E6C000;">[ 0  0  0  0]</span>
<span style="color: #E6C000;">#  </span><span style="color: #E6C000;">[ 4 10  8 14]]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-orga32c8c2" class="outline-3">
<h3 id="orga32c8c2"><span class="section-number-3">3.7</span> Element-wise (Hadamard) vector product</h3>
<div class="outline-text-3" id="text-3-7">
<ul class="org-ul">
<li><p>
所谓Element-wise vector product,就是两个一样的vector的每个位置上的scalar相乘,得到一个和前两者一
样的新vector,公式如下
</p>
\begin{equation}
\mathbf{c} = \mathbf{a} \odot \mathbf{b} = [a_1 b_1 \; a_2 b_2 \; \cdot\cdot\cdot a_n b_n] \tag{3.37}
\end{equation}</li>
<li>这个操作其实不算是线性代数的操作,更像是一组有序的scalar multiplication</li>
<li><p>
<b>Code</b> 计算element-wise multiplication的代码如下
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #96CBFE; font-style: italic;">import</span> numpy <span style="color: #96CBFE; font-style: italic;">as</span> np

<span style="color: #FFB8D1;">v1</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span>2, 5, 4, 7<span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>
<span style="color: #FFB8D1;">v2</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span>4, 1, 0, 2<span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>
<span style="color: #FFB8D1;">v3</span> = v1 * v2
<span style="color: #96CBFE; font-style: italic;">print</span><span style="color: #55b3cc;">(</span><span style="color: #FFEA00;">"""[v3] ==&gt;"""</span>, v3<span style="color: #55b3cc;">)</span>

<span style="color: #E6C000;"># </span><span style="color: #E6C000;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #E6C000;"># </span><span style="color: #E6C000;">[v3] ==&gt; [ 8  5  0 14]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-orgd1f5f02" class="outline-3">
<h3 id="orgd1f5f02"><span class="section-number-3">3.8</span> Cross product</h3>
<div class="outline-text-3" id="text-3-8">
<ul class="org-ul">
<li>cross product的限定条件是:
<ul class="org-ul">
<li>参与者必须是3维vector</li>
<li>结果自然也是3维vector</li>
</ul></li>
<li><p>
计算公式如下
</p>
\begin{equation}
\mathbf{a} \times \mathbf{b} = \begin{bmatrix}
                        a_2 b_3  - a_3 b_2\\
                        a_3 b_1  - a_1 b_3\\
                        a_1 b_2  - a_2 b_1\\
                        \end{bmatrix} \tag{3.38}
\end{equation}</li>

<li><p>
cross product的magnitude等于参与乘法的两个vector的magnitude再乘以两者的sin
</p>
\begin{equation}
\| \mathbf{a} \times \mathbf{b} \| = \| \mathbf{a} \| \| \mathbf{b} \| \sin(\theta_{ab}) \tag{3.39}
\end{equation}</li>
<li>cross product主要应用在几何领域(而不是线性代数领域),主要作用是创建一个vector \(\mathbf{c}\) , 使得
能够正交于 vector \(\mathbf{a}\)  和 vector \(\mathbf{b}\) 所定义的平面</li>
<li>cross product主要用于多变量微积分,而对于如下行业却完全不适用,所以我们后面不会再提及这个概念,这
里写出来是为了知识的完整性:
<ul class="org-ul">
<li>data analysis</li>
<li>statistics</li>
<li>machine-learning</li>
<li>signal-processing</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org662be6e" class="outline-3">
<h3 id="org662be6e"><span class="section-number-3">3.9</span> Unit vectors</h3>
<div class="outline-text-3" id="text-3-9">
<ul class="org-ul">
<li>拥有长度为1的vector( \(\| \mathbf{v} \| = 1\)  ), 有时候是非常重要的</li>
<li>长度为1的vector就叫做unit vector</li>
<li>unit vector还可以用来创建特殊的matrix: orthogonal matrix</li>
<li>本节主要是要我们设计公式来求一个普通vector的unit vector:
<ul class="org-ul">
<li>这个普通vector并不一定是unit vector</li>
<li>求出啦的unit vector和这个普通vector的方向是一样的,只不过长度为1</li>
</ul></li>
<li><p>
求unit vector其实就是求一个scalar \(\mu\) ,使得其满足如下公式
</p>
\begin{equation}
\mu \mathbf{v} \; s.t. \; \| \mu \mathbf{v} \| = 1 \tag{3.40}
\end{equation}</li>
<li>我们求得 \(\mu\) 的值之后,就可以计算unit vector啦, \(\mu\) 的值其实就是:1除以普通vector的magnitude</li>
<li><p>
公式如下,其中unit vector使用一个hat标识 \(\hat{\mathbf{v}}\)
</p>
\begin{equation}
\hat{\mathbf{v}} = \cfrac{1}{\| \mathbf{v} \|} \mathbf{v} = \cfrac{1}{\sqrt{\sum_{i=1}^n v_i^2}} \mathbf{v} \tag{3.41}
\end{equation}</li>
<li><p>
一个例子如下
</p>
\begin{equation}
\mathbf{v}  = \begin{bmatrix}
          0 \\
          2 \\
         \end{bmatrix},
\hat{\mathbf{v}} = \cfrac{1}{\sqrt{0^2 + 2^2}}
\begin{bmatrix}
0 \\
2 \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
1 \\
\end{bmatrix}
\end{equation}</li>
<li>注意,求某个普通vector的unit vector的时候, 普通vector的长度必须不是0</li>
<li><p>
<b>Code</b> 代码如下
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #96CBFE; font-style: italic;">import</span> numpy <span style="color: #96CBFE; font-style: italic;">as</span> np

<span style="color: #FFB8D1;">v</span> = np.array<span style="color: #55b3cc;">(</span><span style="color: #FFB8D1;">[</span>2, 5, 4, 7<span style="color: #FFB8D1;">]</span><span style="color: #55b3cc;">)</span>
<span style="color: #FFB8D1;">vMag</span> = np.linalg.norm<span style="color: #55b3cc;">(</span>v<span style="color: #55b3cc;">)</span>
<span style="color: #96CBFE; font-style: italic;">print</span><span style="color: #55b3cc;">(</span><span style="color: #FFEA00;">"""[vMag] ==&gt;"""</span>, vMag<span style="color: #55b3cc;">)</span>
<span style="color: #FFB8D1;">v_unit</span> = v / vMag
<span style="color: #96CBFE; font-style: italic;">print</span><span style="color: #55b3cc;">(</span><span style="color: #FFEA00;">"""[v_unit] ==&gt;"""</span>, v_unit<span style="color: #55b3cc;">)</span>

<span style="color: #E6C000;"># </span><span style="color: #E6C000;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #E6C000;"># </span><span style="color: #E6C000;">[vMag] ==&gt; 9.695359714832659</span>
<span style="color: #E6C000;"># </span><span style="color: #E6C000;">[v_unit] ==&gt; [0.20628425 0.51571062 0.4125685  0.72199487]</span>
</pre>
</div></li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: harrifeng@outlook.com</p>
<p class="date">Created: 2022-07-21 Thu 19:23</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
