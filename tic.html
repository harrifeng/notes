<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-10-09 Sun 15:35 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>tic</title>
<meta name="author" content="harrifeng@outlook.com" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">tic</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgd40a0a8">1. Introduction to this book</a>
<ul>
<li><a href="#org191c0e2">1.1. What is linear algebra and why learn it?</a></li>
<li><a href="#org64d5aa3">1.2. About this book</a></li>
<li><a href="#org225da84">1.3. Prerequisites</a></li>
<li><a href="#orge02880a">1.4. Practice, exercises and code challenges</a></li>
<li><a href="#orgc170a7d">1.5. Online and other resources</a></li>
</ul>
</li>
<li><a href="#orge491c6c">2. Vectors</a>
<ul>
<li><a href="#org7ec187e">2.1. Scalars</a></li>
<li><a href="#org4349862">2.2. Vectors: geometry and algebra</a></li>
<li><a href="#org0f90fde">2.3. Transpose operation</a></li>
<li><a href="#orgca43ba6">2.4. Vector addition and subtraction</a></li>
<li><a href="#org4487dda">2.5. Vector-scalar multiplication</a></li>
</ul>
</li>
<li><a href="#orgcdae35e">3. Vector multiplications</a>
<ul>
<li><a href="#org6eae6c0">3.1. Vector dot product: Algebra</a></li>
<li><a href="#org04dbe57">3.2. Dot product properties</a>
<ul>
<li><a href="#org4e812fd">3.2.1. vector product和scalar的结合律</a></li>
<li><a href="#org89e0dba">3.2.2. vector product和vector的结合律</a></li>
<li><a href="#org7a65776">3.2.3. commutative property</a></li>
<li><a href="#orgedc5201">3.2.4. Distributive property</a></li>
</ul>
</li>
<li><a href="#orgb24d747">3.3. Vector dot product: Geometry</a></li>
<li><a href="#orgac22b4e">3.4. Algebraic and geometric equivalence</a></li>
<li><a href="#org23e262f">3.5. Linear weighted combination</a></li>
<li><a href="#org620c030">3.6. The outer product</a></li>
<li><a href="#org3dd0ace">3.7. Element-wise (Hadamard) vector product</a></li>
<li><a href="#orgbe81c82">3.8. Cross product</a></li>
<li><a href="#org899e795">3.9. Unit vectors</a></li>
</ul>
</li>
<li><a href="#org373cbe5">4. Vector Spaces</a>
<ul>
<li><a href="#org45824c1">4.1. Dimensions and fields in linear algebra</a></li>
<li><a href="#org8e9ca9f">4.2. Vector spaces</a></li>
<li><a href="#org559d8e2">4.3. Subspaces and ambient spaces</a></li>
<li><a href="#orgbe3a458">4.4. Subsets</a></li>
<li><a href="#org0d24c15">4.5. Span</a></li>
<li><a href="#org11a7879">4.6. Linear independence</a></li>
<li><a href="#org1123071">4.7. Basis</a></li>
</ul>
</li>
<li><a href="#org53db85a">5. Matrices</a>
<ul>
<li><a href="#org4773731">5.1. Interpretations and uses of matrices</a></li>
<li><a href="#orgfbee675">5.2. Matrix terminology and notation</a></li>
<li><a href="#org24022f8">5.3. Matrix dimensionalities</a></li>
<li><a href="#org806576e">5.4. The transpose opertion</a></li>
<li><a href="#orgc3f83ad">5.5. Matrix zoology</a></li>
<li><a href="#org42b284a">5.6. Matrix addition and subtraction</a></li>
<li><a href="#org3f5092a">5.7. Scalar-matrix multiplication</a></li>
<li><a href="#org2d4e7b0">5.8. "Shifting" a matrix</a></li>
<li><a href="#org94871be">5.9. Diagonal and trace</a></li>
</ul>
</li>
<li><a href="#org8247cbe">6. Matrix multiplication</a>
<ul>
<li><a href="#orge37b9cf">6.1. "Standard" matrix multiplication</a>
<ul>
<li><a href="#org92659a4">6.1.1. The "element perspective"</a></li>
<li><a href="#org973580f">6.1.2. The "layer perspective"</a></li>
<li><a href="#org7da8d68">6.1.3. The "column perspective"</a></li>
<li><a href="#org896f0f8">6.1.4. The "row perspective"</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgd40a0a8" class="outline-2">
<h2 id="orgd40a0a8"><span class="section-number-2">1.</span> Introduction to this book</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org191c0e2" class="outline-3">
<h3 id="org191c0e2"><span class="section-number-3">1.1.</span> What is linear algebra and why learn it?</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>线性代数是数学中关于vector和matrix的分支</li>
<li>在现代,线性代数的重要性得到加强,因为很多数据都是以matrix的形势存储的,比如:
<ul class="org-ul">
<li>统计学</li>
<li>机器学习</li>
<li>计算机图形学</li>
<li>压缩算法</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org64d5aa3" class="outline-3">
<h3 id="org64d5aa3"><span class="section-number-3">1.2.</span> About this book</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>本书对机器学习爱好者很有益处</li>
<li>本书仅仅需要高中数学知识</li>
<li>对于希望了解了线性代数之后进行深度学习,统计学的人来说,太过于抽象的线性代数学习比较浪费时间</li>
<li>本书注重实践,而不是理论</li>
<li>本书是一本数学书,所以请不要奇怪书中有公式.但是数学不仅仅是公式:
<ul class="org-ul">
<li>在我看来,数学的目的是理解概念</li>
<li>公式是展示概念的一种方式</li>
<li>但是文章,图片,甚至是代码都非常重要</li>
</ul></li>
<li>公式和其他表现形式有个微妙的平衡:
<ul class="org-ul">
<li>公式提供了正规而严格的表现形式,但是无法提供直觉力</li>
<li>其他表达形式(文章,类比,图表,代码)提供了直觉力,但是不够严格和正规</li>
</ul></li>
<li>本书的公式按照重要性分为三个等级:
<ol class="org-ol">
<li>简单的,或者是为了回忆之前讨论过的公式.那么就是优先度最低的公式,他们会和文本在一块,比如 \(x(yz) = (xy)z\)</li>
<li><p>
更加重要的公式,会有自己单独的行
</p>
\begin{equation}
\sigma = x(yz) = (xy)z\tag{1.1}
\end{equation}</li>
<li>最最重要的公式会有自己的区域来说明
<ul class="org-ul">
<li><p>
公式1.2
</p>
\begin{equation}
\sigma = x(yz) = (xy)z\tag{1.2}
\end{equation}</li>
<li>这个公式的要点1</li>
<li>这个公式的要点2</li>
</ul></li>
</ol></li>
<li>线性代数的很多概念可以使用如下两种数学分支的公式来表示:
<ul class="org-ul">
<li>Geometric: 几何方法,优点是提供图形化的直观展示,缺点是人类只能理解2D和3D的图像</li>
<li>Algebraic: 代数方法,优点是严谨的证明和计算机的介入,可以非常容易的扩展到N维</li>
</ul></li>
<li>注意,并不是所有的线性代数概念都可以使用几何和代数法来展示</li>
</ul>
</div>
</div>
<div id="outline-container-org225da84" class="outline-3">
<h3 id="org225da84"><span class="section-number-3">1.3.</span> Prerequisites</h3>
<div class="outline-text-3" id="text-1-3">
<ul class="org-ul">
<li>需要有学习线性代数的主动性</li>
<li>需要有高中数学基础</li>
<li>不需要有微积分知识</li>
<li>不需要任何线性代数知识,知道矩阵的计算肯定有好处</li>
<li>在计算机发明以前,数学里面的高阶概念,通常都是天才们依靠自己"能够把公式想象成图像"的能力来理解的,
现在有了计算机,我们可以享受到天才们的超能力了</li>
<li>本书使用Matlab(Octave)和Python来解决问题,其中Matlab更为容易实现线性代数</li>
</ul>
</div>
</div>
<div id="outline-container-orge02880a" class="outline-3">
<h3 id="orge02880a"><span class="section-number-3">1.4.</span> Practice, exercises and code challenges</h3>
<div class="outline-text-3" id="text-1-4">
<ul class="org-ul">
<li>为了真正理解线性代数,必须做题</li>
<li>本书习题不多,目的是希望你全部都做完</li>
<li>本书习题分为三类:
<ul class="org-ul">
<li>Practice problem: 在subsection之后的,easy级别,答案就在后面,如果做不出来,
那么不需要继续向前读</li>
<li>Exercise: 在chapter之后的,中等难度,答案就在后面,需要手算,而不是用计算机算</li>
<li>Codechallenges: 需要使用计算机编程来实现的,比较难,也有答案</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgc170a7d" class="outline-3">
<h3 id="orgc170a7d"><span class="section-number-3">1.5.</span> Online and other resources</h3>
<div class="outline-text-3" id="text-1-5">
<ul class="org-ul">
<li>本书中的解释如果你理解不了,可以从网络上搜索从其他角度的解释来让你明白</li>
<li>本书有配套网络课程,喜欢网络课程学习方法的可以关注</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orge491c6c" class="outline-2">
<h2 id="orge491c6c"><span class="section-number-2">2.</span> Vectors</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org7ec187e" class="outline-3">
<h3 id="org7ec187e"><span class="section-number-3">2.1.</span> Scalars</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>我们不是从向量(vector)开始,而是从标量(scalar)开始</li>
<li>所谓标量(scalar)就是一个单独的数字,比如4或者-17.3等等</li>
<li>在数学的其他领域,标量有时候会被称之为常量(constant)</li>
<li>标量虽然简单,但是在线性代数里面却扮演者很多重要的角色:
<ul class="org-ul">
<li>subspaces</li>
<li>linear combination</li>
<li>eigendecomposition</li>
</ul></li>
<li>标量的名字(scalar)是scale的名词形式:
<ul class="org-ul">
<li>scale就有伸展,拉长的意思</li>
<li>scalar就有伸展拉长vector和metrix,并且不改变他们的方向(direction)</li>
</ul></li>
<li>标量在图上线上就是线上的一个空心的point,比如下图中的scalar就是一个1.5</li>
<li>注意:本书中标量都使用希腊小写字母( \(\lambda, \alpha, \gamma\) ),以便和vector和matrix区分</li>
<li><p>
使用python来表示标量,就是一个变量
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #f8f8f0;">aScalar</span> = 5
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org4349862" class="outline-3">
<h3 id="org4349862"><span class="section-number-3">2.2.</span> Vectors: geometry and algebra</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li><b>Geometry</b> vector是一个line,由两个属性决定:
<ul class="org-ul">
<li>magnitude(长度)</li>
<li>direction(方向)</li>
</ul></li>
<li>line可以在任意维度存在(1维,2维,3维,&#x2026;N维)</li>
<li>如图
<ul class="org-ul">
<li><p>
图2-2
</p>

<div id="org3ff56d9" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/2-2.png" alt="2-2.png" />
</p>
<p><span class="figure-number">Figure 1: </span>tic/2-2.png</p>
</div></li>
<li>上图左边是在2维空间的vector[2,3]</li>
<li>上图右边是在3维空间的vector[2,3,5]</li>
</ul></li>
<li>需要注意的是,vector的定义不包含它的起止位置(position)的,这是和坐标系不同的地方</li>
<li>在坐标系里面,每个坐标都是在空间中唯一的</li>
<li>从另外一个角度上讲,如果假设vector是从[0,0]开的话,那么vector和coordinate就是同一回事了.</li>
<li><p>
所以,起点(英文叫tail,注意是尾巴的意思,英文认为终点的是箭头,起点是尾巴)为[0,0]的vector被叫做在他
的standard position
</p>
<pre class="example" id="org2feaa2d">
A vector with its tail at the origin is said to be in its standard position
</pre></li>
<li>如图
<ul class="org-ul">
<li><p>
图2-3
</p>

<div id="orgca62971" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/2-3.png" alt="2-3.png" />
</p>
<p><span class="figure-number">Figure 2: </span>tic/2-3.png</p>
</div></li>
<li>上图中三个vector(line)都是相同的,因为他们的长度和方向都一样</li>
<li>上图中的三个坐标(圆圈)都是不相同的,因为坐标本来就全局唯一,没有两个一样的坐标</li>
<li>比较黑的line就是vector in its standard position. 这种情况下的vector[1,-2]的head和坐标[1,-2]相重叠</li>
</ul></li>
<li><p>
使用如下代码画vector
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">import</span> numpy <span style="color: #26a6a6;">as</span> np
<span style="color: #26a6a6;">import</span> matplotlib.pyplot <span style="color: #26a6a6;">as</span> plt

<span style="color: #f8f8f0;">v</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>2, -1<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
plt.plot<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>0, v<span style="color: #98be65;">[</span>0<span style="color: #98be65;">]</span><span style="color: #c678dd;">]</span>, <span style="color: #c678dd;">[</span>0, v<span style="color: #98be65;">[</span>1<span style="color: #98be65;">]</span><span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
plt.axis<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>-3, 3, -3, 3<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
plt.show<span style="color: #51afef;">()</span>
</pre>
</div></li>
<li><b>Algebra</b> 从代数的角度上说,vector就是一个ordered list(成员是number)</li>
<li>一个vector内部number的数量就叫做vector的dimensionality,比如:
<ul class="org-ul">
<li><p>
2D的vector例子
</p>
<pre class="example" id="orge7ba249">
[1 -2], [4 1], [10000 0]
</pre></li>
<li><p>
3D的vector例子
</p>
<pre class="example" id="org973db5f">
[3.14 e 0], [3 1 4], [2 -7 8]
</pre></li>
</ul></li>
<li>vector内部number的顺序是非常重要的,不同的顺序代表不同的vector,比如下面两个vector就不同,虽然他们
的dimensionality一样,数据也一样:
<ul class="org-ul">
<li>[3 1]</li>
<li>[1 3]</li>
</ul></li>
<li><b>Brackets</b> vector可以使用square bracket(方括号)或者是parentheses(园括号)</li>
<li>我个人认为方括号更加优雅,也不容易混淆,所以一直用方括号</li>
<li>但是有些情况下,你可能会遇到使用圆括号来代替方括号,比如下面两者在这种情况下是等价的:
<ul class="org-ul">
<li>[2 5 5]</li>
<li>(2 5 5)</li>
</ul></li>
<li>vector的几何表示,在2D表达中非常有用,在3D表达中也马马虎虎,但是更多维度就不行了</li>
<li><p>
vector的代数表示,却可以让我们在任何维度上,扩展vector,比如下面的公式就非常清晰的解释了什么是6D vector
</p>
<pre class="example" id="org5d2585c">
[3 4 6 1 -4 5]
</pre></li>
<li><p>
vector成员也不仅限于number,其成员还可以是function,比如下面的例子
</p>
\begin{equation}
\mathbf{v} = [\cos(t)\; \sin(t)\; t]
\end{equation}</li>
<li>本书不讨论上面的情况,本书中vector的所有成员都是普通number</li>
<li><b>Vector orientation</b> vector可以"站着",也可以"躺着":
<ul class="org-ul">
<li><p>
站着的vector被叫做column vector,如下
</p>
\begin{equation}
\left[ {\begin{array}{cccc}
7 \\
3 \\
5 \\
0 \\
\end{array} } \right]
\end{equation}</li>
<li><p>
躺着的vector被叫做row vector,如下
</p>
\begin{equation}
[0 \;1 \;3]
\end{equation}</li>
</ul></li>
<li><b>IMPORTANT</b> 默认情况下,vector是column orientation的,原因可能是在和matrix进行相乘的时候,vector在
matrix右边(作为被乘matrix,一个某个方向上只有一维的matrix)才有意义, 一般matrix都是MxN的大小,那么
在matrix右边,必须是Nx1,而不能是1xN的形状</li>
<li><p>
在matrix中,使用空格分离是row vector, 使用`;`分离,是column vector
</p>
<pre class="example" id="org4883043">
v1 = [2 5 4 7] % row vector
v2 = [2; 5; 4; 7] % column vector
</pre></li>
<li><p>
在python中, list(以及numpy array)没有默认的orientation,所以在某些情况下一定要指定orientation的
时候,numpy要用比较麻烦的方式实现
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #f8f8f0;">v1</span> = <span style="color: #51afef;">[</span>2, 5, 4, 7<span style="color: #51afef;">]</span>               <span style="color: #5B6268;"># list</span>
<span style="color: #f8f8f0;">v2</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>2, 5, 4, 7<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>     <span style="color: #5B6268;"># array, no orientation</span>
<span style="color: #f8f8f0;">v3</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span><span style="color: #98be65;">[</span>2<span style="color: #98be65;">]</span>, <span style="color: #98be65;">[</span>5<span style="color: #98be65;">]</span>, <span style="color: #98be65;">[</span>4<span style="color: #98be65;">]</span>, <span style="color: #98be65;">[</span>7<span style="color: #98be65;">]</span><span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span> <span style="color: #5B6268;"># column vector</span>
<span style="color: #f8f8f0;">v4</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span><span style="color: #98be65;">[</span>2, 5, 4, 7<span style="color: #98be65;">]</span><span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>       <span style="color: #5B6268;"># row vector</span>
</pre>
</div></li>
<li><b>Notation</b> 在书面书写中,我们只需要boldface字母就可以表示vector了,比如 \(\mathbf{v}\), 但是如果是论
文中,我们一定需要在vector上面加上剪头,比如 \(\vec{\mathbf{v}}\)</li>
<li>为了表达vector里面的一个特定成员,我们会使用下标,比如 \(\mathbf{v} = [4\;0\;2]\), 的第二个成员表示
为 \(v_2 = 0\), 第ith个表示为 \(v_i\) ,注意这里的小写字母没有加粗</li>
<li><p>
如果小写字母加粗的下划线加i,也就是 \(\mathbf{v_i}\) 那么表示相关的如下vectors
</p>
\begin{equation}
(\mathbf{v_1},\mathbf{v_2},...,\mathbf{v_i})
\end{equation}</li>
<li><b>Zeros vector</b>, 所有成员都是0的vector叫做zeros vector,注意是所有成员,缺一个都不叫zeros vector</li>
<li>zeros vector有一些特殊的地方,比如:
<ul class="org-ul">
<li>zeros 没有direction, 我的意思不是说它的direction为0,我是说它的direction未知(undefined),因为
zeros vector的magnitude为0,讨论一个magnitude为0的vector的direction是没有意义的</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org0f90fde" class="outline-3">
<h3 id="org0f90fde"><span class="section-number-3">2.3.</span> Transpose operation</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>把vector在column vector和row vector之间相互转换的操作,叫做transpose</li>
<li>transpose只更改orientation,其他的element内容和排序都不变</li>
<li>我们使用一个上标T来代表这个操作,那么就有如下三个例子:
<ul class="org-ul">
<li><p>
row vector转换成 column vector
</p>
\begin{equation}
 [7\;3\;5]^T  = \left[ {\begin{array}{cccc}
7 \\
3 \\
5 \\
\end{array} } \right]
\end{equation}</li>
<li><p>
column vector转换成 row vector
</p>
\begin{equation}
\left[ {\begin{array}{cccc}
7 \\
3 \\
5 \\
\end{array} } \right]^T = [7\;3\;5]
\end{equation}</li>
<li><p>
两次TT操作,可以抵消
</p>
\begin{equation}
[7\;3\;5]^{TT}=[7\;3\;5]
\end{equation}</li>
</ul></li>
<li>我们之前说过,我们assume, vector是column vector,所以:
<ul class="org-ul">
<li>\(\mathbf{v}\) 就是column vector</li>
<li>\(\mathbf{v}^T\) 就是row vector</li>
</ul></li>
<li>在印刷书籍中,在文字间写column vector非常不方便,所以文字书籍中往往是把column vector写成row vector
的转置形式,比如 \(\mathbf{w} = [1\;2\;3]^T\)</li>
<li><p>
在代码中转置很方便
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">import</span> numpy <span style="color: #26a6a6;">as</span> np

<span style="color: #f8f8f0;">v1</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span><span style="color: #98be65;">[</span>2, 5, 4, 7<span style="color: #98be65;">]</span><span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>  <span style="color: #5B6268;"># row vector</span>
<span style="color: #f8f8f0;">v2</span> = v1.T  <span style="color: #5B6268;"># column vector</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-orgca43ba6" class="outline-3">
<h3 id="orgca43ba6"><span class="section-number-3">2.4.</span> Vector addition and subtraction</h3>
<div class="outline-text-3" id="text-2-4">
<ul class="org-ul">
<li>*Geometry*我们主要通过下面的四个图来理解vector的加和减
<ul class="org-ul">
<li><p>
图2-5
</p>

<div id="orgcbb3371" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/2-5.png" alt="2-5.png" />
</p>
<p><span class="figure-number">Figure 3: </span>tic/2-5.png</p>
</div></li>
<li>第一幅图是介绍我们这次加减法的两个成员:
<ol class="org-ol">
<li>v1: [0 2]</li>
<li>v2: [1 1]</li>
</ol></li>
<li>第二幅图是介绍如何计算加法: 把v2的起点从standard position移动到v1的终点,那么从v1的起点到v2的终
点,就是新的vector</li>
<li>第三幅图是介绍减法的第一种做法,就把v2乘以-1,变成[-1,-1], 那么v1-v2就成了v1 + (-1 * v2),加法计算
方法和图二一致</li>
<li>第四幅图是介绍减法的第二种做法,就是从被减数的终点(作为起点)引出一条vector,终点是减数的终点,其
实就是v2 - v1 = v3 转换成v2 = v1 + v3</li>
</ul></li>
<li><p>
vector的加法满足交换律,也就是说
</p>
\begin{equation}
\mathbf{a}  + \mathbf{b} = \mathbf{b} + \mathbf{a}
\end{equation}</li>
<li><b>Algebra</b> 加法和减法的代数解释那就简单了,就是相对应的element进行加或者减:
<ul class="org-ul">
<li><p>
比如
</p>
\begin{equation}
[1\;2] + [3\;4] = [4\;6]
\end{equation}</li>
<li><p>
用公式来解释就是
</p>
\begin{equation}
\mathbf{c} = \mathbf{a} + \mathbf{b} = [a_1 + b_1 \; a_2 + b_2 \;...\; a_n + b_n]^T
\end{equation}</li>
</ul></li>
<li><b>Important</b> 加法和减法有意义的前提是参与运算的两个vector有同样的维度</li>
</ul>
</div>
</div>
<div id="outline-container-org4487dda" class="outline-3">
<h3 id="org4487dda"><span class="section-number-3">2.5.</span> Vector-scalar multiplication</h3>
<div class="outline-text-3" id="text-2-5">
<ul class="org-ul">
<li><b>Geometry</b> Scaling一个vector,就是:
<ul class="org-ul">
<li>增加或者减少这个vector的长度</li>
<li>并且不改变这个vector的angle</li>
</ul></li>
<li>scalar multiplication也不会改变原始的orientation</li>
<li>当然,如果scalar为0的话,最后的结果全部变成0,但是这种情况下,vector转换成了一个point,我们不能说point
有任何的的angle</li>
<li><b>Algebra</b> Scalar-vector的乘法,就是把vector的每个成员都乘以scalar</li>
<li><p>
对于scalar \(\lambda\) 和 vector \(\mathbf{v}\) , 我们有如下的公式
</p>
\begin{equation}
\lambda \mathbf{v} = [\lambda \mathbf{v}_1 \; \lambda \mathbf{v}_2 \; ... \; \lambda \mathbf{v}_n]^T \tag{2.3}
\end{equation}</li>
<li><p>
一个简单的例子如下
</p>
<pre class="example" id="orge6a396b">
3 [-1 3 0 2] = [-3 9 0 6]
</pre></li>
<li><p>
scalar-vector multipleication满足交换律,也就是说
</p>
\begin{equation}
\lambda \mathbf{v} =  \mathbf{v} \lambda
\end{equation}</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgcdae35e" class="outline-2">
<h2 id="orgcdae35e"><span class="section-number-2">3.</span> Vector multiplications</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>有四种方法来对两个vector进行乘法:
<ul class="org-ul">
<li>dot product</li>
<li>outer product</li>
<li>element-wise multiplication</li>
<li>cross product</li>
</ul></li>
<li>其中最重要的也是我们讲的最多的,就是dot product</li>
</ul>
</div>
<div id="outline-container-org6eae6c0" class="outline-3">
<h3 id="org6eae6c0"><span class="section-number-3">3.1.</span> Vector dot product: Algebra</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>dot product也叫做inner product, 是线性代数里面最重要的操作</li>
<li>dot product是如下高级操作的基础:
<ul class="org-ul">
<li>convolution(卷积)</li>
<li>correlation</li>
<li>Fourier transform</li>
<li>matrix multiplication</li>
<li>signal filtering</li>
</ul></li>
<li>dot product是使用一个number来提供两个vector之间relationship的方法</li>
<li><p>
由于两个vector的dot product结果是一个scalar, 所以dot product又称之为scalar product
</p>
<pre class="example" id="org54bcc1a">
注意,是scalar product,而不是scalar-vector product
</pre></li>
<li>至于inner product,这是在"非欧几里得空间"里面对dot product的命名,在欧几里何空间,我们可以认为inner
product和dot product等价.</li>
<li>inner product和dot product(scalar product)的实际关系如下</li>
<li>本书只使用dot product这一个称呼</li>
<li>从几何角度上来说,计算dot product,只需要如下两步:
<ul class="org-ul">
<li>把两个vector对应的N个element相乘,得到N个数字</li>
<li>把这N个数字相加</li>
</ul></li>
<li><p>
dot product的过程可以使用如下公式表达,注意,公式中中间三个是对dot product的三种表达方式(我们经常
使用的是 \(\mathbf{a}^T\mathbf{b}\),因为这个体现了矩阵乘法的原理)
</p>
\begin{equation}
\alpha = \mathbf{a} \cdot \mathbf{b} = \left \langle \mathbf{a}, \mathbf{b} \right \rangle =\mathbf{a}^T \mathbf{b} = \sum_{i=1}^n a_i b_i \tag{3.1}
\end{equation}</li>
<li><p>
我们举个例子来计算一下
</p>
<pre class="example" id="org61bfc12">
[1 2 3 4] * [5 6 7 8] = 1*5 + 2*6 + 3*7 + 4*8
                      = 5 + 12 + 21 + 32
                      = 70
</pre></li>
<li>由于dot product计算过程的特性,那么我们需要dot product参与的两个vector都是相同的dimensionality</li>
<li>vector和它自己的dimensionality肯定是相同的,所以,我们可以计算vector和它自己的dot product
<ul class="org-ul">
<li><p>
这个操作可以在公式3.2中显示
</p>
\begin{equation}
\mathbf{a}^T\mathbf{a} = \left \| \mathbf{a} \right \|^2 = \sum_{i=1}^n a_i a_i = \sum_{i=1}^n a_i^2 \tag{3.2}
\end{equation}</li>
<li>\(\left \| \mathbf{a} \right \|\) 叫做 vector \(\mathbf{a}\) 的length, magnitude或者是norm</li>
<li>vector自己和自己dot product的结果是\(\left \| \mathbf{a}^2 \right \|\) , 其实就是 vector的length-squared,
magnitude-squared或者是sauared-norm</li>
</ul></li>
<li><p>
使用如下代码计算dot product
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #f8f8f0;">v1</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>2, 5, 4, 7<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">v2</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>4, 1, 0, 2<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">dp</span> = np.dot<span style="color: #51afef;">(</span>v1, v2<span style="color: #51afef;">)</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org04dbe57" class="outline-3">
<h3 id="org04dbe57"><span class="section-number-3">3.2.</span> Dot product properties</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li><b>Associative property</b> 我们想看看结合律是否在dot production上面适用,需要从两个角度看,必须两个角
度都满足才能说dot production 满足结合律</li>
</ul>
</div>
<div id="outline-container-org4e812fd" class="outline-4">
<h4 id="org4e812fd"><span class="section-number-4">3.2.1.</span> vector product和scalar的结合律</h4>
<div class="outline-text-4" id="text-3-2-1">
<ul class="org-ul">
<li>这种情况其实就是scalar-vector multiplication嵌套在dot product里面</li>
<li>这种情况下显然满足结合律,因为scalar和每个vector的结果都是"维度不变(长度变化)的新vector"</li>
<li><p>
用公式表达,就是公式3.3是成立的.
</p>
\begin{equation}
\gamma(\mathbf{u}^T \mathbf{v}) = (\gamma \mathbf{u}^T) \mathbf{v} = \mathbf{u}^T (\gamma \mathbf{v}) = (\mathbf{u}^T \mathbf{v})\gamma \tag{3.3}
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-org89e0dba" class="outline-4">
<h4 id="org89e0dba"><span class="section-number-4">3.2.2.</span> vector product和vector的结合律</h4>
<div class="outline-text-4" id="text-3-2-2">
<ul class="org-ul">
<li>先说结论,这种情况下的结合律是不满足的</li>
<li><p>
用公式表达,就是公式3.4是不成立的!
</p>
\begin{equation}
\mathbf{u}^T(\mathbf{v}^T \mathbf{w}) = (\mathbf{u}^T\mathbf{v})^T \mathbf{w}
\end{equation}</li>
<li>要想理解这个不可能,我们可以从很多方向来理解:
<ul class="org-ul">
<li>我们首先假设三个vector的维度相同,那么我们会发现,上面公式的左右两边甚至都不是dot product,因为
<ol class="org-ol">
<li>左边是row vector \(\mathbf{u}^T\) 和一个scalar(两个vector相乘得到的)相乘, 所以左边是一个row vector</li>
<li>右边是一个scalr(两个vector相乘得到的)和一个column vector相乘,所以右边是一个 column vector(注
意对于scalar来说 \(4^T = 4\)</li>
<li><p>
其实不仅仅是row vector和column vector的不一样,他们的成员其实也有可能是不一样的,比如下面的例子
</p>
\begin{equation}
\mathbf{u} = \begin{bmatrix}
             1 \\
             2 \\
             \end{bmatrix} ,
\mathbf{v} = \begin{bmatrix}
             1 \\
             3 \\
             \end{bmatrix} ,
\mathbf{w} = \begin{bmatrix}
             2 \\
             3 \\
             \end{bmatrix}
\end{equation}</li>
<li><p>
左边的结果为
</p>
\begin{equation}
\mathbf{u}^T(\mathbf{v}^T \mathbf{w}) =
             \begin{bmatrix}
             1 \; 2 \\
             \end{bmatrix}
             \left(
             \begin{bmatrix}
             1 \; 3 \\
             \end{bmatrix}
             \begin{bmatrix}
             2 \\
             3 \\
             \end{bmatrix}
             \right)
             =
             \begin{bmatrix}
             11 \; 22 \\
             \end{bmatrix} \tag{3.5}
\end{equation}</li>
<li><p>
右边的结果为,可见,显然和左边的不一样,不仅orientation不一样,element维度也不一样
</p>
 \begin{equation}
( \mathbf{u}^T\mathbf{v})^T \mathbf{w} =
              \left(
              \begin{bmatrix}
              1 \; 2 \\
              \end{bmatrix}
              \begin{bmatrix}
              1 \\ 3 \\
              \end{bmatrix}
              \right)^T
              \begin{bmatrix}
              2 \\
              3 \\
              \end{bmatrix}
              =
              \begin{bmatrix}
              14 \\ 21 \\
              \end{bmatrix} \tag{3.6}
 \end{equation}</li>
</ol></li>
<li>如果这三个vector的维度不同,那么甚至有一边的计算都是invalid的,都不用考虑是否相等了</li>
</ul></li>
<li>综上所述,我们可以得到结论,就是vector dot product不遵守结合律.(但是,matrix的乘法遵守结合律,所以
后面不要和这里混淆)</li>
</ul>
</div>
</div>
<div id="outline-container-org7a65776" class="outline-4">
<h4 id="org7a65776"><span class="section-number-4">3.2.3.</span> commutative property</h4>
<div class="outline-text-4" id="text-3-2-3">
<ul class="org-ul">
<li><p>
dot product 满足交换律,用公式表达如下
</p>
\begin{equation}
\mathbf{a}^T \mathbf{b} = \mathbf{b} \mathbf{a}^T \tag{3.7}
\end{equation}</li>
<li><p>
dot product 满足交换律是很显然的事情,因为dot production是在element维度完成的,两element的相乘,
其实就是两个scalar的乘积,而scalar乘法是符合交换律的(如公式3.8),那么我们也可以说dot product也是
符合交换律的
</p>
\begin{equation}
\sum_{i=1}^na_ib_i = \sum_{i=1}^nb_ia_i \tag{3.8}
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-orgedc5201" class="outline-4">
<h4 id="orgedc5201"><span class="section-number-4">3.2.4.</span> Distributive property</h4>
<div class="outline-text-4" id="text-3-2-4">
<ul class="org-ul">
<li>首先抛出结论: dot product是符合分配率的,符合分配率这件事情能让"代数表达"和"几何表达"联系起来</li>
<li><p>
分配率可以用如下公式解释:(当然了,这里的vector必须维度相同)
</p>
\begin{equation}
\mathbf{w}^T (\mathbf{u} + \mathbf{v}) = \mathbf{w}^T \mathbf{u} + \mathbf{w}^T \mathbf{v} \tag{3.9}
\end{equation}</li>
<li>分配率说的是这么一个事儿:我们可以把一个dot product分成两个dot product的和,只需要把那个vector拆
成两个就好了</li>
<li>当然了也可以反过来用,假设两个vector都和同一个vector相乘,而这两vector的维度一样,那么就可以先把
这两个vector加起来</li>
<li>我们可以用一个例子来加深我们的理解:
<ul class="org-ul">
<li><p>
假设三个vector如下
</p>
\begin{equation}
\mathbf{u} = \begin{bmatrix}
             1 \\
             2 \\
             \end{bmatrix} ,
\mathbf{v} = \begin{bmatrix}
             1 \\
             3 \\
             \end{bmatrix} ,
\mathbf{w} = \begin{bmatrix}
             2 \\
             3 \\
             \end{bmatrix}
\end{equation}</li>
<li><p>
公式左边的计算结果是19
</p>
\begin{equation}
\mathbf{w}^T(\mathbf{u} + \mathbf{v}) =
             \begin{bmatrix}
             2 \; 3 \\
             \end{bmatrix}
             \left(
             \begin{bmatrix}
             1 \\ 2 \\
             \end{bmatrix}
+
              \begin{bmatrix}
              1 \\
              3 \\
              \end{bmatrix}
              \right)
              =
              \begin{bmatrix}
              2 \; 3 \\
              \end{bmatrix}
              \times
              \begin{bmatrix}
              2 \\
              5 \\
              \end{bmatrix}
              = 19
              \tag{3.11}
\end{equation}</li>
<li><p>
公式右边的计算结果也是19
</p>
\begin{equation}
\mathbf{w}^T \mathbf{u} + \mathbf{w}^T \mathbf{v} =
             \begin{bmatrix}
             2 \; 3 \\
             \end{bmatrix}
             \begin{bmatrix}
             1 \\ 2 \\
             \end{bmatrix}
+
              \begin{bmatrix}
              2 \; 3 \\
              \end{bmatrix}
              \begin{bmatrix}
              1 \\
              3 \\
              \end{bmatrix}
              =
              8 + 11
              = 19
              \tag{3.12}
\end{equation}</li>
</ul></li>
<li>下面我们把结合律应用到一种特殊的情况,那就是vector自己分成两个sub_vector, 然后这两个sub_vector再
乘以自己.</li>
<li><p>
由于分配率的存在,我们可以得到如下的等式
</p>
\begin{align}
(\mathbf{u} + \mathbf{v})^T(\mathbf{u} + \mathbf{v}) &= \| \mathbf{u} + \mathbf{v} \|^2 \\
                                                     &= \mathbf{u}^T\mathbf{u} + 2\mathbf{u}^T\mathbf{v} + \mathbf{v}^T\mathbf{v} \\
                                                     &= \| \mathbf{u} \|^2 + \| \mathbf{v} \|^2 + 2\mathbf{u}^T\mathbf{v}
\end{align}</li>
<li>上述公式是连接"代数解释"和"几何解释"之间的桥梁</li>
<li><p>
<b>Cauchy-Schwarz inqauality</b> 柯西-斯瓦茨不等式提供了两个vector进行dot product的上限,,不等式如下
</p>
\begin{equation}
| \mathbf{v}^T \mathbf{w} | \leq \| v \| \| w \| \tag{3.14}
\end{equation}</li>
<li><p>
用英语来说,上面的不等式就是说
</p>
<pre class="example" id="org234a684">
两个vector的dot product的magnitude不会比这两个vector magitude的product值大
</pre></li>
<li>这个不等式的等于会在下面的情况下得到满足的时候,出现:
<ul class="org-ul">
<li>一个vector是另外一个vector的scaled version,也就是说 \(\mathbf{v} = \lambda \mathbf{w}\)</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb24d747" class="outline-3">
<h3 id="orgb24d747"><span class="section-number-3">3.3.</span> Vector dot product: Geometry</h3>
<div class="outline-text-3" id="text-3-3">
<ul class="org-ul">
<li><p>
从几何上来说
</p>
<pre class="example" id="orge6a235d">
dot product就是两个vector之间的cosinec乘以这两个vector
</pre></li>
<li>其实,从几何和代数的方向上看这个公式,其实是使用不同的方式来表达同样的concept</li>
<li><p>
dot product 的几何定义
</p>
\begin{equation}
\mathbf{a}^T \mathbf{b} = \| \mathbf{a} \| \| b \| \cos(\theta_{ab}) \tag{3.15}
\end{equation}</li>
<li>如果上面的vector长度为1(也就是 \(\| \mathbf{a} \| = \| b \| = 1\) ),那么dot product的结果也就变成
了vector的cosine值</li>
<li>公式3.15可以转为如下两种写法:
<ul class="org-ul">
<li><p>
求cosine值
</p>
\begin{equation}
\cos(\theta_{ab}) = \cfrac{\mathbf{a}^T \mathbf{b}}{\| \mathbf{a} \| \| b \|}\tag{3.16}
\end{equation}</li>
<li><p>
求vector之间的角度
</p>
\begin{equation}
\theta_{ab} = \cos^{-1}\left(\cfrac{\mathbf{a}^T \mathbf{b}}{\| \mathbf{a} \| \| b \|}\right)\tag{3.17}
\end{equation}</li>
</ul></li>
<li>公式3.17意义非凡,在一个2D空间求两个vector之间夹角的方法,放到3维,4维,甚至更高维度都是同样成立的</li>
<li><p>
如果只考虑dot product的正负,那么我们可以得到一个结论:
</p>
<pre class="example" id="org05a833e">
dot product的正负只由两个vector之间的夹角来决定
</pre></li>
<li>原因很简单 dot product是cosine乘以两个vector的长度,长度都是正数,那么它的乘积也是正数,在判断结果
正负的时候,就可以不考虑他们</li>
<li>根据vector之间的角度,我们可以把dot product分成五种,我们以 \(\theta\) 代表vector之间的角度, \(\alpha\)
代表dot product的结果:
<ol class="org-ol">
<li>\(\theta < 90^{\circ} \rightarrow \alpha > 0\): 锐角的cosine总是正数,所以dot product也是正数</li>
<li>\(\theta > 90^{\circ} \rightarrow \alpha < 0\): 钝角的cosine总是负数,所以dot product也是负数</li>
<li>\(\theta = 90^{\circ} \rightarrow \alpha = 0\): 直角的cosine是0,所以dot product也是0.这种情况下
非常重要,所以有一个自己的名字: <b>orthogonal</b> (正交), 而且正交还有一个特殊的符号,如果两个vector
是正交的,那么,我们可以使用如下的公式表示 \(\mathbf{w} \bot \mathbf{v}\)</li>
<li>\(\theta = 0^{\circ} \rightarrow \alpha = \| \mathbf{a} \| \| \mathbf{b} \|\): 0度角的cosine是1
所以,dot product就是两个vector长度的乘积,这种情况也有一个单独的名字: <b>collinear</b> (共线)</li>
<li>\(\theta = 180^{\circ} \rightarrow \alpha = -\| \mathbf{a} \| \| \mathbf{b} \|\):180度角的cosine
是-1,所以dot product是两个vector长度的乘积再乘以-1,这种情况也叫共线</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgac22b4e" class="outline-3">
<h3 id="orgac22b4e"><span class="section-number-3">3.4.</span> Algebraic and geometric equivalence</h3>
<div class="outline-text-3" id="text-3-4">
<ul class="org-ul">
<li>对于dot product的解释,几何解释和代数解释非常的不同,但是实质上是一样的.我们本节就是
来讨论这个问题的</li>
<li><p>
下面公式的3.18,中间是代数解释,右边是几何解释
</p>
\begin{equation}
\mathbf{a}^T \mathbf{b} = \sum_{i=1}^n \mathbf{a}_i \mathbf{b}_i = \| \mathbf{a} \| \| \mathbf{b} \| \cos(\theta_{ab}) \tag{3.18}
\end{equation}</li>
<li>论证代数和几何表达的内涵统一性:
<ul class="org-ul">
<li>首先要了解dot product是满足交换律和分配率的</li>
<li>其次要了解Law of Cosine</li>
</ul></li>
<li>勾股定理(Pythagorean theorem)是大家非常熟悉的 \(\mathbf{a}^2 + \mathbf{b}^2 = \mathbf{c}^2\)</li>
<li><p>
勾股定理其实是Law of Cosine的一个特例
</p>
\begin{equation}
\mathbf{c}^2 = \mathbf{a}^2 + \mathbf{b}^2 - 2\mathbf{a}\mathbf{b} \cos \theta_{ \mathbf{a} \mathbf{b}}
\end{equation}</li>
<li>下面是证明dot product代数和结合表示内涵相同的过程:
<ul class="org-ul">
<li><p>
从代数方向上看,由余弦定理计算 \(\mathbf{c}^2\) 得到:
</p>
\begin{equation}
\mathbf{c}^2 = \mathbf{a}^2 + \mathbf{b}^2 - 2\mathbf{a}\mathbf{b} \cos (\theta_{ \mathbf{a} \mathbf{b}})
\end{equation}</li>
<li>从几何方向上看, 我们可以把vector \(\mathbf{c}\) 看成是 vector \(\mathbf{a}\) 和 vector \(\mathbf{b}\)
之间的差值:
<ol class="org-ol">
<li>原始方程 $ \| c \| = \| a - b \|$</li>
<li><p>
我们也去求 \(\mathbf{c}\) 的平方
</p>
\begin{align}
\| \mathbf{a} - \mathbf{b} \| &= ( \mathbf{a} - \mathbf{b} )^T ( \mathbf{a} - \mathbf{b} )   \\
            &= \mathbf{a} ^T \mathbf{a} - 2 \mathbf{a} ^T \mathbf{b} + \mathbf{b} ^T \mathbf{b} \\
            &= \| \mathbf{a} \|^2 + \| \mathbf{b} \|^2 - 2 \mathbf{a} ^T \mathbf{b} \\
\end{align}</li>
<li><p>
综合前面代数方向得到的 \(\mathbf{c}^2\),就得到
</p>
\begin{equation}
\| \mathbf{a} \|^2 + \| \mathbf{b} \|^2 - 2 \mathbf{a} ^T \mathbf{b} = \| \mathbf{a} \|^2 + \| \mathbf{b} \|^2  - 2 \| \mathbf{a} \| \| \mathbf{b} \| \cos \theta
\end{equation}</li>
<li><p>
两边消掉共同项,就得到
</p>
\begin{equation}
\mathbf{a} ^T \mathbf{b} = \| \mathbf{a} \| \| \mathbf{b} \| \cos \theta \tag{3.15}
\end{equation}</li>
</ol></li>
</ul></li>
<li><b>Proof of Cauchy-Schwarz inequality</b> 一旦得到了公式3-15这个结论,那么柯西不等式的证明就很简单了:
<ul class="org-ul">
<li><p>
由于如下等式成立
</p>
\begin{equation}
\mathbf{a} ^T \mathbf{b} = \| \mathbf{a} \| \| \mathbf{b} \| \cos \theta \tag{3.33}
\end{equation}</li>
<li>又由于cosine区间是[0,1]</li>
<li><p>
所以得以证明柯西不等式
</p>
\begin{equation}
\mathbf{a} ^T \mathbf{b} \le \| \mathbf{a} \| \| \mathbf{b} \| \tag{3.34}
\end{equation}</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org23e262f" class="outline-3">
<h3 id="org23e262f"><span class="section-number-3">3.5.</span> Linear weighted combination</h3>
<div class="outline-text-3" id="text-3-5">
<ul class="org-ul">
<li>线性加权组合由于非常常见,所以我们也单独给了他一个章节</li>
<li>线性加权组合还有其他常见的名字,比如:
<ul class="org-ul">
<li>linear mixture</li>
<li>weighted combination</li>
<li>linear coefficient combination</li>
</ul></li>
<li>线性加权组合其实就是:
<ul class="org-ul">
<li>scalar和vector相乘, 得到新的vector</li>
<li>新得到的N个vector再相加</li>
<li>这里就要assume所有的vector的维度相同</li>
<li><p>
公式如下
</p>
\begin{equation}
\mathbf{w} = \lambda_1 \mathbf{v}_1 + \lambda_2 \mathbf{v}_2 + \cdot\cdot\cdot + \lambda_n \mathbf{v}_n \tag{3.35}
\end{equation}</li>
</ul></li>
<li><p>
使用代码来实现linear weighted combination非常容易
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">import</span> numpy <span style="color: #26a6a6;">as</span> np

<span style="color: #f8f8f0;">l1</span> = 1
<span style="color: #f8f8f0;">l2</span> = 2
<span style="color: #f8f8f0;">l3</span> = -3
<span style="color: #f8f8f0;">v1</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>4, 5, 1<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">v2</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>-4, 0, -4<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">v3</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>1, 3, 2<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">ret</span> = l1 * v1 + l2 * v2 + l3 * v3
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[ret] ==&gt;"""</span>, ret<span style="color: #51afef;">)</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[ret] ==&gt; [ -7  -4 -13]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org620c030" class="outline-3">
<h3 id="org620c030"><span class="section-number-3">3.6.</span> The outer product</h3>
<div class="outline-text-3" id="text-3-6">
<ul class="org-ul">
<li>所谓outer product,就是一种把两个vector合并,产生一个matrix的操作</li>
<li>outer product 最容易让人难以理解的地方,就是表达符号,实在是和dot product太像了(注意这里 \(\mathbf{v}\)
是M-element column vector, \(\mathbf{m}\) 是N-element column vector ):
<ul class="org-ul">
<li><p>
我们首先看看dot product(注意,在dot product里, V和M必须相等)
</p>
\begin{equation}
\mathbf{v}^T \mathbf{w} = 1 \times 1
\end{equation}</li>
<li><p>
再来看看out product,只不过是带T的放到了后面
</p>
\begin{equation}
\mathbf{v} \mathbf{w}^T = M \times N
\end{equation}</li>
<li><p>
从上帝视角,我们其实学过矩阵乘法了,从矩阵乘法的角度其实可以理解最后为什么结果一个是scalar,一个
是matrix,因为
</p>
<pre class="example" id="org5963ac8">
Dot product and outer product are special cases of matrix multiplication.
</pre></li>
</ul></li>
<li>我们后面会从三个角度来理解outer product:
<ul class="org-ul">
<li>element perspective</li>
<li>column perspective</li>
<li>row perspective</li>
</ul></li>
<li><b>Element perspetive</b>, 从element的角度考虑,每个对于outer product矩阵里面的成员 element_ij, 其是
如下两个element的scalar multilication值:
<ul class="org-ul">
<li>第一个vector的ith element</li>
<li>第二个vector的jth element</li>
</ul></li>
<li><p>
由此,我们可以总结出element perspective的公式:
</p>
\begin{equation}
(\mathbf{v} \mathbf{w}^T)_{i,j} = v_i w_j\tag{3.36}
\end{equation}</li>
<li><p>
下面是使用字母替代数字来显示outer product的过程
</p>
\begin{equation}
\begin{bmatrix}
a \\
b \\
c \\
\end{bmatrix}
\begin{bmatrix}
d e f  \\
\end{bmatrix}
=
\begin{bmatrix}
ad \;ae \;af \;\\
bd \;be \;bf \;\\
cd \;ce \;cf \;\\
\end{bmatrix}
\end{equation}</li>
<li><b>Column perspective</b> 我们再来从column 维度来看看每个outer product是怎么构成的,从column纬度看outer
product, outer product里面的每一列,都可以看做是一个scalar-vector multiplication:
<ul class="org-ul">
<li>其中,vector是左边的column vector(每次重复使用)</li>
<li>另外,scalar是右边的row vector里面每次取一个</li>
<li>所以,outer product的column的数目和右侧row vector的个数相同</li>
<li><p>
同时,outer product的每个column都是left column vector 的scaled version
</p>
<pre class="example" id="org102f3da">
Each column of the outer product matrix is a scaled version of the left column vector
</pre></li>
</ul></li>
<li><b>Row perspective</b>, 其实类比column perspective就可以得出结论了,outer product里面的每一行,都可以
看做是一个scalar-vector multiplication:
<ul class="org-ul">
<li>其中,vector是右边的row vector(每次重复使用)</li>
<li>另外,scalar是左边的column vector里面每次取一个</li>
<li>所以,outer product的row数目和左侧的left vector的个数相同</li>
<li>同时,outer product的每个row都是right row vector的scaled version</li>
</ul></li>
<li>如果swap 两个vector的order(注意swap里之后,在左边的还是要以column vector的形式,右边的还是要以row
vector的形式),那么我们就会发现新的两个matrix非常像,只不过row和column也给swap了</li>
<li><p>
<b>Code</b> 使用如下代码来完成out product的计算,顺便看一下上一条说的matrix的swap现象
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">import</span> numpy <span style="color: #26a6a6;">as</span> np

<span style="color: #f8f8f0;">v1</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>2, 5, 4, 7<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">v2</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>4, 1, 0, 2<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">op</span> = np.outer<span style="color: #51afef;">(</span>v1, v2<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span>op<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">op</span> = np.outer<span style="color: #51afef;">(</span>v2, v1<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span>op<span style="color: #51afef;">)</span>

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[[ 8  2  0  4]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[20  5  0 10]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[16  4  0  8]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[28  7  0 14]]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[[ 8 20 16 28]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 2  5  4  7]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 0  0  0  0]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 4 10  8 14]]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org3dd0ace" class="outline-3">
<h3 id="org3dd0ace"><span class="section-number-3">3.7.</span> Element-wise (Hadamard) vector product</h3>
<div class="outline-text-3" id="text-3-7">
<ul class="org-ul">
<li><p>
所谓Element-wise vector product,就是两个一样的vector的每个位置上的scalar相乘,得到一个和前两者一
样的新vector,公式如下
</p>
\begin{equation}
\mathbf{c} = \mathbf{a} \odot \mathbf{b} = [a_1 b_1 \; a_2 b_2 \; \cdot\cdot\cdot a_n b_n] \tag{3.37}
\end{equation}</li>
<li>这个操作其实不算是线性代数的操作,更像是一组有序的scalar multiplication</li>
<li><p>
<b>Code</b> 计算element-wise multiplication的代码如下
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">import</span> numpy <span style="color: #26a6a6;">as</span> np

<span style="color: #f8f8f0;">v1</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>2, 5, 4, 7<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">v2</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>4, 1, 0, 2<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">v3</span> = v1 * v2
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[v3] ==&gt;"""</span>, v3<span style="color: #51afef;">)</span>

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[v3] ==&gt; [ 8  5  0 14]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-orgbe81c82" class="outline-3">
<h3 id="orgbe81c82"><span class="section-number-3">3.8.</span> Cross product</h3>
<div class="outline-text-3" id="text-3-8">
<ul class="org-ul">
<li>cross product的限定条件是:
<ul class="org-ul">
<li>参与者必须是3维vector</li>
<li>结果自然也是3维vector</li>
</ul></li>
<li><p>
计算公式如下
</p>
\begin{equation}
\mathbf{a} \times \mathbf{b} = \begin{bmatrix}
                        a_2 b_3  - a_3 b_2\\
                        a_3 b_1  - a_1 b_3\\
                        a_1 b_2  - a_2 b_1\\
                        \end{bmatrix} \tag{3.38}
\end{equation}</li>

<li><p>
cross product的magnitude等于参与乘法的两个vector的magnitude再乘以两者的sin
</p>
\begin{equation}
\| \mathbf{a} \times \mathbf{b} \| = \| \mathbf{a} \| \| \mathbf{b} \| \sin(\theta_{ab}) \tag{3.39}
\end{equation}</li>
<li>cross product主要应用在几何领域(而不是线性代数领域),主要作用是创建一个vector \(\mathbf{c}\) , 使得
能够正交于 vector \(\mathbf{a}\)  和 vector \(\mathbf{b}\) 所定义的平面</li>
<li>cross product主要用于多变量微积分,而对于如下行业却完全不适用,所以我们后面不会再提及这个概念,这
里写出来是为了知识的完整性:
<ul class="org-ul">
<li>data analysis</li>
<li>statistics</li>
<li>machine-learning</li>
<li>signal-processing</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org899e795" class="outline-3">
<h3 id="org899e795"><span class="section-number-3">3.9.</span> Unit vectors</h3>
<div class="outline-text-3" id="text-3-9">
<ul class="org-ul">
<li>拥有长度为1的vector( \(\| \mathbf{v} \| = 1\)  ), 有时候是非常重要的</li>
<li>长度为1的vector就叫做unit vector</li>
<li>unit vector还可以用来创建特殊的matrix: orthogonal matrix</li>
<li>本节主要是要我们设计公式来求一个普通vector的unit vector:
<ul class="org-ul">
<li>这个普通vector并不一定是unit vector</li>
<li>求出啦的unit vector和这个普通vector的方向是一样的,只不过长度为1</li>
</ul></li>
<li><p>
求unit vector其实就是求一个scalar \(\mu\) ,使得其满足如下公式
</p>
\begin{equation}
\mu \mathbf{v} \; s.t. \; \| \mu \mathbf{v} \| = 1 \tag{3.40}
\end{equation}</li>
<li>我们求得 \(\mu\) 的值之后,就可以计算unit vector啦, \(\mu\) 的值其实就是:1除以普通vector的magnitude</li>
<li><p>
公式如下,其中unit vector使用一个hat标识 \(\hat{\mathbf{v}}\)
</p>
\begin{equation}
\hat{\mathbf{v}} = \cfrac{1}{\| \mathbf{v} \|} \mathbf{v} = \cfrac{1}{\sqrt{\sum_{i=1}^n v_i^2}} \mathbf{v} \tag{3.41}
\end{equation}</li>
<li><p>
一个例子如下
</p>
\begin{equation}
\mathbf{v}  = \begin{bmatrix}
          0 \\
          2 \\
         \end{bmatrix},
\hat{\mathbf{v}} = \cfrac{1}{\sqrt{0^2 + 2^2}}
\begin{bmatrix}
0 \\
2 \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
1 \\
\end{bmatrix}
\end{equation}</li>
<li>注意,求某个普通vector的unit vector的时候, 普通vector的长度必须不是0</li>
<li><p>
<b>Code</b> 代码如下
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">import</span> numpy <span style="color: #26a6a6;">as</span> np

<span style="color: #f8f8f0;">v</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>2, 5, 4, 7<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">vMag</span> = np.linalg.norm<span style="color: #51afef;">(</span>v<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[vMag] ==&gt;"""</span>, vMag<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">v_unit</span> = v / vMag
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[v_unit] ==&gt;"""</span>, v_unit<span style="color: #51afef;">)</span>

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[vMag] ==&gt; 9.695359714832659</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[v_unit] ==&gt; [0.20628425 0.51571062 0.4125685  0.72199487]</span>
</pre>
</div></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org373cbe5" class="outline-2">
<h2 id="org373cbe5"><span class="section-number-2">4.</span> Vector Spaces</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org45824c1" class="outline-3">
<h3 id="org45824c1"><span class="section-number-3">4.1.</span> Dimensions and fields in linear algebra</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li><b>Dimension</b> 前面我们说过,dimension的定义很简单,就是vector里面有几个数字,就是几个dimension</li>
<li>但是由于这个概念太重要,值的我们从更详细的维度进行定义:
<ul class="org-ul">
<li><b>Algebraically</b>:
<ol class="org-ol">
<li>从代数的角度,dimension就是vector里面元素的个数</li>
<li>而且顺序很重要,比如vector [3, 1, 5]里面的,在第二个dimension的element是1</li>
<li>同时,不同dimension上的数字之间是平等的,没有哪个dimension比另外的dimension重要</li>
</ol></li>
<li><b>Geometrically</b>:
<ol class="org-ol">
<li>从几何角度上讲,纬度就是vectro所在的坐标轴系里面的坐标数目</li>
<li>在2D space里面, 2D vector是一个line</li>
<li>在3D space里面, 3D vector也是一个line</li>
<li>2D vectro, 3D vectro,甚至ND vector都是line,只不过dimension不同</li>
</ol></li>
</ul></li>
<li>如图
<ul class="org-ul">
<li>图4-3</li>
<li>我们往往会从正交的笛卡尔平面(正交坐标系)来理解vector,但是其实坐标系也有非平面的,如上图</li>
</ul></li>
<li><b>Field</b>, 在数学中,field是指一系列的number, 在这些number上面定义了最基本的算数操作(加减乘除等)</li>
<li>常见的field有:
<ul class="org-ul">
<li>\(\mathbb{R}\) 代表实数</li>
<li>\(\mathbb{C}\) 代表复数</li>
<li>\(\mathbb{Q}\) 代表有理数</li>
</ul></li>
<li>本书主要使用field来表示dimension,比如:
<ul class="org-ul">
<li>\(\mathbb{R}^4\) 代表一个四维vector</li>
<li>如果这个四维vector的样子如[a b c d], 那么,a,b,c,d这四个数字都是实数</li>
</ul></li>
<li>有些时候,具体dimension是多少不重要,但是需要和其他dimension进行比较,这个时候可以用大写字母来代替
dimension,比如,如果说如下两个vector能够计算dot product的唯一可能是 \(M = N\) :
<ul class="org-ul">
<li>\(\mathbf{w}\) in \(\mathbb{R}^M\)</li>
<li>\(\mathbf{v}\) in \(\mathbb{R}^N\)</li>
</ul></li>
<li>再计算机上,如果没有latex,我们的field也可以写成如下两种形式:
<ul class="org-ul">
<li>R2</li>
<li>R^6</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org8e9ca9f" class="outline-3">
<h3 id="org8e9ca9f"><span class="section-number-3">4.2.</span> Vector spaces</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>所谓的vector space,是指一系列对象,这些对象上面定义了addition 和 scalar multiplication</li>
<li>addition和scalar multiplication遵守如下的公理:
<ul class="org-ul">
<li>Additive inverse: \(\mathbf{v} + ( - \mathbf{v}) = 0\)</li>
<li>Associativity(结合律): \((\mathbf{v} + \mathbf{w}) + \mathbf{u} = \mathbf{v} + (\mathbf{w} + \mathbf{u})\)</li>
<li>Commutativity(交换律): \(\mathbf{v} + \mathbf{w} = \mathbf{w} + \mathbf{v}\)</li>
<li>Additive identity: \(\mathbf{v} + \mathbf{0} = \mathbf{v}\)</li>
<li>Multiplicative identity: \(\mathbf{v1} = \mathbf{v}\)</li>
<li>Distributivity(分配率): \((\alpha + \beta)( \mathbf{v} + \mathbf{w}) = \alpha \mathbf{v} + \alpha \mathbf{w} + \beta \mathbf{v} + \beta \mathbf{w}\)</li>
</ul></li>
<li>vector space其实在本书几乎不会再次被提及,之所以在这里介绍vector space,是为了不让大家把他和另外
一个更加重要的概念混淆,那就是下面要讲的vector subspace</li>
</ul>
</div>
</div>
<div id="outline-container-org559d8e2" class="outline-3">
<h3 id="org559d8e2"><span class="section-number-3">4.3.</span> Subspaces and ambient spaces</h3>
<div class="outline-text-3" id="text-4-3">
<ul class="org-ul">
<li>vector subspace是所有线性代数高级概念的核心,其同样具有代数和几何两种方向上的理解</li>
<li><p>
<b>Geometry</b> 从几何上来说,subspace是一系列point的集合,这些point可以通过延长(scalar multiplication)
并且组合(addition)一系列的vector来达到
</p>
<pre class="example" id="orgfb1dc75">
A subspace is the set of all points that you can reach by stretching and combining
a collection of vector(that is, addition and scalar multiplication)
</pre></li>
<li>我们下面从一个简单的例子来解释下vector subspace的概念:
<ul class="org-ul">
<li>图4-4</li>
<li>我们图中黑色的线是从原点到一个vector \(\mathbf{v}\) = [-1 2]的线</li>
<li>vector \(\mathbf{v}\) = [-1 2] 自己不是一个subspace,但是通过乘以一个区间是[-inf, inf]的 \(\lambda\),
可以触及到这个黑色线定义的"线路"上,无数的不同vector,也就是这个gray dashed line</li>
<li>我们把gray dashed line定义成一个1D subspace</li>
<li>这个1D subspace,之所以叫做1D subspace,因为它生活在一维世界里面,注意不要和vector的维度相混淆,
vector是2D vector,但是这个vector和一个scalar配合,产生了一个1D subspace</li>
<li>我们最后来契合一下subspace的定义: 显然我们这里的线上有无数的point(vector),他们都可以通过scalar
multiplication操作来生成,生成后把这些point收集起来就形成了这个1D subspace</li>
</ul></li>
<li>现在我们知道了,由一个vector单打独斗的结果是一个1D subspace(infinitely long line)</li>
<li>那么很显然,如果有两个vector(不能是共线的)的情况下:
<ul class="org-ul">
<li>至少能得到两条infinitely long line</li>
<li>然后再结合scaling以及adding,那么我们会得到非常多的新的point</li>
<li>比如下图中的圆点,就是我们使用如下scaling和adding获得的 \(v_1 + .5v_2\)</li>
<li><p>
图4-5
</p>

<div id="org9b95400" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/4-5.png" alt="4-5.png" />
</p>
<p><span class="figure-number">Figure 4: </span>tic/4-5.png</p>
</div></li>
</ul></li>
<li>实际上,对两个vector实行了scaling和adding之后,得到的points其实组成了一个新的2D subspace</li>
<li>之前的1D subspace是一条长度无限线, 那么这里的2D subspace就是一个没有边界的平面(plane)了</li>
<li>在2D上面(图4-5)没有边界的平面就是整个2D坐标空间,没有办法形象的展示"两个vector组成一个plane"这件
事,我们使用下图来展示在3D空间中两个3维vector(注意vector可以在任意空间)组成的一个2D subspace(plane)</li>
<li><p>
图4-6
</p>

<div id="org37428cb" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/4-6.png" alt="4-6.png" />
</p>
<p><span class="figure-number">Figure 5: </span>tic/4-6.png</p>
</div></li>
<li><b>Ambient dimensionality</b> 图4-6同时也展示了如下两个概念的区别:
<ul class="org-ul">
<li>subspace</li>
<li>ambient space (subspace嵌入在ambient space里面)</li>
</ul></li>
<li>在图4-6这个例子中, subspace是2维的,而它所嵌入的ambient space则是3维的</li>
<li>ambient space的维度N则是vector的维度,也就是 \(\mathbb{R}^N\) 里面的N</li>
<li>所以在一个ambient space里面有多少个subspace?
<ul class="org-ul">
<li>答案很显然,对于维度超过1的ambient space来说,其拥有的subspace数目是无限多的</li>
</ul></li>
<li>问一个不太显然的问题:对于一个N维的ambient space来说,其所拥有的subspace分属于几个维度?
<ul class="org-ul">
<li>答案是N+1</li>
<li>我们以 \(\mathbb{R}^3\) 为例来数一下:
<ol class="org-ol">
<li>最小的subspace是一个0维的point,也就是[0 0 0] (注意所有subspace必须通过 vector \(\mathbf{0}\),
对三维vector来说就是[0 0 0])</li>
<li>接下来是1维的line,在三维空间里面,所有通过原点的line,都是一个subspace.在三维空间有无数的一
维subspace</li>
<li>再来是2维的plane,在三维空间里面,所有通过原点的plane,都是一个subspace.在三维空间有无数的二
维subspace</li>
<li>最后是3维的subspace, ambient space自己就是自己的3维subspace.在三维空间,只有一个3维subspace,
就是ambient space自己</li>
</ol></li>
</ul></li>
<li>最后一个问题是:所有的两个vector都能组成一个plane么?
<ul class="org-ul">
<li>答案是否定的</li>
<li>如果两个vector在同一个line上面,那么他们显然无法组成一个新的plane.这是我们通过直觉得到的判断</li>
<li>在后面,我们会学到,如果两个vector能够组成一个新的plane,那么我们说这两个vector是linear independence的</li>
</ul></li>
<li><b>Algebra</b> 除了额外的公式以外, 代数定义和几何定义都是一样的,只不过代数定义因为有公式,所以显得更
加正式一点</li>
<li>我们通常使用斜体的大写斜体字母来代替subspace,比如 \(V\)</li>
<li>需要注意的是,大写斜体字母还会用来指代a set of vectors, 所以遇到大写斜体字母的时候,你需要结合上
下文来判断其具体意思</li>
<li>在使用公式表述subspace之前,我们先用语言来定义一下subspace: 一个subspace是一系列point是的集合,而
且这些集合满足如下条件:
<ol class="org-ol">
<li>Closed under addition and scalar multiplication</li>
<li>Contains the zeros vector \(\mathbf{0}\)</li>
</ol></li>
<li>第二条很容易理解,就是所有的subspace,无论几维,都必须穿过原点</li>
<li>第一条里面的closed有点难以理解,其实意思就是:集合里的point经过特定的操作(也就是addition和scalar
multiplication)之后,还在集合里面</li>
<li>举个例子,假设有 \(\mathbf{v} \in V\):
<ul class="org-ul">
<li>如果 \(\mathbf{v}\) 乘以一个scalar \(\lambda\), 得到的结果还是在subspace \(V\)</li>
<li>如果 \(\mathbf{v}\) 乘以一个scalar \(\lambda\),再加上另外一个在subspace \(V\) 里面的另外一个vector
\(\alpha \mathbf{w}\), 得到的结果还是在subspace \(V\)</li>
</ul></li>
<li>经过前面的铺垫,我们终于可以使用公式来表达subspace的定义了:
<ul class="org-ul">
<li><p>
公式如下:
</p>
\begin{equation}
\forall \mathbf{v}, \mathbf{w} \in V, \forall \lambda, \alpha \in \mathbb{R}; \lambda \mathbf{v} + \alpha \mathbf{w} \in V, \tag{4.1}
\end{equation}</li>
<li>其中的 \(\forall\), 代表对所有</li>
<li>那么上面的例子翻译出来就是:对任意在subspace里面的vector \(\mathbf{v\;w}\),以及任意的实数 scalar
\(\lambda \; \alpha\), 所有对 \(\mathbf{v\;w}\) 的linearly weighted combination依然存在于subspace \(V\) 里面</li>
</ul></li>
<li>我们下面来看两个例子来加深对定义的理解:</li>
<li>第一个例子是我们知道有一个1D subspace \(V\),定义在3D ambient space \(\mathbb{R}^3\), 我们来判断某些
vector是否属于这个subspace:
<ul class="org-ul">
<li><p>
\(V\) 的定义如下
</p>
\begin{equation}
V = {\lambda [1\;3\;4], \lambda \in \mathbb{R} },\tag{4.2}
\end{equation}</li>
<li>第一个vector是[3 9 12], 它是属于 subspace $V$的,因为我们可以取 \(\lambda\) 为3</li>
<li>第一个vector是[-2 -6 -8], 它是属于 subspace $V$的,因为我们可以取 \(\lambda\) 为-2</li>
<li>但是对于第三个vector [1 3 5],我们就找不到一个 \(\lambda\) 来让他属于 \(V\) 啦,所以[1 3 5]不属于
subspace \(V\)</li>
</ul></li>
<li>第二个例子是一直有一些points,我们来判断这些points能否组成一个新的subspace,我们要看的这个例子描
述如下: 所有 \(\mathbb{R}^2\) 中y轴为正的points</li>
<li>我们来逐个分析看看这些points是否组成了一个新的subspace:
<ul class="org-ul">
<li>第二个条件比较容易判断,这鞋point包含原点[0 0]</li>
<li>再来看第一个条件,我们最终发现它是不满足条件的,因为我们可以举出一个反例:
<ol class="org-ol">
<li>\(\mathbf{v} = [2,3]\) 显然是在这一堆point里面的</li>
<li>我们找到一个实数 \(\lambda = -1\), 从而让 \(\lambda \mathbf{v}\) 不再这一堆point里面了</li>
</ol></li>
<li>综上,这一堆piont没有在scalar multiplication运算下闭合,所以这一堆point不是subspace</li>
<li>当然了我们也没白忙活,因为这一堆point其实是一个subset的例子,也就是我们下一节要介绍的概念</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgbe3a458" class="outline-3">
<h3 id="orgbe3a458"><span class="section-number-3">4.4.</span> Subsets</h3>
<div class="outline-text-3" id="text-4-4">
<ul class="org-ul">
<li>在线性代数里面,subset不是一个重要的概念,之所以拿出篇幅来介绍,是因为不想让如下两个概念相互混淆:
<ul class="org-ul">
<li>subset</li>
<li>subspace</li>
</ul></li>
<li>和subspace不同的两点是:
<ol class="org-ol">
<li>subset是一个(可以有,也可以没有)边界的region,而不是无限长的</li>
<li>subset不需要通过原点</li>
</ol></li>
<li>常见的subspace的例子有:
<ol class="org-ol">
<li>在XY平面里面所有满足x&gt;0并且y&gt;0的point</li>
<li>所有满足如下公式的点 \(4 > x > 2, y > x^2\)</li>
<li>满足如下公式的点 \(y = 4x\), x的取值范围是[-inf, inf]</li>
</ol></li>
<li>上面的第三个不仅仅是一个subset,而且还是一个1D subspace,满足1D subspace的定义: 无限长的线,并且通
过原点</li>
</ul>
</div>
</div>
<div id="outline-container-org0d24c15" class="outline-3">
<h3 id="org0d24c15"><span class="section-number-3">4.5.</span> Span</h3>
<div class="outline-text-3" id="text-4-5">
<ul class="org-ul">
<li><b>Geometry</b> span和subspace其实是非常相近的概念,我们可以使用语法的方式来区别两者:
<ul class="org-ul">
<li>subspace是一个名词: A subspace is the region of ambient space that can be reached by any
linear combination</li>
<li>span是一个动词: Thoose vectors span that subspace</li>
</ul></li>
<li><p>
总结起来可以说:
</p>
<pre class="example" id="org68e1c54">
A set of vectors spancs, and the result of their spanning is a subspace
</pre></li>
<li>举个例子, 所有的 \(\mathbb{R}^2\) 上的点定义的subspace,可以通过如下两个vector的span得到:
<ul class="org-ul">
<li>[0 1]</li>
<li>[1 0]</li>
</ul></li>
<li>另外两个例子:
<ul class="org-ul">
<li>vector [0 1] span了一个1D subspace</li>
<li>vector [1 2] 也span了一个1D subspace, 但是是和 vector [0 1] span出来的不是同一个subspace</li>
</ul></li>
<li><p>
<b>Algebra</b> 一系列vector的span的结果,是一个set, 包含所有的vector进行计算(linear weighted combination)
得到的点,公式如下
</p>
\begin{equation}
span(\{\mathbf{v_1},\cdot\cdot\cdot,\mathbf{v_n}\}) = \{\alpha_1 \mathbf{v_1} + \cdot\cdot\cdot + \alpha_n\mathbf{v_n}, \alpha \in \mathbb{R} \},\tag{4.3}
\end{equation}</li>
<li>上面公式的右边就是linear weighted combination,从公式3.35引入</li>
<li>span在matrix里面会用到很多,比如在matrix里面有个概念叫column space,其实就是matrix的column进行span
得到的subspace</li>
<li><p>
<b>In the span?</b> 在线性代数里面最常见的问题就是是否一个vector 是被其他多个vector span来的?
</p>
<pre class="example" id="orgd9b1595">
Whether one vector is "in the span" of another vector or set of vectors
</pre></li>
<li><p>
上面的数学说法,用大白话就是
</p>
<pre class="example" id="org5d42d14">
对于特定的vector w,你是否可以通过scalar-multiplying and adding 其他vector(从set S)来得到
</pre></li>
<li>举个例子:
<ul class="org-ul">
<li><p>
有Set s
</p>
\begin{equation}
S= \Biggl\{
   \begin{bmatrix}
   1 \\
   1 \\
   0 \\
   \end{bmatrix},
   \begin{bmatrix}
   1 \\
   7 \\
   0 \\
   \end{bmatrix}
   \Biggl\}
\end{equation}</li>
<li><p>
有vector \(\mathbf{v}\)
</p>
\begin{equation}
\mathbf{v} = \begin{bmatrix}
1 \\
2 \\
0 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
有vector \(\mathbf{w}\)
</p>
\begin{equation}
\mathbf{w} = \begin{bmatrix}
3 \\
2 \\
1 \\
\end{bmatrix}
\end{equation}</li>
</ul></li>
<li><p>
很显然, v在S的span中,因为可以得到
</p>
\begin{equation}
v \in span(S) \; because \; \begin{bmatrix}
                            1 \\
                            2 \\
                            0 \\
                            \end{bmatrix}
                           =
                           \cfrac{5}{6}
                           \begin{bmatrix}
                           1 \\
                           1 \\
                           0 \\
                           \end{bmatrix}
+
                             \cfrac{1}{6}
                             \begin{bmatrix}
                             1 \\
                             7 \\
                             0 \\
                             \end{bmatrix}

\end{equation}</li>
<li>所以 \(\mathbf{v}\) 其实就是 \(S\) 中的vector进行weighted combination的结果,其中的weighting数据为
5/6, 1/6</li>
<li>想要得到这些weights其实不是一件容易的事情,我们有算法来计算得到,但是现在还不能告诉你这个算法,因
为这个算法涉及的如下知识,我们还没有学到:
<ul class="org-ul">
<li>determinant</li>
<li>Gaussian elimination</li>
</ul></li>
<li>找到 \(\mathbf{w}\) 所对应的weights非常困难,但是我们却一眼可以看出 $\mathbf{w}$不在S的span里面:
<ul class="org-ul">
<li>因为, \(S\) 的第三个成员都是0,而 \(\mathbf{w}\) 的第三个参数是非0, 两个0无论用什么weight也构造不
出非0值</li>
<li><p>
图4-8
</p>

<div id="orge48b0f8" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/4-8.png" alt="4-8.png" />
</p>
<p><span class="figure-number">Figure 6: </span>tic/4-8.png</p>
</div></li>
<li>从上图可以看出, span of S其实是一个3D ambient space里面的2D plane</li>
<li>而 \(\mathbf{v}\) 则是这个plane里面的一个vector</li>
<li>虽然 \(\mathbf{w}\) 也是一个vector,但是它飞出了这个平面</li>
</ul></li>
<li><p>
还有一点要牢记在心,那就是set里面的某个vector也可以是set里面其他vector linear combination得来的
</p>
<pre class="example" id="org203f25d">
It doesn't matter if the vectors in the set are linear combinations of other vectors
in that same set.
</pre></li>
<li>比如:
<ul class="org-ul">
<li><p>
下面是一个valid set
</p>
\begin{equation}
\left \{
\begin{bmatrix}
1 \\
2 \\
0 \\
1 \\
\end{bmatrix},
\begin{bmatrix}
1 \\
2 \\
0 \\
1 \\
\end{bmatrix},
\begin{bmatrix}
1 \\
2 \\
0 \\
1 \\
\end{bmatrix},
\begin{bmatrix}
-3 \\
-6 \\
0 \\
3 \\
\end{bmatrix},
\begin{bmatrix}
10 \\
20 \\
0 \\
10 \\
\end{bmatrix},
\begin{bmatrix}
0 \\
4 \\
1 \\
0 \\
\end{bmatrix},
\begin{bmatrix}
0 \\
2 \\
5 \\
0 \\
\end{bmatrix}
\right \}
\end{equation}</li>
<li>这个set定义在ambient \(\mathbb{R}^4\)</li>
<li>但是这个set的前三个共线,后两个共线,所以它其实只组成了一个2D plane(通过两个独立的vector)</li>
</ul></li>
<li>我们可以把span理解成机器人以什么样的灵活度来挥舞激光:
<ul class="org-ul">
<li>只有一个vector的情况下, 机器人只能让激光照亮一个方向</li>
<li>有两个vector的情况下,机器人可以挥舞激光,从而形成一个平面</li>
<li>有三个vector的情况下,机器人可以在屋里随意的挥舞,如果有蚊子,那么最终都能打死那个蚊子(如果蚊子
不动的情况下)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org11a7879" class="outline-3">
<h3 id="org11a7879"><span class="section-number-3">4.6.</span> Linear independence</h3>
<div class="outline-text-3" id="text-4-6">
<ul class="org-ul">
<li>Linear independence(简称为indepence)对理解线性代数中的如下概念非常重要:
<ul class="org-ul">
<li>matrix rank</li>
<li>statistics</li>
<li>singular value decomposition</li>
</ul></li>
<li><p>
对于理解linear independence来说,最重要的是要理解到一个事实:线性无关是针对一组vector来说的
</p>
<pre class="example" id="orga7fb7fd">
Linear independence is a property of a set of vectors.
</pre></li>
<li>对于一个vector来说,linear independence与否无法成立</li>
<li><b>Geometry</b> 如果如下两个数值成立,那么参与的一系列vector就是independent的:
<ul class="org-ul">
<li>这一系列的vector span出来的subspace的dimensionality</li>
<li>这一系列的vector的数量</li>
</ul></li>
<li>换句话说:
<ul class="org-ul">
<li>含有两个vector的independent set中的所有vector能够span出来一个plane</li>
<li>含有三个vector的independent set中的所有vector能够span出来一个3D space</li>
</ul></li>
<li>但是反过来说,并不是说一个set of vectors里面有三个成员就一定能span一个3D space,所以,仅仅有三个成
员并不一定保证independent,我们用三个例子可以更清楚的理解这件事情:
<ul class="org-ul">
<li>图4-9</li>
<li>图A包含两个共线的vector,这个set of vector就是linearly dependent,因为你可以把一个vector
看做是另外的vector的scaled version</li>
<li>图B也包含两个vector,但是他们指向了不同方向,那么就不可能使用其中一个vector加个scalar就形成另外
一个</li>
<li>图C中包含了三个vector(都在 \(\mathbb{R}^2\) ),但是他们是linearly dependent的,因为任意一个vector
都能通过其他两个vector linear combination之后得到</li>
</ul></li>
<li>从4-9中的三个例子我们其实可以得到一个定理:
<ul class="org-ul">
<li>如果一个set中vector的数目M大于当前vector所在的维度N (\(\mathbb{R}^N\)), 那么这些vector必然linearly
dependent</li>
<li>如果一个set中vector的数目M大于d等于当前vector所在的维度N (\(\mathbb{R}^N\)), 那么这些vector有可
能linearly independent</li>
</ul></li>
<li>截止到本章学习的内容,不足以让我们完成对这个定理的证明,但是我们可以从直觉出发去理解这个问题:
<ul class="org-ul">
<li>比如定理第一部分,我们在4-9中的图C就可以证明这件事情,我们在2D subspace(N=)里面有三个vector(M=3),他
们无论如何无法创建出一个立方体(3D subspace),所以他们必然不可能做到linear independent</li>
<li><p>
当M&lt;=N的时候,就具备了linear indepedent的可能性,但是具体是否independent要看vectors自己的属性,
举个极端的例子
</p>
<pre class="example" id="org6d0f1cc">
在 ambient $\mathbb{R}^25$ subspace里面,有一个set of 20个vector,但是这20个vector都是
在同一个line里面,那么他们也就能组成一个1D subspace
</pre></li>
</ul></li>
<li>最后,我们可以给一个比较书面化的定义: 一系列vector是independent的,那么必须满足如下两个数值相等:
<ul class="org-ul">
<li>这系列里面vector的数目(number of vectors in the set)</li>
<li>这个set span出来的subspace的维度(dimensionality of the subspace spanned by that set)</li>
</ul></li>
<li><p>
<b>Algebra</b> 从代数的角度上讲, 一系列的vector如果dependent的条件是:
</p>
<pre class="example" id="org13f6c06">
至少一个vector (set中的)可以表示成其他vector的linear weighted combination形式
</pre></li>
<li>下面是两个dependent set的例子,每个vector里面都找到了能用其他vector combine(linear combination)成
自己的方法:
<ul class="org-ul">
<li><p>
\(\mathbf{w}_2\) 可以被 \(\mathbf{w}_1\) 表示
</p>
\begin{equation}
  \{\mathbf{w_1}, \mathbf{w_2} \} = \left \{
      \begin{bmatrix}
        1 \\
        2 \\
        3 \\
      \end{bmatrix},
      \begin{bmatrix}
        2 \\
        4 \\
        6 \\
      \end{bmatrix}
    \right \}
  . \mathbf{w_2} = 2\mathbf{w_1} \tag{4.4}
\end{equation}</li>

<li><p>
\(\mathbf{v}_2\) 可以被 \(\mathbf{v}_1\) \(\mathbf{v}_2\) 表示
</p>
\begin{equation}
 \{\mathbf{v_1}, \mathbf{v_2}, \mathbf{v_3} \} = \left \{
     \begin{bmatrix}
       0 \\
       2 \\
       5 \\
     \end{bmatrix},
     \begin{bmatrix}
       -27 \\
       5 \\
       -37 \\
     \end{bmatrix},
     \begin{bmatrix}
       3 \\
       1 \\
       8 \\
     \end{bmatrix}
   \right \}
 . \mathbf{v_2} = 7\mathbf{v_1} - 9\mathbf{v_3} \tag{4.5}
\end{equation}</li>
</ul></li>
<li>下面两个例子是linear independent set的例子:
<ul class="org-ul">
<li><p>
\(\mathbf{w}_2\) 可以被 \(\mathbf{w}_1\) 表示
</p>
\begin{equation}
  \{\mathbf{w_1}, \mathbf{w_2} \} = \left \{
      \begin{bmatrix}
        1 \\
        2 \\
        3 \\
      \end{bmatrix},
      \begin{bmatrix}
        2 \\
        4 \\
        7 \\
      \end{bmatrix}
    \right \}
\end{equation}</li>

<li><p>
\(\mathbf{v}_2\) 可以被 \(\mathbf{v}_1\) \(\mathbf{v}_2\) 表示
</p>
\begin{equation}
 \{\mathbf{v_1}, \mathbf{v_2}, \mathbf{v_3} \} = \left \{
     \begin{bmatrix}
       0 \\
       2 \\
       5 \\
     \end{bmatrix},
     \begin{bmatrix}
       -27 \\
       0 \\
       -37 \\
     \end{bmatrix},
     \begin{bmatrix}
       3 \\
       1 \\
       9 \\
     \end{bmatrix}
   \right \}
\end{equation}</li>
</ul></li>
<li>你会发现无论你怎样努力都找不到一个vector使用linear combination被其他vector所替代的例子</li>
<li>其实判断一个set of vector是不是linear independent在线性代数领域非常非常的重要,我们后面将学习如
何判断(叫做augment-rank),大致的思路如下:
<ul class="org-ul">
<li>把所有的vector放进一个matrix</li>
<li>计算matrix的rank(秩)</li>
<li>如果矩阵的秩和vector的数目一致,那么这个set的vectors是independent的,否则则是dependent的</li>
</ul></li>
<li>下面我们来用公式来定义一下linear dependence:
<ul class="org-ul">
<li><p>
公式如下
</p>
\begin{equation}
\mathbf{0} = \lambda_1 \mathbf{v_1} + \lambda_2 \mathbf{v_2} + \cdot\cdot\cdot + \lambda_n\mathbf{v_n}, \lambda \in \mathbb{R}, \tag{4.6}
\end{equation}</li>
<li>书面解释如下: 如果所有的vector能使用linear combination(参数不能都为0)得到zero vector,那么这些
vector是dependent的</li>
<li>如果 \(\lambda\) 都为0,那么显然所有的vector set都是zero vector,所以我们要排除这种情况</li>
<li>如果公式4.6不成立(至少一个 \(\lambda\) 为非0), 那么这个vector set就是linear independent的</li>
</ul></li>
<li>这个公式的定义看上去非常的拗口,但是其实你如果使用下面的例子就非常容易理解这个至少一非0 \(\lambda\)
是怎么来的:
<ul class="org-ul">
<li>假设 \(\lambda_1\) 就是那个非0的参数,后面会看到这个数会作为分母,所以必然不能为0</li>
<li><p>
我们从公式4-6左右都减去 \(\lambda_1 \mathbf{v_1}\) 得到
</p>
\begin{equation}
-\lambda_1 \mathbf{v_1} = \lambda_2 \mathbf{v_2} + \cdot\cdot\cdot + \lambda_n \mathbf{v_n}
\end{equation}</li>
<li><p>
我们左右都除以 \(-\lambda_1\) (使用 \(\lambda_{n1}\) 代替) 得到
</p>
\begin{equation}
  \mathbf{v_1} = \cfrac{\lambda_2}{\lambda_{n1}} \mathbf{v_2} + \cdot\cdot\cdot
+ \cfrac{\lambda_n}{\lambda_{n1}} \mathbf{v_n}, \lambda \in \mathbb{R}, \lambda_{n1} \neq 0, \tag{4.7}
\end{equation}</li>
</ul></li>
<li>公式4-6还揭示了一个有趣的属性,那就是: 一旦在一系列vector里面包含了zero vector,那么这系列的vector
必然是linear dependent的!</li>
<li>原因很简单,我们可以把zero vector的scalar设置为非0, 其他vector的scalar全部设置为了0,那么所有的linear
combination的结果必然为0.</li>
<li><b>Determining Whether a set is linearly dependent or indepent</b> 在我们学习matrix-based的算法来判断
一个系列的vector是否linear independent之前,我们先来看看一个比较笨四步判断法来判断一个系列的vector
是否是linear independent的.注意,这个方法只是为了加深理解概念,实践中不会使用:
<ol class="org-ol">
<li>第一步就是判断vector的数目M是否小于等于维度N(也就是vector的成员个数),只有M小于等于N才有可能independent</li>
<li>第二步检查所有的vector成员,有一个是zero vector,那么肯定是dependent</li>
<li>第三步就是检查某些vector中的0值,这些会对第四步起作用</li>
<li>第四步就是"一行行"的开始实验了,比如下面的这个例子:
<ul class="org-ul">
<li><p>
例子如下
</p>
\begin{equation}
\left \{
\begin{bmatrix}
1 \\
2 \\
3 \\
\end{bmatrix},
\begin{bmatrix}
2 \\
1 \\
3 \\
\end{bmatrix},
\begin{bmatrix}
4 \\
5 \\
8 \\
\end{bmatrix}
\right \}
\end{equation}</li>
<li>我们看第一行(1, 2, 4) 很容易想到 2*1+1*2 = 4,</li>
<li>然后第二行也符合这个scalar组合(2, 1), 2*2 +1*1 = 5</li>
<li>但是第三行就不符合了这个scalar组合(2,1)了, 2*3 + 1*3 = 9, 而不是8</li>
<li>所以这个vector的组合不是independent的</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org1123071" class="outline-3">
<h3 id="org1123071"><span class="section-number-3">4.7.</span> Basis</h3>
<div class="outline-text-3" id="text-4-7">
<ul class="org-ul">
<li>basis的定义是span和independence两个概念的组合:</li>
<li>一系列的在 \(\mathbb{R}^N\) 的vector如果满足下面的条件,就说他们组成了basis:
<ul class="org-ul">
<li>vector span了这个subsapce</li>
<li>并且这些个vector是independent的</li>
</ul></li>
<li>basis这个名字就能看出来,这个概念是用来描述一个space的基础和规则的,有了basis,你就能知道测量你的
space使用的基础单位(length, direction)了</li>
<li><p>
最常见的basis set是笛卡尔轴(也叫直角坐标系),只包含0和1
</p>
\begin{equation}
\mathbb{R}^2 : \left\{
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix},
\begin{bmatrix}
0 \\
1 \\
\end{bmatrix}
\right\},
\mathbb{R}^3 : \left\{
\begin{bmatrix}
1 \\
0 \\
0 \\
\end{bmatrix},
\begin{bmatrix}
0 \\
1 \\
0 \\
\end{bmatrix},
\begin{bmatrix}
0 \\
0 \\
1 \\
\end{bmatrix}
\right\}
\end{equation}</li>
<li>这类basis set非常常用因为他有如下优点:
<ul class="org-ul">
<li>每个vector都是unit length</li>
<li>所有的vector都是正交的(也就是说任意两个vector的dot product都是0)</li>
</ul></li>
<li>basis的定义很容易让大家误以为必须要span整个 \(\mathbb{R}^N\) 才能做basis,其实不是的,因为basis的定
义除了indepdent以外,主要关注的就是要vector span一个subspace,span subspace只需要通过原点就可以,
并不一定要充满整个 \(\mathbb{R}^N\)</li>
<li>所以下面两个也是basis sets:
<ul class="org-ul">
<li><p>
Set S1是一个independent set但是没有cover整个 \(\mathbb{R}^2\), 它只span了 \(\mathbb{R}^2\) 一个1D
subspace
</p>
\begin{equation}
S_1 = \left\{
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}
\right\}
\end{equation}</li>
<li>Set S2是一个XY轴的plane(2D)的basis</li>
</ul></li>
<li>下面再来讨论一下,为什么linearly independent非常重要.这是因为如果set成员vector不是independent的,
那么可能有好几种方法来组合成一个vector:
<ul class="org-ul">
<li><p>
假设我们的set如下,注意这不是一个basis for \(\mathbf{R}^2\)
</p>
\begin{equation}
\left\{
\begin{bmatrix}
1 \\
1 \\
\end{bmatrix},
\begin{bmatrix}
1 \\
3 \\
\end{bmatrix},
\begin{bmatrix}
0 \\
2 \\
\end{bmatrix}
\right\}
\end{equation}</li>
<li>如果我们想要得到vector[-2 6],那么可以有如下三种scalar组合:
<ol class="org-ol">
<li>(-6,4,0)</li>
<li>(0,2,6)</li>
<li>(-2,0,4)</li>
</ol></li>
<li>有多种可能性对于数学家来说是非常令人费解的,所以数学家决定basis里面的成员要想组成一个vector,必
须只能有一种组合,那么就只能要求set必须linearly independent了</li>
</ul></li>
<li><b>Infinite base</b> 虽然某个vector在特定的basis里面有unique的解,但是却存在无数的这种basis能够提供解,
实际上,任意linearly independent的2D vectors(穿过原点),都是 \(\mathbb{R}^2\) 的basis</li>
<li>那么为什么数学家又不烦了呢,为啥不规定只有一部分vector set来做basis呢?比如笛卡尔轴(直角坐标系).原因是因为:
<ul class="org-ul">
<li><p>
某些问题使用特定的basis更简单
</p>
<pre class="example" id="org23d3480">
Some problems are easier to solve in certain bases and harder to solve in other bases.
</pre></li>
<li>在multivariate data science, data compression等问题中,寻找最佳basis set是最重要的问题</li>
</ul></li>
<li><p>
除了线性代数,其他的科学里面也有basis的概念,其实basis就是一系列最小的能够表达其他事物的metrics
</p>
<pre class="example" id="org9fd1ca3">
A basis is the set of the minimum number of metrics needed to describe something.
</pre></li>
<li>比如在傅里叶转换中,正弦波就是basis function,因为所有的信号都能被解析为不同的正弦波(频率,周期,振
幅不同的正弦波)的组合</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org53db85a" class="outline-2">
<h2 id="org53db85a"><span class="section-number-2">5.</span> Matrices</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org4773731" class="outline-3">
<h3 id="org4773731"><span class="section-number-3">5.1.</span> Interpretations and uses of matrices</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>本章主要是用来为矩阵入门,了解:
<ul class="org-ul">
<li>矩阵长什么样子</li>
<li>如何代表矩阵</li>
<li>一些重要的,且特殊的矩阵</li>
<li>基本的矩阵运算</li>
</ul></li>
<li>本书我们讨论的矩阵是 rows 乘以 column类型的,换句话说,就是你可以把他们打印在table里面的矩阵(比如
3x2, 7x8)</li>
<li>注意,这种'rows乘以column'类型的矩阵其实不代表这个矩阵是2D矩阵,因为他们可以被解释成存在于更高纬度</li>
<li>矩阵也可以不是在纸上的table就可以画出来的,而是需要3D打印机的那种.那种叫做tensor,我们本书不讨论tensor</li>
<li>下面就是两个矩阵的例子:</li>
<li>我们可以把矩阵理解为:
<ul class="org-ul">
<li>一系列column vector挨个站着</li>
<li>或者是一系列的row vector一个叠一个</li>
</ul></li>
<li>矩阵的应用非常广泛,下面是常见的矩阵应用:
<ul class="org-ul">
<li>表达线性转换或者线性映射</li>
<li>存储多变量系统的偏导数</li>
<li>代表system of equation</li>
<li>存储数据</li>
<li>代表统计模型里面的回归量(regressor)</li>
<li>为图形学存储几何转换</li>
<li>在卷积中存储kernel</li>
<li>存储金融信息</li>
<li>存储模型的参数(parameter for a model)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgfbee675" class="outline-3">
<h3 id="orgfbee675"><span class="section-number-3">5.2.</span> Matrix terminology and notation</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>我们首先来熟悉一下矩阵的术语(terminology):
<ul class="org-ul">
<li>当我们指代一个矩阵的时候,我们会使用粗体的大写字母,比如(matrix \(\mathbf{A}\) )</li>
<li>当我们指代矩阵当中的一个成员的时候,使用小写 \(a_{ij}\)</li>
<li><p>
下图让我们了解矩阵当中的如下几个属于: column, row, element, diagonal
</p>

<div id="org8a0d47b" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/5-1.png" alt="5-1.png" />
</p>
<p><span class="figure-number">Figure 7: </span>tic/5-1.png</p>
</div></li>
</ul></li>
<li>矩阵可以包含小的矩阵,这就能够得到非常有用的block-matrix notation.</li>
<li>注意,block matrices不是本书要了解的特性,但是我们必须要了解他们的符号</li>
<li>如图:
<ul class="org-ul">
<li><p>
图5-2
</p>

<div id="org535d2a7" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/5-2.png" alt="5-2.png" />
</p>
<p><span class="figure-number">Figure 8: </span>tic/5-2.png</p>
</div></li>
<li>bloack matrix是能够包含较小矩阵的矩阵,这能够提高计算和检查效率</li>
<li>上图中的矩阵 \(\mathbf{A}\) 就包含了四个小矩阵</li>
</ul></li>
<li>当我们描述一个matrix大小的时候,我们要按照如下顺序进行描述(默认的convention):
<ol class="org-ol">
<li>首先是row的数目, 通常以M来指代</li>
<li>其次是column的数目, 通常以N来指代</li>
</ol></li>
<li><p>
我们可以使用一个词组来记住顺序:
</p>
<pre class="example" id="org2977599">
MR NiCe =&gt; M Row, N Column
</pre></li>
</ul>
</div>
</div>
<div id="outline-container-org24022f8" class="outline-3">
<h3 id="org24022f8"><span class="section-number-3">5.3.</span> Matrix dimensionalities</h3>
<div class="outline-text-3" id="text-5-3">
<ul class="org-ul">
<li>相比vector的dimension的简洁(有几个element就有几个dimension), matrix的dimension就非常的灵活,从而也让人难以理解</li>
<li>matrix的灵活性表现在,它的dimension从不同的角度会有不同的解释,比如对于一个 \(M \times N\) 的matrix来说:
<ul class="org-ul">
<li>如果每个element都有自己的dimenson,那么整个matrix的dimension就是 \(\mathbb{R}^MN\)</li>
<li>如果matrix被理解为一系列的column vector,那么整个matrix的dimension就是M</li>
<li>如果matrix被理解为一系列的row vector,那么整个matrix的dimension就是N</li>
</ul></li>
<li>实践当中,一个matrix的dimension会case-by-case的分析会通过上下文(或者明确写出来)得到需要如何解释一个matrix</li>
</ul>
</div>
</div>
<div id="outline-container-org806576e" class="outline-3">
<h3 id="org806576e"><span class="section-number-3">5.4.</span> The transpose opertion</h3>
<div class="outline-text-3" id="text-5-4">
<ul class="org-ul">
<li><p>
本章会学习matrix 的transpose(转置)操作,其概念是和vector的transpose是一样的,都是颠倒rows和columns
</p>
<pre class="example" id="org1e452ff">
Swap rows for columns and vice-versa
</pre></li>
<li>matrix transpose的notation也和vector transpose的一样, transpose of A写作 \(\mathbf{A}^T\)</li>
<li><p>
假设B是A的转置( \(\mathbf{B} = \mathbf{A}^T\),那么我们可以得到如下的公式
</p>
\begin{equation}
\mathbf{B}_{i,j} = \mathbf{A}_{j,i}  \tag{5.1}
\end{equation}</li>
<li><p>
matrix转置的转置还是自己,这是一个重要特性
</p>
\begin{equation}
\mathbf{A}^{TT} = \mathbf{A} \tag{5.2}
\end{equation}</li>
<li><p>
下面是一个matrix转置的例子
</p>
\begin{equation}
\begin{bmatrix}
1\;2\;3 \\
4\;5\;6 \\
\end{bmatrix}^T
=
\begin{bmatrix}
1\;4 \\
2\;5 \\
3\;6 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
一个非常容易犯的错误,就是不小心把column(或者row)的顺序给颠倒了,比如下面这个错误的转置,错就错在
虽然转置了,但是转置前的 \(E_{1,1}\) 还是转置后的 \(E_{1,1}\)
</p>
\begin{equation}
\begin{bmatrix}
1\;2\;3 \\
4\;5\;6 \\
\end{bmatrix}^T
\neq
\begin{bmatrix}
4\;1 \\
5\;2 \\
6\;3 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
<b>Code</b> 使用python进行转置的代码如下
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">import</span> numpy <span style="color: #26a6a6;">as</span> np

<span style="color: #f8f8f0;">A</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span><span style="color: #98be65;">(</span>1, 2, 3<span style="color: #98be65;">)</span>, <span style="color: #98be65;">(</span>4, 5, 6<span style="color: #98be65;">)</span><span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">'''[A] ==&gt;\n'''</span>, A<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">At1</span> = A.T
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">'''[At1] ==&gt;\n'''</span>, At1<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">At2</span> = np.transpose<span style="color: #51afef;">(</span>A<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">'''[At2] ==&gt;\n'''</span>, At2<span style="color: #51afef;">)</span>

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[A] ==&gt;</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[[1 2 3]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[4 5 6]]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[At1] ==&gt;</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[[1 4]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[2 5]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[3 6]]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[At2] ==&gt;</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[[1 4]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[2 5]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[3 6]]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-orgc3f83ad" class="outline-3">
<h3 id="orgc3f83ad"><span class="section-number-3">5.5.</span> Matrix zoology</h3>
<div class="outline-text-3" id="text-5-5">
<ul class="org-ul">
<li>有很多特殊特点的matrix会以如下方式命名:
<ul class="org-ul">
<li>发现这些特点的人的名字</li>
<li>根据这些特点来命名</li>
</ul></li>
<li>下面我们会列出并且解释这些特殊的矩阵,熟悉他们会对以后的工作非常重要</li>
<li><p>
<b>Square or rectangular</b> 方阵(square matrix)是说,这个矩阵的row和column的数目相同,例子如下
</p>
\begin{equation}
\begin{bmatrix}
  0 & 1 \\
  -1 & 0 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
非方阵(non-square matrix)是叫做矩形矩阵,例子如下
</p>
\begin{equation}
\begin{bmatrix}
  0 & 1 & 2 & 3 & -5 \\
  -1 & 0 & 3 & 23 & 0 \\
  -2 & 3 & 0 & 0 & 0 \\
\end{bmatrix}
\end{equation}</li>
<li>在线性代数领域,矩形矩阵的row和column不相同,也就是说:
<ul class="org-ul">
<li>\(M \neq N\)</li>
<li>方阵不能算是矩形矩阵</li>
</ul></li>
<li>对于一个矩形矩阵来说:
<ul class="org-ul">
<li>如果column大于row,那么它会被称之为"fat matrix"或者是"wide matrix"</li>
<li>如果column小于row,那么它会被称之为"skinny matrix"或者是"tall matrix"</li>
</ul></li>
<li><b>Symmetric</b>, 如果一个矩阵相对于对角线(diagonal)对称,那么我们就说它是对称矩阵(symmetric matrix)</li>
<li><p>
很显然,对称矩阵妈祖如下公式
</p>
\begin{equation}
\mathbf{A} = \mathbf{A}^T \tag{5.3}
\end{equation}</li>
<li><p>
另外很显然的是,只有方阵才有可能是对阵矩阵
</p>
<pre class="example" id="org60a9f63">
Only square matrices can be symmetric
</pre></li>
<li>也正是这个原因,vector不可能具有symmetric的性质</li>
<li><p>
对阵矩阵的例子如下
</p>
\begin{equation}
\begin{bmatrix}
  1 & 4 & \pi \\
  4 & 7 & 2 \\
  \pi & 2 & 0 \\
\end{bmatrix},
\begin{bmatrix}
  a & b  \\
  b & c  \\
\end{bmatrix},
\begin{bmatrix}
  a & e & f & g \\
  e & b & h & i \\
  f & h & c & j \\
  g & i & j & d \\
\end{bmatrix}
\end{equation}</li>
<li>所有其他矩阵都叫做非对称矩阵,所有矩形矩阵都是非对称矩阵</li>
<li>对称矩阵拥有非常多的特殊属性,后面会有大量篇幅介绍</li>
<li><p>
我们可以使用很多方法从非对称(甚至是矩形)矩阵来创建对称矩阵,而且这是统计学和机器学习的核心,后面
的章节就会介绍
</p>
<pre class="example" id="orgfd6ab01">
Indeed, creating symmetric from non-symmetric matrices is central to many statistics
and machine learning applications
</pre></li>
<li><b>Skew-symmetric</b> 反对称矩阵是说一个矩阵:
<ul class="org-ul">
<li>以对角线为边界,上半部分每个成员是下半部分对应成员的值乘以-1</li>
<li>对角线上必须都是0</li>
</ul></li>
<li><p>
下面是一个斜对称矩阵的例子
</p>
\begin{equation}
\begin{bmatrix}
  0 & 1 & 2 \\
  -1 & 0 & 3 \\
  -2 & -3 & 0 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
斜对称矩阵有如下的特性:
</p>
\begin{equation}
\mathbf{A} = -\mathbf{A}^T,\tag{5.5}
\end{equation}</li>
<li><p>
<b>Identity</b> 单位矩阵(identity matrix)是非常重要的概念,所谓单位矩阵是说某个矩阵乘以单位矩阵还是自
己,用公式表示如下
</p>
\begin{equation}
\mathbf{AI} = \mathbf{A}
\end{equation}</li>
<li>一个常见的错误观念会以为单位矩阵是整个矩阵每个成员都是1的矩阵,其实恰恰不是.单位矩阵是这么一种矩阵:
<ul class="org-ul">
<li>对角线全部为1</li>
<li>其他位置全部为0</li>
</ul></li>
<li><p>
单位矩阵的例子如下
</p>
\begin{equation}
\mathbf{I} = \begin{bmatrix}
               1 & 0 & \cdot\cdot\cdot & 0 \\
               0 & 1 & \cdot\cdot\cdot & 0 \\
               \cdot & \cdot &\cdot  & \cdot \\
               0 & 0 & \cdot\cdot\cdot & 1 \\
             \end{bmatrix}
\end{equation}</li>
<li><p>
我们还经常使用角标来提升用户单位矩阵的size,如下
</p>
\begin{equation}
\mathbf{I_2} = \begin{bmatrix}
                 1 & 0 \\
                 0 & 1 \\
               \end{bmatrix},
\mathbf{I_3} = \begin{bmatrix}
                 1 & 0 & 0 \\
                 0 & 1 & 0 \\
                 0 & 0 & 1 \\
               \end{bmatrix},
\end{equation}</li>
<li>单位矩阵的全称应该是"乘法单位矩阵",也就是一个矩阵和它相乘还是自己</li>
<li>相应的其实也有"加法单位矩阵",也就是一个矩阵和它相加还是自己</li>
<li><b>Zeros</b> 零矩阵,就是这么一种"加法单位矩阵",由于所有成员都是0, 所以任何矩阵和它相加都是自己</li>
<li>我们使用 \(\mathbf{0}\) 来代表零矩阵,对应于单位矩阵的 \(\mathbf{I}\)</li>
<li>除了编程(用来初始化矩阵)等特殊情况,绝大部分情况下零矩阵都是方阵(square matrix)</li>
<li><p>
<b>Code</b>, 使用如下代码来初始化单位矩阵,零矩阵,和vector
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">import</span> numpy <span style="color: #26a6a6;">as</span> np
<span style="color: #f8f8f0;">I</span> = np.eye<span style="color: #51afef;">(</span>4<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">'''[I] ==&gt;'''</span>, I<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">O</span> = np.ones<span style="color: #51afef;">(</span>4<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">'''[O] ==&gt;'''</span>, O<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">Z</span> = np.zeros<span style="color: #51afef;">(</span><span style="color: #c678dd;">(</span>4, 4<span style="color: #c678dd;">)</span><span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">'''[Z] ==&gt;'''</span>, Z<span style="color: #51afef;">)</span>

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[I] ==&gt; [[1. 0. 0. 0.]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[0. 1. 0. 0.]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[0. 0. 1. 0.]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[0. 0. 0. 1.]]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[O] ==&gt; [1. 1. 1. 1.]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[Z] ==&gt; [[0. 0. 0. 0.]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[0. 0. 0. 0.]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[0. 0. 0. 0.]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[0. 0. 0. 0.]]</span>

</pre>
</div></li>
<li>\(\mathbf{A^TA}\) 叫做"A transpose A",有时候写作"AtA",是线性代数里面最重要的一个矩阵公式之一</li>
<li>AtA具有很多特性,值得再这里列出来,如下 TODO</li>
<li><b>Diagonal</b> diagonal matrix(对角矩阵)的特性是:
<ul class="org-ul">
<li>只有对角线(从左上到右下)的位置上的数字可能为非0</li>
<li>其他位置上的数字都是0</li>
</ul></li>
<li>单位矩阵 \(\mathbf{I}\) 就是一种对角矩阵</li>
<li><p>
对角矩阵的例子如下
</p>
\begin{equation}
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 2 & 0 \\
  0 & 0 & 3 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
如果对角矩阵的成员都是一样的,那么一个矩阵可以写成常量乘以单位矩阵(identity matrix)的形式
</p>
\begin{equation}
\begin{bmatrix}
  7 & 0 \\
  0 & 7 \\
\end{bmatrix}
= 7
\begin{bmatrix}
  1 & 0 \\
  0 & 1 \\
\end{bmatrix}
=
7 \mathbf{I}_2
\end{equation}</li>
<li><p>
对角矩阵也可以是非方阵,只要保证下标i,j相等的位置不为0即可(这条非常反直觉)
</p>
\begin{equation}
\begin{bmatrix}
  2 & 0 & 0 & 0 \\
  0 & 6 & 0 & 0 \\
\end{bmatrix},
\begin{bmatrix}
  7 & 0 \\
  0 & 1 \\
  0 & 0 \\
  0 & 0 \\
\end{bmatrix}
\end{equation}</li>
<li>对角矩阵非常有用,因为他可以简化如下两种操作:
<ul class="org-ul">
<li>matrix multiplication</li>
<li>matrix powers ( \(\mathbf{A}^n\) )</li>
</ul></li>
<li>而把一个普通矩阵转换成对角矩阵的操作叫做对角线化(diagonalization),这个操作可以通过如下两种技术得到:
<ul class="org-ul">
<li>特征分解(eigendecomposition)</li>
<li>奇异值分解(singular value decomposition)</li>
</ul></li>
<li>大写字母 \(\mathbf{D}\) 常常用来表示对角矩阵,某些情况下 \(\mathbf{\Lambda}, \mathbf{\Sigma}\) 也会用来表示对角矩阵</li>
<li>和对角矩阵相对(opposite)的是空心矩阵(hollow matrix),它的特点是:
<ul class="org-ul">
<li>对角线(从左上到右下)的位置上的数字全部为0</li>
<li>只有其他位置上的数字可能为非0</li>
</ul></li>
<li>反对称矩阵 skew-symmetric也是空心矩阵</li>
<li>空心矩阵常用来做distance matrix(因为每个node和自己的距离是0), 本书中不会再讨论空心矩阵</li>
<li><p>
<b>Code</b> 使用如下代码生成对角矩阵,或者获取对角矩阵的数据
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">import</span> numpy <span style="color: #26a6a6;">as</span> np


<span style="color: #f8f8f0;">D</span> = np.diag<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>1, 2, 3, 5<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>       <span style="color: #5B6268;"># &#21019;&#24314;&#19968;&#20010;&#23545;&#35282;&#30697;&#38453;</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">'''[D] ==&gt;'''</span>, D<span style="color: #51afef;">)</span>

<span style="color: #f8f8f0;">R</span> = np.random.randn<span style="color: #51afef;">(</span>3, 4<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">'''[R] ==&gt;'''</span>, R<span style="color: #51afef;">)</span>         <span style="color: #5B6268;"># &#38543;&#26426;&#29983;&#25104;&#19968;&#20010;3*4&#30697;&#38453;</span>

<span style="color: #f8f8f0;">d</span> = np.diag<span style="color: #51afef;">(</span>R<span style="color: #51afef;">)</span>                  <span style="color: #5B6268;"># &#33719;&#21462;&#38543;&#26426;&#30697;&#38453;&#30340;&#23545;&#35282;&#32447;&#25968;&#25454;</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">'''[d] ==&gt;'''</span>, d<span style="color: #51afef;">)</span>

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[D] ==&gt; [[1 0 0 0]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[0 2 0 0]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[0 0 3 0]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[0 0 0 5]]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[R] ==&gt; [[-2.1938143  -0.85677168 -0.27007231 -1.22768217]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-0.83367635  1.44634774  0.45579445  0.29923832]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 2.26729013 -0.65103393 -0.52931206 -0.39227102]]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[d] ==&gt; [-2.1938143   1.44634774 -0.52931206]</span>
</pre>
</div></li>
<li><p>
<b>Augmented</b> 增广矩阵(augmented matrix)就是把两个矩阵在水平维度上面"加起来",如下
</p>
\begin{equation}
\begin{bmatrix}
  1 & 4 & 2 \\
  3 & 1 & 9 \\
  4 & 2 & 0 \\
\end{bmatrix}
\sqcup
\begin{bmatrix}
  7 & 2 \\
  7 & 2 \\
  7 & 1 \\
\end{bmatrix}
=
\begin{bmatrix}
  1 & 4 & 2 & 7 & 2 \\
  3 & 1 & 9 & 7 & 2 \\
  4 & 2 & 0 & 7 & 1 \\
\end{bmatrix}
\end{equation}</li>
<li>两个matrix能够"增广"的前提,是两个matrix拥有相同的row</li>
<li><p>
<b>Code</b> 在python中,使用concatenate来代表augment这个操作
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">import</span> numpy <span style="color: #26a6a6;">as</span> np

<span style="color: #f8f8f0;">A</span> = np.random.randn<span style="color: #51afef;">(</span>3, 5<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[A] ==&gt;"""</span>, A<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">B</span> = np.random.randn<span style="color: #51afef;">(</span>3, 2<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[B] ==&gt;"""</span>, B<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">AB</span> = np.concatenate<span style="color: #51afef;">(</span><span style="color: #c678dd;">(</span>A, B<span style="color: #c678dd;">)</span>, axis=1<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[AB] ==&gt;"""</span>, AB<span style="color: #51afef;">)</span>

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[A] ==&gt; [[-0.14425489 -1.22366268 -0.27740708  1.07126456  0.70006477]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-1.28607063 -1.29396936  0.34630739  1.46735437  1.28085203]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-0.61064391 -0.35512087 -0.83978548 -1.67325096 -0.03125103]]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[B] ==&gt; [[ 2.56938822  0.08952084]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-0.02576563  0.17668451]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 1.51715998 -1.43679035]]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[AB] ==&gt; [[-0.14425489 -1.22366268 -0.27740708  1.07126456  0.70006477  2.56938822</span>
<span style="color: #5B6268;">#    </span><span style="color: #5B6268;">0.08952084]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-1.28607063 -1.29396936  0.34630739  1.46735437  1.28085203 -0.02576563</span>
<span style="color: #5B6268;">#    </span><span style="color: #5B6268;">0.17668451]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-0.61064391 -0.35512087 -0.83978548 -1.67325096 -0.03125103  1.51715998</span>
<span style="color: #5B6268;">#   </span><span style="color: #5B6268;">-1.43679035]]</span>
</pre>
</div></li>
<li><b>Triangular</b> 三角矩阵是对角矩阵和full matrix的一个中间状态</li>
<li>它的有两种表现形式:
<ol class="org-ol">
<li>upper triangular
<ul class="org-ul">
<li>对角线(从左上到右下)的位置上方,包括对角线可能是非0</li>
<li>其他位置上的数字全部为0</li>
<li><p>
例子如下
</p>
\begin{equation}
\begin{bmatrix}
  1 & 8 & 4 \\
  0 & 2 & 1 \\
  0 & 0 & 9 \\
\end{bmatrix}
\end{equation}</li>
</ul></li>
<li>lower triangular
<ul class="org-ul">
<li>对角线(从左上到右下)的位置下方,包括对角线可能是非0</li>
<li>其他位置上的数字全部为0</li>
<li><p>
例子如下
</p>
\begin{equation}
\begin{bmatrix}
  1 & 0 & 0 & 0 \\
  3 & 2 & 0 & 0 \\
  6 & 5 & 9 & 0 \\
  2 & 0 & 4 & 1 \\
\end{bmatrix}
\end{equation}</li>
</ul></li>
</ol></li>
<li><p>
上面的例子中三角矩阵都是方阵,但其实三角矩阵也可以是非方阵(rectangular),比如下面的例子
</p>
\begin{equation}
\begin{bmatrix}
  1 & 0 & 0 & 0 & 0 \\
  3 & 2 & 0 & 0 & 0 \\
  6 & 0 & 9 & 0 & 0 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
很多时候,由于三角矩阵里面有一半是0,我们可以空着不写那为0的一般(另外一半可能为0的必须写出来,0不能省略)
</p>
\begin{equation}
\begin{bmatrix}
  1 & 3 & 2 & 1 \\
    & 2 & 7 & 9 \\
    &   & 0 & 5 \\
    &   &   & 2 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
<b>Code</b> 使用如下代码来把一个matrix变成upper triangle和lower triangle
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">import</span> numpy <span style="color: #26a6a6;">as</span> np

<span style="color: #f8f8f0;">A</span> = np.random.randn<span style="color: #51afef;">(</span>5, 5<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[A] ==&gt;"""</span>, A<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">L</span> = np.tril<span style="color: #51afef;">(</span>A<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[L] ==&gt;"""</span>, L<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">U</span> = np.triu<span style="color: #51afef;">(</span>A<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[U] ==&gt;"""</span>, U<span style="color: #51afef;">)</span>

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[A] ==&gt; [[ 0.39866979 -1.22350387 -0.33729558 -1.0857667  -1.30051086]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 0.8020152  -0.35123035 -1.34653201  1.48667849 -0.34774193]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-0.01468234  1.83371032  0.27659977 -0.97204034  0.51079151]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 1.01177767  0.35195454 -0.89955996  0.66733842 -0.46424685]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-0.86648848  0.43285399 -1.91123781  0.07286382  0.18129945]]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[L] ==&gt; [[ 0.39866979  0.          0.          0.          0.        ]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 0.8020152  -0.35123035  0.          0.          0.        ]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-0.01468234  1.83371032  0.27659977  0.          0.        ]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 1.01177767  0.35195454 -0.89955996  0.66733842  0.        ]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-0.86648848  0.43285399 -1.91123781  0.07286382  0.18129945]]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[U] ==&gt; [[ 0.39866979 -1.22350387 -0.33729558 -1.0857667  -1.30051086]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 0.         -0.35123035 -1.34653201  1.48667849 -0.34774193]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 0.          0.          0.27659977 -0.97204034  0.51079151]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 0.          0.          0.          0.66733842 -0.46424685]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 0.          0.          0.          0.          0.18129945]]</span>
</pre>
</div></li>
<li><b>Dense and sparse</b> 如果一个matrix的大部分或者全部element是非0的,那么我们给这个matrix起了如下两个名字:
<ul class="org-ul">
<li>dense matrix(稠密矩阵)</li>
<li>full matrix</li>
</ul></li>
<li>dense matrix这个概念是要有上下文的,比如比较如下两个概念的时候:
<ul class="org-ul">
<li>对角矩阵</li>
<li>稠密矩阵(dense matrix)</li>
</ul></li>
<li>稀疏矩阵(sparse matrix)是指一个矩阵里面大部分员为0,只有很少一部分成员不为0的矩阵</li>
<li>稀疏矩阵计算效率超高,所以现代算法里面把稀疏矩阵作为重点来介绍</li>
<li><p>
下面10*10的稀疏矩阵,可以使用简单的列举非0成员而表示
</p>
\begin{equation}
  \begin{bmatrix}
    0 & 0 & 4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 8 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
  \end{bmatrix}
  \Rightarrow
  (1, 3)4,(4, 6)8
\end{equation}</li>
<li><b>Orthogonal</b> 正交矩阵是满足如下两个条件的矩阵:
<ul class="org-ul">
<li>我们把一个矩阵的每一列(column)看成是一个vector,那么正交矩阵的任意两列的vector的dot product都是0</li>
<li>我们把一个矩阵的每一列(column)看成是一个vector,某个vector和自己的dot product 都是1</li>
</ul></li>
<li><p>
一般来说,我们使用 \(\mathbf{Q}\) 来代表正交矩阵,正交矩阵的正式定义如下
</p>
\begin{equation}
< \mathbf{Q_i}, \mathbf{Q_j} > =
  \begin{cases}
  1, if \; i == j \\
  0, if \; j \ne j \\
  \end{cases},\tag{5.8}
\end{equation}</li>
<li>注意,这里的&lt;&gt;是dot product的符号,我们这里每个成员都是一矩阵的一列</li>
<li><p>
公式5-8有一个更加专业的,只用矩阵乘法(后面会学到)的公式,如下
</p>
\begin{equation}
\mathbf{Q}^T \mathbf{Q} = \mathbf{I},\tag{5.9}
\end{equation}</li>
<li><b>Toeplitz</b> 特布里茨矩阵(也叫常对角矩阵),是指每条对角线上的成员数字都一样的矩阵</li>
<li><p>
我们可以使用一个vector来生成常对角矩阵,如下
</p>
\begin{equation}
\begin{bmatrix}
a & b & c & d \\
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
  a & b & c & d \\
  b & a & b & c \\
  c & b & a & b \\
  d & c & b & a \\
\end{bmatrix}
\end{equation}</li>
<li>其实上图很好理解,把初始的vector作为常对角矩阵的第一行(以及竖着的第一列),后面按照规则(每个对角线
必须一致),继续添加就可以了.总体看起来就是:
<ul class="org-ul">
<li>vector的第一个成员是主对角线(main diagonal)上的数字</li>
<li>vector的第N个成员是第N个对角线上的数字</li>
</ul></li>
<li>需要注意的是上面的例子是一个方阵,而且是对称矩阵(symmetric),但是其实常对角矩阵的定义只是要求对角
线数据一致就可以,常对角矩阵既可以是非方阵,也当然可以是非对称的</li>
<li><b>Hankel</b> 汉克矩阵,是指每条副对角线上的成员数字都一样的方阵:
<ul class="org-ul">
<li>汉克矩阵由于每个副对角线都一样,所以相对于对角线对称</li>
</ul></li>
<li>下面两个都是汉克矩阵:
<ul class="org-ul">
<li><p>
使用一个vector生成的汉克矩阵,先固定好第一行第一列,后面自动生成.因为是副对角线
</p>
\begin{equation}
\begin{bmatrix}
a & b & c & d \\
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
  a & b & c & d \\
  b & c & d & 0 \\
  c & d & 0 & 0 \\
  d & 0 & 0 & 0 \\
\end{bmatrix}
\end{equation}</li>
<li><p>
甚至可以wrap abcd(每次wrap一个位置),最终生成的full matrix也是汉克矩阵
</p>
\begin{equation}
\begin{bmatrix}
  a & b & c & d \\
  b & c & d & a \\
  c & d & a & b \\
  d & a & b & c \\
\end{bmatrix}
\end{equation}</li>
</ul></li>
<li><p>
汉克矩阵的生成公式如下:
</p>
\begin{equation}
  \mathbf{Y}_{i,j} = \mathbf{x}_{i+j-1}, \tag{5.10}
\end{equation}</li>
<li>其实上面两种汉克矩阵,从公式看,都可以看成是从vector生成的矩阵,只不过:
<ul class="org-ul">
<li>第一个例子abcd后面都是0,也就是说,我们的vector延长后就是[a b c d 0 0 0]</li>
<li>第二个例子abcd后面就是不停的wrap,我们的vector延长后就是[a b c d a b c]</li>
</ul></li>
<li><p>
<b>Code</b> 使用如下python代码来生成常对角矩阵和汉克矩阵
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">from</span> scipy.linalg <span style="color: #26a6a6;">import</span> hankel, toeplitz

<span style="color: #f8f8f0;">t</span> = <span style="color: #51afef;">[</span>1, 2, 3, 4<span style="color: #51afef;">]</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[t] ==&gt;"""</span>, t<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">T</span> = toeplitz<span style="color: #51afef;">(</span>t<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[T] ==&gt;"""</span>, T<span style="color: #51afef;">)</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31532;&#19968;&#20010;&#21442;&#25968;&#26159;&#31532;&#19968;&#34892;, r&#36825;&#20010;&#21442;&#25968;&#26159;&#26368;&#21518;&#19968;&#34892;,&#22914;&#26524;&#19981;&#21152;r,&#37027;&#20040;&#19979;&#21322;&#37096;&#20998;&#37117;&#26159;0</span>
<span style="color: #f8f8f0;">H</span> = hankel<span style="color: #51afef;">(</span>t, r=<span style="color: #c678dd;">[</span>2, 3, 4, 1<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[H] ==&gt;"""</span>, H<span style="color: #51afef;">)</span>

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[t] ==&gt; [1, 2, 3, 4]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[T] ==&gt; [[1 2 3 4]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[2 1 2 3]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[3 2 1 2]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[4 3 2 1]]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[H] ==&gt; [[1 2 3 4]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[2 3 4 3]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[3 4 3 4]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[4 3 4 1]]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org42b284a" class="outline-3">
<h3 id="org42b284a"><span class="section-number-3">5.6.</span> Matrix addition and subtraction</h3>
<div class="outline-text-3" id="text-5-6">
<ul class="org-ul">
<li>matrix的加法是非常简单的,两个matrix相加,就是两个matrix对应位置的成员相加,所以matrix加法要求:
<ul class="org-ul">
<li>相加的两个matrix必须是一样的size,比如 \(M \times N\)</li>
<li>相加的结果,也是同一个size,也就是\(M \times N\)</li>
</ul></li>
<li><p>
下面是一个加法的例子
</p>
\begin{equation}
\begin{bmatrix}
  1 & 2 & 5 \\
  9 & 8 & 7 \\
\end{bmatrix}
+
\begin{bmatrix}
  -1 & 0 & -5 \\
  3  & a & \pi \\
\end{bmatrix}
=
\begin{bmatrix}
  0  &   2   & 0 \\
  12 & 8 + a & 7 + \pi \\
\end{bmatrix}
\end{equation}</li>
<li>matrix减法和加法一样,只不过"+"换成"-",我们就不再赘述</li>
<li><p>
就像vector addition一样, matrix addition也满足交换律(commutative),也就是说,如下
</p>
\begin{equation}
\mathbf{C} = \mathbf{A} + \mathbf{B} = \mathbf{B} + \mathbf{A}
\end{equation}</li>
<li>看起来非常显然的知识我们还是列出来了,是因为有些显然是成立的,有些不成立,比如matrix的乘法就不满足
交换律</li>
<li>matrix满足交换律对于生成symmetric matrix非常重要</li>
</ul>
</div>
</div>
<div id="outline-container-org3f5092a" class="outline-3">
<h3 id="org3f5092a"><span class="section-number-3">5.7.</span> Scalar-matrix multiplication</h3>
<div class="outline-text-3" id="text-5-7">
<ul class="org-ul">
<li>matrix multiplication非常重要,且内容繁多,我们将使用一整章来介绍,这里我们介绍相对简单的"乘法":
scalar-matrix multiplication</li>
<li><p>
scalar-matrix multiplication简单来说,就是给matrix的每个成员乘以一个系数,和scalar-vector multiplication
的原理是一样的,例子如下
</p>
\begin{equation}
\delta
\begin{bmatrix}
  a & b \\
  c & d \\
\end{bmatrix}
=
\begin{bmatrix}
  \delta a & \delta b \\
  \delta c & \delta d \\
\end{bmatrix}
=
\begin{bmatrix}
  a \delta & b \delta \\
  c \delta & d \delta \\
\end{bmatrix}
=
\begin{bmatrix}
  a & b \\
  c & d \\
\end{bmatrix}
\delta
\tag{5.12}
\end{equation}</li>
<li><p>
由于scalar-matrix multiplication都是在element维度上面进行的,所以它也是满足交换律(commutative)的,
所以scalar可以到处移动,这对于后面的很多证明都是非常重要的.scalar到处移动可以用公式表达为
</p>
\begin{equation}
\lambda \mathbf{AB} = \mathbf{A} \lambda \mathbf{B} = \mathbf{AB} \lambda
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-org2d4e7b0" class="outline-3">
<h3 id="org2d4e7b0"><span class="section-number-3">5.8.</span> "Shifting" a matrix</h3>
<div class="outline-text-3" id="text-5-8">
<ul class="org-ul">
<li>我们可以把如下两个操作结合成shift操作,应用于一个matrix上:
<ul class="org-ul">
<li>matrix addition</li>
<li>scalar-matrix multiplication</li>
</ul></li>
<li>shift操作是把一个identity matrix(单位矩阵)进行scalar之后,加到原来的matrix上面,所以原来的matrix也
必须是方阵,因为identity matrix(单位矩阵)是方阵</li>
<li><p>
新生成的矩阵一般在字母上面加一个波浪线,比如 \(\tilde{\mathbf{A}}\). 下面就是shitfing a matrix操作的公式表达
</p>
\begin{equation}
\tilde{\mathbf{A}} = \mathbf{A} + \lambda \mathbf{I}, \mathbf{A} \in \mathbb{R}^{M \times M}, \lambda \in \mathbb{R}, \tag{5.13}
\end{equation}</li>
<li><p>
我们使用一个真实的例子来展现下shift操作
</p>
\begin{equation}
\begin{bmatrix}
  1 & 3 & 0 \\
  1 & 3 & 0 \\
  2 & 2 & 7 \\
\end{bmatrix}
+ \; .1
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 \\
\end{bmatrix}
=
\begin{bmatrix}
  1.1 & 3 & 0 \\
  1   & 3.1 & 0 \\
  2   & 2 & 7.1 \\
\end{bmatrix}
\end{equation}</li>
<li>从上面的例子,我们可以看到shifting操作的如下特性:
<ul class="org-ul">
<li>只有对角线上的成员受到影响,非对角线上的成员不受影响,因为单位矩阵的非对角线上都是0</li>
<li>在shift之前矩阵的第一行和第二行是一样的,但是shift之后两者不一样了,所以shift matrix可以用来让
矩阵中相同的行变的不太一样</li>
<li>如果 \(\lambda\) 越接近0, 那么 $~{\mathbf{A}} 就和 \(\mathbf{A}\) 非常的接近, 极端情况下
\(\lambda\) 为0的情况下, \(\tilde{\mathbf{A}} = \mathbf{A}\). 在实践当中,选择一个尽可能满足条件的
足够小的 \(\lambda\) 是工作当中的重要工作</li>
</ul></li>
<li>shifting matrix有着广泛的应用,比如:
<ul class="org-ul">
<li>在统计学中为了让模型适应low-rank数据,通常要进行正则化(Regularization), 正则化的过程当中就设计shifting matrix</li>
<li>另外shifting matrix可以把一个rank-deficient matrix转换成一个full-rank matrix</li>
</ul></li>
<li><p>
<b>Code</b> shifting的代码包含scalar multiplication和addition,如下
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">import</span> numpy <span style="color: #26a6a6;">as</span> np

<span style="color: #f8f8f0;">l</span> = 0.01
<span style="color: #f8f8f0;">I</span> = np.eye<span style="color: #51afef;">(</span>4<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">A</span> = np.random.randn<span style="color: #51afef;">(</span>4, 4<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[A] ==&gt;"""</span>, A<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">As</span> = A + l * I
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[As] ==&gt;"""</span>, As<span style="color: #51afef;">)</span>

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[A] ==&gt; [[-1.57995308 -0.26785428 -0.2266074   1.11107041]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-0.06172473  0.10330103 -0.4369862   1.35241956]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-1.86982424  1.14082253  0.87677916  0.81019919]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 0.10328674  0.47264134  0.02081591  2.13364769]]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[As] ==&gt; [[-1.56995308 -0.26785428 -0.2266074   1.11107041]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-0.06172473  0.11330103 -0.4369862   1.35241956]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-1.86982424  1.14082253  0.88677916  0.81019919]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 0.10328674  0.47264134  0.02081591  2.14364769]]</span>
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org94871be" class="outline-3">
<h3 id="org94871be"><span class="section-number-3">5.9.</span> Diagonal and trace</h3>
<div class="outline-text-3" id="text-5-9">
<ul class="org-ul">
<li>matrix对角线上的数据可以取出来,并且放到一个vector里面</li>
<li>上面的操作在某些特定场景下有用,比如:
<ul class="org-ul">
<li>在统计学领域,covariance matrix(协方差矩阵)的对角线成员包含每个variable的variance(方差)</li>
</ul></li>
<li>注意,提取对角线数据的操作不要求操作对象是方阵</li>
<li>如图:
<ul class="org-ul">
<li><p>
图5-6
</p>

<div id="orgedba91d" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/5-6.png" alt="5-6.png" />
</p>
<p><span class="figure-number">Figure 9: </span>tic/5-6.png</p>
</div></li>
</ul></li>
<li><b>Trace</b> trace是把一个方阵(注意只能是方阵)的对角线数据加起来求和</li>
<li><p>
我们通常使用 \(tr(\mathbf{A})\) 来表示trace操作,公式如下
</p>
\begin{equation}
tr(\mathbf{A}) = \sum_{i=1}^M a_{i,i},\tag{5.15}
\end{equation}</li>
<li>由于非对角线数据对于trace操作没有贡献,所以两个差距非常大的方阵可能trace相同</li>
<li>trace操作对于机器学习非常重要:
<ul class="org-ul">
<li>它用来计算两个matrix的Frobenius norm(范数)</li>
<li>范数用来度量两个matrix之间的距离</li>
</ul></li>
<li>trace操作只能用于方阵是由于"历史原因":
<ul class="org-ul">
<li>matrix的trace等于matrix 特征(eigenvalue)的综合</li>
<li>特征分解只作用于方阵,所以从古时候开始,古代数学家就规定了只有方阵才能进行trace操作</li>
</ul></li>
<li><p>
<b>Code</b> 使用如下代码进行trace操作
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">import</span> numpy <span style="color: #26a6a6;">as</span> np

<span style="color: #f8f8f0;">A</span> = np.random.randn<span style="color: #51afef;">(</span>4, 4<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[A] ==&gt;"""</span>, A<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">tr</span> = np.trace<span style="color: #51afef;">(</span>A<span style="color: #51afef;">)</span>
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">"""[tr] ==&gt;"""</span>, tr<span style="color: #51afef;">)</span>

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[A] ==&gt; [[ 0.59060411  1.20311925 -1.02348206 -0.46843217]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 0.21939289 -2.09379843 -1.20578246 -0.26266209]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-1.47517321 -0.55440747  0.54005967 -1.51054155]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-0.51872691  0.98704069  2.48983979  1.31954482]]</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[tr] ==&gt; 0.35641017102727246</span>
</pre>
</div></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org8247cbe" class="outline-2">
<h2 id="org8247cbe"><span class="section-number-2">6.</span> Matrix multiplication</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-orge37b9cf" class="outline-3">
<h3 id="orge37b9cf"><span class="section-number-3">6.1.</span> "Standard" matrix multiplication</h3>
<div class="outline-text-3" id="text-6-1">
<ul class="org-ul">
<li>两个matrix相乘的时候,如果没有明确的说明,那么我们就默认两个matrix进行的乘法(比如 \(\mathbf{AB}\) )
是标准乘法(standard matrix multiplication)</li>
<li><b>Terminology</b> 最先需要知道的是,matrix multiplication不满足交换律(not commutative),所以 \(\mathbf{AB} \neq \mathbf{BA}\)</li>
<li>由于不满足交换律,所以矩阵乘法的定义非常麻烦,以 \(\mathbf{AB}\) 为例,其有如下五种英语表达方式:
<ul class="org-ul">
<li>A times B</li>
<li>A left-multiplies B</li>
<li>A pre-multiplies B</li>
<li>B right-multiplies A</li>
<li>B post-multiplies A</li>
</ul></li>
<li><b>Validity</b> 在了解如何进行乘法之前,我们先要了解什么样的两个矩阵可以进行相乘</li>
<li>matrix mulitiplication只有在如下情况下valid: 两个matrix的inner dimension相同</li>
<li>同时,新得到的matrix的size是由两个matrix的outer dimension决定的</li>
<li>如图
<ul class="org-ul">
<li><p>
图6-1
</p>

<div id="org5de56f7" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/6-1.png" alt="6-1.png" />
</p>
<p><span class="figure-number">Figure 10: </span>tic/6-1.png</p>
</div></li>
<li>N就是两个矩阵的inner dimension,他们必须相同</li>
<li>M和K是两个矩阵的outer dimension,他们不需要相同,但是矩阵乘法得到的新矩阵,其size由这两个dimension决定</li>
</ul></li>
<li>一旦理解了什么样的乘法是valid的,以及resulting matrix的size如何而来,我们就可以理解dot product和
outer product的区别了
<ul class="org-ul">
<li><p>
图6-2
</p>

<div id="org6adbce0" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/6-2.png" alt="6-2.png" />
</p>
<p><span class="figure-number">Figure 11: </span>tic/6-2.png</p>
</div></li>
<li>我们从这里就可以看到:
<ol class="org-ol">
<li>dot product的结果是一个数字,那么必然要用 \(\mathbf{V}^T \mathbf{W}\)</li>
<li>out product的结果是一个矩阵,那么必然要用 \(\mathbf{V} \mathbf{W}^T}\)</li>
</ol></li>
</ul></li>
<li><p>
<b>Code</b> 使用如下python代码来代表矩阵乘法
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #26a6a6;">import</span> numpy <span style="color: #26a6a6;">as</span> np
<span style="color: #f8f8f0;">M1</span> = np.random.randn<span style="color: #51afef;">(</span>4, 3<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">M2</span> = np.random.randn<span style="color: #51afef;">(</span>3, 5<span style="color: #51afef;">)</span>
<span style="color: #f8f8f0;">C</span> = M1 @ M2
<span style="color: #26a6a6;">print</span><span style="color: #51afef;">(</span><span style="color: #bcd42a;">'''[C] ==&gt;'''</span>, C<span style="color: #51afef;">)</span>

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&lt;===================OUTPUT===================&gt;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">[C] ==&gt; [[-0.23848307 -1.54327638 -0.58576378  1.10745737  1.73813371]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 0.12136803  0.06643498 -1.03968181  1.2177658  -0.68164259]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[-0.10025755  1.3981402  -0.9935196  -0.1241486  -1.14903948]</span>
<span style="color: #5B6268;">#  </span><span style="color: #5B6268;">[ 0.73051258 -1.08758839  0.74398132  1.40448759 -0.89103988]]</span>
</pre>
</div></li>
<li>下面我们终于开始学习如何进行乘法了.有四种方法来思考如何实施矩阵乘法,四种计算方法得到的结果一样,
但是提供了不同的视角来理解乘法</li>
</ul>
</div>
<div id="outline-container-org92659a4" class="outline-4">
<h4 id="org92659a4"><span class="section-number-4">6.1.1.</span> The "element perspective"</h4>
<div class="outline-text-4" id="text-6-1-1">
<ul class="org-ul">
<li>公式 \(\mathbf{AB=C}\)</li>
<li>其中每个成员 \(c_{ij}\) 都是如下两个矩阵的dot product
<ul class="org-ul">
<li>\(\mathbf{A}\) 的ith row</li>
<li>\(\mathbf{B}\) 的jth column</li>
</ul></li>
<li><p>
下面的例子解释了这个过程: top-left的成员如何获得
</p>
\begin{equation}
  \begin{bmatrix}
    1 & 2 \\
    3 & 4 \\
  \end{bmatrix}
  \begin{bmatrix}
    a & b \\
    c & d \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    1a+2c &  \\
     &  \\
  \end{bmatrix} \tag{6.1}
\end{equation}</li>
<li><p>
下面是一个完整的例子
</p>
\begin{equation}
\begin{bmatrix}
  3  & 4 \\
  -1 & 2 \\
  0  & 4 \\
\end{bmatrix}
\begin{bmatrix}
  5 & 1 \\
  3 & 1 \\
\end{bmatrix}
=
\begin{bmatrix}
  3*5 + 4*3  & 3*1+4*1 \\
  -1*5 + 2*3 & -1*1+2*1 \\
  0*5 + 4*3  & 0*1+4*1 \\
\end{bmatrix}
=
\begin{bmatrix}
  27 & 7 \\
  1  & 1 \\
  12 & 4 \\
\end{bmatrix}
\end{equation}</li>
<li>如图
<ul class="org-ul">
<li><p>
图6-3-1
</p>

<div id="orgae28845" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/6-3-1.png" alt="6-3-1.png" />
</p>
<p><span class="figure-number">Figure 12: </span>tic/6-3-1.png</p>
</div></li>
<li>我们把matrix的一行(或者一列)抽象成一个向量,从这方式我们可以看到matrix multiplication的三个特性:
<ol class="org-ol">
<li>matrix \(\mathbf{C}\) 的对角线上,包含了这样一些vector的dot product, 这些vector的下标( \(\mathbf{A}\)
里面的i, \(\mathbf{B}\) 里面的j)相同</li>
<li>matrix \(\mathbf{C}\) 的对角线下半部分,包含了这样一些vector的dot product, 这些vector的下标中,( \(\mathbf{A}\)
里面的i 大于 \(\mathbf{B}\) 里面的j)</li>
<li>matrix \(\mathbf{C}\) 的对角线上半部分,包含了这样一些vector的dot product, 这些vector的下标中,( \(\mathbf{A}\)
里面的i 小于 \(\mathbf{B}\) 里面的j)</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org973580f" class="outline-4">
<h4 id="org973580f"><span class="section-number-4">6.1.2.</span> The "layer perspective"</h4>
<div class="outline-text-4" id="text-6-1-2">
<ul class="org-ul">
<li>上面的"element perspective"是每个element计算自己的,不同element中间是互相不干扰的</li>
<li>layer perspective则完全不同, 它是把matrix multiplication的计算分成了多个层(而不是m*n个element),
然后把这些个层叠加起来</li>
<li>如图
<ul class="org-ul">
<li><p>
图6-4
</p>

<div id="org0114d35" class="figure">
<p><img src="https://raw.githubusercontent.com/harrifeng/image/master/tic/6-4.png" alt="6-4.png" />
</p>
<p><span class="figure-number">Figure 13: </span>tic/6-4.png</p>
</div></li>
<li>我们知道out product的结果都是一样的,所以把这些结果加起来就是matrix multiplication的product,
就好比上图中多个透明纸上的图案叠加起来</li>
</ul></li>
<li><p>
下面是一个使用outer product来计算matrix multiplication的过程
</p>
\begin{equation}
\begin{bmatrix}
  3  & 4 \\
  -1 & 2 \\
  0  & 4 \\
\end{bmatrix}
\begin{bmatrix}
  5 & 1 \\
  3 & 1 \\
\end{bmatrix}
=
\begin{bmatrix}
  15 & 3 \\
  -5 & -1 \\
  0  & 0 \\
\end{bmatrix}
+
\begin{bmatrix}
  12 & 4 \\
  6  & 2 \\
  12 & 4 \\
\end{bmatrix}
=
\begin{bmatrix}
  27 & 7 \\
  1  & 1 \\
  12 & 4 \\
\end{bmatrix}
\end{equation}</li>
<li>outer product有个特殊的点,我们可以通过下面的例子看到:
<ul class="org-ul">
<li><p>
例子
</p>
\begin{equation}
\begin{bmatrix}
a \\
b \\
c \\
\end{bmatrix}
\begin{bmatrix}
d e f  \\
\end{bmatrix}
=
\begin{bmatrix}
ad & ae & af & \\
bd & be & bf & \\
cd & ce & cf & \\
\end{bmatrix}
\end{equation}</li>
<li>特点: 由于都是使用同样的前缀,那么很显然每个outer product得到的matrix里面的column vector都是
dependent set</li>
</ul></li>
<li>那么很显然以layer perspective来看:
<ul class="org-ul">
<li>每个layer matrix的column都是dependent set</li>
<li>但是,这些layer matrix的和(也就是matrix multiplication的结果缺是independent set)</li>
</ul></li>
<li>layer perspective的解释对于矩阵的spectral theorem非常有用</li>
<li><p>
所谓的谱定理(spectral theorem)是说: 任意的matrix都可以看成是一系列"秩为1矩阵"的和
</p>
<pre class="example" id="org360b3e6">
Any matrix can be represented as a sum of rank-1 matrices
</pre></li>
<li>我们还没学到秩,但是我们可以简单的理解为所谓秩为1,是指只有一个column有意义,其他column都是这个有
意义column的scaled version</li>
<li>谱定理(spectral theorem)是奇异值分解(singular value decomposition)的基础,这个我们后面会介绍到</li>
</ul>
</div>
</div>
<div id="outline-container-org7da8d68" class="outline-4">
<h4 id="org7da8d68"><span class="section-number-4">6.1.3.</span> The "column perspective"</h4>
<div class="outline-text-4" id="text-6-1-3">
<ul class="org-ul">
<li>从column perspective来看,所有的matrix(无论是multiplying matrix还是product matrix),搜索可以看做
是一系列column vector的set</li>
<li>我们这里单看product matrix,它可以看做每次创建一个column:
<ul class="org-ul">
<li>product matrix里面的第一个column是一个linear weighted combination的治,这个linear weighted combination:
<ol class="org-ol">
<li>weight是right matrix的第一个column</li>
<li>对应的vector是left matrix的vector set(每一个column是一个vector)</li>
</ol></li>
<li>product matrix里面的第二个column是一个linear weighted combination的治,这个linear weighted combination:
<ol class="org-ol">
<li>weight是right matrix的第二个column</li>
<li>对应的vector是left matrix的vector set(每一个column是一个vector)</li>
</ol></li>
</ul></li>
<li><p>
上面的情况,我们使用例子更能深刻理解,如下
</p>
\begin{equation}
\begin{bmatrix}
1 & 2 \\
3 & 4 \\
\end{bmatrix}
\begin{bmatrix}
a & b \\
c & d \\
\end{bmatrix}
=
\left[
a
\begin{bmatrix}
1 \\

3 \\
\end{bmatrix}
+
c
\begin{bmatrix}
2 \\
4 \\
\end{bmatrix}
\;b
\begin{bmatrix}
1 \\
3 \\
\end{bmatrix}
+
d
\begin{bmatrix}
2 \\
4 \\
\end{bmatrix}
\right],\tag{6.2}
\end{equation}</li>
<li>column perspective的解释在统计学中非常有用:
<ul class="org-ul">
<li>左侧matrix的column包含了一系列的regressor(data的简单模型)</li>
<li>右侧matrix包含coefficient, coefficient包含了对于每个regressor的重要性评估</li>
<li>统计学中的model-fitting其实就寻找最佳的coefficient,能够使得weighted combination of regressor
能够更好的match data</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org896f0f8" class="outline-4">
<h4 id="org896f0f8"><span class="section-number-4">6.1.4.</span> The "row perspective"</h4>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: harrifeng@outlook.com</p>
<p class="date">Created: 2022-10-09 Sun 15:35</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
