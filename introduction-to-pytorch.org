#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
#+OPTIONS: ^:{}
#+TITLE: pytorch-basic
* Quickstart
** Working with data
   + pytorch有两个primitive来处理data:
     - torch.utils.data.Dataset: Dataset存储sample和对应的label
     - torch.utils.data.DataLoader: DataLoader在Dataset的基础上wrap了一层iterable
   + 代码如下
     #+begin_src python
       import torch
       from torch import nn
       from torch.utils.data import DataLoader
       from torchvision import datasets
       from torchvision.transforms import ToTensor, Lambda, Compose
       import matplotlib.pyplot as plt
     #+end_src
   + 我们本tutorial会使用TorchVision dataset
   + torchvision.datasets module包括了很多Dataset objects,很多是现实世界的数据集.本tutorial使用FashionMNIST
     dataset
   + 每个TorchVision Dataset都有两个argument:
     - transform: 修改sample
     - target_transform: 修改label
   + 下载FashionMNIST的:
     - 代码如下
       #+begin_src python
         import torch
         from torch import nn
         from torch.utils.data import DataLoader
         from torchvision import datasets
         from torchvision.transforms import ToTensor, Lambda, Compose
         import matplotlib.pyplot as plt


         # Download training data from open datasets.
         training_data = datasets.FashionMNIST(
             root="data",
             train=True,
             download=True,
             transform=ToTensor(),
         )

         # Download test data from open datasets.
         test_data = datasets.FashionMNIST(
             root="data",
             train=False,
             download=True,
             transform=ToTensor(),
         )
       #+end_src
     - 输出如下:
       #+begin_example
         /Users/fenghaoran/.virtualenvs/3ENV/bin/python3.7 /Users/fenghaoran/github/pytorch/code/001.py
         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz
         100.0%
         Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw

         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz
         100.6%
         Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw

         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz
         100.0%
         Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw

         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz
         119.3%
         Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw
       #+end_example
   + 我们把Dataset作为参数传入给DataLoader.这个会把我们的dataset转换成一个iterable,这个iterable支持:
     - automatic batching
     - sampling
     - shuffling
     - multiprocess
   + 下面的例子就是我们定义了batch_size为64, 然后每次循环, dataloader会返回一个包含64个feature和label
     - 代码如下
       #+begin_src python
         batch_size = 64

         # Create data loaders
         train_dataloader = DataLoader(training_data, batch_size=batch_size)
         test_dataloader = DataLoader(test_data, batch_size=batch_size)

         for X, y in test_dataloader:
             print("Shape of X [N, C, H, W]: ", X.shape)
             print("Shape of y: ", y.dtype)
             break
       #+end_src
     - 输出如下
       #+begin_example
         Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])
         Shape of y:  torch.int64
       #+end_example
** Creating Models
   + 为了创建一个neural network,我们要创建一个继承自nn.Module的class:
     - 我们需要在__init__ function里面定义我们的network的layer
     - 我们还需要在forward函数里面data如何在network上面传输
     - 如果存在GPU,我们还可以把神经网络的代码移动到GPU
     - 代码如下
       #+begin_src python
         # Define model
         class NeuralNetwork(nn.Module):
             def __init__(self):
                 super(NeuralNetwork, self).__init__()
                 self.flatten = nn.Flatten()
                 self.linear_relu_stack = nn.Sequential(
                     nn.Linear(28 * 28, 512),
                     nn.ReLU(),
                     nn.Linear(512, 512),
                     nn.ReLU(),
                     nn.Linear(512, 10),
                     nn.ReLU(),
                 )

             def forward(self, x):
                 x = self.flatten(x)
                 logits = self.linear_relu_stack(x)
                 return logits


         model = NeuralNetwork().to(device)
         print(model)
       #+end_src
     - 输出如下
       #+begin_example
         Using cpu device
         NeuralNetwork(
           (flatten): Flatten(start_dim=1, end_dim=-1)
           (linear_relu_stack): Sequential(
             (0): Linear(in_features=784, out_features=512, bias=True)
             (1): ReLU()
             (2): Linear(in_features=512, out_features=512, bias=True)
             (3): ReLU()
             (4): Linear(in_features=512, out_features=10, bias=True)
             (5): ReLU()
           )
         )
       #+end_example
   + TODO
* Tensors
  + tensors是一种特殊的数据结构,和array或者matrix非常相似.
  + 在pytorch里面,我们使用tensor来encode:
    - model的参数
    - model的input
    - model的output
  + Tensor拥有NumPy的ndarray几乎所有的功能,额外还能支持GPU加速
  + 在实际应用中, tensor和NumPy array经常share underlying memory(这样可以减少数据库拷贝)
  + Tensor还设计了能够自动计算梯度
  + 本节需要的头文件
    #+begin_src python
      import torch
      import numpy as np
    #+end_src
** Initializing a Tensor
   + Tensor可以使用以多种方法初始化
*** Directly from data
    + 可以从data直接初始化tensor
      #+begin_src python
        data = [[1, 2], [3, 4]]
        x_data = torch.tensor(data)
      #+end_src
*** From a NumPy array
    + 也可以从NumPy array初始化tensor,也可以从tensor转换成NumPy array
      #+begin_src python
        np_array = np.array(data)
        x_np = torch.from_numpy(np_array)
      #+end_src
*** From another tensor
    + 新的tensor会保留一些特性(shape, datatype), 当然我们也可以明确的override这些特性.不过值就不继承了,
      比如ones_like就会把原来的matrix的所有值换成1
      - 代码如下:
        #+begin_src python
          x_ones = torch.ones_like(x_data)  # retains the properties of x_data
          print(f"Ones Tensor: \n {x_ones} \n")

          x_rand = torch.rand_like(
              x_data, dtype=torch.float
          )  # override the datatype of x_data
          print(f"Random Tensor: \n {x_rand} \n")
        #+end_src
      - 输出如下:
        #+begin_example
          Ones Tensor:
           tensor([[1, 1],
                  [1, 1]])

          Random Tensor:
           tensor([[0.7919, 0.3389],
                  [0.3505, 0.1489]])
        #+end_example
*** With random or constant values:
    + 我们可以使用一个tuple来定义tensor的dimensions,然后作为一个参数传递给初始化函数:
      - 代码如下:
        #+begin_src python
          shape = (
              2,
              3,
          )
          rand_tensor = torch.rand(shape)
          ones_tensor = torch.ones(shape)
          zeros_tensor = torch.zeros(shape)

          print(f"Random Tensor: \n {rand_tensor} \n")
          print(f"Ones Tensor: \n {ones_tensor} \n")
          print(f"Zeros Tensor: \n {zeros_tensor} \n")
        #+end_src
      - 输出如下:
        #+begin_example
          Random Tensor:
           tensor([[0.6669, 0.3043, 0.1660],
                  [0.6319, 0.2304, 0.6747]])

          Ones Tensor:
           tensor([[1., 1., 1.],
                  [1., 1., 1.]])

          Zeros Tensor:
           tensor([[0., 0., 0.],
                  [0., 0., 0.]])
        #+end_example
** Attributes of Tensor
   + tensor的attribute描述了他们的shape,datatype,和他们所存储的device
     - 代码如下
       #+begin_src python
         tensor = torch.rand(3, 4)

         print(f"Shape of tensor: {tensor.shape}")
         print(f"Datatype of tensor: {tensor.dtype}")
         print(f"Device tensor is stored on: {tensor.device}")
       #+end_src
     - 输出如下
       #+begin_example
         Shape of tensor: torch.Size([3, 4])
         Datatype of tensor: torch.float32
         Device tensor is stored on: cpu
       #+end_example
** Operations on Tensors
   + tensor支持100多种操作,包括:
     - arithmetic
     - linear algebra
     - matrix manipulation(transposing, indexing, slicing)
     - sampling
   + 这些操作可以在GPU上面或者CPU上面运行.默认情况下是在cpu运行.如果想在GPU运行,需要明确的使用`to`
     函数.需要注意的是跨设备(从CPU到GPU)拷贝tensor是非常expensive的操作,费时间,费内存
     #+begin_src python
       if torch.cuda.is_available():
           tensor = tensor.to("cuda")
     #+end_src
   + 下面我们列举一些处理数据的api,和NumPy比起来是非常类似的
*** Standard numpy-like indexing and slicing
    + 代码如下
      #+begin_src python
        print("Frist row: ", tensor[0])
        print("Frist column: ", tensor[:,0]) # 第一到最后一行的,第0个元素
        print("Last column: ", tensor[...,-1]) # 第一到最后一行的,最后一个元素
        tensor[:,1] = 0
        print(tensor)
      #+end_src
    + 输出如下
      #+begin_example
        Frist row:  tensor([1., 1., 1., 1.])
        Frist column:  tensor([1., 1., 1., 1.])
        Last column:  tensor([1., 1., 1., 1.])
        tensor([[1., 0., 1., 1.],
                [1., 0., 1., 1.],
                [1., 0., 1., 1.],
                [1., 0., 1., 1.]])
      #+end_example
*** Joining tensors
    + 你可以使用torch.cat来concatenate一系列的tensor
      - 代码如下
        #+begin_src python
          t1 = torch.cat([tensor, tensor, tensor], dim=1)
          print(t1)
        #+end_src
      - 输出如下
        #+begin_example
          tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
                  [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
                  [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
                  [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])
        #+end_example
*** Arithmetic operations
    + 如下操作都可以看做是两个matrix的multiplicaiton
      - 代码如下
        #+begin_src python
          y1 = tensor @ tensor.T
          y2 = tensor.matmul(tensor.T)
          y3 = torch.rand_like(tensor)
          torch.matmul(tensor, tensor.T, out=y3)
          print(y1)
          print(y2)
          print(y3)
        #+end_src
      - 输出如下
        #+begin_example
          tensor([[3., 3., 3., 3.],
                  [3., 3., 3., 3.],
                  [3., 3., 3., 3.],
                  [3., 3., 3., 3.]])
          tensor([[3., 3., 3., 3.],
                  [3., 3., 3., 3.],
                  [3., 3., 3., 3.],
                  [3., 3., 3., 3.]])
          tensor([[3., 3., 3., 3.],
                  [3., 3., 3., 3.],
                  [3., 3., 3., 3.],
                  [3., 3., 3., 3.]])
        #+end_example
    + 如下操作是两个matrix对应item的乘法
      - 代码如下
        #+begin_src python
          z1 = tensor * tensor
          z2 = tensor.mul(tensor)
          z3 = torch.rand_like(tensor)
          torch.mul(tensor, tensor, out=z3)
          print(z1)
          print(z2)
          print(z3)
        #+end_src
      - 输出如下
        #+begin_example
          tensor([[1., 0., 1., 1.],
                  [1., 0., 1., 1.],
                  [1., 0., 1., 1.],
                  [1., 0., 1., 1.]])
          tensor([[1., 0., 1., 1.],
                  [1., 0., 1., 1.],
                  [1., 0., 1., 1.],
                  [1., 0., 1., 1.]])
          tensor([[1., 0., 1., 1.],
                  [1., 0., 1., 1.],
                  [1., 0., 1., 1.],
                  [1., 0., 1., 1.]])
        #+end_example
    + 把一个tensor里面的数据累加到一个数据里面,我们要用到函数item()
      - 代码如下
        #+begin_src python
          agg = tensor.sum()
          agg_item = agg.item()
          print(agg_item, type(agg_item))
        #+end_src
      - 输出如下
        #+begin_example
          12.0 <class 'float'>
        #+end_example
    + 很多时候,我们还希望有in-place的操作,也就是把更改值直接作用到调用对象上面, 这种函数一般以"_"结尾
      - 代码如下
        #+begin_src python
          print(tensor, "\n")
          tensor.add_(5)
          print(tensor)
        #+end_src
      - 输出如下
        #+begin_example
          tensor([[1., 0., 1., 1.],
                  [1., 0., 1., 1.],
                  [1., 0., 1., 1.],
                  [1., 0., 1., 1.]])

          tensor([[6., 5., 6., 6.],
                  [6., 5., 6., 6.],
                  [6., 5., 6., 6.],
                  [6., 5., 6., 6.]])
        #+end_example
      - 由于in-place操作会丢失history,所以实践当中不推荐使用
** Bridge with Numpy
   + 在CPU上的时候, Tensor和NumPY array可以共享内存,这也就意味着一个内容改变了,另外一个也会跟着改变
*** Tensor to Numpy array
    + 从tensor到numpy共享内存的例子
      - 代码如下
        #+begin_src python
          t = torch.ones(5)
          print(f"t: {t}")
          n = t.numpy()
          print(f"n: {n}")

          t.add_(1)
          print(f"t: {t}")
          print(f"n: {n}")
        #+end_src
      - 输出如下
        #+begin_example
          t: tensor([1., 1., 1., 1., 1.])
          n: [1. 1. 1. 1. 1.]
          t: tensor([2., 2., 2., 2., 2.])
          n: [2. 2. 2. 2. 2.]
        #+end_example
*** Numpy array to Tensor
    + 从numpy到tensor共享内存的例子
      - 代码如下
        #+begin_src python
          n = np.ones(5)
          t = torch.from_numpy(n)
          print(f"t: {t}")
          print(f"n: {n}")
          np.add(n, 1, out=n)
          print(f"t: {t}")
          print(f"n: {n}")
        #+end_src
      - 输出如下
        #+begin_example
          t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
          n: [1. 1. 1. 1. 1.]
          t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
          n: [2. 2. 2. 2. 2.]
        #+end_example
* Datasets & Dataloaders
  + 处理data sample的代码可能会非常的麻烦,我们希望我们的dataset 代码和model training 代码区分开,为了
    达到这个目的,pytorch设计了两个data primitive, 这两个primitive可以处理你自己的数据,也可以使用pre-loaded
    的那些公共数据:
    - torch.utils.data.Dataset: 存储sample和sample对应的的label
    - torch.utils.data.DataLoader: 在Dataset的基础上增加了iterable的功能
  + PyTorch为了让用户做实验,内置了很多公共库,比如FashionMNIST,这种公共库都继承了torch.utils.data.Dataset
** Loading a Dataset
   + 下面我们就来看看pytorch如何load Fashion-MNIST数据. 这个数据:
     - 有60000个training example
     - 有10000个test example
     - 每个example包含28*28 grayscale image
     - 每个example有对应的label(正确答案)
   + 我们下面的代码用到了如下的变量:
     - root是我们的train/test data存储的地方
     - train代表了training或者test dataset
     - download=True表示我们如果在root没有数据的话,要从互联网下载
     - transform和target_transorm代表feature和label
   + 首先看下载Fashion-MNIST数据集的例子:
     - 代码如下
       #+begin_src python
         import torch

         from torch.utils.data import Dataset
         from torchvision import datasets
         from torchvision.transforms import ToTensor
         import matplotlib.pyplot as plt

         training_data = datasets.FashionMNIST(
             root="data", train=True, download=True, transform=ToTensor()
         )

         test_data = datasets.FashionMNIST(
             root="data", train=False, download=True, transform=ToTensor()
         )
       #+end_src
     - 输出如下
       #+begin_example
         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz
         100.0%
         Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw

         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz
         100.6%
         Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw

         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz
         100.0%
         Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw

         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
         Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz
         119.3%
         Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw
       #+end_example
** Iterating and Visualizing the Dataset
   + 我们可以像使用其他list一样使用Dataset,比如training_data[index], 我们使用matplotlib来visualize一些数据
     - 代码如下
       #+begin_src python
         labels_map = {
             0: "T-Shirt",
             1: "Trouser",
             2: "Pullover",
             3: "Dress",
             4: "Coat",
             5: "Sandal",
             6: "Shirt",
             7: "Sneaker",
             8: "Bag",
             9: "Ankle Boot",
         }

         figure = plt.figure(figsize=(8, 8))
         cols, rows = 3, 3
         for i in range(1, cols * rows + 1):
             sample_idx = torch.randint(len(training_data), size=(1,)).item()
             print('''[sample_idx] ==>''', sample_idx)
             img, label = training_data[sample_idx]
             figure.add_subplot(rows, cols, i)
             plt.title(labels_map[label])
             plt.axis("off")
             plt.imshow(img.squeeze(), cmap="gray")
         plt.show()
       #+end_src
     - 输出如下
       #+begin_example
         [sample_idx] ==> 56165
         [sample_idx] ==> 46042
         [sample_idx] ==> 19185
         [sample_idx] ==> 57231
         [sample_idx] ==> 41509
         [sample_idx] ==> 57455
         [sample_idx] ==> 7884
         [sample_idx] ==> 30338
         [sample_idx] ==> 40487
       #+end_example
** Creating a Custom Dataset for your files
   + 如果想创建自己的Dataset,那么就要继承torch.utils.data.Dataset,并且实现如下函数:
     - __init__
     - __len__
     - __geitem__
   + 代码如下
     #+begin_src python
       import os
       import pandas as pd
       from torchvision.io import read_image


       class CustomImageDataset(Dataset):
           def __init__(
               self, annotations_file, img_dir, transform=None, target_transform=None
           ):
               self.img_labels = pd.read_csv(annotations_file)
               self.img_dir = img_dir
               self.transform = transform
               self.target_transform = target_transform

           def __len__(self):
               return len(self.img_labels)

           def __getitem__(self, idx):
               img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
               image = read_image(img_path)
               label = self.img_labels.iloc[idx, 1]
               if self.transform:
                   image = self.transform(image)
               if self.target_transform:
                   label = self.target_transform(label)
               return image, label
     #+end_src
*** __init__
    + __init__函数会在Dataset对象实例化的时候运行一次,参数解释如下:
      - annotations_file: 一个csv文件,里面存放image文件和label的对应关系,例子如下
        #+begin_example
          tshirt1.jpg, 0
          tshirt2.jpg, 0
          ......
          ankleboot999.jpg, 9
        #+end_example
      - img_dir:放置图片的文件夹
*** __len__
    + __len__返回sample的总数,也就是label的总数
*** __getitem__
    + __getitem__返回在idx位置的sample
    + 根据idx,我们要:
      1) 判断出image的disk位置,转换成tensor(使用read_image)
      2) 并且要获得这个image的label
      3) 如果需要还要调用transform函数
      4) 以tuple的形式返回(tensor_image, corresponding_label)
** Preparing your data for training with DataLoaders
   + Dataset每次取一个sample,但是在训练的时候,我们会有如下多个sample一起取的需求:
     - 以minibatch的方法取一批sample
     - 每个epoch都会reshuffle数据来减小overfitting
     - 使用Python的multiprocessing来加速数据获取
   + DataLoader就是为了上面的需求而设计的iterable的Dataset
     #+begin_src python
       from torch.utils.data import DataLoader

       train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
       test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)
     #+end_src
** Iterate through the DataLoader
   + 我们使用DataLoader就可以使用next来循环了
   + 返回的两个对象,train_feature和train_label都是包含64个成员
   + 由于我们设置了shuffle=True,所以我们每次循环之后,所有的batch都会重新洗牌
     - 代码如下
       #+begin_src python
         # Display image and label.
         train_features, train_labels = next(iter(train_dataloader))
         print(f"Feature batch shape: {train_features.size()}")
         print(f"Label batch shape: {train_labels.size()}")
         img = train_features[0].squeeze()
         label = train_labels[0]
         plt.imshow(img, cmap="gray")
         plt.show()
         print(f"Label: {label}")
       #+end_src
     - 输出如下
       #+begin_example
         Feature batch shape: torch.Size([64, 1, 28, 28])
         Label batch shape: torch.Size([64])
         Label: 7
       #+end_example
* Transforms
  + 网上的数据,或者我们提供的数据,并不一定是我们训练的时候需要的格式(或者不同的训练可能会需要不同的格
    式),那么在下载之后,使用之前,我们要做一些转换,在pytorch这里,这个转换叫做transforms
  + 所有的TorchVision dataset都有两个参数:
    - transform用来修改feature的格式
    - target_transform用来修改label的格式
  + 以FashionMNIST为例:
    - feature都是PIL 格式,我们需要把feature都转成normalized tensor
    - label都是integer格式,我们需要把label转成one-hot encoded tensor
    - 代码如下
      #+begin_src python
        import torch
        from torchvision import datasets
        from torchvision.transforms import ToTensor, Lambda

        ds = datasets.FashionMNIST(
            root="data",
            train=True,
            download=True,
            transform=ToTensor(),
            target_transform=Lambda(
                lambda y: torch.zeros(10, dtype=torch.float).scatter_(
                    0, torch.tensor(y), value=1
                )
            ),
        )
      #+end_src
** ToTensor()
   + ToTensor会把PIL image(或者NumPy ndarray)转换成FloatTensor,并且把image的pixel intensity控制在[0,1]
     之间
** Lambda Transforms
   + 如果没有ToTensor这种函数,那么我们需要自己创建一个函数来作为transform,Lambda是一种临时只用一次的函数
   + 这里的Lambda函数:
     - 首选会创建一个长度为10的zero tensor
     - 然后调用scatter_来给这个tensor的某个index赋值为1
* Build The Neural Network
  + Neural network(神经网络)在pytorch里面叫一个module.
  + 每个module要么包含其他module,要么就是最底层的module,这个module包含很多layer,这些layer分别对data
    做operation. 两者相互关系如下:
    #+begin_example
      moduleA
      |-- moduleB
      |   `-- LayerA
      `-- moduleC
          |-- LayerB
          `-- LayerC
    #+end_example
  + pytorch里面,所有的和neural network相关的building block都在torch.nn这个namespace下面
  + pytorch里面的module都继承自nn.Module
  + 本章需要的头文件如下
    #+begin_src python
      import os
      import torch
      from torch import nn
      from torch.utils.data import DataLoader
      from torchvision import datasets, transforms
    #+end_src
** Get Device for Training
   + 如果cuda存在,可以使用显卡来训练,否则使用cpu来训练
     #+begin_src python
       device = "cuda" if torch.cuda.is_available() else "cpu"
       print("Using {} device".format(device))
     #+end_src
** Define the Class
   + 我们自己的神经网络继承自nn.Module:
     - 通过__init__初始化neural network
     - 通过实现forward函数来实现对input data的操作
   + 创建神经网络代码如下
     #+begin_src python
       class NeuralNetwork(nn.Module):
           def __init__(self):
               super(NeuralNetwork, self).__init__()
               self.flatten = nn.Flatten()
               self.linear_relu_stack = nn.Sequential(
                   nn.Linear(28 * 28, 512),
                   nn.ReLU(),
                   nn.Linear(512, 512),
                   nn.ReLU(),
                   nn.Linear(512, 10),
               )

           def forward(self, x):
               x = self.flatten(x)
               logits = self.linear_relu_stack(x)
               return logits

     #+end_src
   + 神经网络代码创建后,可以打印出来,表示我们的model创建成功了,效果如下:
     - 代码
       #+begin_src python
         model = NeuralNetwork().to(device)
         print(model)
       #+end_src
     - 输出
       #+begin_example
         NeuralNetwork(
           (flatten): Flatten(start_dim=1, end_dim=-1)
           (linear_relu_stack): Sequential(
             (0): Linear(in_features=784, out_features=512, bias=True)
             (1): ReLU()
             (2): Linear(in_features=512, out_features=512, bias=True)
             (3): ReLU()
             (4): Linear(in_features=512, out_features=10, bias=True)
           )
         )
       #+end_example
   + 创建model之后,我们要开始使用model了,在pytorch里面,model是一个继承自nn.Module的class,但是,由于它
     实现了__call__,所以是callable的,直接把model当函数使用就可以了,参数是input data:
     - 代码如下:
       #+begin_src python
         X = torch.rand(1, 28, 28, device=device)
         print("X>", X)
         logits = model(X)
         print("logits>", logits)
       #+end_src
     - 输出如下:
       #+begin_example
         X> tensor([[[3.4855e-01, 2.2404e-01, 9.4086e-04, 9.5183e-01, 6.4413e-01,
                   6.3813e-01, 2.9121e-01, 2.7684e-01, 7.4389e-01, 7.8992e-01,
                   1.1403e-01, 3.6536e-01, 8.8994e-01, 1.5893e-01, 7.1634e-01,
                   4.5617e-01, 9.4580e-01, 5.7355e-01, 5.2298e-01, 6.5089e-01,
                   6.2488e-01, 7.5967e-01, 5.6150e-01, 3.3489e-01, 6.6690e-01,
                   3.4567e-01, 8.5913e-01, 9.1230e-01],
                  [5.5698e-01, 5.6784e-01, 6.7565e-01, 1.1774e-01, 9.3881e-01,
                   6.7115e-01, 7.1790e-01, 1.5813e-01, 1.0446e-01, 8.2648e-02,
                   1.6147e-01, 8.2475e-01, 8.3832e-01, 9.9920e-01, 4.1542e-01,
                   3.3176e-01, 2.5911e-01, 6.8579e-01, 1.9526e-01, 5.8544e-01,
                   7.4770e-01, 6.9535e-01, 9.8096e-01, 1.5287e-01, 6.6194e-01,
                   9.0889e-01, 8.8180e-01, 5.2309e-01],
                   ...
                  [6.6676e-01, 5.1000e-01, 3.9123e-01, 6.8837e-03, 9.5137e-01,
                   6.5119e-01, 9.5212e-01, 4.0678e-01, 4.6109e-02, 1.5674e-01,
                   4.6521e-01, 2.0206e-01, 8.5580e-01, 5.1843e-01, 1.8716e-01,
                   1.0377e-03, 3.9218e-01, 5.4533e-01, 1.7358e-01, 5.1870e-01,
                   9.3065e-01, 7.2249e-01, 7.2775e-01, 7.2690e-01, 7.4644e-01,
                   8.2682e-01, 4.8469e-01, 5.6445e-01]]])
         logits> tensor([[ 0.0330, -0.0429,  0.0134,  0.0410,  0.0819,  0.0849,  0.0231, -0.0021,
                  -0.0254, -0.0529]], grad_fn=<AddmmBackward>)
       #+end_example
     - X也就是我们的input,是一个28*28的tensor
     - logits就是我们model对这个input的预测结果,这个预测结果是raw的predicted value,每个value表示每种
       分类的可能性,比如,分类1的可能性是0.0330, 分类2的可能性是-0.0429
   + raw的可能性有负值,而且和并不是1,所以需要使用其他方法修正为和为1的概率分布
     - 代码如下
       #+begin_src python
         pred_probab = nn.Softmax(dim=1)(logits)
         print("pred_probab>", pred_probab)
       #+end_src
     - 输出如下
       #+begin_example
         pred_probab> tensor([[0.1017, 0.0942, 0.0997, 0.1025, 0.1068, 0.1071, 0.1007, 0.0982, 0.0959,
                  0.0933]], grad_fn=<SoftmaxBackward>)
       #+end_example
   + 这里的Softmax也是一个module:
     - 继承自nn.Module
     - 通常作为神经网络的最后一个activation function来正则化输出.
     - 这里的dim是dimension的缩写,是说针对哪个维度来进行正则化:
       1) 针对列: 每一列所有概率加起来是1
          #+begin_src python
            y1 = F.softmax(x, dim=0)
            print(y1)
            # <===================OUTPUT===================>
            # tensor([[0.3333, 0.3333, 0.3333, 0.3333],
            #         [0.3333, 0.3333, 0.3333, 0.3333],
            #         [0.3333, 0.3333, 0.3333, 0.3333]])

          #+end_src
       2) 针对行: 每一行所有概率加起来是1
          #+begin_src python
            y2 = F.softmax(x, dim=1)
            print(y2)
            # <===================OUTPUT===================>
            # tensor([[0.0321, 0.0871, 0.2369, 0.6439],
            #         [0.0321, 0.0871, 0.2369, 0.6439],
            #         [0.0321, 0.0871, 0.2369, 0.6439]])
          #+end_src
   + 最后输出tensor里面最大的一个index,这里使用了numpy里面的argmax来寻找醉倒index的值
     - 代码如下
       #+begin_src python
         y_pred = pred_probab.argmax(1)
         print(f"Predicted class: {y_pred}")
       #+end_src
     - 输出如下
       #+begin_example
         Predicted class: tensor([5])
       #+end_example
** Model Layers
   + 我们使用额外的代码来了解下layer是如何组织的
   + 首先,我们使用随机的方式生成一个image
     - 代码如下
       #+begin_src python
         input_image = torch.rand(3, 4, 5)
         print("""[input_image] ==>""", input_image)
         print("""[input_image.size()] ==>""", input_image.size())
       #+end_src
     - 输出如下
       #+begin_example
         [input_image] ==> tensor([[[0.1550, 0.1822, 0.0137, 0.9355, 0.8929],
                  [0.8082, 0.2442, 0.7184, 0.0184, 0.0828],
                  [0.1097, 0.8672, 0.9809, 0.0283, 0.5656],
                  [0.8645, 0.2560, 0.2821, 0.0864, 0.2733]],

                 [[0.4551, 0.2518, 0.7734, 0.8949, 0.0994],
                  [0.6500, 0.3321, 0.3630, 0.1329, 0.8804],
                  [0.1265, 0.1371, 0.5087, 0.7530, 0.1164],
                  [0.0825, 0.6535, 0.6242, 0.1958, 0.1738]],

                 [[0.1569, 0.3409, 0.7097, 0.9930, 0.4367],
                  [0.0491, 0.4994, 0.8175, 0.8694, 0.2794],
                  [0.3276, 0.8073, 0.9999, 0.0745, 0.6946],
                  [0.4413, 0.6856, 0.1619, 0.5948, 0.5922]]])
         [input_image.size()] ==> torch.Size([3, 4, 5])
       #+end_example
*** nn.Flatten
    + 我们上面是一个3*4*5的三维矩阵,其实每个image是4*5的,所以我们其实想要一个3个像素为20的image,换句
      话说,就是3*4*5矩阵的第一个dimension(dim=0)保留,剩下的打平
      - 代码如下
        #+begin_src python
          flatten = nn.Flatten()
          flat_image = flatten(input_image)
          print("""[flat_image] ==>""", flat_image)
          print("""[flat_image.size()] ==>""", flat_image.size())
        #+end_src
      - 输出如下
        #+begin_example
          [flat_image] ==> tensor([[0.1550, 0.1822, 0.0137, 0.9355, 0.8929, 0.8082, 0.2442, 0.7184, 0.0184,
                   0.0828, 0.1097, 0.8672, 0.9809, 0.0283, 0.5656, 0.8645, 0.2560, 0.2821,
                   0.0864, 0.2733],
                  [0.4551, 0.2518, 0.7734, 0.8949, 0.0994, 0.6500, 0.3321, 0.3630, 0.1329,
                   0.8804, 0.1265, 0.1371, 0.5087, 0.7530, 0.1164, 0.0825, 0.6535, 0.6242,
                   0.1958, 0.1738],
                  [0.1569, 0.3409, 0.7097, 0.9930, 0.4367, 0.0491, 0.4994, 0.8175, 0.8694,
                   0.2794, 0.3276, 0.8073, 0.9999, 0.0745, 0.6946, 0.4413, 0.6856, 0.1619,
                   0.5948, 0.5922]])
          [flat_image.size()] ==> torch.Size([3, 20])
        #+end_example
*** nn.Linear
    + 现在我们获得了3个像素为20(也就是20个float值)的input数据,如果我们想把每个input数据(20像素)映射成
      一个6像素的数据,那么我们就需要linear transformation
    + pytorch为我们准备了这样的linear transformation: nn.Linear
      - 接入代码如下
        #+begin_src python
          layer1 = nn.Linear(in_features=4 * 5, out_features=6)
          hidden1 = layer1(flat_image)
          print("""[hidden1] ==>""", hidden1)
          print("""[hidden1.size()] ==>""", hidden1.size())
        #+end_src
      - 输出如下
        #+begin_example
          [hidden1] ==> tensor([[ 0.0794,  0.5231, -0.2751,  0.3251, -0.1497,  0.2084],
                  [ 0.0444,  0.3368, -0.2555,  0.3189, -0.0683,  0.1535],
                  [-0.0784,  0.6062, -0.8009,  0.4397,  0.0825,  0.1439]],
                 grad_fn=<AddmmBackward>)
          [hidden1.size()] ==> torch.Size([3, 6])
        #+end_example
*** nn.ReLU
    + ReLU是一种activation,所谓activation,是用来引入复杂的(非线性)的规则的
    + activation通常在Linear transformation之后,帮助神经网络学习更复杂的规律(非线性规律)
    + 这里的ReLU是最常见的一个activation,就是说负数的话就映射为0,正数就是保持原来的值:
      - 公式如下:
        \begin{equation}
        ReLU(x) = max(0,x)
        \end{equation}
      - 接入代码如下
        #+begin_src python
          print(f"Before ReLU: {hidden1}\n\n")
          hidden1 = nn.ReLU()(hidden1)
          print(f"After ReLU: {hidden1}")
        #+end_src
      - 输出如下
        #+begin_example
          Before ReLU: tensor([[ 0.0794,  0.5231, -0.2751,  0.3251, -0.1497,  0.2084],
                  [ 0.0444,  0.3368, -0.2555,  0.3189, -0.0683,  0.1535],
                  [-0.0784,  0.6062, -0.8009,  0.4397,  0.0825,  0.1439]],
                 grad_fn=<AddmmBackward>)


          After ReLU: tensor([[0.0794, 0.5231, 0.0000, 0.3251, 0.0000, 0.2084],
                  [0.0444, 0.3368, 0.0000, 0.3189, 0.0000, 0.1535],
                  [0.0000, 0.6062, 0.0000, 0.4397, 0.0825, 0.1439]],
                 grad_fn=<ReluBackward0>)
        #+end_example
*** nn.Sequential
    + 我们来看看刚才的过程:
      1) 首先是3*(4*5)的三个raw input
      2) 第一个module: flatten, 输入是3*(4*5),输出是3*20
      3) 第二个module: nn.Linear(layer1), 输入是3*20,输出是3*6
      4) 第三个module: nn.ReLU, 输入是3*6,输出还是3*6,所有负数都变成了零
    + 我们可以把上面的四个过程统一使用一个module处理,就是nn.Sequential:
      - 代码如下,我们最后还多了一层,把output定格为了7
        #+begin_src python
          seq_modules = nn.Sequential(flatten, layer1, nn.ReLU(), nn.Linear(6, 7))

          input_image = torch.rand(3, 4, 5)
          logits = seq_modules(input_image)

          print("""[logits] ==>""", logits)
          print("""[logits.size()] ==>""", logits.size())
        #+end_src
      - 输出如下
        #+begin_example
          [logits] ==> tensor([[-2.6497e-01, -9.1267e-02,  6.0936e-02, -1.5278e-02, -3.5053e-01,
                    8.0587e-02, -2.1164e-01],
                  [-2.3255e-01, -4.9769e-02,  3.9146e-02,  5.0154e-02, -4.0463e-01,
                    3.8066e-04, -1.5409e-01],
                  [-4.0593e-01, -2.6705e-01, -5.0617e-02, -7.1221e-02, -3.6326e-01,
                   -1.1169e-01,  2.1804e-02]], grad_fn=<AddmmBackward>)
          [logits.size()] ==> torch.Size([3, 7])
        #+end_example
*** nn.Softmax
    + 为了让我们的分类问题最后的7个output能够满足一个概率分布,他们7个的概率和必须是1,所以要使用module:
      nn.Softmax:
      - 代码如下
        #+begin_src python
          softmax = nn.Softmax(dim=1)
          pred_probab = softmax(logits)
          print("""[pred_probab] ==>""", pred_probab)

        #+end_src
      - 输出如下
        #+begin_example
          [pred_probab] ==> tensor([[0.1213, 0.1443, 0.1680, 0.1557, 0.1114, 0.1714, 0.1279],
                  [0.1246, 0.1496, 0.1635, 0.1653, 0.1049, 0.1573, 0.1348],
                  [0.1125, 0.1292, 0.1604, 0.1572, 0.1174, 0.1509, 0.1725]],
                 grad_fn=<SoftmaxBackward>)
        #+end_example
** Model Parameters
   + 机器学习是让input和output去拟合一个人类可以理解的函数
   + 深度学习其实也是让input和output去拟合函数,只不过这个函数特别的负载,人类已经无法去理解,因为:
     - 这个函数有很多层
     - 每一层都有对应的weight和bias
     - 这些weight和bias在模型创建的时候都是随机的,通过后面讲的训练会把这些weight和bias朝更拟合output
       的方向调整
     - 到目前为止,我们还没有将训练的过程,所以我们的model里面的weight和bias都是随机的
   + 下面我们使用代码来看看我们生成的model里面随机的weight和bias都是什么样的:
     - 代码如下
       #+begin_src python
         print("Model structure: ", model, "\n\n")

         for name, param in model.named_parameters():
             print(f"Layer: {name} | Size: {param.size()} \nValues: {param[:2]} \n")
       #+end_src
     - 输出如下
       #+begin_example
         Model structure:  NeuralNetwork(
           (flatten): Flatten(start_dim=1, end_dim=-1)
           (linear_relu_stack): Sequential(
             (0): Linear(in_features=784, out_features=512, bias=True)
             (1): ReLU()
             (2): Linear(in_features=512, out_features=512, bias=True)
             (3): ReLU()
             (4): Linear(in_features=512, out_features=10, bias=True)
           )
         )


         Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784])
         Values: tensor([[ 0.0354, -0.0001,  0.0253,  ..., -0.0030, -0.0246, -0.0285],
                 [ 0.0091,  0.0070,  0.0264,  ..., -0.0156, -0.0108,  0.0153]],
                grad_fn=<SliceBackward>)

         Layer: linear_relu_stack.0.bias | Size: torch.Size([512])
         Values: tensor([ 0.0056, -0.0319], grad_fn=<SliceBackward>)

         Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512])
         Values: tensor([[-0.0228,  0.0358,  0.0308,  ..., -0.0433, -0.0227, -0.0242],
                 [-0.0052,  0.0408, -0.0430,  ..., -0.0049, -0.0225,  0.0050]],
                grad_fn=<SliceBackward>)

         Layer: linear_relu_stack.2.bias | Size: torch.Size([512])
         Values: tensor([0.0017, 0.0280], grad_fn=<SliceBackward>)

         Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512])
         Values: tensor([[ 0.0390,  0.0072,  0.0054,  ...,  0.0183,  0.0282,  0.0360],
                 [-0.0165,  0.0336, -0.0274,  ..., -0.0119, -0.0261,  0.0167]],
                grad_fn=<SliceBackward>)

         Layer: linear_relu_stack.4.bias | Size: torch.Size([10])
         Values: tensor([ 0.0213, -0.0026], grad_fn=<SliceBackward>)
       #+end_example
* Automatic Differentitaion With torch.autograd
  + 在训练神经网络的时候,最常用的算法是反向传播(back propagation),在这个算法的运行过程当中,weight和bias
    都会朝着梯度的相反方向进行修正(修正的过程使用optimizer)
  + 为了能够进行反向传播,pytorch必须在每一层都能够自动计算梯度,pytorch里面有个package torch.autograd
    来提供这个功能
  + 我们来看一个最简单的一层layer的例子:
    - input tensor是x, 是一个1*5的matrix
    - 权重是w(weight),是一个5*3的matrix
    - 偏差是b(bias),是一个1*3的matrix
    - output tensor是y,是一个1*3的matrix
    - z是一个简单的神经网络,使用x*w+b得到
    - loss是损失函数
    - 代码如下
      #+begin_src python
        import torch

        x = torch.ones(5)
        y = torch.zeros(3)
        w = torch.randn(5, 3, requires_grad=True)
        b = torch.randn(3, requires_grad=True)
        z = torch.matmul(x, w) + b
        loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)

        print("""[x] ==>""", x)
        print("""[y] ==>""", y)
        print("""[w] ==>""", w)
        print("""[b] ==>""", b)
        print("""[z] ==>""", z)
        print('''[loss] ==>''', loss)
      #+end_src
    - 输出如下
      #+begin_example
        [x] ==> tensor([1., 1., 1., 1., 1.])
        [y] ==> tensor([0., 0., 0.])
        [w] ==> tensor([[-1.8722,  1.5433, -1.6344],
                [ 0.8174,  0.1571, -1.0487],
                [-1.0059, -0.0180, -1.3587],
                [-0.7847, -0.1392,  0.3352],
                [-0.1005, -0.1010, -0.6419]], requires_grad=True)
        [b] ==> tensor([ 0.1220, -0.6997, -0.6994], requires_grad=True)
        [z] ==> tensor([-2.8239,  0.7424, -5.0480], grad_fn=<AddBackward0>)
        [loss] ==> tensor(0.3986, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)
      #+end_example
** Computing Gradients
   + 为了优化weight和bias,我们需要计算梯度:
     - $\frac{\partial loss}{\partial w}$
     - $\frac{\partial loss}{\partial d}$
   + 计算方法是loss函数(创建的时候就传入了z)调用backward()函数
     - 代码如下
       #+begin_src python
         loss.backward()
         print("""[w.grad] ==>""", w.grad)
         print("""[b.grad] ==>""", b.grad)
       #+end_src
     - 输出如下
       #+begin_example
         [w.grad] ==> tensor([[0.0187, 0.2258, 0.0021],
                 [0.0187, 0.2258, 0.0021],
                 [0.0187, 0.2258, 0.0021],
                 [0.0187, 0.2258, 0.0021],
                 [0.0187, 0.2258, 0.0021]])
         [b.grad] ==> tensor([0.0187, 0.2258, 0.0021])
       #+end_example
** Disabling Gradient Tracking
   + 默认情况下,所有的tensor都有requires_grad=True的设置,也就会同时支持梯度计算
   + 但是有些情况下你可能不想计算梯度:
     - 比如你把一些神经网络的parameter设置为frozen parameter, 这在训练一些pretrained network的时候经常用到
     - 只想做forward pass,不想做back propagation那么停止计算梯度能够提高效率
   + 停止自动计算梯度的方法有两种:
     - 设置no_grad()
       #+begin_src python
         z = torch.matmul(x, w) + b
         print(z.requires_grad)

         with torch.no_grad():
             z = torch.matmul(x, w) + b
         print(z.requires_grad)

         # <===================OUTPUT===================>
         # True
         # False
       #+end_src
     - 调用detach()
       #+begin_src python
         z = torch.matmul(x, w) + b
         z_det = z.detach()
         print(z_det.requires_grad)

         # <===================OUTPUT===================>
         # False
       #+end_src
** More on Computational Graphs
   + pytorch自动求梯度的原理如下:
     - autograd把所有的tensor的executed operation(和数据)都存在了一个DAG里面
     - DAG的root是output tensor
     - DAG的leaves是input tensor
     - 从root到leaf的过程就是自动求梯度的过程
   + TODO
* Optimizing Model Parameters
  + 我们已经有了model和data,那么下一步就是train,validate和test了,这个过程当中再来优化参数
  + 训练模型是一个交互式的过程,分成多个iteration,每个iteration叫一个epoch
  + 每个epoch都会:
    1) 对output做出猜测
    2) 然后通过loss function计算和真实值的差距
    3) 根据差距计算梯度
    4) 根据梯度下降法来optimize参数(weight和bias)
** Prereuisite Code
   + 从前面的代码中拷贝如下代码
     #+begin_src python
       import torch
       from torch import nn
       from torch.utils.data import DataLoader
       from torchvision import datasets
       from torchvision.transforms import ToTensor, Lambda

       training_data = datasets.FashionMNIST(
           root="data", train=True, download=True, transform=ToTensor()
       )

       test_data = datasets.FashionMNIST(
           root="data", train=False, download=True, transform=ToTensor()
       )

       train_dataloader = DataLoader(training_data, batch_size=64)
       test_dataloader = DataLoader(test_data, batch_size=64)


       class NeuralNetwork(nn.Module):
           def __init__(self):
               super(NeuralNetwork, self).__init__()
               self.flatten = nn.Flatten()
               self.linear_relu_stack = nn.Sequential(
                   nn.Linear(28 * 28, 512),
                   nn.ReLU(),
                   nn.Linear(512, 512),
                   nn.ReLU(),
                   nn.Linear(512, 10),
               )

           def forward(self, x):
               x = self.flatten(x)
               logits = self.linear_relu_stack(x)
               return logits


       model = NeuralNetwork()
     #+end_src
** Hyperparameters
   + 所谓超参(hyperparameter)是相对于参数(weight和bias)而言的
     - 参数(weight和bias)是神经网络自己维持的参数,是pytorch的使用人员无法更改的
     - 超参(hyperparameter)则不同,其是用来控制optimization process的
   + 我们这里定义三个超参:
     - Number of Epochs: 迭代多少次,一般来说,迭代的越多,最后模型的准确率越高
     - Batch Size: parameter更新之前,有多少的data sample穿过神经网络
     - Learning Rate: 每个epoch,我们超"梯度的反方向"更新参数的时候,更新大. 这个数目小了,容易降低learning
       speed,但是如果太大了,会导致unpredictable behavior
     - 代码如下
       #+begin_src python
         learning_rate = 1e-3
         batch_size = 64
         epochs = 5
       #+end_src
** Optimization Loop
   + 每个loop主要包含两个部分:
     - Train Loop, 遍历training data set,试图优化参数
     - The Validation/Test Loop, 遍历test data set,看看我们的model performance是否提高了(如果没有提高
       可以中断训练,寻找bug)
*** Loss Function
    + 当给与training data的时候,我们untrained network肯定会给出一个不正确的答案
    + Loss function就是来度量我们的答案和正确答案之间的差距的,因为我们model的目的就是让loss function最小
    + 常见的loss function有:
      - nn.MSELoss(Mean Square Error),主要为regression task服务
      - nn.NLLLoss(Negative Log Likelihood),主要为classification服务
      - nn.CrossEntropyLoss,结合了nn.LogSoftmax和nn.NLLLoss
    + 我们这里使用nn.CrossEntropyLoss
      #+begin_src python
        loss_fn = nn.CrossEntropyLoss()
      #+end_src
*** Optimizer
    + optimization是adjust model参数,并且减小model error的过程
    + optimization algorithm是说我们如何超梯度的方向减少(比如SGD就是随机找一个向量的方向减少,而不是全部,为了减少运算量)
    + optimization在pytorch里面被封装成optimizer,常见的optimizer有:
      - SGD
      - ADAM
      - RMSProp
    + 初始化optimizer需要输入model的参数和learning_rate
      #+begin_src python
        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
      #+end_src
    + 在一个training loop里面,我们要做到如下三个step:
      1) 调用optimizer.zero_grad()来reset 梯度的值,因为梯度的值是累加的,不清空会成倍累计
      2) 使用loss.backwards()来进行反向传播的过程,然后把每个参数的梯度记录下来
      3) 一旦有了梯度,我们就调用optimizer.step(),来使用特定的optimization algorithm来调整参数
** Full Implementation
   + 全部代码如下
     #+begin_src python
       import torch
       from torch import nn
       from torch.utils.data import DataLoader
       from torchvision import datasets
       from torchvision.transforms import ToTensor, Lambda

       training_data = datasets.FashionMNIST(
           root="data", train=True, download=True, transform=ToTensor()
       )

       test_data = datasets.FashionMNIST(
           root="data", train=False, download=True, transform=ToTensor()
       )

       train_dataloader = DataLoader(training_data, batch_size=64)
       test_dataloader = DataLoader(test_data, batch_size=64)


       class NeuralNetwork(nn.Module):
           def __init__(self):
               super(NeuralNetwork, self).__init__()
               self.flatten = nn.Flatten()
               self.linear_relu_stack = nn.Sequential(
                   nn.Linear(28 * 28, 512),
                   nn.ReLU(),
                   nn.Linear(512, 512),
                   nn.ReLU(),
                   nn.Linear(512, 10),
               )

           def forward(self, x):
               x = self.flatten(x)
               logits = self.linear_relu_stack(x)
               return logits


       model = NeuralNetwork()


       learning_rate = 1e-3
       batch_size = 64
       epochs = 5


       def train_loop(dataloader, model, loss_fn, optimizer):
           size = len(dataloader.dataset)
           for batch, (X, y) in enumerate(dataloader):
               # Compute prediction and loss
               pred = model(X)
               loss = loss_fn(pred, y)

               # Backpropagation
               optimizer.zero_grad()
               loss.backward()
               optimizer.step()

               if batch % 100 == 0:
                   loss, current = loss.item(), batch * len(X)
                   print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")


       def test_loop(dataloader, model, loss_fn):
           size = len(dataloader.dataset)
           num_batches = len(dataloader)
           test_loss, correct = 0, 0

           with torch.no_grad():
               for X, y in dataloader:
                   pred = model(X)
                   test_loss += loss_fn(pred, y).item()
                   correct += (pred.argmax(1) == y).type(torch.float).sum().item()

           test_loss /= num_batches
           correct /= size
           print(
               f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n"
           )


       loss_fn = nn.CrossEntropyLoss()
       optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

       epochs = 10
       for t in range(epochs):
           print(f"Epoch {t+1}\n-------------------------------")
           train_loop(train_dataloader, model, loss_fn, optimizer)
           test_loop(test_dataloader, model, loss_fn)
       print("Done!")
     #+end_src
   + 输出如下
     #+begin_example
       Epoch 1
       -------------------------------
       loss: 2.290681  [    0/60000]
       loss: 2.287182  [ 6400/60000]
       loss: 2.269812  [12800/60000]
       loss: 2.266142  [19200/60000]
       loss: 2.256343  [25600/60000]
       loss: 2.220319  [32000/60000]
       loss: 2.229174  [38400/60000]
       loss: 2.187555  [44800/60000]
       loss: 2.194004  [51200/60000]
       loss: 2.173015  [57600/60000]
       Test Error:
        Accuracy: 45.8%, Avg loss: 2.157823

       Epoch 2
       -------------------------------
       loss: 2.159005  [    0/60000]
       loss: 2.162359  [ 6400/60000]
       loss: 2.105484  [12800/60000]
       loss: 2.121630  [19200/60000]
       loss: 2.078309  [25600/60000]
       loss: 2.014289  [32000/60000]
       loss: 2.043489  [38400/60000]
       loss: 1.956825  [44800/60000]
       loss: 1.975176  [51200/60000]
       loss: 1.920474  [57600/60000]
       Test Error:
        Accuracy: 55.7%, Avg loss: 1.903461

       Epoch 3
       -------------------------------
       loss: 1.928488  [    0/60000]
       loss: 1.913021  [ 6400/60000]
       loss: 1.795003  [12800/60000]
       loss: 1.833542  [19200/60000]
       loss: 1.736217  [25600/60000]
       loss: 1.679621  [32000/60000]
       loss: 1.705046  [38400/60000]
       loss: 1.591978  [44800/60000]
       loss: 1.635176  [51200/60000]
       loss: 1.541151  [57600/60000]
       Test Error:
        Accuracy: 59.6%, Avg loss: 1.542476

       Epoch 4
       -------------------------------
       loss: 1.602455  [    0/60000]
       loss: 1.576380  [ 6400/60000]
       loss: 1.420821  [12800/60000]
       loss: 1.492640  [19200/60000]
       loss: 1.386150  [25600/60000]
       loss: 1.369658  [32000/60000]
       loss: 1.383621  [38400/60000]
       loss: 1.293382  [44800/60000]
       loss: 1.349705  [51200/60000]
       loss: 1.254283  [57600/60000]
       Test Error:
        Accuracy: 62.8%, Avg loss: 1.270659

       Epoch 5
       -------------------------------
       loss: 1.342015  [    0/60000]
       loss: 1.327567  [ 6400/60000]
       loss: 1.159419  [12800/60000]
       loss: 1.263751  [19200/60000]
       loss: 1.154799  [25600/60000]
       loss: 1.169653  [32000/60000]
       loss: 1.184060  [38400/60000]
       loss: 1.109008  [44800/60000]
       loss: 1.170164  [51200/60000]
       loss: 1.086842  [57600/60000]
       Test Error:
        Accuracy: 64.7%, Avg loss: 1.101013

       Epoch 6
       -------------------------------
       loss: 1.168192  [    0/60000]
       loss: 1.168162  [ 6400/60000]
       loss: 0.987777  [12800/60000]
       loss: 1.118827  [19200/60000]
       loss: 1.011244  [25600/60000]
       loss: 1.035202  [32000/60000]
       loss: 1.060468  [38400/60000]
       loss: 0.991097  [44800/60000]
       loss: 1.054724  [51200/60000]
       loss: 0.981641  [57600/60000]
       Test Error:
        Accuracy: 66.0%, Avg loss: 0.991141

       Epoch 7
       -------------------------------
       loss: 1.047844  [    0/60000]
       loss: 1.064381  [ 6400/60000]
       loss: 0.870337  [12800/60000]
       loss: 1.021567  [19200/60000]
       loss: 0.921638  [25600/60000]
       loss: 0.939490  [32000/60000]
       loss: 0.979135  [38400/60000]
       loss: 0.913901  [44800/60000]
       loss: 0.975232  [51200/60000]
       loss: 0.910894  [57600/60000]
       Test Error:
        Accuracy: 67.3%, Avg loss: 0.916158

       Epoch 8
       -------------------------------
       loss: 0.959148  [    0/60000]
       loss: 0.992466  [ 6400/60000]
       loss: 0.786403  [12800/60000]
       loss: 0.952738  [19200/60000]
       loss: 0.862164  [25600/60000]
       loss: 0.869106  [32000/60000]
       loss: 0.921611  [38400/60000]
       loss: 0.862038  [44800/60000]
       loss: 0.917971  [51200/60000]
       loss: 0.860151  [57600/60000]
       Test Error:
        Accuracy: 68.5%, Avg loss: 0.862315

       Epoch 9
       -------------------------------
       loss: 0.891266  [    0/60000]
       loss: 0.939044  [ 6400/60000]
       loss: 0.723808  [12800/60000]
       loss: 0.901962  [19200/60000]
       loss: 0.819886  [25600/60000]
       loss: 0.816061  [32000/60000]
       loss: 0.878376  [38400/60000]
       loss: 0.825895  [44800/60000]
       loss: 0.875392  [51200/60000]
       loss: 0.821557  [57600/60000]
       Test Error:
        Accuracy: 69.7%, Avg loss: 0.821707

       Epoch 10
       -------------------------------
       loss: 0.837337  [    0/60000]
       loss: 0.896555  [ 6400/60000]
       loss: 0.675275  [12800/60000]
       loss: 0.863091  [19200/60000]
       loss: 0.787557  [25600/60000]
       loss: 0.775253  [32000/60000]
       loss: 0.844009  [38400/60000]
       loss: 0.799441  [44800/60000]
       loss: 0.842212  [51200/60000]
       loss: 0.790734  [57600/60000]
       Test Error:
        Accuracy: 71.1%, Avg loss: 0.789619

       Done!
     #+end_example
* Save and Load the Model
** Saving and Loading Model Weights
   + pytorch会把学到的参数存储在内部的一个字典里面,叫做state_dict,我们可以只持久化存储这些参数
     #+begin_src python
       import torch
       import torchvision.models as models

       model = models.vgg16(pretrained=True)
       torc.save(model.state_dict(), 'model_weights.pth')
     #+end_src
   + 到用到的志恒,我们之间创建以一样的model,然后使用函数load_state_dict来load这些参数,当然要记得eval()后才能使用
     #+begin_src python
       model = models.vgg16()
       model.load_state_dict(torch.load('model_weights.pth'))
       model.eval()
     #+end_src
** Saving and Loading Models with Shapes
   + 我们上面只save了model的参数,其实我们的model也可以和参数一块被save
     #+begin_src python
       torch.save(model, 'model.pth')
     #+end_src
   + load的时候如下
     #+begin_src python
       model = torch.load('model.pth')
     #+end_src
   + 这个方法的内部原理是使用了python的序列化库pickle,所以在load的时候要保证当前的python环境能够load到class definition
